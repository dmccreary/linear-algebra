{
  "title": "Activation Functions",
  "description": "Interactive comparison of neural network activation functions including ReLU, Sigmoid, Tanh, Leaky ReLU, and Softplus with derivative visualization.",
  "subject": "Neural Networks, Machine Learning",
  "keywords": ["activation function", "ReLU", "sigmoid", "tanh", "derivative", "gradient", "vanishing gradient"],
  "author": "Dan McCreary",
  "dateCreated": "2026-01-21",
  "language": "en",
  "rights": "MIT License",
  "gradeLevel": "Undergraduate",
  "educationalLevel": "College",
  "bloomsTaxonomy": "Analyze",
  "learningObjectives": [
    "Compare activation functions by their shape, range, and gradient behavior",
    "Identify regions of vanishing gradients",
    "Understand why nonlinear activations enable deep learning"
  ],
  "duration": "10-15 minutes",
  "prerequisites": ["Functions", "Derivatives", "Neural network basics"],
  "framework": "p5.js",
  "canvasDimensions": "responsive x 480",
  "controls": [
    {
      "type": "select",
      "label": "Function",
      "options": ["ReLU", "Sigmoid", "Tanh", "Leaky ReLU", "Softplus"]
    },
    {
      "type": "checkbox",
      "label": "Show Derivative",
      "default": true
    },
    {
      "type": "checkbox",
      "label": "Compare All",
      "default": false
    },
    {
      "type": "slider",
      "label": "Input x",
      "min": -5,
      "max": 5,
      "default": 0,
      "step": 0.01
    }
  ]
}
