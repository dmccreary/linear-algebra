# Vector Spaces and Inner Products

## Summary

Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications. You will learn about abstract vector spaces, inner products, orthogonality, the Gram-Schmidt orthogonalization process, and projections. This chapter also covers the four fundamental subspaces of a matrix and the pseudoinverse, which are essential for least squares and machine learning.

## Concepts Covered

This chapter covers the following 19 concepts from the learning graph:

1. Abstract Vector Space
2. Subspace
3. Vector Space Axioms
4. Inner Product
5. Inner Product Space
6. Norm from Inner Product
7. Cauchy-Schwarz Inequality
8. Orthogonality
9. Orthogonal Vectors
10. Orthonormal Set
11. Orthonormal Basis
12. Gram-Schmidt Process
13. Projection onto Subspace
14. Least Squares Problem
15. Normal Equations
16. Row Space
17. Left Null Space
18. Four Subspaces
19. Pseudoinverse

## Prerequisites

This chapter builds on concepts from:

- [Chapter 1: Vectors and Vector Spaces](../01-vectors-and-vector-spaces/index.md)
- [Chapter 2: Matrices and Matrix Operations](../02-matrices-and-matrix-operations/index.md)
- [Chapter 4: Linear Transformations](../04-linear-transformations/index.md)

---

TODO: Generate Chapter Content
