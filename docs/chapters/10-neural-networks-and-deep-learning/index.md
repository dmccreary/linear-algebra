# Neural Networks and Deep Learning

## Summary

Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning, covering neurons, activation functions, weight matrices, forward propagation, and backpropagation. You will also learn about specialized architectures including convolutional layers, batch normalization, and tensor operations.

## Concepts Covered

This chapter covers the following 26 concepts from the learning graph:

1. Perceptron
2. Neuron Model
3. Activation Function
4. ReLU
5. Sigmoid
6. Tanh
7. Softmax
8. Weight Matrix
9. Bias Vector
10. Forward Propagation
11. Backpropagation
12. Chain Rule Matrices
13. Loss Function
14. Cross-Entropy Loss
15. Neural Network Layer
16. Hidden Layer
17. Deep Network
18. Convolutional Layer
19. Convolution Kernel
20. Stride
21. Padding
22. Pooling Layer
23. Batch Normalization
24. Layer Normalization
25. Tensor
26. Tensor Operations

## Prerequisites

This chapter builds on concepts from:

- [Chapter 1: Vectors and Vector Spaces](../01-vectors-and-vector-spaces/index.md)
- [Chapter 2: Matrices and Matrix Operations](../02-matrices-and-matrix-operations/index.md)
- [Chapter 4: Linear Transformations](../04-linear-transformations/index.md)
- [Chapter 9: Machine Learning Foundations](../09-machine-learning-foundations/index.md)
- [Chapter 13: Image Processing](../13-image-processing-and-computer-vision/index.md) (for convolution concepts)

---

TODO: Generate Chapter Content
