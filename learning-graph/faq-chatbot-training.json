{
  "faq_version": "1.0",
  "generated_date": "2026-01-17",
  "source_textbook": "Applied Linear Algebra for AI and Machine Learning",
  "total_questions": 65,
  "categories": {
    "getting_started": 10,
    "core_concepts": 13,
    "technical_details": 10,
    "common_challenges": 9,
    "best_practices": 8,
    "advanced_topics": 9
  },
  "questions": [
    {
      "id": "faq-001",
      "category": "Getting Started",
      "question": "What is this course about?",
      "answer": "This course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. You'll develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Course Overview", "Linear Algebra", "Machine Learning"],
      "keywords": ["course", "overview", "linear algebra", "AI", "machine learning"],
      "source_links": ["docs/course-description.md", "docs/index.md"],
      "has_example": false,
      "word_count": 56
    },
    {
      "id": "faq-002",
      "category": "Getting Started",
      "question": "Who is this course designed for?",
      "answer": "This course is designed for Computer Science majors pursuing AI/ML specializations, Data Science students seeking mathematical foundations, Engineering students interested in robotics and autonomous systems, Applied Mathematics students wanting practical applications, and Graduate students needing linear algebra foundations for research.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Target Audience", "Prerequisites"],
      "keywords": ["audience", "students", "prerequisites", "who"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 48
    },
    {
      "id": "faq-003",
      "category": "Getting Started",
      "question": "What are the prerequisites for this course?",
      "answer": "To succeed in this course, you should have College Algebra or equivalent, basic programming experience (Python recommended), and familiarity with calculus concepts including derivatives and integrals at a conceptual level.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Prerequisites", "College Algebra", "Calculus"],
      "keywords": ["prerequisites", "requirements", "algebra", "calculus", "programming"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-004",
      "category": "Getting Started",
      "question": "How is the course structured?",
      "answer": "The course is divided into four major parts spanning 15 chapters: Part 1 covers Foundations (vectors, matrices, systems, transformations), Part 2 covers Advanced Matrix Theory (determinants, eigenvalues, decompositions), Part 3 covers Machine Learning applications, and Part 4 covers Computer Vision and Autonomous Systems.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Course Structure", "Chapters", "Parts"],
      "keywords": ["structure", "chapters", "parts", "organization"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 52
    },
    {
      "id": "faq-005",
      "category": "Getting Started",
      "question": "How do I use the interactive MicroSims?",
      "answer": "MicroSims are browser-based interactive simulations. Navigate to the MicroSims section in the sidebar, select a simulation, use sliders and controls to adjust parameters, observe real-time changes, and connect the visual behavior to mathematical concepts.",
      "bloom_level": "Apply",
      "difficulty": "easy",
      "concepts": ["MicroSims", "Interactive Learning", "Visualization"],
      "keywords": ["microsims", "interactive", "simulation", "browser"],
      "source_links": ["docs/sims/index.md"],
      "has_example": false,
      "word_count": 42
    },
    {
      "id": "faq-006",
      "category": "Getting Started",
      "question": "Why is linear algebra important for AI and machine learning?",
      "answer": "Linear algebra is the mathematical language in which modern AI systems are written. Understanding it enables you to debug ML models, optimize performance, innovate with new applications, communicate with researchers, and adapt to new techniques that build on these foundations.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Linear Algebra", "Machine Learning", "Applications"],
      "keywords": ["importance", "AI", "machine learning", "why"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 47
    },
    {
      "id": "faq-007",
      "category": "Getting Started",
      "question": "How long does it take to complete each chapter?",
      "answer": "Each chapter is designed for approximately one week of study, including 2-3 hours reading, 2-3 hours working through examples, 1-2 hours with MicroSims, and 2-3 hours on practice problems. The entire course spans 15 weeks at this pace.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Time Commitment", "Study Schedule"],
      "keywords": ["time", "duration", "week", "schedule"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 48
    },
    {
      "id": "faq-008",
      "category": "Getting Started",
      "question": "What software do I need?",
      "answer": "For reading the textbook and using MicroSims, you only need a modern web browser. For programming exercises, you'll benefit from Python 3.x with NumPy, Matplotlib, and scikit-learn. GPU access is optional for deep learning exercises.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Software", "Python", "NumPy"],
      "keywords": ["software", "python", "numpy", "browser", "requirements"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 43
    },
    {
      "id": "faq-009",
      "category": "Getting Started",
      "question": "How can I check my understanding of the material?",
      "answer": "Each chapter provides concept check questions, interactive MicroSims for testing predictions, practice problems, the glossary for reviewing terminology, and quiz questions for self-assessment.",
      "bloom_level": "Apply",
      "difficulty": "easy",
      "concepts": ["Assessment", "Self-Check", "Practice"],
      "keywords": ["check", "understanding", "quiz", "practice"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 31
    },
    {
      "id": "faq-010",
      "category": "Getting Started",
      "question": "Where can I get help if I'm stuck?",
      "answer": "Review relevant glossary definitions, use MicroSims to build geometric intuition, re-read prerequisite material, check the learning graph for prerequisite concepts, and report textbook issues on the GitHub Issues page.",
      "bloom_level": "Apply",
      "difficulty": "easy",
      "concepts": ["Help", "Resources", "Support"],
      "keywords": ["help", "stuck", "support", "resources"],
      "source_links": ["docs/glossary.md", "docs/learning-graph/index.md"],
      "has_example": false,
      "word_count": 34
    },
    {
      "id": "faq-011",
      "category": "Core Concepts",
      "question": "What is the difference between a scalar and a vector?",
      "answer": "A scalar is a single numerical value representing magnitude only (like temperature or mass). A vector is an ordered collection of scalars that represents both magnitude and direction.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Scalar", "Vector"],
      "keywords": ["scalar", "vector", "difference", "magnitude", "direction"],
      "source_links": ["docs/chapters/01-vectors-and-vector-spaces/index.md", "docs/glossary.md#scalar", "docs/glossary.md#vector"],
      "has_example": true,
      "word_count": 35
    },
    {
      "id": "faq-012",
      "category": "Core Concepts",
      "question": "What is a matrix and how does it relate to vectors?",
      "answer": "A matrix is a rectangular array of numbers in rows and columns. It can be viewed as a collection of column vectors, row vectors, or a representation of a linear transformation. Matrix-vector multiplication applies a transformation to map input vectors to output vectors.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Matrix", "Vector", "Linear Transformation"],
      "keywords": ["matrix", "vectors", "rows", "columns", "transformation"],
      "source_links": ["docs/chapters/02-matrices-and-matrix-operations/index.md", "docs/glossary.md#matrix"],
      "has_example": true,
      "word_count": 48
    },
    {
      "id": "faq-013",
      "category": "Core Concepts",
      "question": "What does it mean for vectors to be linearly independent?",
      "answer": "Vectors are linearly independent if no vector in the set can be written as a linear combination of the others. The only way to combine them to get zero is with all zero coefficients. Linear independence means vectors provide 'new directions' in space.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Linear Independence", "Linear Combination"],
      "keywords": ["linear independence", "linearly independent", "combination"],
      "source_links": ["docs/chapters/01-vectors-and-vector-spaces/index.md", "docs/glossary.md#linear-independence"],
      "has_example": true,
      "word_count": 46
    },
    {
      "id": "faq-014",
      "category": "Core Concepts",
      "question": "What is a basis and why is it important?",
      "answer": "A basis is a set of linearly independent vectors that span an entire vector space. Every vector in the space can be written as a unique linear combination of basis vectors. The basis provides a coordinate system for the space.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Basis Vector", "Span", "Vector Space"],
      "keywords": ["basis", "span", "coordinate system", "independent"],
      "source_links": ["docs/chapters/01-vectors-and-vector-spaces/index.md", "docs/glossary.md#basis-vector"],
      "has_example": true,
      "word_count": 45
    },
    {
      "id": "faq-015",
      "category": "Core Concepts",
      "question": "What is the dot product and what does it tell us?",
      "answer": "The dot product of two vectors produces a scalar as the sum of products of corresponding components. Geometrically, it equals |a||b|cos(θ). Positive means similar directions, zero means perpendicular, negative means opposite directions.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Dot Product", "Inner Product", "Orthogonality"],
      "keywords": ["dot product", "inner product", "perpendicular", "angle"],
      "source_links": ["docs/chapters/01-vectors-and-vector-spaces/index.md", "docs/glossary.md#dot-product"],
      "has_example": true,
      "word_count": 41
    },
    {
      "id": "faq-016",
      "category": "Core Concepts",
      "question": "What is a linear transformation?",
      "answer": "A linear transformation is a function between vector spaces that preserves vector addition and scalar multiplication. Every linear transformation can be represented by a matrix. Examples include rotations, reflections, scaling, shearing, and projections.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Linear Transformation", "Transformation Matrix"],
      "keywords": ["linear transformation", "matrix", "rotation", "scaling"],
      "source_links": ["docs/chapters/04-linear-transformations/index.md", "docs/glossary.md#linear-transformation"],
      "has_example": true,
      "word_count": 38
    },
    {
      "id": "faq-017",
      "category": "Core Concepts",
      "question": "What is the determinant and what does it tell us?",
      "answer": "The determinant is a scalar computed from a square matrix that indicates whether the matrix is invertible (nonzero = invertible), the volume scaling factor of the transformation, and orientation change (negative = orientation flip).",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Determinant", "Invertible Matrix", "Volume Scaling Factor"],
      "keywords": ["determinant", "invertible", "volume", "orientation"],
      "source_links": ["docs/chapters/05-determinants-and-matrix-properties/index.md", "docs/glossary.md#determinant"],
      "has_example": true,
      "word_count": 40
    },
    {
      "id": "faq-018",
      "category": "Core Concepts",
      "question": "What are eigenvalues and eigenvectors?",
      "answer": "An eigenvector of matrix A is a nonzero vector that only gets scaled (not rotated) when transformed by A. The scaling factor is the eigenvalue. Eigenvectors reveal 'natural directions' where transformation acts as simple scaling.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Eigenvalue", "Eigenvector", "Eigenspace"],
      "keywords": ["eigenvalue", "eigenvector", "scaling", "transformation"],
      "source_links": ["docs/chapters/06-eigenvalues-and-eigenvectors/index.md", "docs/glossary.md#eigenvalue"],
      "has_example": true,
      "word_count": 41
    },
    {
      "id": "faq-019",
      "category": "Core Concepts",
      "question": "What is Singular Value Decomposition (SVD)?",
      "answer": "SVD decomposes any matrix A into A = UΣV^T where U contains left singular vectors, Σ is diagonal with singular values, and V^T contains right singular vectors. SVD enables low-rank approximation, compression, and dimensionality reduction.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["SVD", "Singular Value", "Left Singular Vector", "Right Singular Vector"],
      "keywords": ["SVD", "singular value decomposition", "factorization"],
      "source_links": ["docs/chapters/07-matrix-decompositions/index.md", "docs/glossary.md#svd"],
      "has_example": true,
      "word_count": 45
    },
    {
      "id": "faq-020",
      "category": "Core Concepts",
      "question": "What is Principal Component Analysis (PCA)?",
      "answer": "PCA finds directions of maximum variance in data by computing eigenvectors of the covariance matrix. It's used for dimensionality reduction, data visualization, feature extraction, and noise reduction by keeping top k components.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["PCA", "Principal Component", "Variance Explained", "Covariance Matrix"],
      "keywords": ["PCA", "principal component", "dimensionality reduction", "variance"],
      "source_links": ["docs/chapters/09-machine-learning-foundations/index.md", "docs/glossary.md#pca"],
      "has_example": true,
      "word_count": 38
    },
    {
      "id": "faq-021",
      "category": "Core Concepts",
      "question": "How do neural networks use linear algebra?",
      "answer": "Neural networks use weight matrices connecting layers through matrix-vector multiplication, bias vectors for constant offsets, forward propagation as chained matrix multiplications with activations, and backpropagation using chain rule with Jacobian matrices.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Weight Matrix", "Bias Vector", "Forward Propagation", "Backpropagation"],
      "keywords": ["neural network", "weight matrix", "forward propagation", "backpropagation"],
      "source_links": ["docs/chapters/10-neural-networks-and-deep-learning/index.md"],
      "has_example": true,
      "word_count": 38
    },
    {
      "id": "faq-022",
      "category": "Core Concepts",
      "question": "What is the attention mechanism in transformers?",
      "answer": "Attention computes weighted combinations of values based on relevance between queries and keys. The formula is Attention(Q,K,V) = softmax(QK^T/√d)V. Multi-head attention runs multiple attention operations in parallel.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Attention Mechanism", "Query Matrix", "Key Matrix", "Value Matrix", "Multi-Head Attention"],
      "keywords": ["attention", "transformer", "query", "key", "value"],
      "source_links": ["docs/chapters/11-generative-ai-and-llms/index.md", "docs/glossary.md#attention-mechanism"],
      "has_example": false,
      "word_count": 41
    },
    {
      "id": "faq-023",
      "category": "Core Concepts",
      "question": "What is a Kalman filter?",
      "answer": "A Kalman filter optimally estimates system state from noisy measurements using two steps: Predict (project state forward using system model) and Update (correct prediction using new measurements). The Kalman gain weights prediction versus measurement based on uncertainties.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Kalman Filter", "State Estimation", "Prediction Step", "Update Step", "Kalman Gain"],
      "keywords": ["Kalman filter", "state estimation", "sensor fusion", "prediction"],
      "source_links": ["docs/chapters/15-autonomous-systems-and-sensor-fusion/index.md", "docs/glossary.md#kalman-filter"],
      "has_example": true,
      "word_count": 43
    },
    {
      "id": "faq-024",
      "category": "Technical Details",
      "question": "What is the difference between L1, L2, and L-infinity norms?",
      "answer": "L1 (Manhattan) is the sum of absolute values. L2 (Euclidean) is the square root of sum of squares. L-infinity (Max) is the largest absolute component value. For vector (3, -4): L1=7, L2=5, L∞=4.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["L1 Norm", "L2 Norm", "L-Infinity Norm"],
      "keywords": ["L1 norm", "L2 norm", "L-infinity", "Manhattan", "Euclidean"],
      "source_links": ["docs/chapters/01-vectors-and-vector-spaces/index.md", "docs/glossary.md#l1-norm"],
      "has_example": true,
      "word_count": 44
    },
    {
      "id": "faq-025",
      "category": "Technical Details",
      "question": "What is the difference between a symmetric and orthogonal matrix?",
      "answer": "A symmetric matrix equals its transpose (A = A^T) with real eigenvalues and orthogonal eigenvectors. An orthogonal matrix has orthonormal columns/rows where Q^TQ = I, preserving lengths and angles. Covariance matrices are symmetric; rotation matrices are orthogonal.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Symmetric Matrix", "Orthogonal Matrix"],
      "keywords": ["symmetric", "orthogonal", "transpose", "eigenvalues"],
      "source_links": ["docs/glossary.md#symmetric-matrix", "docs/glossary.md#orthogonal-matrix"],
      "has_example": true,
      "word_count": 43
    },
    {
      "id": "faq-026",
      "category": "Technical Details",
      "question": "What is the difference between rank and nullity?",
      "answer": "Rank is the dimension of the column space (number of linearly independent columns). Nullity is the dimension of the null space (vectors mapping to zero). The Rank-Nullity Theorem states: rank + nullity = number of columns.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Rank", "Nullity", "Rank-Nullity Theorem"],
      "keywords": ["rank", "nullity", "column space", "null space"],
      "source_links": ["docs/glossary.md#rank", "docs/glossary.md#nullity"],
      "has_example": true,
      "word_count": 42
    },
    {
      "id": "faq-027",
      "category": "Technical Details",
      "question": "What is the condition number and why does it matter?",
      "answer": "The condition number is the ratio of largest to smallest singular value, measuring solution sensitivity to input changes. Near 1 is well-conditioned (stable). Greater than 10^10 is ill-conditioned (numerically unstable) and can amplify rounding errors dramatically.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Condition Number", "Numerical Stability", "Singular Value"],
      "keywords": ["condition number", "stability", "ill-conditioned", "singular values"],
      "source_links": ["docs/glossary.md#condition-number"],
      "has_example": true,
      "word_count": 42
    },
    {
      "id": "faq-028",
      "category": "Technical Details",
      "question": "What is the difference between row echelon form and reduced row echelon form?",
      "answer": "Row echelon form (REF) has leading 1s (pivots) with zeros below each pivot and rows ordered by pivot position. Reduced row echelon form (RREF) additionally has each pivot as the only nonzero entry in its column, making solutions easier to read.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Row Echelon Form", "Reduced Row Echelon Form", "Pivot Position"],
      "keywords": ["row echelon", "reduced row echelon", "RREF", "pivot"],
      "source_links": ["docs/chapters/03-systems-of-linear-equations/index.md", "docs/glossary.md#row-echelon-form"],
      "has_example": false,
      "word_count": 49
    },
    {
      "id": "faq-029",
      "category": "Technical Details",
      "question": "What is the difference between QR and LU decomposition?",
      "answer": "LU decomposes into Lower × Upper triangular matrices, works on square matrices, and is generally faster. QR decomposes into Orthogonal × Upper triangular, works on any matrix, and is inherently more stable for ill-conditioned problems.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["LU Decomposition", "QR Decomposition"],
      "keywords": ["LU decomposition", "QR decomposition", "triangular", "orthogonal"],
      "source_links": ["docs/chapters/07-matrix-decompositions/index.md", "docs/glossary.md#lu-decomposition"],
      "has_example": false,
      "word_count": 42
    },
    {
      "id": "faq-030",
      "category": "Technical Details",
      "question": "What is the pseudoinverse?",
      "answer": "The pseudoinverse A^+ generalizes matrix inversion to non-square and singular matrices, computed from SVD as A^+ = VΣ^+U^T. It solves least squares problems: x = A^+b minimizes ||Ax - b||.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Pseudoinverse", "SVD", "Least Squares Problem"],
      "keywords": ["pseudoinverse", "Moore-Penrose", "least squares", "SVD"],
      "source_links": ["docs/glossary.md#pseudoinverse"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-031",
      "category": "Technical Details",
      "question": "What is the difference between gradient descent and Newton's method?",
      "answer": "Gradient descent uses first derivatives (gradient) with linear convergence. Newton's method uses first and second derivatives (Hessian) with quadratic convergence near the minimum, but higher per-iteration cost. Quasi-Newton methods like BFGS approximate the Hessian.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Gradient Descent", "Newton's Method", "Hessian Matrix", "BFGS Algorithm"],
      "keywords": ["gradient descent", "Newton's method", "Hessian", "optimization"],
      "source_links": ["docs/chapters/12-optimization-and-learning-algorithms/index.md", "docs/glossary.md#gradient-descent"],
      "has_example": false,
      "word_count": 42
    },
    {
      "id": "faq-032",
      "category": "Technical Details",
      "question": "What is the difference between convolution and correlation in image processing?",
      "answer": "Convolution flips the kernel before sliding across the image. Correlation does not flip. For symmetric kernels they're identical. Most deep learning frameworks implement correlation but call it 'convolution' due to historical convention.",
      "bloom_level": "Analyze",
      "difficulty": "medium",
      "concepts": ["Image Convolution", "Convolution Kernel"],
      "keywords": ["convolution", "correlation", "kernel", "flip"],
      "source_links": ["docs/chapters/13-image-processing-and-computer-vision/index.md"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-033",
      "category": "Technical Details",
      "question": "What are homogeneous coordinates?",
      "answer": "Homogeneous coordinates add an extra dimension: 2D point (x,y) becomes (x,y,1). This enables representing translations as matrix multiplication, unified treatment of transformations, representing points at infinity, and simplifying perspective projection.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Homogeneous Coordinates", "Perspective Projection"],
      "keywords": ["homogeneous coordinates", "projective", "translation", "perspective"],
      "source_links": ["docs/chapters/14-3d-geometry-and-transformations/index.md", "docs/glossary.md#homogeneous-coordinates"],
      "has_example": true,
      "word_count": 36
    },
    {
      "id": "faq-034",
      "category": "Common Challenges",
      "question": "Why do I get different results when multiplying matrices in different orders?",
      "answer": "Matrix multiplication is not commutative: AB ≠ BA in general. The order matters because transformations are applied in sequence. Dimensions may not even allow reverse multiplication. Geometrically, rotating then scaling differs from scaling then rotating.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Matrix Multiplication", "Composition of Transforms"],
      "keywords": ["matrix multiplication", "order", "commutative", "composition"],
      "source_links": ["docs/chapters/02-matrices-and-matrix-operations/index.md"],
      "has_example": true,
      "word_count": 42
    },
    {
      "id": "faq-035",
      "category": "Common Challenges",
      "question": "How do I know if a system of equations has a unique solution, no solution, or infinitely many solutions?",
      "answer": "Analyze the augmented matrix after row reduction. Pivot in every coefficient column means unique solution. Pivot in the constant column means no solution. Fewer pivots than variables means infinitely many solutions.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["System of Equations", "Unique Solution", "No Solution", "Infinite Solutions"],
      "keywords": ["solution", "unique", "infinite", "no solution", "pivot"],
      "source_links": ["docs/chapters/03-systems-of-linear-equations/index.md"],
      "has_example": false,
      "word_count": 40
    },
    {
      "id": "faq-036",
      "category": "Common Challenges",
      "question": "Why does my matrix inversion give numerical errors?",
      "answer": "Numerical errors occur when the matrix is singular or near-singular, has poor conditioning (large condition number), or accumulated errors compound. Solutions include using decompositions instead of explicit inversion, partial pivoting, or higher precision.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Matrix Inverse", "Numerical Stability", "Condition Number"],
      "keywords": ["inversion", "numerical error", "stability", "conditioning"],
      "source_links": ["docs/glossary.md#numerical-stability"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-037",
      "category": "Common Challenges",
      "question": "How do I handle non-square matrices?",
      "answer": "Non-square matrices can't be inverted directly. For overdetermined systems (more rows than columns), use pseudoinverse or least squares. For underdetermined (fewer rows), use minimum norm solution. SVD works on all matrices and provides the pseudoinverse.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Matrix", "Pseudoinverse", "Least Squares Problem"],
      "keywords": ["non-square", "overdetermined", "underdetermined", "pseudoinverse"],
      "source_links": ["docs/glossary.md#pseudoinverse"],
      "has_example": false,
      "word_count": 42
    },
    {
      "id": "faq-038",
      "category": "Common Challenges",
      "question": "Why do eigenvalue computations sometimes give complex numbers?",
      "answer": "Complex eigenvalues occur when a real matrix includes rotational components. A 2D rotation has eigenvalues cos(θ) ± i·sin(θ). Complex eigenvalues always come in conjugate pairs for real matrices and indicate oscillatory behavior.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Complex Eigenvalue", "Eigenvalue", "Rotation Matrix"],
      "keywords": ["complex eigenvalue", "rotation", "conjugate pair"],
      "source_links": ["docs/glossary.md#complex-eigenvalue"],
      "has_example": true,
      "word_count": 40
    },
    {
      "id": "faq-039",
      "category": "Common Challenges",
      "question": "How do I choose the right matrix decomposition?",
      "answer": "For solving Ax=b generally, use LU. For symmetric positive definite, use Cholesky. For least squares, use QR. For eigenvalues, use specialized algorithms. For dimensionality reduction or low-rank approximation, use SVD.",
      "bloom_level": "Evaluate",
      "difficulty": "hard",
      "concepts": ["LU Decomposition", "QR Decomposition", "Cholesky Decomposition", "SVD"],
      "keywords": ["decomposition", "choose", "LU", "QR", "Cholesky", "SVD"],
      "source_links": ["docs/chapters/07-matrix-decompositions/index.md"],
      "has_example": false,
      "word_count": 39
    },
    {
      "id": "faq-040",
      "category": "Common Challenges",
      "question": "Why is gradient descent slow for some problems?",
      "answer": "Gradient descent can be slow due to ill-conditioning (different scales across dimensions), saddle points, plateaus, or learning rate issues. Solutions include adaptive methods (Adam, RMSprop), preconditioning, or feature normalization.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Gradient Descent", "Adam Optimizer", "RMSprop", "Condition Number"],
      "keywords": ["gradient descent", "slow", "ill-conditioning", "Adam"],
      "source_links": ["docs/chapters/12-optimization-and-learning-algorithms/index.md"],
      "has_example": false,
      "word_count": 35
    },
    {
      "id": "faq-041",
      "category": "Common Challenges",
      "question": "How do I debug dimension mismatch errors in neural networks?",
      "answer": "Common causes include inner dimensions not matching for matrix multiplication, batch dimension confusion, wrong reshape before fully connected layers, or convolution output size miscalculation. Trace dimensions through each layer systematically.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Matrix Dimensions", "Neural Network Layer", "Convolutional Layer"],
      "keywords": ["dimension mismatch", "debug", "neural network", "shape"],
      "source_links": ["docs/chapters/10-neural-networks-and-deep-learning/index.md"],
      "has_example": false,
      "word_count": 35
    },
    {
      "id": "faq-042",
      "category": "Common Challenges",
      "question": "How do eigenvalue computations fail?",
      "answer": "Eigenvalue computations can fail or be unreliable when the matrix is very large (iterative methods needed), has repeated eigenvalues (may lack complete eigenvector set), is nearly defective, or has extremely large/small entries causing overflow.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Eigenvalue", "Power Iteration", "Numerical Stability"],
      "keywords": ["eigenvalue", "fail", "numerical", "repeated"],
      "source_links": ["docs/chapters/06-eigenvalues-and-eigenvectors/index.md"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-043",
      "category": "Best Practices",
      "question": "When should I use sparse matrix representations?",
      "answer": "Use sparse matrices when more than 90% of entries are zero, the matrix is large (thousands of rows/columns), memory is constrained, or operations preserve sparsity. Common formats include CSR (fast row slicing) and CSC (fast column slicing).",
      "bloom_level": "Evaluate",
      "difficulty": "medium",
      "concepts": ["Sparse Matrix", "Dense Matrix"],
      "keywords": ["sparse", "matrix", "CSR", "CSC", "memory"],
      "source_links": ["docs/glossary.md#sparse-matrix"],
      "has_example": true,
      "word_count": 44
    },
    {
      "id": "faq-044",
      "category": "Best Practices",
      "question": "How do I choose the number of principal components to keep?",
      "answer": "Common approaches: variance threshold (keep 95% or 99% of total variance), scree plot (look for 'elbow'), cross-validation (minimize reconstruction error), or domain knowledge (keep interpretable components). No universal rule exists.",
      "bloom_level": "Evaluate",
      "difficulty": "medium",
      "concepts": ["PCA", "Variance Explained", "Scree Plot"],
      "keywords": ["principal components", "PCA", "variance", "scree plot"],
      "source_links": ["docs/chapters/09-machine-learning-foundations/index.md", "docs/glossary.md#pca"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-045",
      "category": "Best Practices",
      "question": "What regularization strength should I use?",
      "answer": "Finding optimal regularization strength typically requires cross-validation, grid search over logarithmic range (0.001 to 10), domain knowledge, and monitoring for underfitting or overfitting. Start with λ=1 and adjust based on validation performance.",
      "bloom_level": "Evaluate",
      "difficulty": "medium",
      "concepts": ["Regularization", "Ridge Regression", "Lasso Regression"],
      "keywords": ["regularization", "lambda", "cross-validation", "overfitting"],
      "source_links": ["docs/chapters/09-machine-learning-foundations/index.md", "docs/glossary.md#regularization"],
      "has_example": false,
      "word_count": 37
    },
    {
      "id": "faq-046",
      "category": "Best Practices",
      "question": "How should I normalize features before applying linear algebra algorithms?",
      "answer": "Standardization (x-μ)/σ works for different scales and PCA. Min-Max normalization gives bounded 0-1 output. L2 normalization when direction matters more than magnitude. Always apply the same transformation to training and test data.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Standardization", "Feature Matrix"],
      "keywords": ["normalize", "standardization", "min-max", "scaling"],
      "source_links": ["docs/glossary.md#standardization"],
      "has_example": false,
      "word_count": 41
    },
    {
      "id": "faq-047",
      "category": "Best Practices",
      "question": "How do I handle missing data in matrix computations?",
      "answer": "Options include imputation (fill with mean/median/predicted values), matrix completion (low-rank methods for missing entries), masking (weight valid entries only), or dropping rows/columns if missingness is sparse and random.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Data Matrix", "Low-Rank Approximation"],
      "keywords": ["missing data", "imputation", "matrix completion"],
      "source_links": ["docs/chapters/09-machine-learning-foundations/index.md"],
      "has_example": false,
      "word_count": 35
    },
    {
      "id": "faq-048",
      "category": "Best Practices",
      "question": "What's the best way to implement matrix operations efficiently?",
      "answer": "Use optimized libraries (NumPy, BLAS), vectorize operations avoiding explicit loops, consider memory layout, batch operations, exploit matrix structure (symmetric, sparse, banded), and avoid unnecessary copies with in-place operations.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Matrix", "Tensor Operations"],
      "keywords": ["efficient", "NumPy", "BLAS", "vectorize", "batch"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 33
    },
    {
      "id": "faq-049",
      "category": "Best Practices",
      "question": "How should I choose between different attention mechanisms?",
      "answer": "Dot-product attention has O(n²d) complexity for moderate sequences. Multi-head learns multiple relationship types. Linear attention O(nd²) suits very long sequences. Sparse attention O(nd) works for extremely long sequences with local patterns.",
      "bloom_level": "Evaluate",
      "difficulty": "hard",
      "concepts": ["Attention Mechanism", "Multi-Head Attention", "Self-Attention"],
      "keywords": ["attention", "multi-head", "complexity", "sparse"],
      "source_links": ["docs/chapters/11-generative-ai-and-llms/index.md"],
      "has_example": false,
      "word_count": 39
    },
    {
      "id": "faq-050",
      "category": "Best Practices",
      "question": "How can I verify my linear algebra implementations are correct?",
      "answer": "Test with identity checks (multiply by I returns input), inverse checks (AA⁻¹=I), orthogonality checks, numerical comparison with NumPy/SciPy, known solutions, property preservation, and gradient checking against numerical differences.",
      "bloom_level": "Evaluate",
      "difficulty": "medium",
      "concepts": ["Identity Matrix", "Matrix Inverse", "Orthogonal Matrix"],
      "keywords": ["verify", "test", "implementation", "correctness"],
      "source_links": [],
      "has_example": false,
      "word_count": 34
    },
    {
      "id": "faq-051",
      "category": "Advanced Topics",
      "question": "How does LoRA reduce the cost of fine-tuning large language models?",
      "answer": "LoRA decomposes weight updates as ΔW = BA where B is (d×r) and A is (r×k) with small rank r. This reduces trainable parameters by 10-100x, memory requirements, and storage for multiple fine-tuned models.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["LoRA", "Low-Rank Approximation", "Weight Matrix"],
      "keywords": ["LoRA", "fine-tuning", "low-rank", "parameters"],
      "source_links": ["docs/chapters/11-generative-ai-and-llms/index.md", "docs/glossary.md#lora"],
      "has_example": true,
      "word_count": 43
    },
    {
      "id": "faq-052",
      "category": "Advanced Topics",
      "question": "What is the relationship between SVD and eigendecomposition?",
      "answer": "For matrix A: A^T·A has eigenvalues σ² (squared singular values) with eigenvectors V. A·A^T has eigenvalues σ² with eigenvectors U. Singular values are square roots of eigenvalues of A^T·A. For symmetric matrices, SVD and eigendecomposition are essentially the same.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["SVD", "Eigendecomposition", "Eigenvalue"],
      "keywords": ["SVD", "eigendecomposition", "relationship", "singular values"],
      "source_links": ["docs/chapters/07-matrix-decompositions/index.md"],
      "has_example": false,
      "word_count": 47
    },
    {
      "id": "faq-053",
      "category": "Advanced Topics",
      "question": "How does convolution in neural networks differ from mathematical convolution?",
      "answer": "Mathematical convolution flips the kernel. Neural network 'convolution' doesn't flip (technically correlation). Neural network kernels are learned, operate on 2D/3D with channels, and aim for feature extraction rather than signal processing.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["Image Convolution", "Convolutional Layer", "Convolution Kernel"],
      "keywords": ["convolution", "neural network", "correlation", "kernel"],
      "source_links": ["docs/chapters/13-image-processing-and-computer-vision/index.md"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-054",
      "category": "Advanced Topics",
      "question": "How do quaternions avoid gimbal lock?",
      "answer": "Gimbal lock occurs when Euler angle axes align, losing a degree of freedom. Quaternions avoid this by representing rotations as 4D unit vectors without axis-angle singularities, enabling smooth interpolation (SLERP) between orientations.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Quaternion", "Gimbal Lock", "Euler Angles"],
      "keywords": ["quaternion", "gimbal lock", "rotation", "SLERP"],
      "source_links": ["docs/chapters/14-3d-geometry-and-transformations/index.md", "docs/glossary.md#quaternion"],
      "has_example": false,
      "word_count": 38
    },
    {
      "id": "faq-055",
      "category": "Advanced Topics",
      "question": "What makes SLAM computationally challenging?",
      "answer": "SLAM challenges include the chicken-and-egg problem (need position for map, need map for position), growing state over time, loop closure requiring global matching, real-time constraints, and managing correlated uncertainties.",
      "bloom_level": "Analyze",
      "difficulty": "hard",
      "concepts": ["SLAM", "Localization", "Mapping", "State Estimation"],
      "keywords": ["SLAM", "computational", "loop closure", "real-time"],
      "source_links": ["docs/chapters/15-autonomous-systems-and-sensor-fusion/index.md", "docs/glossary.md#slam"],
      "has_example": false,
      "word_count": 37
    },
    {
      "id": "faq-056",
      "category": "Advanced Topics",
      "question": "How do I design a custom loss function using matrix operations?",
      "answer": "Express the loss in matrix form for batch computation, ensure differentiability for backpropagation, avoid numerical instabilities (log(0), division by zero), consider convexity for easier optimization, and match the loss to the problem type.",
      "bloom_level": "Create",
      "difficulty": "hard",
      "concepts": ["Loss Function", "Gradient Vector", "Convexity"],
      "keywords": ["loss function", "custom", "matrix", "differentiability"],
      "source_links": ["docs/chapters/12-optimization-and-learning-algorithms/index.md"],
      "has_example": true,
      "word_count": 40
    },
    {
      "id": "faq-057",
      "category": "Advanced Topics",
      "question": "What are the trade-offs between different sensor fusion approaches?",
      "answer": "Kalman Filter is optimal for linear Gaussian but assumes linearity. Extended Kalman handles nonlinearity but may diverge. Particle Filter handles any distribution but is expensive. Factor graphs handle complex relationships but are complex to implement.",
      "bloom_level": "Evaluate",
      "difficulty": "hard",
      "concepts": ["Sensor Fusion", "Kalman Filter", "Extended Kalman Filter"],
      "keywords": ["sensor fusion", "Kalman", "particle filter", "trade-offs"],
      "source_links": ["docs/chapters/15-autonomous-systems-and-sensor-fusion/index.md"],
      "has_example": false,
      "word_count": 42
    },
    {
      "id": "faq-058",
      "category": "Advanced Topics",
      "question": "What emerging applications of linear algebra should I be aware of?",
      "answer": "Active areas include quantum computing (states as vectors, unitary matrices), graph neural networks (sparse matrix message passing), NeRF for 3D scenes, diffusion models, mixture of experts with sparse gating, and efficient state space models.",
      "bloom_level": "Remember",
      "difficulty": "medium",
      "concepts": ["Linear Algebra", "Machine Learning"],
      "keywords": ["emerging", "quantum", "graph neural networks", "diffusion"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 39
    },
    {
      "id": "faq-059",
      "category": "Advanced Topics",
      "question": "How can I extend linear algebra concepts to tensors?",
      "answer": "Tensors generalize matrices to higher dimensions. Key operations include tensor products, contractions (generalized matrix multiplication), reshaping, and mode-n products. Libraries like NumPy and TensorFlow handle tensor algebra for deep learning.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Tensor", "Tensor Operations"],
      "keywords": ["tensor", "generalize", "contraction", "mode-n"],
      "source_links": ["docs/glossary.md#tensor"],
      "has_example": false,
      "word_count": 35
    }
  ]
}
