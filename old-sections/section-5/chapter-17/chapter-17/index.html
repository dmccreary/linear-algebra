
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An in-depth exploration of linear algebra concepts and their applications in machine learning, computer vision, and autonomous systems.">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/linear-Algebra/old-sections/section-5/chapter-17/chapter-17/">
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.43">
    
    
      
        <title>Chapter 17: Principal Component Analysis and Beyond - Linear Algebra</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../css/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-KC2L3G6KXH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-KC2L3G6KXH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-KC2L3G6KXH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Chapter 17: Principal Component Analysis and Beyond - Linear Algebra" >
      
        <meta  property="og:description"  content="An in-depth exploration of linear algebra concepts and their applications in machine learning, computer vision, and autonomous systems." >
      
        <meta  property="og:image"  content="https://dmccreary.github.io/linear-Algebra/assets/images/social/old-sections/section-5/chapter-17/chapter-17.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://dmccreary.github.io/linear-Algebra/old-sections/section-5/chapter-17/chapter-17/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Chapter 17: Principal Component Analysis and Beyond - Linear Algebra" >
      
        <meta  name="twitter:description"  content="An in-depth exploration of linear algebra concepts and their applications in machine learning, computer vision, and autonomous systems." >
      
        <meta  name="twitter:image"  content="https://dmccreary.github.io/linear-Algebra/assets/images/social/old-sections/section-5/chapter-17/chapter-17.png" >
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="navy" data-md-color-accent="yellow">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-17-principal-component-analysis-and-beyond" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="Linear Algebra" class="md-header__button md-logo" aria-label="Linear Algebra" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Linear Algebra
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 17: Principal Component Analysis and Beyond
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/linear-Algebra" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="Linear Algebra" class="md-nav__button md-logo" aria-label="Linear Algebra" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Linear Algebra
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/linear-Algebra" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../course-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Course Description
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../../chapters/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Chapters
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../../learning-graph/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Learning Graph
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../../../../sims/graph-viewer/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    MicroSims
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-covariance-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      1. Covariance Matrices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Covariance Matrices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-covariance" class="md-nav__link">
    <span class="md-ellipsis">
      What is Covariance?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#building-the-covariance-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Building the Covariance Matrix
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-principal-component-analysis-pca" class="md-nav__link">
    <span class="md-ellipsis">
      2. Principal Component Analysis (PCA)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Principal Component Analysis (PCA)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intuitive-idea" class="md-nav__link">
    <span class="md-ellipsis">
      Intuitive Idea
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-it-works" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-pca-works" class="md-nav__link">
    <span class="md-ellipsis">
      Why PCA Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-whitening-transformations" class="md-nav__link">
    <span class="md-ellipsis">
      3. Whitening Transformations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-low-rank-matrix-approximation" class="md-nav__link">
    <span class="md-ellipsis">
      4. Low-Rank Matrix Approximation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Low-Rank Matrix Approximation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#how-it-works_1" class="md-nav__link">
    <span class="md-ellipsis">
      How It Works
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-matrix-completion" class="md-nav__link">
    <span class="md-ellipsis">
      5. Matrix Completion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#quiz-question" class="md-nav__link">
    <span class="md-ellipsis">
      Quiz Question
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/dmccreary/linear-Algebra/edit/master/docs/old-sections/section-5/chapter-17/chapter-17.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="chapter-17-principal-component-analysis-and-beyond">Chapter 17: Principal Component Analysis and Beyond</h1>
<h2 id="introduction">Introduction</h2>
<p>Imagine standing in a massive library where every book represents a feature of your dataset. Some books are filled with crucial knowledge; others, not so much. Principal Component Analysis (PCA) helps us figure out which "books" contain the most useful information, allowing us to focus only on the essentials.</p>
<p>In this chapter, we explore how PCA uses linear algebra — specifically eigenvalues and eigenvectors — to reduce the complexity of data while preserving its most important structures. We'll also touch on related ideas like whitening transformations, low-rank approximations, and matrix completion, expanding your toolkit for working with real-world, messy datasets.</p>
<h2 id="1-covariance-matrices">1. Covariance Matrices</h2>
<p>Before we dive into PCA, we must understand <strong>covariance matrices</strong>.</p>
<h3 id="what-is-covariance">What is Covariance?</h3>
<p>Covariance measures how two variables change together:
- <strong>Positive covariance</strong>: Variables increase together.
- <strong>Negative covariance</strong>: As one increases, the other decreases.</p>
<p>Mathematically, for variables <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>:</p>
<div class="arithmatex">\[
\text{Cov}(X, Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)]
\]</div>
<p>where <span class="arithmatex">\(\mu_X\)</span> and <span class="arithmatex">\(\mu_Y\)</span> are the means of <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>.</p>
<h3 id="building-the-covariance-matrix">Building the Covariance Matrix</h3>
<p>Given a dataset with multiple features, the <strong>covariance matrix</strong> summarizes covariances between all pairs of features. For a data matrix <span class="arithmatex">\(X\)</span> (after centering by subtracting the mean):</p>
<div class="arithmatex">\[
\Sigma = \frac{1}{n-1}X^T X
\]</div>
<p><strong>Key Insight:</strong>
The covariance matrix is symmetric and its structure tells us how features relate. Eigenvalues and eigenvectors of this matrix reveal the most important directions in the data.</p>
<h2 id="2-principal-component-analysis-pca">2. Principal Component Analysis (PCA)</h2>
<h3 id="intuitive-idea">Intuitive Idea</h3>
<p>PCA finds new axes (directions) where the data variance is maximized. Think of spinning a cloud of data points: PCA aligns a new coordinate system along the directions of greatest spread.</p>
<h3 id="how-it-works">How It Works</h3>
<ol>
<li><strong>Center the data</strong>: Subtract the mean from each feature.</li>
<li><strong>Compute the covariance matrix</strong> <span class="arithmatex">\(\Sigma\)</span>.</li>
<li><strong>Find eigenvectors and eigenvalues</strong>:<ul>
<li>Eigenvectors = Principal components (new axes)</li>
<li>Eigenvalues = How much variance each principal component captures</li>
</ul>
</li>
<li><strong>Sort eigenvectors</strong> by eigenvalues in descending order.</li>
<li><strong>Project data</strong> onto the top <span class="arithmatex">\(k\)</span> eigenvectors to reduce dimensionality.</li>
</ol>
<h3 id="why-pca-works">Why PCA Works</h3>
<ul>
<li><strong>Energy (variance)</strong> is preserved: We keep most of the "action" of the data.</li>
<li><strong>Orthogonality</strong> of principal components ensures that projections are independent and non-redundant.</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Quick Visualization</p>
<p>Imagine shining a flashlight onto a 3D object. The resulting 2D shadow captures the shape. PCA finds the best way to cast that shadow to preserve maximum detail.</p>
</div>
<h2 id="3-whitening-transformations">3. Whitening Transformations</h2>
<p>PCA decorrelates features but may still have different variances along each axis. <strong>Whitening</strong> goes a step further: it scales the principal components so that they have unit variance.</p>
<p>Mathematically:</p>
<div class="arithmatex">\[
X_{\text{white}} = \Lambda^{-1/2} U^T X
\]</div>
<p>where <span class="arithmatex">\(U\)</span> contains the eigenvectors and <span class="arithmatex">\(\Lambda\)</span> is the diagonal matrix of eigenvalues.</p>
<p><strong>Use Cases:</strong>
- Improve stability and convergence in machine learning algorithms.
- Standardize features without introducing correlations.</p>
<h2 id="4-low-rank-matrix-approximation">4. Low-Rank Matrix Approximation</h2>
<p>Often, data can be well-approximated by a matrix with <strong>lower rank</strong> — fewer "independent dimensions" than the original.</p>
<h3 id="how-it-works_1">How It Works</h3>
<ul>
<li><strong>Singular Value Decomposition (SVD)</strong> decomposes a matrix into <span class="arithmatex">\(U \Sigma V^T\)</span>.</li>
<li>By keeping only the largest singular values and corresponding vectors, we build a lower-rank approximation.</li>
</ul>
<div class="admonition example">
<p class="admonition-title">Movie Ratings Dataset</p>
<p>Suppose a movie ratings matrix has missing entries. A low-rank approximation guesses missing ratings based on patterns in the data.</p>
</div>
<h2 id="5-matrix-completion">5. Matrix Completion</h2>
<p>Matrix completion fills missing values in a matrix, assuming the data lies in a low-dimensional space.</p>
<ul>
<li>Widely used in <strong>recommender systems</strong> (e.g., Netflix recommendations).</li>
<li>Methods include minimizing the nuclear norm (sum of singular values) subject to constraints from known entries.</li>
</ul>
<p>The idea is that with only a few observations, if the data is simple enough (low-rank), we can guess the missing pieces accurately.</p>
<h2 id="summary">Summary</h2>
<ul>
<li><strong>Covariance matrices</strong> reveal relationships between features.</li>
<li><strong>PCA</strong> finds the directions of maximum variance to compress data.</li>
<li><strong>Whitening</strong> standardizes data after PCA.</li>
<li><strong>Low-rank approximation</strong> captures essential structure with fewer dimensions.</li>
<li><strong>Matrix completion</strong> guesses missing data using low-rank assumptions.</li>
</ul>
<p>These techniques are central to fields like data science, image compression, genomics, and more.</p>
<hr />
<h2 id="quiz-question">Quiz Question</h2>
<p><strong>Which matrix operation is central to finding the principal components in PCA?</strong></p>
<div class="upper-alpha">
<p>A. Matrix inversion<br />
B. Eigen decomposition of the covariance matrix<br />
C. LU decomposition<br />
D. QR factorization</p>
</div>
<details class="question">
<summary>Show Answer</summary>
<p>The correct answer is <strong>B</strong>. PCA relies on the eigen decomposition of the data’s covariance matrix to find directions (principal components) that capture the most variance.</p>
</details>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../../../js/extra.js"></script>
      
        <script src="../../../../js/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>