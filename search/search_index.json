{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linear Algebra","text":"<p>Course Description</p> <p>In Linear Algebra, you won\u2019t just learn matrices and vectors\u2014you'll experience them in action. This course empowers undergraduate students in Computer Science and Artificial Intelligence to develop a deep, functional understanding of linear algebra\u2019s essential role in modern technology.</p>"},{"location":"about/","title":"About the Linear Algebra Book","text":"<p>In January of 2025 I was invited to lead a group of four students in a senior design project.  These were all student in their senior year at the University of Minnesota Department of Electrical Engineering and Computer Design.  My team all were assigned to create an intelligent textbook for either a class they took or a subject they were interested in.  One of these students selected Linear Algebra.  This course is a fork of that textbook, but it has been redesigned significantly to focus on the application of linear algebra on machine learning.</p> <p>Our tools have also matured since January 2025.  This version of our intelligent textbook was generated using Claude Code Skills.  We also put a much stronger focus on creating high-quality MicroSims.</p> <ul> <li>Dan McCreary</li> <li>January 17th, 2025</li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"course-description/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"course-description/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"course-description/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"course-description/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"course-description/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"course-description/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"course-description/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"course-description/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"course-description/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"course-description/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"course-description/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"course-description/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"course-description/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"course-description/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"course-description/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"course-description/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"course-description/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"course-description/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"course-description/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"course-description/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"course-description/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"course-description/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"course-description/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"course-description/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"course-description/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"course-description/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"course-description/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"course-description/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"course-description/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"course-description/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"course-description/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"faq/","title":"Applied Linear Algebra for AI and Machine Learning FAQ","text":"<p>This FAQ addresses common questions about the course content, concepts, and applications. Questions are organized by category to help you find answers quickly.</p>"},{"location":"faq/#getting-started-questions","title":"Getting Started Questions","text":""},{"location":"faq/#what-is-this-course-about","title":"What is this course about?","text":"<p>This course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. You'll develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life. The course covers vectors, matrices, linear transformations, eigenvalues, matrix decompositions, and their applications in neural networks, generative AI, image processing, and autonomous systems.</p>"},{"location":"faq/#who-is-this-course-designed-for","title":"Who is this course designed for?","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul> <p>The material is presented at a college undergraduate level, making it accessible to anyone with the prerequisites who wants to understand the mathematics behind modern AI systems.</p>"},{"location":"faq/#what-are-the-prerequisites-for-this-course","title":"What are the prerequisites for this course?","text":"<p>To succeed in this course, you should have:</p> <ul> <li>College Algebra or equivalent: Familiarity with functions, equations, and basic mathematical notation</li> <li>Basic programming experience: Python is recommended but not required</li> <li>Familiarity with calculus concepts: Understanding of derivatives and integrals at a conceptual level</li> </ul> <p>You don't need prior exposure to linear algebra\u2014this course starts from the fundamentals and builds up systematically.</p>"},{"location":"faq/#how-is-the-course-structured","title":"How is the course structured?","text":"<p>The course is divided into four major parts spanning 15 chapters:</p> <ol> <li>Part 1: Foundations of Linear Algebra (Chapters 1-4): Vectors, matrices, systems of equations, and linear transformations</li> <li>Part 2: Advanced Matrix Theory (Chapters 5-8): Determinants, eigenvalues, matrix decompositions, and abstract vector spaces</li> <li>Part 3: Linear Algebra in Machine Learning (Chapters 9-12): ML foundations, neural networks, generative AI, and optimization</li> <li>Part 4: Computer Vision and Autonomous Systems (Chapters 13-15): Image processing, 3D geometry, and sensor fusion</li> </ol> <p>Each chapter includes interactive MicroSims to reinforce concepts through hands-on exploration.</p>"},{"location":"faq/#how-do-i-use-the-interactive-microsims","title":"How do I use the interactive MicroSims?","text":"<p>MicroSims are browser-based interactive simulations that let you visualize and experiment with linear algebra concepts. To use them:</p> <ol> <li>Navigate to the MicroSims section in the sidebar</li> <li>Select a simulation relevant to what you're studying</li> <li>Use the sliders, buttons, and controls to adjust parameters</li> <li>Observe how changes affect the visualization in real-time</li> <li>Connect the visual behavior to the mathematical concepts you're learning</li> </ol> <p>No software installation is required\u2014all MicroSims run directly in your web browser.</p>"},{"location":"faq/#why-is-linear-algebra-important-for-ai-and-machine-learning","title":"Why is linear algebra important for AI and machine learning?","text":"<p>Linear algebra is the mathematical language in which modern AI systems are written. Understanding it enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically inside them</li> <li>Optimize performance by choosing efficient matrix operations and representations</li> <li>Innovate by seeing new ways to apply linear algebra concepts to novel problems</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundational concepts</li> </ul> <p>From the matrix multiplications in neural networks to the transformations in computer vision, virtually every AI algorithm relies heavily on linear algebra operations.</p>"},{"location":"faq/#how-long-does-it-take-to-complete-each-chapter","title":"How long does it take to complete each chapter?","text":"<p>Each chapter is designed for approximately one week of study, including:</p> <ul> <li>Reading the chapter content (2-3 hours)</li> <li>Working through examples and exercises (2-3 hours)</li> <li>Exploring interactive MicroSims (1-2 hours)</li> <li>Completing practice problems (2-3 hours)</li> </ul> <p>The entire course spans 15 weeks at this pace, though self-study learners can adjust their schedule as needed.</p>"},{"location":"faq/#what-software-do-i-need","title":"What software do I need?","text":"<p>For reading the textbook and using MicroSims, you only need a modern web browser. For hands-on programming exercises, you'll benefit from:</p> <ul> <li>Python 3.x with the following libraries:</li> <li>NumPy: For numerical computations and array operations</li> <li>Matplotlib: For creating visualizations</li> <li>scikit-learn: For machine learning examples</li> <li>Optional: GPU access for deep learning exercises in later chapters</li> </ul> <p>All code examples in the textbook use Python with NumPy.</p>"},{"location":"faq/#how-can-i-check-my-understanding-of-the-material","title":"How can I check my understanding of the material?","text":"<p>Each chapter provides multiple ways to assess your understanding:</p> <ul> <li>Concept check questions embedded throughout the text</li> <li>Interactive MicroSims where you can test your predictions</li> <li>Practice problems with varying difficulty levels</li> <li>The glossary for reviewing terminology</li> <li>Quiz questions for self-assessment</li> </ul> <p>Working through these resources actively, rather than passively reading, is the key to building deep understanding.</p>"},{"location":"faq/#where-can-i-get-help-if-im-stuck","title":"Where can I get help if I'm stuck?","text":"<p>If you're struggling with a concept:</p> <ol> <li>Review the relevant glossary definitions for terminology clarity</li> <li>Use the MicroSims to build geometric intuition</li> <li>Re-read prerequisite material if foundational concepts are unclear</li> <li>Check the learning graph to ensure you've covered prerequisite concepts</li> <li>For textbook issues, report problems on the GitHub Issues page</li> </ol>"},{"location":"faq/#core-concept-questions","title":"Core Concept Questions","text":""},{"location":"faq/#what-is-the-difference-between-a-scalar-and-a-vector","title":"What is the difference between a scalar and a vector?","text":"<p>A scalar is a single numerical value representing magnitude only (like temperature or mass). A vector is an ordered collection of scalars that represents both magnitude and direction. While the scalar 5 tells you \"how much,\" the vector (3, 4) tells you \"how much and in which direction.\"</p> <p>Example: Speed of 60 mph is a scalar; velocity of 60 mph heading northeast is represented as a vector with components in the x and y directions.</p> <p>See also: Chapter 1: Vectors and Vector Spaces</p>"},{"location":"faq/#what-is-a-matrix-and-how-does-it-relate-to-vectors","title":"What is a matrix and how does it relate to vectors?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. You can think of a matrix as:</p> <ul> <li>A collection of column vectors side by side</li> <li>A collection of row vectors stacked vertically</li> <li>A representation of a linear transformation</li> </ul> <p>A matrix with dimensions m\u00d7n has m rows and n columns. When you multiply a matrix by a vector, you're applying a linear transformation that maps the input vector to an output vector.</p> <p>Example: A 3\u00d72 matrix contains 3 rows and 2 columns, and can be viewed as 2 column vectors in 3-dimensional space.</p> <p>See also: Chapter 2: Matrices and Matrix Operations</p>"},{"location":"faq/#what-does-it-mean-for-vectors-to-be-linearly-independent","title":"What does it mean for vectors to be linearly independent?","text":"<p>Vectors are linearly independent if no vector in the set can be written as a linear combination of the others. Equivalently, the only way to combine them to get the zero vector is with all zero coefficients.</p> <p>Example: The vectors (1, 0) and (0, 1) are linearly independent because neither is a multiple of the other. However, (1, 2) and (2, 4) are linearly dependent because (2, 4) = 2\u00b7(1, 2).</p> <p>Linear independence is crucial because independent vectors provide \"new directions\" in space, while dependent vectors are redundant.</p>"},{"location":"faq/#what-is-a-basis-and-why-is-it-important","title":"What is a basis and why is it important?","text":"<p>A basis is a set of linearly independent vectors that span an entire vector space. Every vector in the space can be written as a unique linear combination of basis vectors. The number of vectors in a basis equals the dimension of the space.</p> <p>The basis is important because it provides a coordinate system for the vector space. The standard basis in 3D consists of the unit vectors along each axis: (1,0,0), (0,1,0), and (0,0,1).</p> <p>Example: Any point in 3D space can be written as x(1,0,0) + y(0,1,0) + z(0,0,1) where (x, y, z) are the coordinates.</p>"},{"location":"faq/#what-is-the-dot-product-and-what-does-it-tell-us","title":"What is the dot product and what does it tell us?","text":"<p>The dot product (also called inner product) of two vectors produces a scalar value computed as the sum of products of corresponding components:</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\ldots + a_nb_n\\] <p>Geometrically, the dot product equals |a||b|cos(\u03b8) where \u03b8 is the angle between the vectors. This tells us:</p> <ul> <li>If the dot product is positive, vectors point in similar directions</li> <li>If the dot product is zero, vectors are perpendicular (orthogonal)</li> <li>If the dot product is negative, vectors point in opposite directions</li> </ul> <p>Example: The dot product of (1, 2) and (3, 4) is 1\u00d73 + 2\u00d74 = 11.</p>"},{"location":"faq/#what-is-a-linear-transformation","title":"What is a linear transformation?","text":"<p>A linear transformation is a function between vector spaces that preserves vector addition and scalar multiplication. If T is a linear transformation, then:</p> <ul> <li>T(u + v) = T(u) + T(v) for all vectors u and v</li> <li>T(cv) = cT(v) for all vectors v and scalars c</li> </ul> <p>Every linear transformation can be represented by a matrix. Common examples include rotations, reflections, scaling, shearing, and projections.</p> <p>Example: Rotating a 2D vector by 45\u00b0 is a linear transformation represented by the rotation matrix [[cos(45\u00b0), -sin(45\u00b0)], [sin(45\u00b0), cos(45\u00b0)]].</p> <p>See also: Chapter 4: Linear Transformations</p>"},{"location":"faq/#what-is-the-determinant-and-what-does-it-tell-us","title":"What is the determinant and what does it tell us?","text":"<p>The determinant is a scalar value computed from a square matrix that tells us:</p> <ol> <li>Whether the matrix is invertible (nonzero determinant = invertible)</li> <li>The volume scaling factor of the associated transformation</li> <li>The orientation change (negative determinant = orientation flip)</li> </ol> <p>For a 2\u00d72 matrix [[a, b], [c, d]], the determinant is ad - bc.</p> <p>Example: A rotation matrix always has determinant 1 (preserves area and orientation). A reflection matrix has determinant -1 (preserves area but flips orientation).</p> <p>See also: Chapter 5: Determinants and Matrix Properties</p>"},{"location":"faq/#what-are-eigenvalues-and-eigenvectors","title":"What are eigenvalues and eigenvectors?","text":"<p>An eigenvector of a matrix A is a nonzero vector v that, when transformed by A, points in the same direction (or exactly opposite)\u2014it only gets scaled by a factor \u03bb called the eigenvalue:</p> \\[A\\mathbf{v} = \\lambda\\mathbf{v}\\] <p>Eigenvectors reveal the \"natural directions\" of a transformation where the transformation acts as simple scaling rather than rotation or shearing.</p> <p>Example: For a horizontal stretch matrix that doubles the x-coordinate, any vector along the x-axis is an eigenvector with eigenvalue 2, and any vector along the y-axis is an eigenvector with eigenvalue 1.</p> <p>See also: Chapter 6: Eigenvalues and Eigenvectors</p>"},{"location":"faq/#what-is-singular-value-decomposition-svd","title":"What is Singular Value Decomposition (SVD)?","text":"<p>SVD decomposes any matrix A into three matrices: A = U\u03a3V^T where:</p> <ul> <li>U contains left singular vectors (orthonormal columns)</li> <li>\u03a3 is a diagonal matrix of singular values (non-negative, decreasing)</li> <li>V^T contains right singular vectors (orthonormal rows)</li> </ul> <p>SVD reveals the fundamental structure of any matrix and enables:</p> <ul> <li>Low-rank approximation (keeping only largest singular values)</li> <li>Image compression</li> <li>Pseudoinverse computation</li> <li>Dimensionality reduction</li> </ul> <p>Example: Truncating an image's SVD to keep only the 50 largest singular values can reduce storage by 90% while maintaining recognizable quality.</p> <p>See also: Chapter 7: Matrix Decompositions</p>"},{"location":"faq/#what-is-principal-component-analysis-pca","title":"What is Principal Component Analysis (PCA)?","text":"<p>PCA is a technique that finds the directions of maximum variance in data by computing eigenvectors of the covariance matrix. The first principal component points in the direction of greatest variance, the second in the direction of greatest remaining variance (perpendicular to the first), and so on.</p> <p>PCA is used for:</p> <ul> <li>Dimensionality reduction (keeping top k components)</li> <li>Data visualization (projecting high-dimensional data to 2D or 3D)</li> <li>Feature extraction (finding the most informative directions)</li> <li>Noise reduction (removing low-variance components)</li> </ul> <p>Example: Applying PCA to face images produces \"eigenfaces\"\u2014the principal components that capture the most variation in facial appearance.</p> <p>See also: Chapter 9: Machine Learning Foundations</p>"},{"location":"faq/#how-do-neural-networks-use-linear-algebra","title":"How do neural networks use linear algebra?","text":"<p>Neural networks are fundamentally composed of linear algebra operations:</p> <ul> <li>Weight matrices connect layers through matrix-vector multiplication</li> <li>Bias vectors add constant offsets to layer outputs</li> <li>Forward propagation chains matrix multiplications with nonlinear activations</li> <li>Backpropagation uses chain rule with Jacobian matrices to compute gradients</li> <li>Batch processing uses matrix-matrix multiplication for efficiency</li> </ul> <p>Each layer computes: output = activation(W\u00b7input + b) where W is the weight matrix and b is the bias vector.</p> <p>Example: A layer connecting 100 inputs to 50 outputs uses a 50\u00d7100 weight matrix containing 5,000 learnable parameters.</p> <p>See also: Chapter 10: Neural Networks and Deep Learning</p>"},{"location":"faq/#what-is-the-attention-mechanism-in-transformers","title":"What is the attention mechanism in transformers?","text":"<p>The attention mechanism computes weighted combinations of values based on the relevance between queries and keys. Given Query (Q), Key (K), and Value (V) matrices:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>This allows each position in a sequence to \"attend to\" relevant positions elsewhere. The dot product Q\u00b7K^T measures similarity, softmax normalizes to weights, and the weighted sum of V produces the output.</p> <p>Multi-head attention runs multiple attention operations in parallel, allowing the model to attend to different types of relationships simultaneously.</p> <p>See also: Chapter 11: Generative AI and LLMs</p>"},{"location":"faq/#what-is-a-kalman-filter","title":"What is a Kalman filter?","text":"<p>A Kalman filter is an optimal algorithm for estimating the state of a system from noisy measurements. It works in two steps:</p> <ol> <li>Predict: Use a system model to predict the next state</li> <li>Update: Correct the prediction using new measurements</li> </ol> <p>The Kalman gain determines how much to trust the prediction versus the measurement. The filter optimally combines both sources of information based on their uncertainties.</p> <p>Example: A GPS receiver uses Kalman filtering to estimate position by fusing satellite measurements (accurate but slow) with inertial sensors (fast but drifts).</p> <p>See also: Chapter 15: Autonomous Systems and Sensor Fusion</p>"},{"location":"faq/#technical-detail-questions","title":"Technical Detail Questions","text":""},{"location":"faq/#what-is-the-difference-between-l1-l2-and-l-infinity-norms","title":"What is the difference between L1, L2, and L-infinity norms?","text":"<p>These are different ways to measure vector length:</p> Norm Formula Geometric Interpretation L1 (Manhattan) $|v|_1 = \\sum v_i L2 (Euclidean) \\(\\|v\\|_2 = \\sqrt{\\sum v_i^2}\\) Straight-line distance L\u221e (Max) $|v|_\\infty = \\max v_i <p>Example: For vector (3, -4), L1 = 7, L2 = 5, L\u221e = 4.</p>"},{"location":"faq/#what-is-the-difference-between-a-symmetric-and-orthogonal-matrix","title":"What is the difference between a symmetric and orthogonal matrix?","text":"<p>A symmetric matrix equals its own transpose: A = A^T. This means the element in row i, column j equals the element in row j, column i. Symmetric matrices have real eigenvalues and orthogonal eigenvectors.</p> <p>An orthogonal matrix has columns (and rows) that are orthonormal: Q^TQ = QQ^T = I. This means Q^(-1) = Q^T. Orthogonal matrices preserve lengths and angles\u2014rotations and reflections are orthogonal.</p> <p>Example: Covariance matrices are symmetric. Rotation matrices are orthogonal.</p>"},{"location":"faq/#what-is-the-difference-between-rank-and-nullity","title":"What is the difference between rank and nullity?","text":"<p>The rank of a matrix is the dimension of its column space\u2014the number of linearly independent columns. The nullity is the dimension of its null space\u2014the number of independent vectors that map to zero.</p> <p>The Rank-Nullity Theorem states: rank(A) + nullity(A) = number of columns.</p> <p>Example: A 3\u00d75 matrix with rank 3 has nullity 5 - 3 = 2, meaning two free variables exist in the solution to Ax = 0.</p>"},{"location":"faq/#what-is-the-condition-number-and-why-does-it-matter","title":"What is the condition number and why does it matter?","text":"<p>The condition number of a matrix is the ratio of its largest to smallest singular value. It measures how sensitive solutions are to small changes in input:</p> <ul> <li>Condition number \u2248 1: Well-conditioned (stable)</li> <li>Condition number &gt; 10^10: Ill-conditioned (numerically unstable)</li> </ul> <p>An ill-conditioned matrix amplifies rounding errors, potentially making computed solutions unreliable.</p> <p>Example: A matrix with condition number 10^6 can amplify input errors by up to a million times in the output.</p>"},{"location":"faq/#what-is-the-difference-between-row-echelon-form-and-reduced-row-echelon-form","title":"What is the difference between row echelon form and reduced row echelon form?","text":"<p>Row echelon form (REF): - Leading entries (pivots) are 1 - Each pivot is to the right of the pivot above - Rows of all zeros are at the bottom - Entries below each pivot are zero</p> <p>Reduced row echelon form (RREF) adds: - Each pivot is the only nonzero entry in its column</p> <p>RREF makes reading solutions easier but requires more computation to achieve.</p>"},{"location":"faq/#what-is-the-difference-between-qr-and-lu-decomposition","title":"What is the difference between QR and LU decomposition?","text":"Feature LU Decomposition QR Decomposition Form A = LU (Lower \u00d7 Upper triangular) A = QR (Orthogonal \u00d7 Upper triangular) Matrix type Square, some need pivoting Any matrix Stability May need partial pivoting Inherently stable Primary use Solving linear systems Least squares, eigenvalue algorithms Computation Generally faster More stable for ill-conditioned problems"},{"location":"faq/#what-is-the-pseudoinverse","title":"What is the pseudoinverse?","text":"<p>The pseudoinverse A^+ generalizes matrix inversion to non-square and singular matrices. It's computed from SVD as:</p> \\[A^+ = V\\Sigma^+U^T\\] <p>where \u03a3^+ is formed by taking reciprocals of nonzero singular values.</p> <p>The pseudoinverse solves least squares problems: x = A^+b minimizes ||Ax - b||.</p>"},{"location":"faq/#what-is-the-difference-between-gradient-descent-and-newtons-method","title":"What is the difference between gradient descent and Newton's method?","text":"Feature Gradient Descent Newton's Method Uses First derivatives (gradient) First and second derivatives (Hessian) Step direction Steepest descent Newton step using curvature Convergence Linear (slow near minimum) Quadratic (fast near minimum) Per-iteration cost Low (gradient only) High (Hessian inversion) Robustness Works far from minimum May diverge far from minimum <p>For large-scale problems, quasi-Newton methods like BFGS approximate the Hessian without computing it explicitly.</p>"},{"location":"faq/#what-is-the-difference-between-convolution-and-correlation-in-image-processing","title":"What is the difference between convolution and correlation in image processing?","text":"<p>Convolution flips the kernel before sliding it across the image. Correlation does not flip the kernel. For symmetric kernels, they're identical.</p> <p>Mathematically, convolution is associative (order doesn't matter for multiple filters), which is important for neural network design.</p> <p>In practice, most deep learning frameworks implement correlation but call it \"convolution.\"</p>"},{"location":"faq/#what-are-homogeneous-coordinates","title":"What are homogeneous coordinates?","text":"<p>Homogeneous coordinates add an extra dimension to represent points. A 2D point (x, y) becomes (x, y, 1) in homogeneous coordinates. This enables:</p> <ul> <li>Representing translations as matrix multiplication</li> <li>Unified treatment of affine and projective transformations</li> <li>Representing points at infinity</li> <li>Simplifying perspective projection</li> </ul> <p>To convert back: (x, y, w) \u2192 (x/w, y/w)</p> <p>Example: Translation, which is not a linear transformation in Cartesian coordinates, becomes a matrix multiplication in homogeneous coordinates.</p>"},{"location":"faq/#common-challenge-questions","title":"Common Challenge Questions","text":""},{"location":"faq/#why-do-i-get-different-results-when-multiplying-matrices-in-different-orders","title":"Why do I get different results when multiplying matrices in different orders?","text":"<p>Matrix multiplication is not commutative: AB \u2260 BA in general. The order matters because:</p> <ul> <li>A applies to the result of B, not the other way around</li> <li>Dimensions may not even allow reverse multiplication</li> <li>Geometrically, applying transformation A then B differs from B then A</li> </ul> <p>Example: Rotating then scaling gives a different result than scaling then rotating.</p>"},{"location":"faq/#how-do-i-know-if-a-system-of-equations-has-a-unique-solution-no-solution-or-infinitely-many-solutions","title":"How do I know if a system of equations has a unique solution, no solution, or infinitely many solutions?","text":"<p>Analyze the augmented matrix after row reduction:</p> Condition Solution Type Pivot in every column (of coefficient matrix) Unique solution Pivot in last column (constant column) No solution Fewer pivots than variables Infinitely many solutions <p>The rank of the coefficient matrix compared to the augmented matrix determines solvability.</p>"},{"location":"faq/#why-does-my-matrix-inversion-give-numerical-errors","title":"Why does my matrix inversion give numerical errors?","text":"<p>Numerical errors in matrix inversion occur when:</p> <ol> <li>Matrix is singular or near-singular: Small pivots cause division by tiny numbers</li> <li>Poor conditioning: Large condition number amplifies rounding errors</li> <li>Accumulated errors: Long computation chains compound small errors</li> </ol> <p>Solutions: - Use LU or QR decomposition instead of explicit inversion - Apply partial pivoting - Use higher precision arithmetic for critical applications - Reformulate the problem to avoid explicit inversion</p>"},{"location":"faq/#how-do-i-handle-non-square-matrices","title":"How do I handle non-square matrices?","text":"<p>Non-square matrices can't be inverted directly, but you can:</p> <ul> <li>For m\u00d7n with m &gt; n (overdetermined): Use pseudoinverse or least squares</li> <li>For m\u00d7n with m &lt; n (underdetermined): Solution has free variables; use minimum norm solution</li> <li>For any case: SVD works on all matrices and provides the pseudoinverse</li> </ul>"},{"location":"faq/#why-do-eigenvalue-computations-sometimes-give-complex-numbers","title":"Why do eigenvalue computations sometimes give complex numbers?","text":"<p>Complex eigenvalues occur when a real matrix includes rotational components. For example, a pure rotation matrix in 2D has eigenvalues cos(\u03b8) \u00b1 i\u00b7sin(\u03b8).</p> <p>Complex eigenvalues always come in conjugate pairs for real matrices. They indicate oscillatory behavior in dynamical systems.</p>"},{"location":"faq/#how-do-i-choose-the-right-matrix-decomposition","title":"How do I choose the right matrix decomposition?","text":"Problem Best Decomposition Solve Ax = b (general) LU with pivoting Solve Ax = b (symmetric positive definite) Cholesky Least squares QR Eigenvalues/vectors Use specialized eigenvalue algorithms Dimensionality reduction SVD or eigendecomposition Low-rank approximation Truncated SVD Numerical stability critical QR or SVD"},{"location":"faq/#why-is-gradient-descent-slow-for-some-problems","title":"Why is gradient descent slow for some problems?","text":"<p>Gradient descent can be slow when:</p> <ol> <li>Ill-conditioning: Different dimensions have very different scales</li> <li>Saddle points: Gradient is small but not at a minimum</li> <li>Plateaus: Loss surface is nearly flat</li> <li>Learning rate issues: Too small = slow; too large = oscillation</li> </ol> <p>Solutions: Use adaptive methods (Adam, RMSprop), apply preconditioning, or normalize features.</p>"},{"location":"faq/#how-do-i-debug-dimension-mismatch-errors-in-neural-networks","title":"How do I debug dimension mismatch errors in neural networks?","text":"<p>Common dimension mismatch causes:</p> <ol> <li>Matrix multiplication: Inner dimensions must match (m\u00d7n times n\u00d7p)</li> <li>Batch dimension confusion: First dimension is usually batch size</li> <li>Flattening errors: Wrong reshape before fully connected layers</li> <li>Convolution output: Calculate output size using (input - kernel + 2\u00d7padding)/stride + 1</li> </ol> <p>Trace dimensions through each layer systematically to find the mismatch.</p>"},{"location":"faq/#best-practice-questions","title":"Best Practice Questions","text":""},{"location":"faq/#when-should-i-use-sparse-matrix-representations","title":"When should I use sparse matrix representations?","text":"<p>Use sparse matrices when:</p> <ul> <li>More than 90% of entries are zero</li> <li>Matrix is large (thousands of rows/columns)</li> <li>Memory is constrained</li> <li>Operations preserve sparsity</li> </ul> <p>Common sparse formats include CSR (fast row slicing), CSC (fast column slicing), and COO (fast construction).</p> <p>Example: A 10,000\u00d710,000 matrix with only 50,000 nonzero entries uses 99.95% less memory in sparse format.</p>"},{"location":"faq/#how-do-i-choose-the-number-of-principal-components-to-keep","title":"How do I choose the number of principal components to keep?","text":"<p>Common approaches:</p> <ol> <li>Variance threshold: Keep components explaining 95% or 99% of total variance</li> <li>Scree plot: Look for an \"elbow\" where variance explained drops sharply</li> <li>Cross-validation: Choose k that minimizes reconstruction error on held-out data</li> <li>Domain knowledge: Keep components that have interpretable meaning</li> </ol> <p>There's no universal rule\u2014the best choice depends on your specific application.</p>"},{"location":"faq/#what-regularization-strength-should-i-use","title":"What regularization strength should I use?","text":"<p>Finding the right regularization strength (\u03bb) typically requires:</p> <ol> <li>Cross-validation: Try multiple values and evaluate on validation set</li> <li>Grid search: Systematically explore a range (often logarithmic: 0.001, 0.01, 0.1, 1, 10)</li> <li>Domain knowledge: Larger \u03bb when you expect simpler relationships</li> <li>Monitoring: Watch for underfitting (\u03bb too large) or overfitting (\u03bb too small)</li> </ol> <p>Start with \u03bb = 1 and adjust based on validation performance.</p>"},{"location":"faq/#how-should-i-normalize-features-before-applying-linear-algebra-algorithms","title":"How should I normalize features before applying linear algebra algorithms?","text":"<p>Common normalization strategies:</p> Method Formula When to Use Standardization (x - \u03bc) / \u03c3 Features with different scales; PCA Min-Max (x - min) / (max - min) Bounded output needed (0-1) L2 Normalization x / Batch Normalization Layer-wise during training Deep neural networks <p>Always apply the same transformation to training and test data.</p>"},{"location":"faq/#how-do-i-handle-missing-data-in-matrix-computations","title":"How do I handle missing data in matrix computations?","text":"<p>Options for missing data:</p> <ol> <li>Imputation: Fill with mean, median, or predicted values</li> <li>Matrix completion: Use low-rank methods to estimate missing entries</li> <li>Mask and ignore: Weight valid entries only in loss computation</li> <li>Drop rows/columns: If missingness is sparse and random</li> </ol> <p>For recommendation systems, matrix completion methods specifically designed for sparse matrices work well.</p>"},{"location":"faq/#whats-the-best-way-to-implement-matrix-operations-efficiently","title":"What's the best way to implement matrix operations efficiently?","text":"<p>For efficient matrix operations:</p> <ol> <li>Use optimized libraries: NumPy, BLAS, LAPACK, cuBLAS for GPU</li> <li>Avoid explicit loops: Vectorize operations</li> <li>Consider memory layout: Row-major vs column-major affects cache performance</li> <li>Batch operations: Process multiple inputs simultaneously</li> <li>Exploit structure: Use specialized algorithms for symmetric, sparse, or banded matrices</li> <li>Avoid unnecessary copies: Use in-place operations when possible</li> </ol>"},{"location":"faq/#how-should-i-choose-between-different-attention-mechanisms","title":"How should I choose between different attention mechanisms?","text":"Mechanism Complexity Best For Dot-product O(n\u00b2d) Standard transformer, moderate sequences Multi-head O(n\u00b2d) Learning multiple relationship types Linear O(nd\u00b2) Very long sequences Sparse O(nd) Extremely long sequences with local patterns Cross-attention O(nm d) Different-length source and target <p>For most applications, multi-head dot-product attention works well.</p>"},{"location":"faq/#advanced-topic-questions","title":"Advanced Topic Questions","text":""},{"location":"faq/#how-does-lora-reduce-the-cost-of-fine-tuning-large-language-models","title":"How does LoRA reduce the cost of fine-tuning large language models?","text":"<p>Low-Rank Adaptation (LoRA) decomposes weight updates as the product of two small matrices: \u0394W = BA where B is (d \u00d7 r) and A is (r \u00d7 k) with rank r &lt;&lt; min(d, k).</p> <p>Instead of updating millions of parameters in the original weight matrix, LoRA only trains the small A and B matrices. This reduces:</p> <ul> <li>Trainable parameters by 10-100x</li> <li>Memory requirements during training</li> <li>Storage for multiple fine-tuned models (only store A and B)</li> </ul> <p>Example: For a 1000\u00d71000 weight matrix, using rank r=8 reduces parameters from 1,000,000 to 16,000.</p>"},{"location":"faq/#what-is-the-relationship-between-svd-and-eigendecomposition","title":"What is the relationship between SVD and eigendecomposition?","text":"<p>For a matrix A:</p> <ul> <li>A^T A has eigenvalues \u03c3\u00b2 (squared singular values) and eigenvectors V</li> <li>A A^T has eigenvalues \u03c3\u00b2 and eigenvectors U</li> <li>The singular values of A are the square roots of eigenvalues of A^T A</li> </ul> <p>For symmetric matrices, SVD and eigendecomposition are essentially the same, with singular values being absolute values of eigenvalues.</p>"},{"location":"faq/#how-does-convolution-in-neural-networks-differ-from-mathematical-convolution","title":"How does convolution in neural networks differ from mathematical convolution?","text":"Aspect Mathematical Convolution Neural Network \"Convolution\" Kernel flip Yes No (technically correlation) Dimensions Continuous or 1D discrete 2D, 3D with channels Kernel Fixed Learned from data Goal Signal processing Feature extraction <p>The terminology is historical\u2014neural network convolution is mathematically cross-correlation, but the learned kernels make the distinction practically irrelevant.</p>"},{"location":"faq/#how-do-quaternions-avoid-gimbal-lock","title":"How do quaternions avoid gimbal lock?","text":"<p>Gimbal lock occurs with Euler angles when two rotation axes align, losing a degree of freedom. Quaternions avoid this by:</p> <ul> <li>Representing rotations as 4D unit vectors (avoiding singularities)</li> <li>Using a different mathematical structure without axis-angle limitations</li> <li>Enabling smooth interpolation (SLERP) between orientations</li> </ul> <p>The trade-off is less intuitive representation\u2014quaternion components don't directly correspond to roll, pitch, yaw angles.</p>"},{"location":"faq/#what-makes-slam-computationally-challenging","title":"What makes SLAM computationally challenging?","text":"<p>SLAM (Simultaneous Localization and Mapping) is challenging because:</p> <ol> <li>Chicken-and-egg problem: Need position to build map, need map to determine position</li> <li>Growing state: Map size grows over time, increasing computation</li> <li>Loop closure: Recognizing previously visited locations requires global matching</li> <li>Real-time constraints: Must process sensor data fast enough for navigation</li> <li>Uncertainty management: Probabilistic state estimation with correlated errors</li> </ol> <p>Modern SLAM systems use sparse representations, keyframe-based approaches, and graph optimization to manage complexity.</p>"},{"location":"faq/#how-do-i-design-a-custom-loss-function-using-matrix-operations","title":"How do I design a custom loss function using matrix operations?","text":"<p>When designing custom loss functions:</p> <ol> <li>Express in matrix form: Vectorize to enable batch computation</li> <li>Ensure differentiability: Gradient must exist for backpropagation</li> <li>Consider numerical stability: Avoid log(0), division by zero</li> <li>Check convexity: Convex losses are easier to optimize</li> <li>Match the problem: Classification \u2192 cross-entropy; regression \u2192 MSE or Huber</li> </ol> <p>Example: Weighted least squares: L = (y - Xw)^T D (y - Xw) where D is a diagonal weight matrix.</p>"},{"location":"faq/#what-are-the-trade-offs-between-different-sensor-fusion-approaches","title":"What are the trade-offs between different sensor fusion approaches?","text":"Approach Pros Cons Kalman Filter Optimal for linear Gaussian Assumes linearity and Gaussian noise Extended Kalman Handles nonlinearity Linearization errors, may diverge Particle Filter Any distribution Computationally expensive Factor Graph Handles complex relationships Complex implementation Neural Fusion Learns optimal fusion Requires training data, less interpretable <p>Choose based on your system's characteristics and computational constraints.</p>"},{"location":"faq/#how-can-i-verify-my-linear-algebra-implementations-are-correct","title":"How can I verify my linear algebra implementations are correct?","text":"<p>Testing strategies:</p> <ol> <li>Identity checks: Does multiplying by identity return input?</li> <li>Inverse checks: Does AA^(-1) = I?</li> <li>Orthogonality checks: Is Q^T Q = I for orthogonal Q?</li> <li>Numerical comparison: Compare with NumPy/SciPy results</li> <li>Known solutions: Test on problems with analytical solutions</li> <li>Property preservation: Do transformations preserve expected properties?</li> <li>Gradient checking: Compare analytical gradients with numerical differences</li> </ol>"},{"location":"faq/#what-emerging-applications-of-linear-algebra-should-i-be-aware-of","title":"What emerging applications of linear algebra should I be aware of?","text":"<p>Active areas applying linear algebra include:</p> <ul> <li>Quantum computing: Quantum states are vectors; operations are unitary matrices</li> <li>Graph neural networks: Message passing as sparse matrix operations</li> <li>Neural radiance fields (NeRF): 3D scene representation using transformations</li> <li>Diffusion models: Noise addition/removal as matrix operations</li> <li>Mixture of experts: Sparse gating with linear combinations</li> <li>State space models: Efficient alternatives to attention using matrix structure</li> </ul> <p>Understanding linear algebra fundamentals prepares you for these advancing fields.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":"<p>This glossary contains 300 terms from the Applied Linear Algebra for AI and Machine Learning course, organized alphabetically with ISO 11179-compliant definitions.</p>"},{"location":"glossary/#2d-rotation","title":"2D Rotation","text":"<p>A linear transformation that rotates vectors in a two-dimensional plane around the origin by a specified angle.</p> <p>Example: Rotating a point (1, 0) by 90 degrees counterclockwise yields (0, 1).</p> <p>See also: 3D Rotation, Rotation Matrix</p>"},{"location":"glossary/#2d-vector","title":"2D Vector","text":"<p>An ordered pair of real numbers representing a point or direction in a two-dimensional plane.</p> <p>Example: The vector (3, 4) represents a point 3 units along the x-axis and 4 units along the y-axis.</p> <p>See also: 3D Vector, N-Dimensional Vector</p>"},{"location":"glossary/#2x2-determinant","title":"2x2 Determinant","text":"<p>The determinant of a 2\u00d72 matrix, calculated as ad - bc for matrix [[a, b], [c, d]].</p> <p>Example: For matrix [[3, 2], [1, 4]], the determinant is 3\u00d74 - 2\u00d71 = 10.</p> <p>See also: 3x3 Determinant, Determinant</p>"},{"location":"glossary/#3d-coordinate-system","title":"3D Coordinate System","text":"<p>A reference frame using three perpendicular axes (typically x, y, z) to specify positions in three-dimensional space.</p> <p>Example: The point (2, 3, 5) is located 2 units along x, 3 units along y, and 5 units up the z-axis.</p> <p>See also: Homogeneous Coordinates, Coordinate System</p>"},{"location":"glossary/#3d-rotation","title":"3D Rotation","text":"<p>A linear transformation that rotates vectors in three-dimensional space around a specified axis by a given angle.</p> <p>Example: Rotating a vector around the z-axis preserves its z-component while rotating its x and y components.</p> <p>See also: 2D Rotation, Euler Angles, Quaternion Rotation</p>"},{"location":"glossary/#3d-vector","title":"3D Vector","text":"<p>An ordered triple of real numbers representing a point or direction in three-dimensional space.</p> <p>Example: The vector (1, 2, 3) points from the origin to a location one unit along x, two along y, and three along z.</p> <p>See also: 2D Vector, N-Dimensional Vector</p>"},{"location":"glossary/#3x3-determinant","title":"3x3 Determinant","text":"<p>The determinant of a 3\u00d73 matrix, computed using cofactor expansion along any row or column.</p> <p>Example: Using the rule of Sarrus or cofactor expansion to compute the determinant of a 3\u00d73 rotation matrix yields 1.</p> <p>See also: 2x2 Determinant, Cofactor Expansion</p>"},{"location":"glossary/#abstract-vector-space","title":"Abstract Vector Space","text":"<p>A set of elements (vectors) together with operations of addition and scalar multiplication satisfying the vector space axioms.</p> <p>Example: The set of all polynomials of degree at most n forms an abstract vector space with standard polynomial addition.</p> <p>See also: Vector Space, Vector Space Axioms</p>"},{"location":"glossary/#activation-function","title":"Activation Function","text":"<p>A nonlinear function applied element-wise to the output of a neural network layer, introducing nonlinearity into the model.</p> <p>Example: ReLU returns max(0, x), allowing neural networks to learn complex, nonlinear patterns in data.</p> <p>See also: ReLU, Sigmoid, Tanh, Softmax</p>"},{"location":"glossary/#adam-optimizer","title":"Adam Optimizer","text":"<p>An adaptive learning rate optimization algorithm combining momentum and RMSprop for efficient gradient descent.</p> <p>Adam adjusts the learning rate for each parameter individually based on estimates of first and second moments of gradients.</p> <p>Example: Training a deep neural network with Adam typically converges faster than standard SGD.</p> <p>See also: SGD, Momentum, RMSprop</p>"},{"location":"glossary/#algebraic-multiplicity","title":"Algebraic Multiplicity","text":"<p>The number of times an eigenvalue appears as a root of the characteristic polynomial.</p> <p>Example: If \u03bb = 2 is a repeated root appearing twice in the characteristic polynomial, its algebraic multiplicity is 2.</p> <p>See also: Eigenvalue, Geometric Multiplicity, Characteristic Polynomial</p>"},{"location":"glossary/#attention-mechanism","title":"Attention Mechanism","text":"<p>A neural network component that computes weighted combinations of values based on learned relevance scores between queries and keys.</p> <p>Attention allows models to focus on relevant parts of the input when producing each part of the output.</p> <p>Example: In machine translation, attention helps the model focus on specific source words when generating each target word.</p> <p>See also: Self-Attention, Cross-Attention, Multi-Head Attention</p>"},{"location":"glossary/#attention-score","title":"Attention Score","text":"<p>A scalar value indicating the relevance between a query vector and a key vector, typically computed as their scaled dot product.</p> <p>Example: Higher attention scores mean the model considers those key-value pairs more relevant for the current query.</p> <p>See also: Attention Weights, Query Matrix, Key Matrix</p>"},{"location":"glossary/#attention-weights","title":"Attention Weights","text":"<p>Normalized attention scores (typically via softmax) that determine how much each value contributes to the output.</p> <p>Example: If a word has attention weight 0.7 for another word, it strongly influences how that word is processed.</p> <p>See also: Attention Score, Softmax, Value Matrix</p>"},{"location":"glossary/#augmented-matrix","title":"Augmented Matrix","text":"<p>A matrix formed by appending the constant terms of a system of linear equations as an additional column to the coefficient matrix.</p> <p>Example: For the system 2x + 3y = 5 and x - y = 1, the augmented matrix is [[2, 3, 5], [1, -1, 1]].</p> <p>See also: Matrix Equation Form, Gaussian Elimination</p>"},{"location":"glossary/#back-substitution","title":"Back Substitution","text":"<p>A technique for solving triangular systems of equations by solving for variables in reverse order, starting from the last equation.</p> <p>Example: In a row echelon form system, you first find the last variable, then substitute it back to find the others.</p> <p>See also: Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#backpropagation","title":"Backpropagation","text":"<p>An algorithm for computing gradients of the loss function with respect to neural network weights by applying the chain rule layer by layer.</p> <p>Backpropagation efficiently computes gradients by propagating error signals backward through the network.</p> <p>Example: During training, backpropagation calculates how much each weight contributed to the prediction error.</p> <p>See also: Forward Propagation, Chain Rule Matrices, Gradient Descent</p>"},{"location":"glossary/#basic-variable","title":"Basic Variable","text":"<p>A variable in a system of linear equations that corresponds to a pivot column in the reduced row echelon form.</p> <p>Example: In a system with two pivot columns for x and y, these are basic variables while z (if present without a pivot) is free.</p> <p>See also: Free Variable, Pivot Column</p>"},{"location":"glossary/#basis-transition-matrix","title":"Basis Transition Matrix","text":"<p>A matrix that converts vector coordinates from one basis representation to another.</p> <p>Example: To express a vector given in standard basis using a custom basis, multiply by the appropriate transition matrix.</p> <p>See also: Change of Basis, Basis Vector</p>"},{"location":"glossary/#basis-vector","title":"Basis Vector","text":"<p>A vector that is part of a basis, which is a linearly independent set that spans the entire vector space.</p> <p>Example: In 2D, the vectors (1, 0) and (0, 1) form the standard basis.</p> <p>See also: Standard Basis, Linear Independence, Span</p>"},{"location":"glossary/#batch-gradient-descent","title":"Batch Gradient Descent","text":"<p>An optimization method that computes the gradient using the entire training dataset before updating parameters.</p> <p>Example: After processing all 10,000 training examples, batch gradient descent makes one parameter update.</p> <p>See also: SGD, Mini-Batch SGD, Learning Rate</p>"},{"location":"glossary/#batch-normalization","title":"Batch Normalization","text":"<p>A technique that normalizes layer inputs across a mini-batch to accelerate training and improve stability.</p> <p>Example: Before applying the activation function, batch normalization centers and scales the values using mini-batch statistics.</p> <p>See also: Layer Normalization, Neural Network Layer</p>"},{"location":"glossary/#bfgs-algorithm","title":"BFGS Algorithm","text":"<p>A quasi-Newton optimization method that approximates the inverse Hessian matrix to find the minimum of a function efficiently.</p> <p>BFGS builds an approximation of curvature information without computing second derivatives explicitly.</p> <p>Example: BFGS is commonly used for training logistic regression and other convex optimization problems.</p> <p>See also: Quasi-Newton Method, Hessian Matrix, Newton's Method</p>"},{"location":"glossary/#bias-vector","title":"Bias Vector","text":"<p>A vector of constant terms added to the weighted sum in each layer of a neural network, allowing shifts in the activation function.</p> <p>Example: Adding a bias of 0.5 to all neurons allows the network to output non-zero values even when all inputs are zero.</p> <p>See also: Weight Matrix, Neural Network Layer</p>"},{"location":"glossary/#block-matrix","title":"Block Matrix","text":"<p>A matrix partitioned into rectangular submatrices (blocks) that can be manipulated as individual elements.</p> <p>Example: A 4\u00d74 matrix can be viewed as a 2\u00d72 block matrix where each block is a 2\u00d72 matrix.</p> <p>See also: Matrix, Sparse Matrix</p>"},{"location":"glossary/#blur-filter","title":"Blur Filter","text":"<p>A convolution kernel that averages neighboring pixel values to reduce image sharpness and noise.</p> <p>Example: A 3\u00d73 averaging filter replaces each pixel with the mean of its 9 neighbors.</p> <p>See also: Image Convolution, Sharpen Filter, Image Filter</p>"},{"location":"glossary/#bounding-box","title":"Bounding Box","text":"<p>A rectangular region defined by coordinates that encloses an object of interest in an image or 3D space.</p> <p>Example: An object detector outputs bounding boxes as (x_min, y_min, x_max, y_max) coordinates around detected cars.</p> <p>See also: Object Detection, Object Tracking</p>"},{"location":"glossary/#camera-calibration","title":"Camera Calibration","text":"<p>The process of estimating camera parameters (intrinsic and extrinsic) to accurately map 3D world points to 2D image coordinates.</p> <p>Example: Using a checkerboard pattern to determine focal length, principal point, and lens distortion coefficients.</p> <p>See also: Camera Matrix, Intrinsic Parameters, Extrinsic Parameters</p>"},{"location":"glossary/#camera-matrix","title":"Camera Matrix","text":"<p>A matrix that maps 3D world coordinates to 2D image coordinates, combining intrinsic and extrinsic parameters.</p> <p>Example: The 3\u00d74 camera matrix projects a 3D point onto the image plane using perspective projection.</p> <p>See also: Projection Matrix, Intrinsic Parameters, Extrinsic Parameters</p>"},{"location":"glossary/#cauchy-schwarz-inequality","title":"Cauchy-Schwarz Inequality","text":"<p>A fundamental inequality stating that the absolute value of an inner product is at most the product of the norms.</p> <p>Example: For any vectors u and v, |\u27e8u, v\u27e9| \u2264 ||u|| \u00b7 ||v||, with equality when vectors are parallel.</p> <p>See also: Inner Product, Norm from Inner Product</p>"},{"location":"glossary/#chain-rule-matrices","title":"Chain Rule Matrices","text":"<p>Matrix representations of derivative chains used in backpropagation to compute gradients through composed functions.</p> <p>Example: The gradient flows backward through layers as products of Jacobian matrices.</p> <p>See also: Backpropagation, Gradient Vector</p>"},{"location":"glossary/#change-of-basis","title":"Change of Basis","text":"<p>The process of expressing vectors in terms of a different set of basis vectors than the original representation.</p> <p>Example: Converting from Cartesian coordinates to polar-like basis vectors requires a change of basis operation.</p> <p>See also: Basis Transition Matrix, Basis Vector</p>"},{"location":"glossary/#characteristic-equation","title":"Characteristic Equation","text":"<p>An equation obtained by setting the characteristic polynomial equal to zero, whose roots are the eigenvalues.</p> <p>Example: For a 2\u00d72 matrix, solving det(A - \u03bbI) = 0 gives the characteristic equation \u03bb\u00b2 - trace(A)\u03bb + det(A) = 0.</p> <p>See also: Characteristic Polynomial, Eigenvalue</p>"},{"location":"glossary/#characteristic-polynomial","title":"Characteristic Polynomial","text":"<p>A polynomial whose roots are the eigenvalues of a matrix, computed as det(A - \u03bbI).</p> <p>Example: For a 3\u00d73 matrix, the characteristic polynomial is a cubic in \u03bb.</p> <p>See also: Characteristic Equation, Eigenvalue, Determinant</p>"},{"location":"glossary/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>A factorization of a positive definite matrix into the product of a lower triangular matrix and its transpose.</p> <p>Cholesky decomposition is computationally efficient and numerically stable for solving systems with positive definite matrices.</p> <p>Example: A covariance matrix can be decomposed as A = LL^T, where L is lower triangular.</p> <p>See also: Positive Definite Matrix, LU Decomposition</p>"},{"location":"glossary/#codomain","title":"Codomain","text":"<p>The set of all possible output values of a function or transformation.</p> <p>Example: For a transformation from R\u00b2 to R\u00b3, the codomain is R\u00b3.</p> <p>See also: Domain, Image, Range</p>"},{"location":"glossary/#cofactor","title":"Cofactor","text":"<p>The signed minor of a matrix element, used in computing determinants and the adjugate matrix.</p> <p>Example: The cofactor of element a\u2081\u2082 is (-1)^(1+2) times the minor M\u2081\u2082.</p> <p>See also: Minor, Cofactor Expansion</p>"},{"location":"glossary/#cofactor-expansion","title":"Cofactor Expansion","text":"<p>A method for computing the determinant by expanding along a row or column using minors and cofactors.</p> <p>Example: Expanding a 3\u00d73 determinant along the first row: det(A) = a\u2081\u2081C\u2081\u2081 + a\u2081\u2082C\u2081\u2082 + a\u2081\u2083C\u2081\u2083.</p> <p>See also: Determinant, Minor, Cofactor</p>"},{"location":"glossary/#color-space-transform","title":"Color Space Transform","text":"<p>A linear transformation that converts image data between different color representations.</p> <p>Example: Converting from RGB to YUV separates luminance from color information using a matrix transformation.</p> <p>See also: RGB Image, Image Matrix</p>"},{"location":"glossary/#column-space","title":"Column Space","text":"<p>The set of all possible linear combinations of the column vectors of a matrix, also called the range.</p> <p>Example: If a 3\u00d72 matrix has linearly independent columns, its column space is a 2D plane in R\u00b3.</p> <p>See also: Range, Row Space, Rank</p>"},{"location":"glossary/#column-vector","title":"Column Vector","text":"<p>A matrix with a single column, representing a vector as a vertical array of numbers.</p> <p>Example: The column vector [1; 2; 3] has three rows and one column.</p> <p>See also: Row Vector, Vector</p>"},{"location":"glossary/#compact-svd","title":"Compact SVD","text":"<p>A form of singular value decomposition that retains only the non-zero singular values and corresponding vectors.</p> <p>Example: For a rank-r matrix, compact SVD uses r singular values instead of the full min(m,n).</p> <p>See also: SVD, Full SVD, Truncated SVD</p>"},{"location":"glossary/#complex-eigenvalue","title":"Complex Eigenvalue","text":"<p>An eigenvalue that has a nonzero imaginary component, occurring in conjugate pairs for real matrices.</p> <p>Example: A 2D rotation matrix has complex eigenvalues cos(\u03b8) \u00b1 i\u00b7sin(\u03b8).</p> <p>See also: Eigenvalue, Characteristic Polynomial</p>"},{"location":"glossary/#composition-of-transforms","title":"Composition of Transforms","text":"<p>The result of applying one linear transformation followed by another, represented by matrix multiplication.</p> <p>Example: Rotating then scaling is represented by multiplying the rotation matrix by the scaling matrix.</p> <p>See also: Linear Transformation, Transformation Matrix</p>"},{"location":"glossary/#condition-number","title":"Condition Number","text":"<p>A measure of how sensitive a matrix computation is to small changes in input, defined as the ratio of largest to smallest singular values.</p> <p>Large condition numbers indicate ill-conditioned matrices prone to numerical instability.</p> <p>Example: A condition number of 10\u2076 means errors can be amplified by up to a million times.</p> <p>See also: Singular Value, Numerical Stability</p>"},{"location":"glossary/#constrained-optimization","title":"Constrained Optimization","text":"<p>Optimization of an objective function subject to equality or inequality constraints on the variables.</p> <p>Example: Minimizing cost while meeting quality requirements is a constrained optimization problem.</p> <p>See also: Lagrange Multiplier, KKT Conditions</p>"},{"location":"glossary/#convex-function","title":"Convex Function","text":"<p>A function where any line segment between two points on its graph lies above or on the graph.</p> <p>Convex functions have a single global minimum, making optimization straightforward.</p> <p>Example: f(x) = x\u00b2 is convex because the parabola curves upward everywhere.</p> <p>See also: Convexity, Hessian Matrix</p>"},{"location":"glossary/#convexity","title":"Convexity","text":"<p>The property of a set or function ensuring that any weighted average of two elements remains within the set or below the function graph.</p> <p>Example: A convex optimization problem has no local minima that are not also global minima.</p> <p>See also: Convex Function, Constrained Optimization</p>"},{"location":"glossary/#convolution-kernel","title":"Convolution Kernel","text":"<p>A small matrix of weights used in convolution operations to detect features or patterns in input data.</p> <p>Example: A 3\u00d73 edge detection kernel highlights boundaries between regions of different intensities.</p> <p>See also: Convolutional Layer, Image Convolution</p>"},{"location":"glossary/#convolutional-layer","title":"Convolutional Layer","text":"<p>A neural network layer that applies learnable convolution kernels to detect local patterns in input data.</p> <p>Example: Early convolutional layers might detect edges, while deeper layers detect complex features like faces.</p> <p>See also: Convolution Kernel, Stride, Padding</p>"},{"location":"glossary/#coordinate-system","title":"Coordinate System","text":"<p>A system that uses numerical values (coordinates) to uniquely specify the position of points in a space.</p> <p>Example: The Cartesian coordinate system uses perpendicular x and y axes to locate points in a plane.</p> <p>See also: 3D Coordinate System, Basis Vector</p>"},{"location":"glossary/#correlation-matrix","title":"Correlation Matrix","text":"<p>A matrix of pairwise Pearson correlation coefficients between features, with values ranging from -1 to 1.</p> <p>Example: A correlation matrix shows which stock prices tend to move together.</p> <p>See also: Covariance Matrix, Feature Matrix</p>"},{"location":"glossary/#cosine-similarity","title":"Cosine Similarity","text":"<p>A measure of similarity between two vectors computed as the cosine of the angle between them.</p> <p>Example: Two parallel vectors have cosine similarity of 1, while orthogonal vectors have similarity of 0.</p> <p>See also: Dot Product, Semantic Similarity</p>"},{"location":"glossary/#covariance-matrix","title":"Covariance Matrix","text":"<p>A symmetric matrix containing covariances between pairs of variables, with variances on the diagonal.</p> <p>The covariance matrix captures how features vary together in a dataset.</p> <p>Example: A 3\u00d73 covariance matrix for height, weight, and age shows how these variables co-vary.</p> <p>See also: Correlation Matrix, PCA</p>"},{"location":"glossary/#cramers-rule","title":"Cramers Rule","text":"<p>A formula for solving systems of linear equations using ratios of determinants.</p> <p>Example: For a 2\u00d72 system, x = det(A_x)/det(A), where A_x replaces the first column with the constant terms.</p> <p>See also: Determinant, System of Equations</p>"},{"location":"glossary/#cross-attention","title":"Cross-Attention","text":"<p>An attention mechanism where queries come from one sequence and keys/values come from a different sequence.</p> <p>Example: In translation, cross-attention allows the decoder to attend to relevant parts of the encoded source sentence.</p> <p>See also: Self-Attention, Attention Mechanism</p>"},{"location":"glossary/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>A loss function measuring the difference between predicted probability distributions and true labels.</p> <p>Example: For classification, cross-entropy heavily penalizes confident wrong predictions.</p> <p>See also: Loss Function, Softmax</p>"},{"location":"glossary/#cross-product","title":"Cross Product","text":"<p>A binary operation on two 3D vectors that produces a vector perpendicular to both inputs.</p> <p>Example: The cross product of (1, 0, 0) and (0, 1, 0) is (0, 0, 1).</p> <p>See also: Dot Product, Vector</p>"},{"location":"glossary/#data-matrix","title":"Data Matrix","text":"<p>A matrix where rows represent observations (samples) and columns represent features (variables).</p> <p>Example: A data matrix for 1000 patients with 50 measurements has dimensions 1000\u00d750.</p> <p>See also: Feature Matrix, Design Matrix</p>"},{"location":"glossary/#deep-network","title":"Deep Network","text":"<p>A neural network with multiple hidden layers, enabling learning of hierarchical feature representations.</p> <p>Example: A 50-layer convolutional network can learn increasingly abstract features for image classification.</p> <p>See also: Hidden Layer, Neural Network Layer</p>"},{"location":"glossary/#dense-matrix","title":"Dense Matrix","text":"<p>A matrix in which most elements are non-zero, typically stored as a full 2D array.</p> <p>Example: A random matrix with normally distributed entries is typically dense.</p> <p>See also: Sparse Matrix, Matrix</p>"},{"location":"glossary/#design-matrix","title":"Design Matrix","text":"<p>A matrix of input features used in regression, with rows as observations and columns as predictors.</p> <p>Example: In linear regression with 100 samples and 5 features, the design matrix is 100\u00d75.</p> <p>See also: Data Matrix, Linear Regression</p>"},{"location":"glossary/#determinant","title":"Determinant","text":"<p>A scalar value computed from a square matrix that indicates whether the matrix is invertible and measures volume scaling.</p> <p>Example: A matrix with determinant zero is singular and cannot be inverted.</p> <p>See also: 2x2 Determinant, 3x3 Determinant, Singular Matrix</p>"},{"location":"glossary/#determinant-properties","title":"Determinant Properties","text":"<p>Rules governing how determinants behave under matrix operations, including row operations and multiplication.</p> <p>Example: The determinant of a product equals the product of determinants: det(AB) = det(A)\u00b7det(B).</p> <p>See also: Determinant, Multiplicative Property</p>"},{"location":"glossary/#diagonal-form","title":"Diagonal Form","text":"<p>A diagonal matrix that a matrix can be transformed into through diagonalization, with eigenvalues on the diagonal.</p> <p>Example: If A = PDP\u207b\u00b9 where D is diagonal, then D is the diagonal form of A.</p> <p>See also: Diagonalization, Eigenvalue</p>"},{"location":"glossary/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A square matrix where all non-diagonal elements are zero.</p> <p>Example: The matrix [[3, 0], [0, 5]] is diagonal with entries 3 and 5.</p> <p>See also: Identity Matrix, Triangular Matrix</p>"},{"location":"glossary/#diagonalization","title":"Diagonalization","text":"<p>The process of expressing a matrix as A = PDP\u207b\u00b9, where D is diagonal and P contains eigenvectors.</p> <p>Example: Diagonalizing a symmetric matrix simplifies computing matrix powers: A^n = PD^nP\u207b\u00b9.</p> <p>See also: Eigenvalue, Eigenvector, Diagonal Form</p>"},{"location":"glossary/#dimension-of-space","title":"Dimension of Space","text":"<p>The number of vectors in any basis for a vector space, indicating the degrees of freedom.</p> <p>Example: R\u00b3 has dimension 3 because any basis requires exactly three vectors.</p> <p>See also: Vector Space, Basis Vector</p>"},{"location":"glossary/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Techniques that reduce the number of features while preserving important information in the data.</p> <p>Example: Using PCA to reduce 100 features to 10 principal components that capture 95% of variance.</p> <p>See also: PCA, Truncated SVD</p>"},{"location":"glossary/#domain","title":"Domain","text":"<p>The set of all possible input values for a function or transformation.</p> <p>Example: A transformation from R\u00b3 to R\u00b2 has domain R\u00b3.</p> <p>See also: Codomain, Range</p>"},{"location":"glossary/#dominant-eigenvalue","title":"Dominant Eigenvalue","text":"<p>The eigenvalue with the largest absolute value, which determines the long-term behavior of iterative matrix powers.</p> <p>Example: In power iteration, multiplying by a matrix repeatedly converges to the direction of the dominant eigenvector.</p> <p>See also: Eigenvalue, Power Iteration</p>"},{"location":"glossary/#dot-product","title":"Dot Product","text":"<p>A binary operation on two vectors that produces a scalar, computed as the sum of componentwise products.</p> <p>Example: The dot product of (1, 2, 3) and (4, 5, 6) is 1\u00d74 + 2\u00d75 + 3\u00d76 = 32.</p> <p>See also: Inner Product, Cosine Similarity</p>"},{"location":"glossary/#edge-detection","title":"Edge Detection","text":"<p>Image processing techniques that identify boundaries between regions with different intensities or colors.</p> <p>Example: The Sobel operator detects edges by computing intensity gradients in horizontal and vertical directions.</p> <p>See also: Sobel Operator, Image Convolution</p>"},{"location":"glossary/#eigen-equation","title":"Eigen Equation","text":"<p>The fundamental equation Av = \u03bbv that defines eigenvalues \u03bb and eigenvectors v of a matrix A.</p> <p>Example: If Av = 3v, then v is an eigenvector with eigenvalue 3.</p> <p>See also: Eigenvalue, Eigenvector</p>"},{"location":"glossary/#eigendecomposition","title":"Eigendecomposition","text":"<p>The representation of a matrix as a product of its eigenvector matrix, diagonal eigenvalue matrix, and inverse eigenvector matrix.</p> <p>Example: For a symmetric matrix, eigendecomposition is A = Q\u039bQ^T where Q is orthogonal.</p> <p>See also: Diagonalization, Eigenvalue, Eigenvector</p>"},{"location":"glossary/#eigenspace","title":"Eigenspace","text":"<p>The set of all eigenvectors corresponding to a particular eigenvalue, together with the zero vector.</p> <p>Example: For a 3\u00d73 matrix with eigenvalue 2 having geometric multiplicity 2, the eigenspace is a plane.</p> <p>See also: Eigenvector, Geometric Multiplicity</p>"},{"location":"glossary/#eigenvalue","title":"Eigenvalue","text":"<p>A scalar \u03bb such that Av = \u03bbv for some nonzero vector v, indicating a direction scaled by the transformation.</p> <p>Example: A 2D rotation has complex eigenvalues, indicating no real directions are preserved.</p> <p>See also: Eigenvector, Characteristic Polynomial</p>"},{"location":"glossary/#eigenvector","title":"Eigenvector","text":"<p>A nonzero vector v such that Av = \u03bbv for some scalar \u03bb, indicating a direction unchanged by the transformation except for scaling.</p> <p>Example: For a horizontal stretch matrix, any vector along the x-axis is an eigenvector.</p> <p>See also: Eigenvalue, Eigenspace</p>"},{"location":"glossary/#embedding","title":"Embedding","text":"<p>A learned vector representation that maps discrete objects (like words or items) to continuous vector space.</p> <p>Embeddings capture semantic relationships where similar items have similar vector representations.</p> <p>Example: Word embeddings place \"king\" and \"queen\" close together in vector space.</p> <p>See also: Embedding Space, Word Embedding</p>"},{"location":"glossary/#embedding-space","title":"Embedding Space","text":"<p>A continuous vector space where embedded objects reside, with geometric relationships encoding semantic meaning.</p> <p>Example: In an embedding space, arithmetic like \"king - man + woman \u2248 queen\" can hold.</p> <p>See also: Embedding, Latent Space</p>"},{"location":"glossary/#epipolar-geometry","title":"Epipolar Geometry","text":"<p>The geometric relationship between two camera views of the same 3D scene, constraining point correspondences.</p> <p>Example: A point in one image must lie on an epipolar line in the other image.</p> <p>See also: Stereo Vision, Triangulation</p>"},{"location":"glossary/#euclidean-distance","title":"Euclidean Distance","text":"<p>The straight-line distance between two points, computed as the L2 norm of their difference.</p> <p>Example: The Euclidean distance between (0, 0) and (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L2 Norm, Vector Magnitude</p>"},{"location":"glossary/#euler-angles","title":"Euler Angles","text":"<p>Three angles (typically roll, pitch, yaw) that describe the orientation of a rigid body in 3D space.</p> <p>Example: An aircraft's orientation is often described as 10\u00b0 roll, 5\u00b0 pitch, and 30\u00b0 yaw.</p> <p>See also: Gimbal Lock, Quaternion, 3D Rotation</p>"},{"location":"glossary/#extended-kalman-filter","title":"Extended Kalman Filter","text":"<p>A nonlinear extension of the Kalman filter that linearizes the system model around the current estimate.</p> <p>Example: EKF is used when sensor measurements are nonlinear functions of the state, like bearing angles.</p> <p>See also: Kalman Filter, State Estimation</p>"},{"location":"glossary/#extrinsic-parameters","title":"Extrinsic Parameters","text":"<p>Camera parameters describing the position and orientation of the camera in world coordinates.</p> <p>Example: Extrinsic parameters include a rotation matrix and translation vector from world to camera frame.</p> <p>See also: Intrinsic Parameters, Camera Matrix</p>"},{"location":"glossary/#feature-detection","title":"Feature Detection","text":"<p>Algorithms that identify distinctive points or regions in images that can be tracked or matched.</p> <p>Example: SIFT and ORB algorithms detect corner-like features that are robust to rotation and scale.</p> <p>See also: Edge Detection, Homography</p>"},{"location":"glossary/#feature-matrix","title":"Feature Matrix","text":"<p>A matrix organizing feature values where rows represent samples and columns represent individual features.</p> <p>Example: A feature matrix for housing data might have columns for square footage, bedrooms, and price.</p> <p>See also: Feature Vector, Data Matrix</p>"},{"location":"glossary/#feature-vector","title":"Feature Vector","text":"<p>A vector containing the numerical feature values that describe a single data sample.</p> <p>Example: A feature vector for an image might contain pixel intensities or extracted features like edges.</p> <p>See also: Feature Matrix, Embedding</p>"},{"location":"glossary/#forward-propagation","title":"Forward Propagation","text":"<p>The process of computing outputs by passing inputs through each layer of a neural network sequentially.</p> <p>Example: In forward propagation, input data flows through hidden layers to produce a prediction.</p> <p>See also: Backpropagation, Neural Network Layer</p>"},{"location":"glossary/#four-subspaces","title":"Four Subspaces","text":"<p>The column space, row space, null space, and left null space of a matrix, with fundamental relationships.</p> <p>Example: The column space and left null space are orthogonal complements in the output space.</p> <p>See also: Column Space, Row Space, Null Space, Left Null Space</p>"},{"location":"glossary/#fourier-transform","title":"Fourier Transform","text":"<p>A transformation that decomposes a signal into its constituent frequencies using sinusoidal basis functions.</p> <p>Example: The Fourier transform of an image reveals its frequency content for filtering or compression.</p> <p>See also: Frequency Domain, Image Compression</p>"},{"location":"glossary/#free-variable","title":"Free Variable","text":"<p>A variable in a system of linear equations that can take any value, corresponding to a non-pivot column.</p> <p>Example: In a system with infinitely many solutions, free variables parameterize the solution set.</p> <p>See also: Basic Variable, Infinite Solutions</p>"},{"location":"glossary/#frequency-domain","title":"Frequency Domain","text":"<p>A representation of data in terms of frequency components rather than spatial or temporal values.</p> <p>Example: High-frequency components in an image correspond to edges and fine details.</p> <p>See also: Fourier Transform, Image Compression</p>"},{"location":"glossary/#full-svd","title":"Full SVD","text":"<p>The complete singular value decomposition with U, \u03a3, and V^T matrices of full dimensions.</p> <p>Example: For an m\u00d7n matrix, full SVD has U as m\u00d7m, \u03a3 as m\u00d7n, and V^T as n\u00d7n.</p> <p>See also: SVD, Compact SVD, Truncated SVD</p>"},{"location":"glossary/#function","title":"Function","text":"<p>A rule that assigns exactly one output value to each input value from the domain.</p> <p>Example: f(x) = x\u00b2 is a function mapping each real number to its square.</p> <p>See also: Domain, Codomain, Linear Transformation</p>"},{"location":"glossary/#gaussian-elimination","title":"Gaussian Elimination","text":"<p>An algorithm for transforming a matrix to row echelon form using elementary row operations.</p> <p>Example: Gaussian elimination converts an augmented matrix to a form where back substitution can solve the system.</p> <p>See also: Row Operations, Row Echelon Form</p>"},{"location":"glossary/#geometric-multiplicity","title":"Geometric Multiplicity","text":"<p>The dimension of the eigenspace corresponding to an eigenvalue, equal to the number of linearly independent eigenvectors.</p> <p>Example: If an eigenvalue has geometric multiplicity 2, there are two independent eigenvector directions.</p> <p>See also: Algebraic Multiplicity, Eigenspace</p>"},{"location":"glossary/#gimbal-lock","title":"Gimbal Lock","text":"<p>A phenomenon where two rotation axes become aligned, causing loss of one degree of rotational freedom.</p> <p>Example: When pitch reaches 90\u00b0, roll and yaw rotations become equivalent, losing independent control.</p> <p>See also: Euler Angles, Quaternion</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An iterative optimization algorithm that moves parameters in the direction opposite to the gradient to minimize a function.</p> <p>Example: Training neural networks uses gradient descent to reduce the loss function over many iterations.</p> <p>See also: Gradient Vector, Learning Rate, SGD</p>"},{"location":"glossary/#gradient-vector","title":"Gradient Vector","text":"<p>A vector of partial derivatives indicating the direction and rate of steepest increase of a function.</p> <p>Example: The gradient of f(x, y) = x\u00b2 + y\u00b2 at point (1, 2) is (2, 4).</p> <p>See also: Gradient Descent, Hessian Matrix</p>"},{"location":"glossary/#gram-schmidt-process","title":"Gram-Schmidt Process","text":"<p>An algorithm for orthonormalizing a set of vectors by sequentially removing projections onto previously processed vectors.</p> <p>Example: Starting with vectors (1, 1) and (1, 2), Gram-Schmidt produces orthonormal vectors.</p> <p>See also: Orthonormal Basis, QR Decomposition</p>"},{"location":"glossary/#gram-schmidt-qr","title":"Gram-Schmidt QR","text":"<p>QR decomposition computed using the Gram-Schmidt orthogonalization process.</p> <p>Example: The Q matrix from Gram-Schmidt contains orthonormal columns spanning the column space of A.</p> <p>See also: Gram-Schmidt Process, QR Decomposition, Householder QR</p>"},{"location":"glossary/#grayscale-image","title":"Grayscale Image","text":"<p>A digital image represented by a single channel matrix of intensity values, typically ranging from 0 (black) to 255 (white).</p> <p>Example: A 512\u00d7512 grayscale image is stored as a single 512\u00d7512 matrix of pixel values.</p> <p>See also: Image Matrix, RGB Image</p>"},{"location":"glossary/#hessian-matrix","title":"Hessian Matrix","text":"<p>A square matrix of second partial derivatives describing the local curvature of a multivariable function.</p> <p>Example: For f(x, y), the Hessian is [[\u2202\u00b2f/\u2202x\u00b2, \u2202\u00b2f/\u2202x\u2202y], [\u2202\u00b2f/\u2202y\u2202x, \u2202\u00b2f/\u2202y\u00b2]].</p> <p>See also: Gradient Vector, Newton's Method</p>"},{"location":"glossary/#hidden-layer","title":"Hidden Layer","text":"<p>A neural network layer between the input and output layers that learns intermediate representations.</p> <p>Example: A network with two hidden layers of 64 neurons each can learn hierarchical features.</p> <p>See also: Neural Network Layer, Deep Network</p>"},{"location":"glossary/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>An extended coordinate system using n+1 coordinates to represent points in n-dimensional space, enabling uniform treatment of transformations.</p> <p>Example: The 2D point (3, 4) becomes (3, 4, 1) in homogeneous coordinates.</p> <p>See also: Projection Matrix, SE3 Transform</p>"},{"location":"glossary/#homogeneous-system","title":"Homogeneous System","text":"<p>A system of linear equations where all constant terms are zero, always having at least the trivial solution.</p> <p>Example: The system 2x + 3y = 0 and x - y = 0 is homogeneous with solution x = y = 0.</p> <p>See also: Trivial Solution, Null Space</p>"},{"location":"glossary/#homography","title":"Homography","text":"<p>A projective transformation that maps points between two planes, represented by a 3\u00d73 matrix.</p> <p>Example: A homography can correct perspective distortion when photographing a document at an angle.</p> <p>See also: Perspective Projection, Feature Detection</p>"},{"location":"glossary/#householder-qr","title":"Householder QR","text":"<p>QR decomposition using Householder reflections, which is numerically more stable than Gram-Schmidt.</p> <p>Example: Householder QR is preferred for solving least squares problems due to better numerical properties.</p> <p>See also: QR Decomposition, Gram-Schmidt QR</p>"},{"location":"glossary/#identity-matrix","title":"Identity Matrix","text":"<p>A square diagonal matrix with ones on the main diagonal and zeros elsewhere, acting as the multiplicative identity.</p> <p>Example: Multiplying any matrix by the identity matrix of compatible size returns the original matrix.</p> <p>See also: Diagonal Matrix, Matrix Inverse</p>"},{"location":"glossary/#image","title":"Image","text":"<p>The set of all output values that a function or transformation actually produces from its domain.</p> <p>Example: For f(x) = x\u00b2, the image is all non-negative real numbers.</p> <p>See also: Range, Codomain</p>"},{"location":"glossary/#image-compression","title":"Image Compression","text":"<p>Techniques that reduce the storage size of images while preserving visual quality, often using matrix decompositions.</p> <p>Example: SVD-based compression retains only the largest singular values, approximating the image with fewer parameters.</p> <p>See also: Truncated SVD, Low-Rank Approximation</p>"},{"location":"glossary/#image-convolution","title":"Image Convolution","text":"<p>An operation that applies a kernel to each position in an image, computing weighted sums of neighboring pixels.</p> <p>Example: Convolving with a 3\u00d73 averaging kernel blurs the image by smoothing local variations.</p> <p>See also: Convolution Kernel, Image Filter</p>"},{"location":"glossary/#image-filter","title":"Image Filter","text":"<p>A convolution kernel designed for specific image processing tasks like blurring, sharpening, or edge detection.</p> <p>Example: A Gaussian filter smooths images while preserving edges better than simple averaging.</p> <p>See also: Image Convolution, Blur Filter, Sharpen Filter</p>"},{"location":"glossary/#image-matrix","title":"Image Matrix","text":"<p>A numerical representation of a digital image as a matrix (grayscale) or tensor (color) of pixel values.</p> <p>Example: An 800\u00d7600 RGB image is stored as a 800\u00d7600\u00d73 tensor of values.</p> <p>See also: Grayscale Image, RGB Image, Image Tensor</p>"},{"location":"glossary/#image-tensor","title":"Image Tensor","text":"<p>A multi-dimensional array representing image data, typically with dimensions for height, width, and color channels.</p> <p>Example: A batch of 32 RGB images of size 224\u00d7224 forms a 32\u00d7224\u00d7224\u00d73 tensor.</p> <p>See also: Image Matrix, Tensor</p>"},{"location":"glossary/#infinite-solutions","title":"Infinite Solutions","text":"<p>A condition where a system of linear equations has infinitely many solutions, forming a line, plane, or higher-dimensional set.</p> <p>Example: Parallel planes that coincide have infinitely many intersection points.</p> <p>See also: Solution Set, Free Variable</p>"},{"location":"glossary/#inner-product","title":"Inner Product","text":"<p>A generalization of the dot product that defines angles and lengths in abstract vector spaces.</p> <p>Example: The standard inner product in R^n is the dot product \u27e8u, v\u27e9 = \u03a3u_i v_i.</p> <p>See also: Dot Product, Inner Product Space</p>"},{"location":"glossary/#inner-product-space","title":"Inner Product Space","text":"<p>A vector space equipped with an inner product operation satisfying linearity, symmetry, and positive-definiteness.</p> <p>Example: R^n with the dot product is an inner product space.</p> <p>See also: Inner Product, Vector Space</p>"},{"location":"glossary/#interpolation","title":"Interpolation","text":"<p>Creating intermediate values or states between known data points using mathematical techniques.</p> <p>Example: Linear interpolation between embeddings generates semantically meaningful intermediate representations.</p> <p>See also: Latent Space, Embedding</p>"},{"location":"glossary/#intrinsic-parameters","title":"Intrinsic Parameters","text":"<p>Camera parameters describing internal properties like focal length, principal point, and lens distortion.</p> <p>Example: A camera with focal length 50mm and sensor size 36mm has specific intrinsic parameters.</p> <p>See also: Extrinsic Parameters, Camera Matrix</p>"},{"location":"glossary/#invertible-matrix","title":"Invertible Matrix","text":"<p>A square matrix that has an inverse, equivalent to having a nonzero determinant and full rank.</p> <p>Example: The matrix [[1, 2], [3, 4]] is invertible because its determinant 1\u00d74 - 2\u00d73 = -2 \u2260 0.</p> <p>See also: Matrix Inverse, Singular Matrix, Determinant</p>"},{"location":"glossary/#invertible-transform","title":"Invertible Transform","text":"<p>A linear transformation that can be reversed, mapping each output uniquely back to its input.</p> <p>Example: Rotation is invertible\u2014rotating by angle \u03b8 can be reversed by rotating by -\u03b8.</p> <p>See also: Linear Transformation, Matrix Inverse</p>"},{"location":"glossary/#kalman-filter","title":"Kalman Filter","text":"<p>An optimal recursive algorithm for estimating the state of a linear dynamic system from noisy measurements.</p> <p>The Kalman filter combines predictions from a system model with sensor measurements, weighting by their uncertainties.</p> <p>Example: GPS receivers use Kalman filters to estimate position by fusing measurements over time.</p> <p>See also: State Estimation, Kalman Gain, Extended Kalman Filter</p>"},{"location":"glossary/#kalman-gain","title":"Kalman Gain","text":"<p>A matrix that determines how much the state estimate should be updated based on the measurement residual.</p> <p>Example: High Kalman gain means trusting measurements more; low gain means trusting predictions more.</p> <p>See also: Kalman Filter, Update Step</p>"},{"location":"glossary/#kernel","title":"Kernel","text":"<p>The set of all vectors mapped to zero by a linear transformation, also called the null space.</p> <p>Example: For a projection matrix onto a line, the kernel is the orthogonal complement of that line.</p> <p>See also: Null Space, Rank-Nullity Theorem</p>"},{"location":"glossary/#key-matrix","title":"Key Matrix","text":"<p>A learned matrix that transforms input embeddings into key vectors used for computing attention scores.</p> <p>Example: In transformers, the key matrix creates representations that queries match against.</p> <p>See also: Query Matrix, Value Matrix, Attention Mechanism</p>"},{"location":"glossary/#kkt-conditions","title":"KKT Conditions","text":"<p>Necessary conditions for optimality in constrained optimization problems with both equality and inequality constraints.</p> <p>KKT conditions generalize Lagrange multiplier conditions to handle inequality constraints.</p> <p>Example: At an optimal point, KKT conditions require zero gradient and complementary slackness for active constraints.</p> <p>See also: Lagrange Multiplier, Constrained Optimization</p>"},{"location":"glossary/#l1-norm","title":"L1 Norm","text":"<p>The sum of absolute values of vector components, also called the Manhattan distance or taxicab norm.</p> <p>Example: The L1 norm of vector (3, -4, 2) is |3| + |-4| + |2| = 9.</p> <p>See also: L2 Norm, L-Infinity Norm</p>"},{"location":"glossary/#l2-norm","title":"L2 Norm","text":"<p>The square root of the sum of squared components, equal to the Euclidean length of a vector.</p> <p>Example: The L2 norm of vector (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L1 Norm, Vector Magnitude, Euclidean Distance</p>"},{"location":"glossary/#l-infinity-norm","title":"L-Infinity Norm","text":"<p>The maximum absolute value among all vector components.</p> <p>Example: The L-infinity norm of vector (3, -7, 2) is max(|3|, |-7|, |2|) = 7.</p> <p>See also: L1 Norm, L2 Norm</p>"},{"location":"glossary/#lagrange-multiplier","title":"Lagrange Multiplier","text":"<p>A scalar variable introduced to convert a constrained optimization problem into an unconstrained one.</p> <p>Example: To minimize f(x) subject to g(x) = 0, solve \u2207f = \u03bb\u2207g where \u03bb is the Lagrange multiplier.</p> <p>See also: Constrained Optimization, KKT Conditions</p>"},{"location":"glossary/#lasso-regression","title":"Lasso Regression","text":"<p>Linear regression with L1 regularization, promoting sparse solutions by shrinking some coefficients to exactly zero.</p> <p>Example: Lasso with 100 features might zero out 80 of them, performing automatic feature selection.</p> <p>See also: Ridge Regression, Regularization</p>"},{"location":"glossary/#latent-space","title":"Latent Space","text":"<p>A lower-dimensional space containing learned representations that capture meaningful variations in data.</p> <p>Example: A variational autoencoder learns a latent space where each dimension controls a meaningful image attribute.</p> <p>See also: Embedding Space, Dimensionality Reduction</p>"},{"location":"glossary/#layer-normalization","title":"Layer Normalization","text":"<p>A technique that normalizes activations across features for each individual sample, independent of batch size.</p> <p>Example: Layer normalization is preferred over batch normalization for transformers and recurrent networks.</p> <p>See also: Batch Normalization, Neural Network Layer</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling the step size in gradient-based optimization.</p> <p>Example: A learning rate of 0.001 means each gradient update moves parameters by 0.001 times the gradient.</p> <p>See also: Gradient Descent, SGD</p>"},{"location":"glossary/#least-squares-problem","title":"Least Squares Problem","text":"<p>An optimization problem seeking to minimize the sum of squared differences between observed and predicted values.</p> <p>Example: Fitting a line to scattered points by minimizing the sum of squared vertical distances.</p> <p>See also: Normal Equations, Linear Regression</p>"},{"location":"glossary/#left-null-space","title":"Left Null Space","text":"<p>The null space of a matrix transpose, containing all vectors orthogonal to the row space.</p> <p>Example: Vectors in the left null space of A satisfy A^T x = 0.</p> <p>See also: Null Space, Four Subspaces</p>"},{"location":"glossary/#left-singular-vector","title":"Left Singular Vector","text":"<p>A column of the U matrix in singular value decomposition, representing a direction in the output space.</p> <p>Example: Left singular vectors form an orthonormal basis for the column space of the matrix.</p> <p>See also: SVD, Right Singular Vector</p>"},{"location":"glossary/#lidar-point-cloud","title":"LIDAR Point Cloud","text":"<p>A set of 3D points measured by a laser scanner, representing the geometry of the surrounding environment.</p> <p>Example: Autonomous vehicles use LIDAR point clouds to detect obstacles and map road surfaces.</p> <p>See also: Point Cloud, Sensor Fusion</p>"},{"location":"glossary/#linear-combination","title":"Linear Combination","text":"<p>A sum of vectors multiplied by scalar coefficients.</p> <p>Example: 2\u00b7(1, 0) + 3\u00b7(0, 1) = (2, 3) is a linear combination of the standard basis vectors.</p> <p>See also: Span, Linear Independence</p>"},{"location":"glossary/#linear-dependence","title":"Linear Dependence","text":"<p>A property of a set of vectors where at least one vector can be written as a linear combination of the others.</p> <p>Example: Vectors (1, 2), (2, 4), and (3, 6) are linearly dependent because (3, 6) = 3\u00b7(1, 2).</p> <p>See also: Linear Independence, Span</p>"},{"location":"glossary/#linear-equation","title":"Linear Equation","text":"<p>An equation where variables appear only to the first power with constant coefficients.</p> <p>Example: 3x + 2y - z = 7 is a linear equation in three variables.</p> <p>See also: System of Equations, Matrix Equation Form</p>"},{"location":"glossary/#linear-independence","title":"Linear Independence","text":"<p>A property of a set of vectors where no vector can be written as a linear combination of the others.</p> <p>Example: Vectors (1, 0) and (0, 1) are linearly independent in R\u00b2.</p> <p>See also: Linear Dependence, Basis Vector</p>"},{"location":"glossary/#linear-regression","title":"Linear Regression","text":"<p>A method for modeling the relationship between variables by fitting a linear equation to observed data.</p> <p>Example: Predicting house prices from square footage using a line: price = m \u00d7 sqft + b.</p> <p>See also: Least Squares Problem, Design Matrix</p>"},{"location":"glossary/#linear-transformation","title":"Linear Transformation","text":"<p>A function between vector spaces that preserves vector addition and scalar multiplication.</p> <p>Example: A rotation is a linear transformation because rotating a sum equals the sum of rotations.</p> <p>See also: Transformation Matrix, Matrix</p>"},{"location":"glossary/#localization","title":"Localization","text":"<p>The process of determining an agent's position and orientation within a known map or environment.</p> <p>Example: A robot uses sensor data to estimate it is at coordinates (10, 5) facing north.</p> <p>See also: SLAM, Mapping, State Estimation</p>"},{"location":"glossary/#lora","title":"LoRA","text":"<p>Low-Rank Adaptation, a technique for efficiently fine-tuning large language models by learning low-rank updates to weight matrices.</p> <p>Example: LoRA reduces fine-tuning parameters from billions to millions by decomposing updates as BA where B and A are small matrices.</p> <p>See also: Weight Matrix, Low-Rank Approximation</p>"},{"location":"glossary/#loss-function","title":"Loss Function","text":"<p>A function measuring the discrepancy between model predictions and true values, guiding optimization.</p> <p>Example: Mean squared error loss penalizes large prediction errors quadratically.</p> <p>See also: Cross-Entropy Loss, Gradient Descent</p>"},{"location":"glossary/#low-rank-approximation","title":"Low-Rank Approximation","text":"<p>An approximation of a matrix using a product of smaller matrices, capturing the most significant patterns.</p> <p>Example: Truncating SVD to k singular values gives the best rank-k approximation in Frobenius norm.</p> <p>See also: Truncated SVD, Matrix Rank</p>"},{"location":"glossary/#lower-triangular","title":"Lower Triangular","text":"<p>A matrix where all entries above the main diagonal are zero.</p> <p>Example: The matrix [[2, 0, 0], [3, 1, 0], [4, 5, 6]] is lower triangular.</p> <p>See also: Upper Triangular, Triangular Matrix</p>"},{"location":"glossary/#lu-decomposition","title":"LU Decomposition","text":"<p>Factorization of a matrix into the product of a lower triangular and an upper triangular matrix.</p> <p>Example: LU decomposition enables efficient solving of multiple systems with the same coefficient matrix.</p> <p>See also: Cholesky Decomposition, Gaussian Elimination</p>"},{"location":"glossary/#mapping","title":"Mapping","text":"<p>The process of building a representation of the environment from sensor observations.</p> <p>Example: A robot creates a 2D occupancy grid showing walls and open spaces as it explores.</p> <p>See also: SLAM, Localization</p>"},{"location":"glossary/#matrix","title":"Matrix","text":"<p>A rectangular array of numbers arranged in rows and columns, representing linear transformations or data.</p> <p>Example: A 2\u00d73 matrix has 2 rows and 3 columns, containing 6 entries.</p> <p>See also: Matrix Dimensions, Vector</p>"},{"location":"glossary/#matrix-addition","title":"Matrix Addition","text":"<p>Element-wise addition of two matrices with the same dimensions.</p> <p>Example: [[1, 2], [3, 4]] + [[5, 6], [7, 8]] = [[6, 8], [10, 12]].</p> <p>See also: Matrix, Matrix Scalar Multiply</p>"},{"location":"glossary/#matrix-dimensions","title":"Matrix Dimensions","text":"<p>The number of rows and columns in a matrix, written as m\u00d7n where m is rows and n is columns.</p> <p>Example: A 4\u00d73 matrix has 4 rows and 3 columns.</p> <p>See also: Matrix, Row Vector, Column Vector</p>"},{"location":"glossary/#matrix-entry","title":"Matrix Entry","text":"<p>A single element of a matrix, identified by its row and column position.</p> <p>Example: In matrix A, the entry a\u2082\u2083 is in row 2 and column 3.</p> <p>See also: Matrix, Matrix Notation</p>"},{"location":"glossary/#matrix-equation-form","title":"Matrix Equation Form","text":"<p>Expression of a system of linear equations as a single matrix equation Ax = b.</p> <p>Example: The system 2x + y = 5 and x - y = 1 becomes [[2, 1], [1, -1]]\u00b7[x, y]^T = [5, 1]^T.</p> <p>See also: System of Equations, Augmented Matrix</p>"},{"location":"glossary/#matrix-factorization","title":"Matrix Factorization","text":"<p>Decomposition of a matrix into a product of simpler matrices revealing structure or enabling computation.</p> <p>Example: Factoring a rating matrix into user and item matrices enables recommendation systems.</p> <p>See also: LU Decomposition, SVD</p>"},{"location":"glossary/#matrix-inverse","title":"Matrix Inverse","text":"<p>A matrix A\u207b\u00b9 such that AA\u207b\u00b9 = A\u207b\u00b9A = I, existing only for square invertible matrices.</p> <p>Example: For [[a, b], [c, d]], the inverse is (1/(ad-bc))\u00b7[[d, -b], [-c, a]].</p> <p>See also: Invertible Matrix, Identity Matrix</p>"},{"location":"glossary/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>An operation combining two matrices by taking dot products of rows of the first with columns of the second.</p> <p>Example: A (2\u00d73) matrix times a (3\u00d74) matrix yields a (2\u00d74) matrix.</p> <p>See also: Matrix-Vector Product, Composition of Transforms</p>"},{"location":"glossary/#matrix-notation","title":"Matrix Notation","text":"<p>Conventions for writing matrices, including bracket notation and subscript indexing.</p> <p>Example: A matrix A with element a_{ij} in row i and column j is written as A = [a_{ij}].</p> <p>See also: Matrix, Matrix Entry</p>"},{"location":"glossary/#matrix-rank","title":"Matrix Rank","text":"<p>The maximum number of linearly independent rows or columns in a matrix.</p> <p>Example: A 5\u00d73 matrix has rank at most 3, the smaller of its dimensions.</p> <p>See also: Rank, Column Space, Row Space</p>"},{"location":"glossary/#matrix-scalar-multiply","title":"Matrix Scalar Multiply","text":"<p>Multiplication of every entry of a matrix by a scalar value.</p> <p>Example: 3\u00b7[[1, 2], [3, 4]] = [[3, 6], [9, 12]].</p> <p>See also: Scalar Multiplication, Matrix</p>"},{"location":"glossary/#matrix-transpose","title":"Matrix Transpose","text":"<p>The matrix obtained by interchanging rows and columns of the original matrix.</p> <p>Example: The transpose of [[1, 2, 3], [4, 5, 6]] is [[1, 4], [2, 5], [3, 6]].</p> <p>See also: Symmetric Matrix, Transpose Determinant</p>"},{"location":"glossary/#matrix-vector-product","title":"Matrix-Vector Product","text":"<p>Multiplication of a matrix by a vector, producing a new vector.</p> <p>Example: [[1, 2], [3, 4]]\u00b7[5, 6]^T = [17, 39]^T.</p> <p>See also: Matrix Multiplication, Linear Transformation</p>"},{"location":"glossary/#measurement-vector","title":"Measurement Vector","text":"<p>A vector containing sensor observations at a given time step, used for updating state estimates.</p> <p>Example: A measurement vector might contain GPS position and compass heading readings.</p> <p>See also: State Vector, Kalman Filter</p>"},{"location":"glossary/#mini-batch-sgd","title":"Mini-Batch SGD","text":"<p>Stochastic gradient descent using small batches of training examples to estimate the gradient.</p> <p>Example: With batch size 32, gradients are computed from 32 samples before each parameter update.</p> <p>See also: SGD, Batch Gradient Descent</p>"},{"location":"glossary/#minor","title":"Minor","text":"<p>The determinant of a submatrix formed by deleting one row and one column from a matrix.</p> <p>Example: For a 3\u00d73 matrix, M\u2081\u2082 is the determinant of the 2\u00d72 submatrix with row 1 and column 2 removed.</p> <p>See also: Cofactor, Cofactor Expansion</p>"},{"location":"glossary/#momentum","title":"Momentum","text":"<p>An optimization technique that accelerates gradient descent by accumulating a velocity vector of past gradients.</p> <p>Example: Momentum helps overcome local minima and oscillations by adding inertia to parameter updates.</p> <p>See also: SGD, Adam Optimizer</p>"},{"location":"glossary/#motion-planning","title":"Motion Planning","text":"<p>Computing a sequence of configurations or trajectories to move from start to goal while avoiding obstacles.</p> <p>Example: A robotic arm plans a collision-free path from its current position to grasp an object.</p> <p>See also: Path Planning, Trajectory Optimization</p>"},{"location":"glossary/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Parallel attention mechanisms that attend to different representation subspaces, capturing diverse relationships.</p> <p>Example: With 8 attention heads, a transformer can simultaneously focus on syntax, semantics, and other aspects.</p> <p>See also: Attention Mechanism, Self-Attention</p>"},{"location":"glossary/#multiplicative-property","title":"Multiplicative Property","text":"<p>The property that the determinant of a product equals the product of determinants: det(AB) = det(A)\u00b7det(B).</p> <p>Example: If det(A) = 2 and det(B) = 3, then det(AB) = 6.</p> <p>See also: Determinant, Determinant Properties</p>"},{"location":"glossary/#n-dimensional-vector","title":"N-Dimensional Vector","text":"<p>An ordered list of n real numbers representing a point or direction in n-dimensional space.</p> <p>Example: A 100-dimensional feature vector in machine learning might represent an image or document.</p> <p>See also: 2D Vector, 3D Vector, Vector</p>"},{"location":"glossary/#neural-network-layer","title":"Neural Network Layer","text":"<p>A computational unit in a neural network that transforms input through weights, biases, and activation functions.</p> <p>Example: A dense layer with 128 neurons applies a 128-dimensional linear transformation followed by activation.</p> <p>See also: Hidden Layer, Activation Function</p>"},{"location":"glossary/#neuron-model","title":"Neuron Model","text":"<p>A mathematical abstraction of a biological neuron, computing a weighted sum of inputs plus bias through an activation.</p> <p>Example: A neuron computes output = \u03c3(w\u2081x\u2081 + w\u2082x\u2082 + ... + b) where \u03c3 is the activation function.</p> <p>See also: Perceptron, Activation Function</p>"},{"location":"glossary/#newtons-method","title":"Newton's Method","text":"<p>An optimization algorithm using second-order derivatives to take optimal steps toward a function's minimum.</p> <p>Example: Newton's method converges quadratically near the optimum but requires computing the Hessian.</p> <p>See also: Hessian Matrix, Quasi-Newton Method, BFGS Algorithm</p>"},{"location":"glossary/#no-solution","title":"No Solution","text":"<p>A condition where a system of linear equations has no values satisfying all equations simultaneously.</p> <p>Example: The system x + y = 1 and x + y = 2 has no solution because the planes are parallel.</p> <p>See also: Solution Set, Infinite Solutions</p>"},{"location":"glossary/#non-uniform-scaling","title":"Non-Uniform Scaling","text":"<p>Scaling a transformation that uses different scale factors along different axes.</p> <p>Example: Scaling by 2 in x and 0.5 in y stretches horizontally while compressing vertically.</p> <p>See also: Uniform Scaling, Scaling Matrix</p>"},{"location":"glossary/#normal-equations","title":"Normal Equations","text":"<p>The system A^T A x = A^T b derived from the least squares problem, giving the optimal solution.</p> <p>Example: Solving the normal equations finds the best-fit line coefficients for linear regression.</p> <p>See also: Least Squares Problem, Pseudoinverse</p>"},{"location":"glossary/#norm-from-inner-product","title":"Norm from Inner Product","text":"<p>A vector norm derived from an inner product as ||v|| = \u221a\u27e8v, v\u27e9.</p> <p>Example: The Euclidean norm is induced by the standard dot product: ||v|| = \u221a(v \u00b7 v).</p> <p>See also: Inner Product, L2 Norm</p>"},{"location":"glossary/#null-space","title":"Null Space","text":"<p>The set of all vectors mapped to zero by a matrix, representing the homogeneous solution set.</p> <p>Example: The null space of [[1, 2], [2, 4]] is all multiples of (-2, 1).</p> <p>See also: Kernel, Rank-Nullity Theorem</p>"},{"location":"glossary/#nullity","title":"Nullity","text":"<p>The dimension of the null space, indicating the number of free variables in the homogeneous solution.</p> <p>Example: A 4\u00d75 matrix with rank 3 has nullity 5 - 3 = 2.</p> <p>See also: Null Space, Rank-Nullity Theorem</p>"},{"location":"glossary/#numerical-rank","title":"Numerical Rank","text":"<p>The number of singular values above a tolerance threshold, accounting for floating-point errors.</p> <p>Example: A matrix might have theoretical rank 10 but numerical rank 8 due to tiny singular values from noise.</p> <p>See also: Matrix Rank, Condition Number</p>"},{"location":"glossary/#numerical-stability","title":"Numerical Stability","text":"<p>The property of an algorithm being resistant to accumulation and amplification of rounding errors.</p> <p>Example: Householder QR is more numerically stable than Gram-Schmidt for solving least squares problems.</p> <p>See also: Condition Number, Partial Pivoting</p>"},{"location":"glossary/#object-detection","title":"Object Detection","text":"<p>Computer vision tasks that identify and localize objects of interest within images or video.</p> <p>Example: YOLO detects cars, pedestrians, and traffic signs in autonomous driving applications.</p> <p>See also: Bounding Box, Object Tracking</p>"},{"location":"glossary/#object-tracking","title":"Object Tracking","text":"<p>Following the movement of detected objects across consecutive frames in video sequences.</p> <p>Example: Kalman filters predict object positions between frames to maintain identity through occlusions.</p> <p>See also: Object Detection, Kalman Filter</p>"},{"location":"glossary/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>A square matrix whose columns (and rows) are orthonormal, satisfying Q^T Q = I.</p> <p>Example: A rotation matrix is orthogonal because rotating preserves lengths and angles.</p> <p>See also: Orthonormal Basis, Matrix Inverse</p>"},{"location":"glossary/#orthogonal-projection","title":"Orthogonal Projection","text":"<p>Mapping a vector onto a subspace by dropping the perpendicular component.</p> <p>Example: Projecting (3, 4) onto the x-axis gives (3, 0).</p> <p>See also: Projection, Projection onto Subspace</p>"},{"location":"glossary/#orthogonal-vectors","title":"Orthogonal Vectors","text":"<p>Vectors whose inner product is zero, meaning they are perpendicular.</p> <p>Example: Vectors (1, 1) and (1, -1) are orthogonal because 1\u00d71 + 1\u00d7(-1) = 0.</p> <p>See also: Orthogonality, Dot Product</p>"},{"location":"glossary/#orthogonality","title":"Orthogonality","text":"<p>The property of being perpendicular, characterized by zero inner product between vectors.</p> <p>Orthogonality is fundamental to many algorithms including QR decomposition and principal component analysis.</p> <p>Example: Eigenvectors of a symmetric matrix corresponding to different eigenvalues are orthogonal.</p> <p>See also: Orthogonal Vectors, Orthonormal Basis</p>"},{"location":"glossary/#orthonormal-basis","title":"Orthonormal Basis","text":"<p>A basis consisting of mutually orthogonal unit vectors.</p> <p>Example: The standard basis {(1, 0, 0), (0, 1, 0), (0, 0, 1)} is orthonormal in R\u00b3.</p> <p>See also: Orthonormal Set, Gram-Schmidt Process</p>"},{"location":"glossary/#orthonormal-set","title":"Orthonormal Set","text":"<p>A set of vectors that are mutually orthogonal and all have unit length.</p> <p>Example: The vectors (1, 0) and (0, 1) form an orthonormal set in R\u00b2.</p> <p>See also: Orthonormal Basis, Unit Vector</p>"},{"location":"glossary/#padding","title":"Padding","text":"<p>Adding extra values (typically zeros) around input data to control output dimensions in convolution.</p> <p>Example: Zero-padding a 5\u00d75 image to 7\u00d77 before 3\u00d73 convolution produces a 5\u00d75 output.</p> <p>See also: Convolutional Layer, Stride</p>"},{"location":"glossary/#partial-pivoting","title":"Partial Pivoting","text":"<p>Row swapping during Gaussian elimination to place the largest magnitude element in the pivot position.</p> <p>Example: Partial pivoting improves numerical stability by avoiding division by small numbers.</p> <p>See also: Gaussian Elimination, LU Decomposition</p>"},{"location":"glossary/#path-planning","title":"Path Planning","text":"<p>Determining a route through space from start to goal, typically avoiding obstacles.</p> <p>Example: A* algorithm finds shortest paths on navigation graphs for robotic path planning.</p> <p>See also: Motion Planning, Trajectory Optimization</p>"},{"location":"glossary/#pca","title":"PCA","text":"<p>Principal Component Analysis, a technique that finds orthogonal directions of maximum variance in data.</p> <p>PCA reduces dimensionality while preserving as much variance as possible.</p> <p>Example: PCA on face images extracts eigenfaces, the principal components of facial variation.</p> <p>See also: Principal Component, Variance Explained, Dimensionality Reduction</p>"},{"location":"glossary/#perceptron","title":"Perceptron","text":"<p>The simplest neural network unit, computing a weighted sum of inputs passed through a step function.</p> <p>Example: A perceptron can learn to classify linearly separable patterns like AND and OR functions.</p> <p>See also: Neuron Model, Activation Function</p>"},{"location":"glossary/#perspective-projection","title":"Perspective Projection","text":"<p>Mapping 3D points to 2D image coordinates using perspective geometry where distant objects appear smaller.</p> <p>Example: Parallel lines in 3D appear to converge at a vanishing point in perspective projection.</p> <p>See also: Projection Matrix, Camera Matrix</p>"},{"location":"glossary/#pivot-column","title":"Pivot Column","text":"<p>A column in a matrix that contains a pivot position (leading 1 in row echelon form).</p> <p>Example: In reduced row echelon form, pivot columns correspond to basic variables.</p> <p>See also: Pivot Position, Basic Variable</p>"},{"location":"glossary/#pivot-position","title":"Pivot Position","text":"<p>The location of a leading entry (first nonzero element) in a row of an echelon form matrix.</p> <p>Example: After Gaussian elimination, pivot positions identify which variables are determined.</p> <p>See also: Pivot Column, Row Echelon Form</p>"},{"location":"glossary/#point-cloud","title":"Point Cloud","text":"<p>A set of data points in 3D space, typically from LIDAR or depth sensors, representing surface geometry.</p> <p>Example: A point cloud of a room might contain millions of points representing walls, furniture, and objects.</p> <p>See also: LIDAR Point Cloud, 3D Coordinate System</p>"},{"location":"glossary/#pooling-layer","title":"Pooling Layer","text":"<p>A neural network layer that reduces spatial dimensions by aggregating values within local regions.</p> <p>Example: Max pooling takes the maximum value in each 2\u00d72 region, reducing resolution by half.</p> <p>See also: Convolutional Layer, Stride</p>"},{"location":"glossary/#position-encoding","title":"Position Encoding","text":"<p>Fixed or learned vectors added to embeddings that convey sequence position information.</p> <p>Example: Sinusoidal position encodings allow transformers to understand word order in sentences.</p> <p>See also: Transformer Architecture, Embedding</p>"},{"location":"glossary/#positive-definite-matrix","title":"Positive Definite Matrix","text":"<p>A symmetric matrix where x^T Ax &gt; 0 for all nonzero vectors x, having all positive eigenvalues.</p> <p>Example: Covariance matrices of non-degenerate distributions are positive definite.</p> <p>See also: Cholesky Decomposition, Eigenvalue</p>"},{"location":"glossary/#power-iteration","title":"Power Iteration","text":"<p>An iterative algorithm for computing the dominant eigenvalue and eigenvector of a matrix.</p> <p>Example: Multiplying a random vector by a matrix repeatedly and normalizing converges to the dominant eigenvector.</p> <p>See also: Dominant Eigenvalue, Eigenvalue</p>"},{"location":"glossary/#prediction-step","title":"Prediction Step","text":"<p>The Kalman filter phase that projects the state estimate forward using the system dynamics model.</p> <p>Example: The prediction step estimates where a tracked vehicle will be at the next time step.</p> <p>See also: Update Step, Kalman Filter</p>"},{"location":"glossary/#principal-component","title":"Principal Component","text":"<p>A direction in data space corresponding to maximum variance, computed as an eigenvector of the covariance matrix.</p> <p>Example: The first principal component of height and weight data might point along the \"body size\" direction.</p> <p>See also: PCA, Variance Explained</p>"},{"location":"glossary/#projection","title":"Projection","text":"<p>A linear transformation that maps vectors onto a subspace, reducing dimension.</p> <p>Example: Projecting 3D points onto the xy-plane removes the z-coordinate.</p> <p>See also: Orthogonal Projection, Projection Matrix</p>"},{"location":"glossary/#projection-matrix","title":"Projection Matrix","text":"<p>A matrix P satisfying P\u00b2 = P, used to project vectors onto a subspace.</p> <p>Example: The projection matrix onto a line through unit vector u is P = uu^T.</p> <p>See also: Projection, Orthogonal Projection</p>"},{"location":"glossary/#projection-onto-subspace","title":"Projection onto Subspace","text":"<p>Mapping a vector to the closest point in a subspace, minimizing the distance from the original vector.</p> <p>Example: Projecting a 3D vector onto a plane gives the point in the plane nearest to the vector.</p> <p>See also: Orthogonal Projection, Least Squares Problem</p>"},{"location":"glossary/#pseudoinverse","title":"Pseudoinverse","text":"<p>A generalization of the matrix inverse to non-square or singular matrices, computed from SVD as A\u207a = V\u03a3\u207aU^T.</p> <p>Example: The pseudoinverse solves the least squares problem: x = A\u207ab minimizes ||Ax - b||.</p> <p>See also: SVD, Least Squares Problem</p>"},{"location":"glossary/#quaternion","title":"Quaternion","text":"<p>A four-component hypercomplex number used to represent 3D rotations without gimbal lock.</p> <p>Example: A unit quaternion q = (cos(\u03b8/2), sin(\u03b8/2)\u00b7axis) represents rotation by angle \u03b8 around the given axis.</p> <p>See also: Quaternion Rotation, Euler Angles</p>"},{"location":"glossary/#quaternion-rotation","title":"Quaternion Rotation","text":"<p>Rotation of a 3D vector using quaternion multiplication, avoiding gimbal lock and enabling smooth interpolation.</p> <p>Example: Quaternion rotation is computed as v' = qvq where q is the quaternion conjugate.</p> <p>See also: Quaternion, 3D Rotation</p>"},{"location":"glossary/#query-matrix","title":"Query Matrix","text":"<p>A learned matrix that transforms input embeddings into query vectors for computing attention.</p> <p>Example: In self-attention, each token's query asks \"what information should I gather?\"</p> <p>See also: Key Matrix, Value Matrix, Attention Mechanism</p>"},{"location":"glossary/#quasi-newton-method","title":"Quasi-Newton Method","text":"<p>Optimization methods that approximate second-order information without explicit Hessian computation.</p> <p>Example: BFGS builds a Hessian approximation from gradients observed during optimization.</p> <p>See also: BFGS Algorithm, Newton's Method</p>"},{"location":"glossary/#range","title":"Range","text":"<p>The set of all output vectors that a linear transformation can produce, equivalent to the column space.</p> <p>Example: For a 3\u00d72 matrix with linearly independent columns, the range is a 2D plane in R\u00b3.</p> <p>See also: Column Space, Image</p>"},{"location":"glossary/#rank","title":"Rank","text":"<p>The dimension of the column space (or row space) of a matrix, indicating its linear independence structure.</p> <p>Example: A 5\u00d75 matrix with rank 3 has 3 linearly independent columns.</p> <p>See also: Matrix Rank, Column Space</p>"},{"location":"glossary/#rank-nullity-theorem","title":"Rank-Nullity Theorem","text":"<p>The theorem stating that rank plus nullity equals the number of columns: rank(A) + nullity(A) = n.</p> <p>Example: A 4\u00d76 matrix with rank 4 has nullity 6 - 4 = 2.</p> <p>See also: Rank, Nullity</p>"},{"location":"glossary/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>A matrix form where each pivot is 1, pivots are the only nonzero entries in their columns, and rows are ordered by pivot position.</p> <p>Example: [[1, 0, 2], [0, 1, -1]] is in reduced row echelon form.</p> <p>See also: Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#reflection-matrix","title":"Reflection Matrix","text":"<p>A matrix representing a linear transformation that mirrors vectors across a line, plane, or hyperplane.</p> <p>Example: The matrix [[1, 0], [0, -1]] reflects vectors across the x-axis.</p> <p>See also: Linear Transformation, Transformation Matrix</p>"},{"location":"glossary/#regularization","title":"Regularization","text":"<p>A technique adding penalty terms to loss functions to prevent overfitting by constraining model complexity.</p> <p>Example: L2 regularization adds \u03bb||w||\u00b2 to the loss, shrinking weights toward zero.</p> <p>See also: Ridge Regression, Lasso Regression</p>"},{"location":"glossary/#relu","title":"ReLU","text":"<p>Rectified Linear Unit, an activation function returning max(0, x), popular in deep learning for its simplicity and effectiveness.</p> <p>Example: ReLU(5) = 5 and ReLU(-3) = 0, providing nonlinearity while avoiding vanishing gradients for positive values.</p> <p>See also: Activation Function, Sigmoid, Tanh</p>"},{"location":"glossary/#rgb-image","title":"RGB Image","text":"<p>A color image represented by three channel matrices for red, green, and blue intensity values.</p> <p>Example: Each pixel in an RGB image has three values, like (255, 128, 0) for orange.</p> <p>See also: Grayscale Image, Image Matrix</p>"},{"location":"glossary/#ridge-regression","title":"Ridge Regression","text":"<p>Linear regression with L2 regularization, shrinking coefficients toward zero to prevent overfitting.</p> <p>Example: Ridge regression with \u03bb = 0.1 adds 0.1\u00d7||w||\u00b2 to the least squares loss.</p> <p>See also: Lasso Regression, Regularization</p>"},{"location":"glossary/#right-singular-vector","title":"Right Singular Vector","text":"<p>A column of the V matrix in singular value decomposition, representing a direction in the input space.</p> <p>Example: Right singular vectors form an orthonormal basis for the row space of the matrix.</p> <p>See also: SVD, Left Singular Vector</p>"},{"location":"glossary/#rigid-body-transform","title":"Rigid Body Transform","text":"<p>A transformation preserving distances and angles, consisting of rotation and translation.</p> <p>Example: Moving and rotating a robot arm's end effector is a rigid body transformation.</p> <p>See also: SE3 Transform, Rotation Matrix</p>"},{"location":"glossary/#rmsprop","title":"RMSprop","text":"<p>An adaptive learning rate optimizer that divides the learning rate by a running average of squared gradients.</p> <p>Example: RMSprop adapts to different gradient magnitudes, learning slowly for large gradients and quickly for small ones.</p> <p>See also: Adam Optimizer, SGD</p>"},{"location":"glossary/#rotation-matrix","title":"Rotation Matrix","text":"<p>A square orthogonal matrix with determinant 1 that rotates vectors by a fixed angle around a fixed axis.</p> <p>Example: A 2D rotation by angle \u03b8 is represented by [[cos(\u03b8), -sin(\u03b8)], [sin(\u03b8), cos(\u03b8)]].</p> <p>See also: 2D Rotation, 3D Rotation, Orthogonal Matrix</p>"},{"location":"glossary/#row-addition","title":"Row Addition","text":"<p>An elementary row operation adding a scalar multiple of one row to another row.</p> <p>Example: Subtracting 2 times row 1 from row 2 eliminates the leading entry in row 2.</p> <p>See also: Row Operations, Gaussian Elimination</p>"},{"location":"glossary/#row-echelon-form","title":"Row Echelon Form","text":"<p>A matrix form where each row's leading entry is to the right of the row above, and all-zero rows are at the bottom.</p> <p>Example: [[2, 3, 1], [0, 4, 2], [0, 0, 5]] is in row echelon form.</p> <p>See also: Reduced Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#row-operations","title":"Row Operations","text":"<p>Elementary transformations on matrix rows: swapping rows, scaling rows, or adding multiples of rows.</p> <p>Example: Row operations transform a matrix to row echelon form without changing the solution set.</p> <p>See also: Row Swap, Row Scaling, Row Addition</p>"},{"location":"glossary/#row-scaling","title":"Row Scaling","text":"<p>An elementary row operation multiplying all entries in a row by a nonzero scalar.</p> <p>Example: Multiplying row 2 by 1/4 makes the leading entry 1.</p> <p>See also: Row Operations, Gaussian Elimination</p>"},{"location":"glossary/#row-space","title":"Row Space","text":"<p>The set of all linear combinations of the row vectors of a matrix.</p> <p>Example: For a 3\u00d72 matrix with linearly independent rows, the row space is a 3D subspace of R\u00b3.</p> <p>See also: Column Space, Four Subspaces</p>"},{"location":"glossary/#row-swap","title":"Row Swap","text":"<p>An elementary row operation exchanging two rows of a matrix.</p> <p>Example: Swapping rows 1 and 2 places the row with larger leading entry on top.</p> <p>See also: Row Operations, Partial Pivoting</p>"},{"location":"glossary/#row-vector","title":"Row Vector","text":"<p>A matrix with a single row, representing a vector as a horizontal array of numbers.</p> <p>Example: The row vector [1, 2, 3] has one row and three columns.</p> <p>See also: Column Vector, Vector</p>"},{"location":"glossary/#scalar","title":"Scalar","text":"<p>A single numerical value, as opposed to a vector or matrix.</p> <p>Example: In the expression 3v, the number 3 is a scalar multiplying vector v.</p> <p>See also: Scalar Multiplication, Vector</p>"},{"location":"glossary/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiplication of a vector by a scalar, scaling all components by that factor.</p> <p>Example: 3 \u00d7 (1, 2, 3) = (3, 6, 9).</p> <p>See also: Scalar, Vector</p>"},{"location":"glossary/#scaling-matrix","title":"Scaling Matrix","text":"<p>A diagonal matrix that scales vectors by different factors along different axes.</p> <p>Example: The diagonal matrix [[2, 0], [0, 3]] scales x by 2 and y by 3.</p> <p>See also: Uniform Scaling, Non-Uniform Scaling</p>"},{"location":"glossary/#scree-plot","title":"Scree Plot","text":"<p>A graph of eigenvalues or variance explained in decreasing order, used to determine how many components to retain.</p> <p>Example: A scree plot showing a sharp \"elbow\" at component 5 suggests retaining 5 principal components.</p> <p>See also: PCA, Variance Explained</p>"},{"location":"glossary/#se3-transform","title":"SE3 Transform","text":"<p>The Special Euclidean group in 3D, representing rigid body transformations as rotation plus translation.</p> <p>Example: An SE3 transform combines a 3\u00d73 rotation matrix and a translation vector into a 4\u00d74 homogeneous matrix.</p> <p>See also: Rigid Body Transform, Homogeneous Coordinates</p>"},{"location":"glossary/#self-attention","title":"Self-Attention","text":"<p>An attention mechanism where queries, keys, and values all derive from the same sequence.</p> <p>Example: In self-attention, each word attends to all other words in the same sentence.</p> <p>See also: Attention Mechanism, Cross-Attention</p>"},{"location":"glossary/#semantic-similarity","title":"Semantic Similarity","text":"<p>The degree to which two pieces of text or concepts share meaning, measured by embedding proximity.</p> <p>Example: \"Happy\" and \"joyful\" have high semantic similarity, reflected in similar embedding vectors.</p> <p>See also: Cosine Similarity, Embedding</p>"},{"location":"glossary/#sensor-fusion","title":"Sensor Fusion","text":"<p>Combining data from multiple sensors to achieve more accurate and reliable state estimates.</p> <p>Example: Fusing GPS, IMU, and camera data improves vehicle localization compared to any single sensor.</p> <p>See also: Kalman Filter, LIDAR Point Cloud</p>"},{"location":"glossary/#sgd","title":"SGD","text":"<p>Stochastic Gradient Descent, an optimization algorithm that updates parameters using gradients from random single samples.</p> <p>Example: SGD makes many small updates per epoch rather than one large update, introducing beneficial noise.</p> <p>See also: Batch Gradient Descent, Mini-Batch SGD</p>"},{"location":"glossary/#sharpen-filter","title":"Sharpen Filter","text":"<p>A convolution kernel that enhances edges and details by emphasizing high-frequency components.</p> <p>Example: A sharpening kernel subtracts blurred versions from the original, boosting differences.</p> <p>See also: Image Convolution, Blur Filter</p>"},{"location":"glossary/#shear-matrix","title":"Shear Matrix","text":"<p>A transformation matrix that skews shapes by shifting points parallel to an axis by an amount proportional to their distance from it.</p> <p>Example: A horizontal shear slides the top of a square right while keeping the bottom fixed, creating a parallelogram.</p> <p>See also: Transformation Matrix, Linear Transformation</p>"},{"location":"glossary/#sigmoid","title":"Sigmoid","text":"<p>An S-shaped activation function mapping real values to the range (0, 1), defined as \u03c3(x) = 1/(1 + e^(-x)).</p> <p>Example: Sigmoid outputs 0.5 at x = 0, approaching 1 for large positive x and 0 for large negative x.</p> <p>See also: Activation Function, Tanh</p>"},{"location":"glossary/#signed-area","title":"Signed Area","text":"<p>The determinant of a 2\u00d72 matrix, representing the oriented area of the parallelogram spanned by two vectors.</p> <p>Example: Vectors (2, 0) and (0, 3) span a parallelogram with signed area 2\u00d73 - 0\u00d70 = 6.</p> <p>See also: 2x2 Determinant, Volume Scaling Factor</p>"},{"location":"glossary/#similar-matrices","title":"Similar Matrices","text":"<p>Matrices A and B related by B = P\u207b\u00b9AP for some invertible P, sharing eigenvalues and determinant.</p> <p>Example: A matrix and its diagonalization are similar, connected by the eigenvector matrix.</p> <p>See also: Diagonalization, Eigenvalue</p>"},{"location":"glossary/#singular-matrix","title":"Singular Matrix","text":"<p>A square matrix with determinant zero, having no inverse and reduced rank.</p> <p>Example: [[1, 2], [2, 4]] is singular because row 2 is twice row 1.</p> <p>See also: Determinant, Invertible Matrix</p>"},{"location":"glossary/#singular-value","title":"Singular Value","text":"<p>A non-negative value on the diagonal of \u03a3 in SVD, equal to the square root of an eigenvalue of A^T A.</p> <p>Example: The largest singular value indicates the maximum scaling factor applied by the matrix.</p> <p>See also: SVD, Left Singular Vector, Right Singular Vector</p>"},{"location":"glossary/#slam","title":"SLAM","text":"<p>Simultaneous Localization and Mapping, algorithms that build a map while tracking the agent's position within it.</p> <p>Example: A robot vacuum uses SLAM to create a floor plan while cleaning, avoiding revisiting areas.</p> <p>See also: Localization, Mapping</p>"},{"location":"glossary/#sobel-operator","title":"Sobel Operator","text":"<p>A convolution kernel for edge detection computing horizontal and vertical intensity gradients.</p> <p>Example: The Sobel x-kernel [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] detects vertical edges.</p> <p>See also: Edge Detection, Image Convolution</p>"},{"location":"glossary/#softmax","title":"Softmax","text":"<p>An activation function that converts a vector of values into a probability distribution summing to 1.</p> <p>Example: Softmax([1, 2, 3]) \u2248 [0.09, 0.24, 0.67], emphasizing the largest value.</p> <p>See also: Activation Function, Attention Weights</p>"},{"location":"glossary/#solution-set","title":"Solution Set","text":"<p>The set of all variable assignments satisfying a system of equations.</p> <p>Example: A system might have a solution set that is a single point, a line, a plane, or empty.</p> <p>See also: Unique Solution, Infinite Solutions, No Solution</p>"},{"location":"glossary/#span","title":"Span","text":"<p>The set of all possible linear combinations of a given set of vectors.</p> <p>Example: The span of (1, 0) and (0, 1) is all of R\u00b2.</p> <p>See also: Linear Combination, Basis Vector</p>"},{"location":"glossary/#sparse-matrix","title":"Sparse Matrix","text":"<p>A matrix where most elements are zero, stored using specialized formats for efficiency.</p> <p>Example: A 10000\u00d710000 matrix with only 50000 nonzero entries is 99.95% sparse.</p> <p>See also: Dense Matrix, Matrix</p>"},{"location":"glossary/#spectral-theorem","title":"Spectral Theorem","text":"<p>A theorem stating that symmetric matrices have orthonormal eigenvectors and real eigenvalues, enabling orthogonal diagonalization.</p> <p>Example: Any real symmetric matrix A can be written as Q\u039bQ^T where Q is orthogonal.</p> <p>See also: Symmetric Eigenvalues, Eigendecomposition</p>"},{"location":"glossary/#standard-basis","title":"Standard Basis","text":"<p>The set of unit vectors along coordinate axes, forming the default basis for R^n.</p> <p>Example: In R\u00b3, the standard basis is {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.</p> <p>See also: Basis Vector, Coordinate System</p>"},{"location":"glossary/#standardization","title":"Standardization","text":"<p>Transforming features to have zero mean and unit variance for consistent scaling.</p> <p>Example: Standardized values are computed as z = (x - \u03bc)/\u03c3 for each feature.</p> <p>See also: Feature Matrix, PCA</p>"},{"location":"glossary/#state-estimation","title":"State Estimation","text":"<p>The process of inferring hidden system states from noisy observations using probabilistic models.</p> <p>Example: Estimating a drone's actual position from GPS measurements with known error characteristics.</p> <p>See also: Kalman Filter, State Vector</p>"},{"location":"glossary/#state-vector","title":"State Vector","text":"<p>A vector containing all variables needed to describe the current state of a dynamic system.</p> <p>Example: A tracking state vector might be [x, y, vx, vy] for position and velocity.</p> <p>See also: State Estimation, Kalman Filter</p>"},{"location":"glossary/#stereo-vision","title":"Stereo Vision","text":"<p>Depth perception using two cameras, analogous to human binocular vision.</p> <p>Example: Stereo matching finds corresponding points in left and right images to compute depth.</p> <p>See also: Triangulation, Epipolar Geometry</p>"},{"location":"glossary/#stride","title":"Stride","text":"<p>The step size by which a convolution kernel moves across the input, affecting output dimensions.</p> <p>Example: With stride 2, the kernel moves 2 pixels at a time, reducing output size by half.</p> <p>See also: Convolutional Layer, Padding</p>"},{"location":"glossary/#subspace","title":"Subspace","text":"<p>A subset of a vector space that is itself a vector space under the same operations.</p> <p>Example: The xy-plane in R\u00b3 is a subspace: any linear combination of xy-plane vectors stays in the plane.</p> <p>See also: Vector Space, Span</p>"},{"location":"glossary/#svd","title":"SVD","text":"<p>Singular Value Decomposition, a factorization A = U\u03a3V^T revealing the fundamental structure of any matrix.</p> <p>SVD is one of the most important decompositions in applied linear algebra, with applications in dimensionality reduction, compression, and pseudoinverse computation.</p> <p>Example: Truncating SVD provides the best low-rank matrix approximation.</p> <p>See also: Singular Value, Left Singular Vector, Right Singular Vector</p>"},{"location":"glossary/#symmetric-eigenvalues","title":"Symmetric Eigenvalues","text":"<p>The property that symmetric matrices have only real eigenvalues (no complex values).</p> <p>Example: The symmetric matrix [[1, 2], [2, 1]] has real eigenvalues 3 and -1.</p> <p>See also: Spectral Theorem, Symmetric Matrix</p>"},{"location":"glossary/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A square matrix equal to its transpose, meaning a_{ij} = a_{ji} for all entries.</p> <p>Example: Covariance matrices are symmetric because Cov(X, Y) = Cov(Y, X).</p> <p>See also: Matrix Transpose, Spectral Theorem</p>"},{"location":"glossary/#system-of-equations","title":"System of Equations","text":"<p>A collection of linear equations that must all be satisfied simultaneously.</p> <p>Example: The system 2x + y = 5, x - y = 1 has two equations in two unknowns.</p> <p>See also: Linear Equation, Matrix Equation Form</p>"},{"location":"glossary/#tanh","title":"Tanh","text":"<p>Hyperbolic tangent activation function mapping values to (-1, 1), defined as tanh(x) = (e^x - e^(-x))/(e^x + e^(-x)).</p> <p>Example: Tanh is centered at zero unlike sigmoid, making it often preferable for hidden layers.</p> <p>See also: Activation Function, Sigmoid, ReLU</p>"},{"location":"glossary/#tensor","title":"Tensor","text":"<p>A multi-dimensional array generalizing scalars, vectors, and matrices to higher dimensions.</p> <p>Example: A 3D tensor with shape (batch_size, height, width) stores a batch of grayscale images.</p> <p>See also: Tensor Operations, Image Tensor</p>"},{"location":"glossary/#tensor-operations","title":"Tensor Operations","text":"<p>Mathematical operations on tensors including element-wise operations, contractions, and products.</p> <p>Example: Batched matrix multiplication multiplies corresponding matrices across the batch dimension.</p> <p>See also: Tensor, Matrix Multiplication</p>"},{"location":"glossary/#trajectory-optimization","title":"Trajectory Optimization","text":"<p>Computing optimal paths through state space that minimize cost while satisfying dynamics and constraints.</p> <p>Example: Planning a fuel-efficient trajectory for a spacecraft to reach Mars.</p> <p>See also: Path Planning, Motion Planning</p>"},{"location":"glossary/#transformation-matrix","title":"Transformation Matrix","text":"<p>A matrix that represents a linear transformation, mapping input vectors to output vectors.</p> <p>Example: A 2D rotation by 45\u00b0 has transformation matrix [[\u221a2/2, -\u221a2/2], [\u221a2/2, \u221a2/2]].</p> <p>See also: Linear Transformation, Rotation Matrix</p>"},{"location":"glossary/#transformer-architecture","title":"Transformer Architecture","text":"<p>A neural network architecture using self-attention to process sequences in parallel without recurrence.</p> <p>Transformers are the foundation of modern large language models and many vision systems.</p> <p>Example: GPT models use transformer decoders to generate text one token at a time.</p> <p>See also: Self-Attention, Multi-Head Attention, Position Encoding</p>"},{"location":"glossary/#transpose-determinant","title":"Transpose Determinant","text":"<p>The property that the determinant of a transpose equals the original determinant: det(A^T) = det(A).</p> <p>Example: This property allows cofactor expansion along any row or column.</p> <p>See also: Determinant, Matrix Transpose</p>"},{"location":"glossary/#triangular-matrix","title":"Triangular Matrix","text":"<p>A square matrix where all entries either above or below the main diagonal are zero.</p> <p>Example: Triangular systems can be solved efficiently using back or forward substitution.</p> <p>See also: Upper Triangular, Lower Triangular</p>"},{"location":"glossary/#triangulation","title":"Triangulation","text":"<p>Computing 3D point positions from multiple 2D observations using geometric constraints.</p> <p>Example: With two calibrated cameras, triangulation determines the 3D location of a visible feature.</p> <p>See also: Stereo Vision, Epipolar Geometry</p>"},{"location":"glossary/#trivial-solution","title":"Trivial Solution","text":"<p>The zero solution x = 0 that always satisfies a homogeneous system of equations.</p> <p>Example: The system 2x + 3y = 0 always has the trivial solution x = 0, y = 0.</p> <p>See also: Homogeneous System, Null Space</p>"},{"location":"glossary/#truncated-svd","title":"Truncated SVD","text":"<p>Singular value decomposition keeping only the k largest singular values and corresponding vectors.</p> <p>Example: Truncated SVD with k = 100 reduces a million-dimensional space to 100 dimensions.</p> <p>See also: SVD, Low-Rank Approximation, Dimensionality Reduction</p>"},{"location":"glossary/#uniform-scaling","title":"Uniform Scaling","text":"<p>Scaling by the same factor in all directions, preserving shape while changing size.</p> <p>Example: Uniform scaling by factor 2 doubles all distances, making a circle twice as large.</p> <p>See also: Non-Uniform Scaling, Scaling Matrix</p>"},{"location":"glossary/#unique-solution","title":"Unique Solution","text":"<p>A system of equations having exactly one solution.</p> <p>Example: Two non-parallel lines in 2D have a unique intersection point.</p> <p>See also: Solution Set, No Solution, Infinite Solutions</p>"},{"location":"glossary/#unit-vector","title":"Unit Vector","text":"<p>A vector with magnitude (length) equal to one.</p> <p>Example: The unit vector in the direction of (3, 4) is (3/5, 4/5) = (0.6, 0.8).</p> <p>See also: Vector Normalization, Vector Magnitude</p>"},{"location":"glossary/#update-step","title":"Update Step","text":"<p>The Kalman filter phase that corrects the prediction using new measurements and the Kalman gain.</p> <p>Example: When a GPS measurement arrives, the update step adjusts the position estimate.</p> <p>See also: Prediction Step, Kalman Gain</p>"},{"location":"glossary/#upper-triangular","title":"Upper Triangular","text":"<p>A matrix where all entries below the main diagonal are zero.</p> <p>Example: The matrix [[1, 2, 3], [0, 4, 5], [0, 0, 6]] is upper triangular.</p> <p>See also: Lower Triangular, Triangular Matrix</p>"},{"location":"glossary/#value-matrix","title":"Value Matrix","text":"<p>A learned matrix that transforms input embeddings into value vectors aggregated by attention weights.</p> <p>Example: Values contain the information that attention mechanisms choose to pass forward.</p> <p>See also: Query Matrix, Key Matrix, Attention Mechanism</p>"},{"location":"glossary/#variance-explained","title":"Variance Explained","text":"<p>The proportion of total data variance captured by principal components, guiding dimensionality reduction decisions.</p> <p>Example: If the first 10 components explain 95% of variance, 90% of features can be discarded.</p> <p>See also: PCA, Scree Plot</p>"},{"location":"glossary/#vector","title":"Vector","text":"<p>An ordered list of numbers representing magnitude and direction in a space.</p> <p>Example: The velocity (5, 3) represents movement of 5 units/s in x and 3 units/s in y.</p> <p>See also: 2D Vector, 3D Vector, Vector Space</p>"},{"location":"glossary/#vector-addition","title":"Vector Addition","text":"<p>Adding two vectors by summing their corresponding components.</p> <p>Example: (1, 2) + (3, 4) = (4, 6).</p> <p>See also: Vector, Vector Subtraction</p>"},{"location":"glossary/#vector-magnitude","title":"Vector Magnitude","text":"<p>The length of a vector, computed as the square root of the sum of squared components.</p> <p>Example: The magnitude of vector (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L2 Norm, Unit Vector</p>"},{"location":"glossary/#vector-normalization","title":"Vector Normalization","text":"<p>Dividing a vector by its magnitude to produce a unit vector with the same direction.</p> <p>Example: Normalizing (3, 4) gives (3/5, 4/5) with magnitude 1.</p> <p>See also: Unit Vector, Vector Magnitude</p>"},{"location":"glossary/#vector-notation","title":"Vector Notation","text":"<p>Conventions for writing vectors, including bold letters, arrows, or component lists.</p> <p>Example: The vector v = (v\u2081, v\u2082, v\u2083) can also be written as v or with an arrow above.</p> <p>See also: Vector, Matrix Notation</p>"},{"location":"glossary/#vector-space","title":"Vector Space","text":"<p>A collection of vectors with defined addition and scalar multiplication operations satisfying axioms.</p> <p>Example: R\u00b2 with standard vector addition and scalar multiplication is a vector space.</p> <p>See also: Abstract Vector Space, Vector Space Axioms</p>"},{"location":"glossary/#vector-space-axioms","title":"Vector Space Axioms","text":"<p>The eight properties that addition and scalar multiplication must satisfy for a set to be a vector space.</p> <p>Example: The axioms include commutativity, associativity, and the existence of zero and inverse vectors.</p> <p>See also: Vector Space, Abstract Vector Space</p>"},{"location":"glossary/#vector-subtraction","title":"Vector Subtraction","text":"<p>Subtracting two vectors by computing the difference of corresponding components.</p> <p>Example: (5, 7) - (2, 3) = (3, 4).</p> <p>See also: Vector Addition, Vector</p>"},{"location":"glossary/#volume-scaling-factor","title":"Volume Scaling Factor","text":"<p>The absolute value of a determinant, indicating how much a transformation scales volumes.</p> <p>Example: A matrix with determinant 3 triples the volume of any region it transforms.</p> <p>See also: Determinant, Signed Area</p>"},{"location":"glossary/#weight-matrix","title":"Weight Matrix","text":"<p>A matrix of learnable parameters connecting layers in a neural network.</p> <p>Example: A layer connecting 100 inputs to 50 outputs has a 50\u00d7100 weight matrix.</p> <p>See also: Bias Vector, Neural Network Layer</p>"},{"location":"glossary/#word-embedding","title":"Word Embedding","text":"<p>A vector representation of a word learned from text data, capturing semantic meaning.</p> <p>Example: Word2Vec learns that \"king\" - \"man\" + \"woman\" \u2248 \"queen\" in embedding space.</p> <p>See also: Embedding, Semantic Similarity</p>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"license/#commercial-licensing","title":"Commercial Licensing","text":"<p>Commercial rights are reserved by the copyright holder. For commercial licensing, publication inquiries, or permission to use this work in commercial contexts, please contact Dan McCreary on LinkedIn.</p>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 15 chapters covering 300 concepts across four major parts.</p>"},{"location":"chapters/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":"<ol> <li> <p>Vectors and Vector Spaces - Introduces vectors as fundamental building blocks, covering operations, norms, and vector space theory.</p> </li> <li> <p>Matrices and Matrix Operations - Covers matrix notation, special types, and core operations including multiplication and inverse.</p> </li> <li> <p>Systems of Linear Equations - Methods for representing and solving linear systems using elimination and matrix techniques.</p> </li> <li> <p>Linear Transformations - How matrices represent geometric transformations including rotation, scaling, and projection.</p> </li> </ol>"},{"location":"chapters/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":"<ol> <li> <p>Determinants and Matrix Properties - Determinant computation, properties, and geometric interpretation.</p> </li> <li> <p>Eigenvalues and Eigenvectors - Eigentheory fundamentals including characteristic polynomials and diagonalization.</p> </li> <li> <p>Matrix Decompositions - Matrix factorization methods including LU, QR, Cholesky, and SVD.</p> </li> <li> <p>Vector Spaces and Inner Products - Abstract inner product spaces, orthogonality, and least squares.</p> </li> </ol>"},{"location":"chapters/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":"<ol> <li> <p>Machine Learning Foundations - Data representation, PCA, linear regression, and gradient descent.</p> </li> <li> <p>Neural Networks and Deep Learning - Deep learning architecture, weight matrices, and backpropagation.</p> </li> <li> <p>Generative AI and Large Language Models - Embeddings, attention mechanisms, and transformer architecture.</p> </li> <li> <p>Optimization and Learning Algorithms - Optimization algorithms for training machine learning models.</p> </li> </ol>"},{"location":"chapters/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":"<ol> <li> <p>Image Processing and Computer Vision - Image representation, convolution, filtering, and feature detection.</p> </li> <li> <p>3D Geometry and Transformations - Three-dimensional geometry, quaternions, and camera models.</p> </li> <li> <p>Autonomous Systems and Sensor Fusion - Kalman filtering, SLAM, and autonomous navigation.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>This textbook is designed for sequential learning - each chapter builds on concepts from previous chapters. The dependency structure ensures that prerequisite knowledge is always introduced before it's needed. While you can jump ahead to specific topics of interest, completing earlier chapters first will provide the strongest foundation.</p> <p>Each chapter includes a list of concepts covered, allowing you to track your progress through the learning graph. Interactive microsimulations throughout the book help reinforce abstract mathematical concepts with visual, hands-on exploration.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/","title":"Vectors and Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#summary","title":"Summary","text":"<p>This chapter introduces vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces. By the end of this chapter, you will understand how to perform vector operations, compute norms and distances, and work with the abstract concepts of span, linear independence, and basis vectors that form the foundation for all subsequent topics.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description: College Algebra and familiarity with basic calculus concepts.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#introduction","title":"Introduction","text":"<p>Linear algebra provides the mathematical foundation for modern artificial intelligence, machine learning, and computer vision. At the heart of this mathematical framework lies the vector\u2014an elegant structure that represents both magnitude and direction. Whether you're representing a data point in a machine learning model, describing the velocity of an autonomous vehicle, or encoding the semantic meaning of a word, vectors serve as your fundamental tool.</p> <p>This chapter establishes the foundational concepts you'll build upon throughout this course. We begin with the simple distinction between scalars and vectors, then progressively develop your understanding through vector operations, norms, and the powerful abstractions of vector spaces.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalars-and-vectors","title":"Scalars and Vectors","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-scalars","title":"Understanding Scalars","text":"<p>A scalar is a single numerical value that represents a quantity with magnitude only. Scalars are the numbers you've worked with throughout your mathematical education\u2014they can be positive, negative, or zero. In the context of linear algebra, scalars typically come from a field (usually the real numbers \\(\\mathbb{R}\\) or complex numbers \\(\\mathbb{C}\\)).</p> <p>Examples of scalars include:</p> <ul> <li>Temperature: 72\u00b0F</li> <li>Mass: 5.2 kg</li> <li>Speed: 60 mph</li> <li>Count: 42 items</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#what-is-a-vector","title":"What is a Vector?","text":"<p>A vector is an ordered collection of scalars that represents both magnitude and direction. While scalars tell us \"how much,\" vectors tell us \"how much and in which direction.\" Geometrically, we can visualize a vector as an arrow in space\u2014the length represents magnitude, and the arrowhead indicates direction.</p> <p>More formally, a vector in \\(n\\)-dimensional space is an ordered \\(n\\)-tuple of real numbers:</p> \\[\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\] <p>where each \\(v_i\\) is a scalar component of the vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-notation","title":"Vector Notation","text":"<p>Mathematical texts use several conventions for denoting vectors. Throughout this course, we use the following:</p> Notation Description Example Bold lowercase Standard textbook notation \\(\\mathbf{v}\\), \\(\\mathbf{u}\\), \\(\\mathbf{w}\\) Arrow above Handwritten notation \\(\\vec{v}\\), \\(\\vec{u}\\) Column matrix Computational notation \\(\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) Row matrix Alternative notation \\([v_1 \\quad v_2]\\) Component form Explicit values \\((3, 4)\\) or \\(\\langle 3, 4 \\rangle\\) <p>Notation in Code</p> <p>In programming contexts like NumPy, vectors are typically represented as one-dimensional arrays: <code>v = np.array([3, 4, 5])</code>.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vectors-in-different-dimensions","title":"Vectors in Different Dimensions","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#2d-vectors","title":"2D Vectors","text":"<p>A 2D vector exists in a two-dimensional plane and consists of two components. We write a 2D vector as:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}\\] <p>where \\(v_x\\) is the horizontal component and \\(v_y\\) is the vertical component. Geometrically, this represents an arrow from the origin to the point \\((v_x, v_y)\\).</p> <p>Common applications of 2D vectors include:</p> <ul> <li>Position coordinates on a screen</li> <li>Velocity in a plane</li> <li>Force components in 2D physics simulations</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#3d-vectors","title":"3D Vectors","text":"<p>A 3D vector extends into three-dimensional space with three components:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix}\\] <p>The additional \\(z\\)-component allows us to represent quantities in our three-dimensional world, such as the position of a drone or the direction of a camera in virtual reality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-2d-and-3d-vector-visualization","title":"Diagram: 2D and 3D Vector Visualization","text":"2D and 3D Vector Visualization MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, visualize</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p> <p>Canvas layout: - Left panel (500px): 2D vector visualization with coordinate grid - Right panel (400px): 3D vector visualization with rotatable axes - Bottom controls: Component sliders and toggle switches</p> <p>Visual elements: - 2D coordinate grid with x and y axes, grid lines at unit intervals - 3D coordinate system with x, y, z axes (perspective view) - Vector arrow from origin to point, with distinct arrowhead - Dashed projection lines from vector tip to each axis - Component labels showing current values - Origin point clearly marked</p> <p>Interactive controls: - Slider: x-component (-5 to 5, default 3) - Slider: y-component (-5 to 5, default 4) - Slider: z-component (-5 to 5, default 2) for 3D view - Toggle: Show/hide projection lines - Toggle: Show/hide component labels - Button: Switch between 2D and 3D views - For 3D: Mouse drag to rotate view</p> <p>Default parameters: - 2D vector: (3, 4) - 3D vector: (3, 4, 2) - Projection lines: visible - Component labels: visible</p> <p>Behavior: - As sliders change, vector arrow updates in real-time - Projection lines dynamically connect to axis intersections - Magnitude value updates and displays - 3D view allows full rotation with mouse drag</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#n-dimensional-vectors","title":"N-Dimensional Vectors","text":"<p>An N-dimensional vector generalizes to any number of dimensions:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\] <p>While we cannot visualize vectors beyond three dimensions geometrically, the mathematical operations extend naturally. In machine learning, it's common to work with vectors having hundreds or thousands of dimensions:</p> <ul> <li>Word embeddings: 300-dimensional vectors representing word meanings</li> <li>Image features: 2048-dimensional vectors from convolutional neural networks</li> <li>User preference vectors: 100+ dimensions in recommendation systems</li> </ul> <p>The power of linear algebra is that the same operations and theorems apply regardless of dimensionality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#basic-vector-operations","title":"Basic Vector Operations","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-addition","title":"Vector Addition","text":"<p>Vector addition combines two vectors of the same dimension by adding their corresponding components:</p> \\[\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\\] <p>Geometrically, vector addition follows the parallelogram rule or the tip-to-tail method. If you place the tail of \\(\\mathbf{v}\\) at the tip of \\(\\mathbf{u}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) points from the tail of \\(\\mathbf{u}\\) to the tip of \\(\\mathbf{v}\\).</p> <p>Vector addition has the following properties:</p> <ul> <li>Commutative: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associative: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Scalar multiplication multiplies each component of a vector by a scalar value:</p> \\[c \\cdot \\mathbf{v} = c \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} c \\cdot v_1 \\\\ c \\cdot v_2 \\\\ \\vdots \\\\ c \\cdot v_n \\end{bmatrix}\\] <p>The geometric effect of scalar multiplication:</p> <ul> <li>\\(c &gt; 1\\): Stretches the vector (increases magnitude)</li> <li>\\(0 &lt; c &lt; 1\\): Shrinks the vector (decreases magnitude)</li> <li>\\(c = 0\\): Produces the zero vector</li> <li>\\(c &lt; 0\\): Reverses direction and scales magnitude</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-subtraction","title":"Vector Subtraction","text":"<p>Vector subtraction is defined in terms of addition and scalar multiplication:</p> \\[\\mathbf{u} - \\mathbf{v} = \\mathbf{u} + (-1)\\mathbf{v}\\] <p>This yields component-wise subtraction:</p> \\[\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_n - v_n \\end{bmatrix}\\]"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-operations-playground","title":"Diagram: Vector Operations Playground","text":"Vector Operations Playground MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, practice, demonstrate</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p> <p>Canvas layout: - Main area (600px): Coordinate grid showing vectors - Right panel (200px): Controls and result display - Bottom: Operation selector and calculation display</p> <p>Visual elements: - Coordinate grid with axes from -10 to 10 - Vector u (blue arrow) with adjustable endpoint - Vector v (red arrow) with adjustable endpoint - Result vector (green arrow) showing operation result - Parallelogram outline when showing addition (dashed lines) - Component labels on each vector - Magnitude display for each vector</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Radio buttons: Addition, Subtraction, Scalar Multiply - Slider: Scalar value c (-3 to 3, default 2) for scalar multiplication - Checkbox: Show parallelogram construction - Checkbox: Show component breakdown - Button: Animate operation step-by-step - Button: Reset to defaults</p> <p>Default parameters: - Vector u: (3, 2) - Vector v: (1, 4) - Operation: Addition - Scalar: 2 - Show parallelogram: true</p> <p>Behavior: - Dragging vector endpoints updates all calculations in real-time - Operation result vector updates immediately - Step-by-step animation shows geometric construction - Component breakdown displays numerical computation - Parallelogram appears for addition to show geometric interpretation</p> <p>Implementation: p5.js with drag-and-drop interaction</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-products","title":"Vector Products","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#the-dot-product","title":"The Dot Product","text":"<p>The dot product (also called the inner product or scalar product) takes two vectors of the same dimension and returns a scalar. For vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\):</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n\\] <p>The geometric interpretation reveals the dot product's significance:</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos\\theta\\] <p>where \\(\\theta\\) is the angle between the vectors and \\(\\|\\cdot\\|\\) denotes magnitude. This relationship connects algebraic computation with geometric meaning.</p> <p>Key properties and insights:</p> Condition Geometric Meaning Dot Product Value \\(\\theta = 0\u00b0\\) Vectors point same direction Maximum positive \\(\\theta = 90\u00b0\\) Vectors are perpendicular Zero \\(\\theta = 180\u00b0\\) Vectors point opposite directions Maximum negative <p>The dot product has important applications:</p> <ul> <li>Computing angles between vectors</li> <li>Projecting one vector onto another</li> <li>Calculating work in physics (force \u00b7 displacement)</li> <li>Measuring similarity between feature vectors</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-cross-product","title":"The Cross Product","text":"<p>The cross product is defined only for 3D vectors and produces a vector perpendicular to both input vectors:</p> \\[\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_2 v_3 - u_3 v_2 \\\\ u_3 v_1 - u_1 v_3 \\\\ u_1 v_2 - u_2 v_1 \\end{bmatrix}\\] <p>The magnitude of the cross product equals the area of the parallelogram formed by the two vectors:</p> \\[\\|\\mathbf{u} \\times \\mathbf{v}\\| = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\sin\\theta\\] <p>The direction follows the right-hand rule: if you curl your fingers from \\(\\mathbf{u}\\) toward \\(\\mathbf{v}\\), your thumb points in the direction of \\(\\mathbf{u} \\times \\mathbf{v}\\).</p> <p>Important properties of the cross product:</p> <ul> <li>Anti-commutative: \\(\\mathbf{u} \\times \\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u})\\)</li> <li>Not associative: \\((\\mathbf{u} \\times \\mathbf{v}) \\times \\mathbf{w} \\neq \\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w})\\)</li> <li>Self-cross is zero: \\(\\mathbf{v} \\times \\mathbf{v} = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-dot-product-and-cross-product-visualizer","title":"Diagram: Dot Product and Cross Product Visualizer","text":"Dot Product and Cross Product Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare, differentiate</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p> <p>Canvas layout: - Left panel (400px): 2D view for dot product with projection visualization - Right panel (400px): 3D view for cross product with parallelogram and result vector - Bottom panel: Numerical results and formulas</p> <p>Visual elements: - 2D panel: Two vectors, projection line, angle arc, shaded projection component - 3D panel: Two vectors, cross product result vector (perpendicular), parallelogram surface - Angle measurement display with theta symbol - Area calculation display for parallelogram - Formula display showing computation steps</p> <p>Interactive controls: - Draggable vector endpoints in both panels - Slider: Angle between vectors (0\u00b0 to 180\u00b0) - Toggle: Show projection in 2D - Toggle: Show parallelogram in 3D - Toggle: Show right-hand rule animation - Button: Animate angle sweep from 0\u00b0 to 180\u00b0 - 3D rotation via mouse drag</p> <p>Default parameters: - Vector u: (3, 2) in 2D, (3, 2, 0) in 3D - Vector v: (4, 1) in 2D, (1, 4, 0) in 3D - Show projection: true - Show parallelogram: true</p> <p>Behavior: - Dot product value updates with cos(theta) relationship visible - When vectors perpendicular, dot product display highlights zero - Cross product vector length changes with parallelogram area - Right-hand rule animation shows finger curl and thumb direction - Numerical display shows step-by-step calculation</p> <p>Implementation: p5.js with WEBGL for 3D panel</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#magnitude-and-norms","title":"Magnitude and Norms","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-magnitude","title":"Vector Magnitude","text":"<p>The magnitude (or length) of a vector measures how far the vector extends from the origin. For a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\), the magnitude is:</p> \\[\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This formula is the generalization of the Pythagorean theorem to \\(n\\) dimensions. For a 2D vector \\((3, 4)\\), the magnitude is \\(\\sqrt{9 + 16} = 5\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-norms","title":"Understanding Norms","text":"<p>A norm is a function that assigns a non-negative length to each vector in a vector space. Different norms measure \"length\" differently, and each has applications in machine learning and optimization.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#l2-norm-euclidean-norm","title":"L2 Norm (Euclidean Norm)","text":"<p>The L2 norm is the standard Euclidean distance from the origin:</p> \\[\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This is the magnitude we computed above. It's the \"straight-line\" distance and is used extensively in:</p> <ul> <li>Least squares regression</li> <li>Euclidean distance calculations</li> <li>Ridge regularization (L2 regularization)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l1-norm-manhattan-norm","title":"L1 Norm (Manhattan Norm)","text":"<p>The L1 norm sums the absolute values of components:</p> \\[\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^{n} |v_i|\\] <p>Named after the grid-like street layout of Manhattan, this norm measures distance as if you could only travel along axes. It's used in:</p> <ul> <li>Lasso regularization (L1 regularization)</li> <li>Robust statistics</li> <li>Sparse signal recovery</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l-infinity-norm-maximum-norm","title":"L-Infinity Norm (Maximum Norm)","text":"<p>The L-infinity norm returns the maximum absolute component value:</p> \\[\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\\] <p>This norm is useful when you need to constrain the maximum deviation in any single dimension.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-norm-comparison-visualizer","title":"Diagram: Norm Comparison Visualizer","text":"Norm Comparison Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p> <p>Canvas layout: - Main area (600px): Coordinate grid with unit \"circles\" for each norm - Right panel (200px): Numerical values and controls</p> <p>Visual elements: - Coordinate grid from -2 to 2 on both axes - Unit circle for L2 norm (actual circle, blue) - Unit \"circle\" for L1 norm (diamond/rhombus shape, green) - Unit \"circle\" for L-infinity norm (square, orange) - Point showing selected vector with lines to origin - Distance measurements for each norm displayed</p> <p>Interactive controls: - Draggable point to select vector - Checkboxes: Show L1, L2, L-infinity unit shapes (all on by default) - Slider: Adjust norm \"radius\" to see scaled shapes - Toggle: Animate point around each unit shape - Display: Current coordinates and all three norm values</p> <p>Default parameters: - Selected point: (0.6, 0.8) - All three norm shapes visible - Radius: 1</p> <p>Behavior: - As point is dragged, all three norm values update - Unit shapes clearly show different \"distance = 1\" definitions - Points on each unit shape highlight when norm equals 1 - Animation moves point around each shape showing equal norm path</p> <p>Implementation: p5.js with parametric curve drawing</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#euclidean-distance","title":"Euclidean Distance","text":"<p>The Euclidean distance between two vectors measures the straight-line separation:</p> \\[d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2}\\] <p>This fundamental metric appears throughout machine learning:</p> <ul> <li>k-Nearest Neighbors classification</li> <li>K-means clustering</li> <li>Distance-based similarity measures</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#unit-vectors-and-normalization","title":"Unit Vectors and Normalization","text":"<p>A unit vector has magnitude exactly 1. Any non-zero vector can be converted to a unit vector through normalization:</p> \\[\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\] <p>The resulting vector \\(\\hat{\\mathbf{v}}\\) (pronounced \"v-hat\") points in the same direction as \\(\\mathbf{v}\\) but has unit length. Normalization is essential for:</p> <ul> <li>Comparing vector directions without magnitude bias</li> <li>Creating orthonormal bases</li> <li>Preparing features for machine learning algorithms</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations-and-span","title":"Linear Combinations and Span","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations","title":"Linear Combinations","text":"<p>A linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) is a sum of scalar multiples:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\] <p>where \\(c_1, c_2, \\ldots, c_k\\) are scalars. Linear combinations allow us to create new vectors from existing ones and form the foundation for understanding vector spaces.</p> <p>Example: Given \\(\\mathbf{v}_1 = (1, 0)\\) and \\(\\mathbf{v}_2 = (0, 1)\\), the linear combination \\(3\\mathbf{v}_1 + 2\\mathbf{v}_2 = (3, 2)\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-span-of-vectors","title":"The Span of Vectors","text":"<p>The span of a set of vectors is the collection of all possible linear combinations of those vectors:</p> \\[\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k : c_i \\in \\mathbb{R}\\}\\] <p>Geometrically, the span represents all points reachable by combining the vectors:</p> <ul> <li>The span of one non-zero vector is a line through the origin</li> <li>The span of two non-parallel vectors in 3D is a plane through the origin</li> <li>The span of three non-coplanar vectors in 3D is all of \\(\\mathbb{R}^3\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-linear-combination-explorer","title":"Diagram: Linear Combination Explorer","text":"Linear Combination Explorer MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: use, demonstrate, calculate</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p> <p>Canvas layout: - Main area (550px): 2D coordinate grid with vectors and span visualization - Right panel (250px): Controls for coefficients and target challenges</p> <p>Visual elements: - Coordinate grid from -6 to 6 - Two basis vectors v1 (red) and v2 (blue) with adjustable directions - Scaled versions c1v1 (light red) and c2v2 (light blue) - Result vector as sum (green, thick arrow) - Shaded region showing span (when two vectors span a plane) - Target point for challenges (yellow star) - Trail showing path as coefficients change</p> <p>Interactive controls: - Slider: Coefficient c1 (-3 to 3, step 0.1) - Slider: Coefficient c2 (-3 to 3, step 0.1) - Draggable endpoints to adjust v1 and v2 directions - Toggle: Show span region (shaded area) - Toggle: Show component arrows tip-to-tail - Button: Random target challenge - Button: Show solution for current target - Display: Current linear combination equation</p> <p>Default parameters: - v1: (2, 1) - v2: (1, 2) - c1: 1, c2: 1 - Show span: true - Show components: true</p> <p>Behavior: - Adjusting c1/c2 sliders moves result vector smoothly - Target challenges require finding correct coefficients - When vectors are parallel, span collapses to a line (visual feedback) - Component arrows show tip-to-tail construction of sum</p> <p>Implementation: p5.js with slider controls</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence-and-dependence","title":"Linear Independence and Dependence","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence","title":"Linear Independence","text":"<p>A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is linearly independent if the only solution to:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>is \\(c_1 = c_2 = \\cdots = c_k = 0\\). In other words, no vector in the set can be written as a linear combination of the others.</p> <p>Geometric interpretation:</p> <ul> <li>Two vectors are linearly independent if they are not parallel</li> <li>Three vectors in 3D are linearly independent if they don't all lie in the same plane</li> <li>Linearly independent vectors point in \"genuinely different\" directions</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-dependence","title":"Linear Dependence","text":"<p>Vectors are linearly dependent if at least one can be expressed as a linear combination of the others. This occurs when there exist scalars, not all zero, such that:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>Example of dependence: The vectors \\(\\mathbf{v}_1 = (1, 2)\\), \\(\\mathbf{v}_2 = (2, 4)\\), and \\(\\mathbf{v}_3 = (3, 6)\\) are linearly dependent because \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) and \\(\\mathbf{v}_3 = 3\\mathbf{v}_1\\). They all lie on the same line.</p> Property Linearly Independent Linearly Dependent Unique representation Each point in span has unique coefficients Multiple ways to reach same point Redundancy No redundant vectors At least one vector is redundant Span efficiency Minimal set to span a subspace Could remove vectors without reducing span"},{"location":"chapters/01-vectors-and-vector-spaces/#basis-and-coordinate-systems","title":"Basis and Coordinate Systems","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#basis-vectors","title":"Basis Vectors","text":"<p>A basis for a vector space is a set of linearly independent vectors that span the entire space. Every vector in the space can be written uniquely as a linear combination of basis vectors.</p> <p>A basis provides:</p> <ul> <li>A coordinate system for the space</li> <li>A way to represent any vector as a list of coefficients</li> <li>The minimum number of vectors needed to describe the space</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-standard-basis","title":"The Standard Basis","text":"<p>The standard basis for \\(\\mathbb{R}^n\\) consists of vectors with a single 1 and all other components 0:</p> <p>For \\(\\mathbb{R}^2\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>For \\(\\mathbb{R}^3\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>The standard basis vectors align with the coordinate axes, making them intuitive for visualization and computation.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#coordinate-systems","title":"Coordinate Systems","text":"<p>A coordinate system assigns a unique tuple of numbers to each point in space. When we choose a basis \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n\\}\\), any vector \\(\\mathbf{v}\\) can be written as:</p> \\[\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2 + \\cdots + c_n\\mathbf{b}_n\\] <p>The coefficients \\((c_1, c_2, \\ldots, c_n)\\) are the coordinates of \\(\\mathbf{v}\\) with respect to this basis. Different bases give different coordinate representations of the same vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-basis-and-coordinate-system-visualizer","title":"Diagram: Basis and Coordinate System Visualizer","text":"Basis and Coordinate System Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret, compare</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p> <p>Canvas layout: - Left panel (400px): Standard basis grid with vector - Right panel (400px): Custom basis grid with same vector - Bottom: Coordinate comparison display</p> <p>Visual elements: - Standard basis: Traditional x-y grid with e1, e2 as unit vectors - Custom basis: Skewed grid based on chosen basis vectors b1, b2 - Same geometric vector shown in both coordinate systems - Grid lines parallel to basis vectors in each panel - Coordinate labels showing (x,y) in standard and (c1,c2) in custom - Dashed lines from vector tip to axes showing coordinates</p> <p>Interactive controls: - Draggable point to select vector (synced between panels) - Draggable endpoints for custom basis vectors b1 and b2 - Preset buttons: Standard, Rotated 45\u00b0, Skewed, Stretched - Toggle: Show grid lines - Toggle: Show coordinate projections - Animation: Morph from standard to custom basis</p> <p>Default parameters: - Vector: (3, 2) in standard basis - Custom basis: b1 = (2, 0), b2 = (1, 1) - Grid lines: visible - Projections: visible</p> <p>Behavior: - Moving point updates coordinates in both systems - Changing custom basis redraws right panel grid - Coordinates update in real-time showing transformation - Morph animation shows how grid deforms between bases</p> <p>Implementation: p5.js with matrix transformation for grid rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-spaces","title":"Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#definition-of-a-vector-space","title":"Definition of a Vector Space","text":"<p>A vector space is a collection of objects called vectors that satisfies ten axioms regarding addition and scalar multiplication. For a set \\(V\\) to be a vector space over a field \\(F\\) (typically \\(\\mathbb{R}\\)), it must satisfy:</p> <p>Addition axioms:</p> <ol> <li>Closure: \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</li> <li>Commutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associativity: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: There exists \\(\\mathbf{0}\\) such that \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: For each \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ol> <p>Scalar multiplication axioms:</p> <ol> <li>Closure: \\(c\\mathbf{v} \\in V\\)</li> <li>Distributivity (vectors): \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> <li>Distributivity (scalars): \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</li> <li>Associativity: \\(c(d\\mathbf{v}) = (cd)\\mathbf{v}\\)</li> <li>Identity: \\(1\\mathbf{v} = \\mathbf{v}\\)</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#examples-of-vector-spaces","title":"Examples of Vector Spaces","text":"<p>The most familiar vector space is \\(\\mathbb{R}^n\\), but vector spaces are more general:</p> Vector Space Elements Operations \\(\\mathbb{R}^n\\) n-tuples of real numbers Component-wise addition/scaling Polynomials of degree \u2264 n Polynomials \\(a_0 + a_1x + \\cdots + a_nx^n\\) Add coefficients, scale coefficients Continuous functions on \\([a,b]\\) Functions \\(f: [a,b] \\to \\mathbb{R}\\) \\((f+g)(x) = f(x) + g(x)\\) \\(m \\times n\\) matrices Matrices with \\(m\\) rows, \\(n\\) columns Matrix addition, scalar multiplication"},{"location":"chapters/01-vectors-and-vector-spaces/#dimension-of-a-vector-space","title":"Dimension of a Vector Space","text":"<p>The dimension of a vector space is the number of vectors in any basis. This count is always the same regardless of which basis you choose\u2014a fundamental theorem of linear algebra.</p> <ul> <li>\\(\\mathbb{R}^2\\) has dimension 2</li> <li>\\(\\mathbb{R}^3\\) has dimension 3</li> <li>The space of polynomials of degree \u2264 3 has dimension 4 (basis: \\(\\{1, x, x^2, x^3\\}\\))</li> </ul> <p>Dimension tells us the \"degrees of freedom\" in a vector space\u2014how many independent directions exist.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-space-axiom-explorer","title":"Diagram: Vector Space Axiom Explorer","text":"Vector Space Axiom Explorer Infographic <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize, list</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p> <p>Layout: Central hub showing \"Vector Space\" with two branches for Addition and Scalar Multiplication axioms</p> <p>Visual elements: - Central node: \"Vector Space V over field F\" - Left branch (blue theme): Five addition axiom nodes - Right branch (green theme): Five scalar multiplication axiom nodes - Each axiom node shows: name, symbolic form, geometric mini-visualization - Connection lines showing axiom groupings</p> <p>Interactive elements: - Hover over axiom node: Tooltip shows full definition and concrete example - Click axiom node: Animates a 2D example demonstrating the axiom - Hover over \"V\": Shows examples of vector spaces - Hover over \"F\": Shows examples of fields (R, C) - Progress tracker: Checkmarks for viewed axioms</p> <p>Hover text content: - Closure (add): \"For any u, v in V, the sum u + v is also in V\" - Commutativity: \"Order doesn't matter: u + v = v + u\" - Associativity (add): \"Grouping doesn't matter: (u + v) + w = u + (v + w)\" - Additive identity: \"The zero vector 0 exists: v + 0 = v\" - Additive inverse: \"Every vector has an opposite: v + (-v) = 0\" - Closure (scalar): \"For any scalar c and v in V, cv is also in V\" - Distributivity (vectors): \"c(u + v) = cu + cv\" - Distributivity (scalars): \"(c + d)v = cv + dv\" - Associativity (scalar): \"c(dv) = (cd)v\" - Scalar identity: \"1v = v\"</p> <p>Visual style: Modern node-and-edge layout with pastel colors</p> <p>Implementation: HTML/CSS/JavaScript with SVG nodes and CSS transitions</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#applications-in-ai-and-machine-learning","title":"Applications in AI and Machine Learning","text":"<p>The concepts introduced in this chapter form the foundation for understanding machine learning algorithms:</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#feature-vectors","title":"Feature Vectors","text":"<p>Machine learning represents data points as vectors. A sample with features \\((x_1, x_2, \\ldots, x_n)\\) becomes a vector in \\(\\mathbb{R}^n\\). Vector operations enable:</p> <ul> <li>Distance-based classification: k-NN uses Euclidean distance between feature vectors</li> <li>Similarity measures: Cosine similarity uses the dot product to compare vectors</li> <li>Centering data: Subtracting the mean vector centers the dataset at the origin</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#word-embeddings","title":"Word Embeddings","text":"<p>Natural language processing represents words as dense vectors (typically 100-300 dimensions). These embedding vectors capture semantic meaning:</p> <ul> <li>Similar words have nearby vectors (small Euclidean distance)</li> <li>Analogies appear as vector arithmetic: \\(\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\)</li> <li>The dot product measures semantic similarity</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#neural-network-parameters","title":"Neural Network Parameters","text":"<p>Neural networks store learned knowledge in weight matrices and bias vectors. Forward propagation consists of:</p> <ol> <li>Linear combination: \\(\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}\\)</li> <li>Nonlinear activation: \\(\\mathbf{a} = \\sigma(\\mathbf{z})\\)</li> </ol> <p>Understanding vector spaces helps interpret what neural networks learn and how they transform data.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#summary_1","title":"Summary","text":"<p>This chapter established the fundamental concepts of linear algebra that underpin all subsequent topics:</p> <p>Core Concepts:</p> <ul> <li>Scalars are single numbers; vectors are ordered collections with magnitude and direction</li> <li>Vectors exist in spaces of any dimension, from 2D and 3D to high-dimensional feature spaces</li> <li>Basic operations include addition, scalar multiplication, and subtraction</li> </ul> <p>Products and Measurements:</p> <ul> <li>The dot product returns a scalar measuring projection and angle</li> <li>The cross product (3D only) returns a perpendicular vector</li> <li>Norms (L1, L2, L-infinity) measure vector length in different ways</li> <li>Normalization creates unit vectors for direction comparison</li> </ul> <p>Abstract Structures:</p> <ul> <li>Linear combinations create new vectors from scaled sums</li> <li>The span is all reachable points through linear combinations</li> <li>Linear independence means no vector is redundant</li> <li>A basis provides a coordinate system for a vector space</li> <li>The dimension counts basis vectors (degrees of freedom)</li> </ul> <p>These concepts provide the vocabulary and tools for the matrix operations, transformations, and decompositions explored in subsequent chapters.</p> Self-Check: Can you answer these questions? <ol> <li>What is the geometric interpretation of the dot product \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\)?</li> <li>If vectors \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), \\(\\mathbf{v}_3\\) are linearly dependent, what does this mean geometrically in 3D?</li> <li>Why must a basis for \\(\\mathbb{R}^3\\) contain exactly three vectors?</li> <li>How does the L1 norm differ from the L2 norm when measuring the vector \\((3, 4)\\)?</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/","title":"Quiz: Vectors and Vector Spaces","text":"<p>Test your understanding of vectors, vector operations, and vector space fundamentals.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#1-what-is-a-vector-in-the-context-of-linear-algebra","title":"1. What is a vector in the context of linear algebra?","text":"<ol> <li>A single number representing magnitude</li> <li>An ordered collection of numbers representing magnitude and direction</li> <li>A matrix with only one row</li> <li>A geometric shape with four sides</li> </ol> Show Answer <p>The correct answer is B. A vector is an ordered collection of numbers (components) that represents both magnitude and direction. Unlike scalars which are single numbers, vectors have multiple components that together define a quantity in multi-dimensional space.</p> <p>Concept Tested: Vector</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#2-which-operation-produces-a-scalar-result-from-two-vectors","title":"2. Which operation produces a scalar result from two vectors?","text":"<ol> <li>Vector addition</li> <li>Scalar multiplication</li> <li>Dot product</li> <li>Cross product</li> </ol> Show Answer <p>The correct answer is C. The dot product (also called inner product or scalar product) of two vectors produces a scalar value. It is computed as the sum of the products of corresponding components: \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\\).</p> <p>Concept Tested: Dot Product</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#3-what-does-the-l2-norm-euclidean-norm-of-a-vector-measure","title":"3. What does the L2 norm (Euclidean norm) of a vector measure?","text":"<ol> <li>The number of non-zero components</li> <li>The sum of all component values</li> <li>The length or magnitude of the vector</li> <li>The angle between the vector and the x-axis</li> </ol> Show Answer <p>The correct answer is C. The L2 norm (Euclidean norm) measures the length or magnitude of a vector in Euclidean space. It is computed as \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}\\), which corresponds to the geometric distance from the origin to the vector's endpoint.</p> <p>Concept Tested: Euclidean Norm</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#4-two-vectors-are-orthogonal-when-their-dot-product-equals","title":"4. Two vectors are orthogonal when their dot product equals:","text":"<ol> <li>One</li> <li>Zero</li> <li>Negative one</li> <li>Their product of magnitudes</li> </ol> Show Answer <p>The correct answer is B. Two vectors are orthogonal (perpendicular) when their dot product equals zero. This is because \\(\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos\\theta\\), and when \\(\\theta = 90\u00b0\\), \\(\\cos(90\u00b0) = 0\\).</p> <p>Concept Tested: Orthogonality</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#5-what-is-a-unit-vector","title":"5. What is a unit vector?","text":"<ol> <li>A vector with all components equal to one</li> <li>A vector with magnitude equal to one</li> <li>The first vector in a basis</li> <li>A vector pointing in the positive x-direction</li> </ol> Show Answer <p>The correct answer is B. A unit vector is a vector with magnitude (norm) equal to one. Any non-zero vector can be converted to a unit vector by dividing it by its magnitude: \\(\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\). Unit vectors indicate direction without magnitude.</p> <p>Concept Tested: Unit Vector</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#6-what-is-a-linear-combination-of-vectors","title":"6. What is a linear combination of vectors?","text":"<ol> <li>Adding vectors without any scaling</li> <li>Multiplying all vectors together</li> <li>Scaling vectors by scalars and then adding them</li> <li>Taking the dot product of multiple vectors</li> </ol> Show Answer <p>The correct answer is C. A linear combination of vectors involves scaling each vector by a scalar coefficient and then summing the results: \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\). This is a fundamental operation in linear algebra used to create new vectors from existing ones.</p> <p>Concept Tested: Linear Combination</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#7-a-set-of-vectors-is-linearly-independent-if","title":"7. A set of vectors is linearly independent if:","text":"<ol> <li>All vectors are orthogonal to each other</li> <li>No vector can be written as a linear combination of the others</li> <li>All vectors have the same magnitude</li> <li>The vectors point in different directions</li> </ol> Show Answer <p>The correct answer is B. Vectors are linearly independent if no vector in the set can be expressed as a linear combination of the others. Equivalently, the only solution to \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n = \\mathbf{0}\\) is \\(c_1 = c_2 = \\cdots = c_n = 0\\).</p> <p>Concept Tested: Linear Independence</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#8-given-vectors-mathbfa-3-4-and-mathbfb-1-2-what-is-mathbfa-cdot-mathbfb","title":"8. Given vectors \\(\\mathbf{a} = [3, 4]\\) and \\(\\mathbf{b} = [1, 2]\\), what is \\(\\mathbf{a} \\cdot \\mathbf{b}\\)?","text":"<ol> <li>7</li> <li>10</li> <li>11</li> <li>14</li> </ol> Show Answer <p>The correct answer is C. The dot product is computed as \\(\\mathbf{a} \\cdot \\mathbf{b} = (3)(1) + (4)(2) = 3 + 8 = 11\\). The dot product sums the products of corresponding components.</p> <p>Concept Tested: Dot Product</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#9-the-span-of-a-set-of-vectors-is","title":"9. The span of a set of vectors is:","text":"<ol> <li>The largest vector in the set</li> <li>The set of all possible linear combinations of those vectors</li> <li>The number of vectors in the set</li> <li>The sum of all vector magnitudes</li> </ol> Show Answer <p>The correct answer is B. The span of a set of vectors is the set of all vectors that can be created through linear combinations of those vectors. It represents all points reachable by scaling and adding the original vectors.</p> <p>Concept Tested: Span</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#10-what-is-the-dimension-of-a-vector-space","title":"10. What is the dimension of a vector space?","text":"<ol> <li>The largest component value among all vectors</li> <li>The number of vectors in the space</li> <li>The number of vectors in a basis for the space</li> <li>The magnitude of the longest vector</li> </ol> Show Answer <p>The correct answer is C. The dimension of a vector space is the number of vectors in any basis for that space. All bases of a given vector space have the same number of vectors, which defines the dimension. For example, \\(\\mathbb{R}^3\\) has dimension 3.</p> <p>Concept Tested: Dimension</p>"},{"location":"chapters/02-matrices-and-matrix-operations/","title":"Matrices and Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#summary","title":"Summary","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations. You will learn about matrix notation, various special matrix types including diagonal, triangular, symmetric, and orthogonal matrices, and master core operations like multiplication, transpose, and inverse. These concepts form the computational backbone of all linear algebra applications.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"chapters/02-matrices-and-matrix-operations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#introduction-to-matrices","title":"Introduction to Matrices","text":"<p>In the previous chapter, we explored vectors as the fundamental objects of linear algebra\u2014ordered lists of numbers representing points or directions in space. Now we extend this foundation to matrices, rectangular arrays of numbers that organize multiple vectors into a single mathematical object. Matrices are ubiquitous in modern computing: they represent images as pixel grids, encode neural network weights, store graph adjacency relationships, and transform coordinates in computer graphics.</p> <p>The matrix perspective transforms our understanding of linear systems. Rather than thinking of equations individually, we can represent entire systems compactly and manipulate them using matrix operations. This algebraic framework enables efficient computation on modern hardware, where matrix operations are highly optimized through libraries like NumPy, TensorFlow, and PyTorch.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#what-is-a-matrix","title":"What is a Matrix?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. We denote matrices with bold uppercase letters such as \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), or \\(\\mathbf{M}\\). The numbers within a matrix are called entries or elements.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-definition","title":"Matrix Definition","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(a_{ij}\\) is the entry in row \\(i\\) and column \\(j\\)</li> <li>\\(m\\) is the number of rows</li> <li>\\(n\\) is the number of columns</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-notation-and-dimensions","title":"Matrix Notation and Dimensions","text":"<p>The dimensions of a matrix describe its shape. An \\(m \\times n\\) matrix (read \"m by n\") has \\(m\\) rows and \\(n\\) columns. The first number always indicates rows, and the second indicates columns.</p> Dimensions Description Example Use Case \\(3 \\times 3\\) Square matrix Rotation in 3D \\(28 \\times 28\\) Square matrix MNIST digit image \\(m \\times n\\) Rectangular Data matrix with \\(m\\) samples, \\(n\\) features \\(1 \\times n\\) Row vector Single observation \\(m \\times 1\\) Column vector Single feature across samples <p>Using standard matrix notation, we refer to individual entries with subscripts. The entry \\(a_{ij}\\) (or \\(A_{ij}\\)) denotes the element in row \\(i\\) and column \\(j\\). This indexing convention\u2014row first, column second\u2014is consistent across mathematics and most programming languages.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#row-and-column-vectors","title":"Row and Column Vectors","text":"<p>Matrices with only one row or one column receive special names. A row vector is a \\(1 \\times n\\) matrix containing \\(n\\) elements arranged horizontally:</p> <p>\\(\\mathbf{r} = \\begin{bmatrix} r_1 &amp; r_2 &amp; \\cdots &amp; r_n \\end{bmatrix}\\)</p> <p>A column vector is an \\(m \\times 1\\) matrix containing \\(m\\) elements arranged vertically:</p> <p>\\(\\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix}\\)</p> <p>In machine learning contexts, column vectors typically represent individual data points or feature vectors, while row vectors represent observations in a data matrix. This distinction matters because matrix multiplication requires compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-row-and-column-vector-visualization","title":"Diagram: Row and Column Vector Visualization","text":"Row and Column Vector Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: compare, contrast</p> <p>Learning Objective: Help students visually distinguish between row vectors (horizontal) and column vectors (vertical), understanding how their orientation affects matrix operations.</p> <p>Canvas layout: - Main drawing area showing both vector types side by side - Controls below for adjusting vector dimensions</p> <p>Visual elements: - Left side: A row vector displayed horizontally with labeled entries - Right side: A column vector displayed vertically with labeled entries - Color coding: row vector in blue, column vector in green - Grid background showing the row/column structure - Dimension labels showing \"1 \u00d7 n\" for row and \"m \u00d7 1\" for column</p> <p>Interactive controls: - Slider: Number of elements (2-6) - Toggle: Show/hide dimension annotations - Button: Randomize values</p> <p>Default parameters: - Elements: 4 - Values: random integers 1-9</p> <p>Behavior: - Adjusting element count updates both vectors simultaneously - Dimension labels update dynamically - Values displayed inside each cell</p> <p>Implementation: p5.js</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-entries-and-indexing","title":"Matrix Entries and Indexing","text":"<p>Each matrix entry \\(a_{ij}\\) occupies a specific position determined by its row index \\(i\\) and column index \\(j\\). Understanding this indexing system is essential for implementing matrix algorithms and interpreting matrix operations.</p> <p>Consider a \\(3 \\times 4\\) matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\end{bmatrix}\\)</p> <p>In this example:</p> <ul> <li>\\(a_{11} = 1\\) (first row, first column)</li> <li>\\(a_{23} = 7\\) (second row, third column)</li> <li>\\(a_{32} = 10\\) (third row, second column)</li> </ul> <p>Zero-Based vs One-Based Indexing</p> <p>Mathematical notation uses one-based indexing (starting from 1), while programming languages like Python and JavaScript use zero-based indexing (starting from 0). In NumPy, accessing element \\(a_{23}\\) requires <code>A[1, 2]</code>.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#viewing-matrices-as-collections-of-vectors","title":"Viewing Matrices as Collections of Vectors","text":"<p>A powerful perspective views matrices as collections of vectors. An \\(m \\times n\\) matrix can be interpreted as:</p> <ul> <li>\\(n\\) column vectors of dimension \\(m\\), or</li> <li>\\(m\\) row vectors of dimension \\(n\\)</li> </ul> <p>This dual interpretation underlies many matrix operations. When we multiply a matrix by a vector, we're computing a linear combination of the matrix's column vectors. When we compute the transpose, we're swapping between row and column interpretations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#basic-matrix-operations","title":"Basic Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition","title":"Matrix Addition","text":"<p>Matrix addition combines two matrices of identical dimensions by adding corresponding entries. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) matrices, their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is also an \\(m \\times n\\) matrix where each entry is:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition-formula","title":"Matrix Addition Formula","text":"<p>\\(c_{ij} = a_{ij} + b_{ij}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in the sum matrix</li> <li>\\(a_{ij}\\) is the corresponding entry in \\(\\mathbf{A}\\)</li> <li>\\(b_{ij}\\) is the corresponding entry in \\(\\mathbf{B}\\)</li> </ul> <p>Matrix addition is both commutative (\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)) and associative (\\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\)).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Matrix scalar multiplication multiplies every entry of a matrix by a single number (scalar). If \\(k\\) is a scalar and \\(\\mathbf{A}\\) is a matrix, then \\(k\\mathbf{A}\\) has entries:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication-formula","title":"Scalar Multiplication Formula","text":"<p>\\((k\\mathbf{A})_{ij} = k \\cdot a_{ij}\\)</p> <p>where:</p> <ul> <li>\\(k\\) is the scalar multiplier</li> <li>\\(a_{ij}\\) is the original matrix entry</li> </ul> <p>Scalar multiplication scales all entries uniformly. In neural networks, this operation appears when applying learning rates to gradient matrices during backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-addition-and-scalar-multiplication","title":"Diagram: Matrix Addition and Scalar Multiplication","text":"Matrix Addition and Scalar Multiplication Interactive <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to practice matrix addition and scalar multiplication interactively, reinforcing the element-wise nature of these operations.</p> <p>Canvas layout: - Top section: Two input matrices A and B displayed side by side - Middle: Result matrix C with operation indicator - Bottom: Control panel</p> <p>Visual elements: - 3\u00d73 matrices displayed as grids with editable cells - Color highlighting showing corresponding entries during addition - Animation showing values flowing into result matrix - Operation symbol (+, \u00d7) displayed between matrices</p> <p>Interactive controls: - Radio buttons: Select operation (Addition / Scalar Multiply) - Slider: Scalar value k (for scalar multiplication, range -3 to 3) - Button: Randomize matrices - Button: Step through calculation - Toggle: Show/hide calculation details</p> <p>Default parameters: - Matrix size: 3\u00d73 - Operation: Addition - Scalar: 2 - Values: small integers (-5 to 5)</p> <p>Behavior: - Clicking a cell allows value editing - Result updates in real-time as inputs change - Step mode highlights each entry calculation sequentially - Animation shows entry-by-entry computation</p> <p>Implementation: p5.js with editable input fields</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication is the most important and nuanced matrix operation. Unlike addition, matrix multiplication has specific dimension requirements and is not commutative.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product","title":"Matrix-Vector Product","text":"<p>The matrix-vector product multiplies an \\(m \\times n\\) matrix by an \\(n \\times 1\\) column vector, producing an \\(m \\times 1\\) column vector. This operation represents applying a linear transformation to a vector.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product-formula","title":"Matrix-Vector Product Formula","text":"<p>\\(\\mathbf{y} = \\mathbf{A}\\mathbf{x}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix</li> <li>\\(\\mathbf{x}\\) is an \\(n \\times 1\\) column vector</li> <li>\\(\\mathbf{y}\\) is the resulting \\(m \\times 1\\) column vector</li> </ul> <p>Each entry of the result is the dot product of a row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\):</p> <p>\\(y_i = \\sum_{j=1}^{n} a_{ij} x_j = a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n\\)</p> <p>Alternatively, the matrix-vector product can be viewed as a linear combination of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\mathbf{A}\\mathbf{x} = x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n\\)</p> <p>where \\(\\mathbf{a}_j\\) denotes the \\(j\\)-th column of \\(\\mathbf{A}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-matrix-multiplication","title":"Matrix-Matrix Multiplication","text":"<p>Matrix multiplication extends the matrix-vector product. The product of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B}\\) is an \\(m \\times p\\) matrix \\(\\mathbf{C}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication-formula","title":"Matrix Multiplication Formula","text":"<p>\\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in row \\(i\\), column \\(j\\) of the product</li> <li>The sum runs over the shared dimension \\(n\\)</li> <li>Each entry requires \\(n\\) multiplications and \\(n-1\\) additions</li> </ul> <p>The dimension compatibility rule states: the number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\).</p> Matrix A Matrix B Product C Valid? \\(2 \\times 3\\) \\(3 \\times 4\\) \\(2 \\times 4\\) Yes \\(3 \\times 2\\) \\(3 \\times 4\\) \u2014 No \\(4 \\times 4\\) \\(4 \\times 4\\) \\(4 \\times 4\\) Yes \\(1 \\times 5\\) \\(5 \\times 1\\) \\(1 \\times 1\\) Yes (scalar) <p>Matrix Multiplication is Not Commutative</p> <p>In general, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). In fact, if \\(\\mathbf{A}\\mathbf{B}\\) exists, \\(\\mathbf{B}\\mathbf{A}\\) may not even be defined (different dimensions). Even for square matrices where both products exist, they typically differ.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-multiplication-visualizer","title":"Diagram: Matrix Multiplication Visualizer","text":"Matrix Multiplication Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand the row-by-column computation process in matrix multiplication by animating each entry calculation.</p> <p>Canvas layout: - Left: Matrix A (highlighted rows) - Center: Matrix B (highlighted columns) - Right: Result matrix C (entries fill in as computed) - Bottom: Control panel and calculation display</p> <p>Visual elements: - Matrix A with current row highlighted in blue - Matrix B with current column highlighted in green - Result matrix C with current entry position highlighted in yellow - Dot product calculation shown step by step below matrices - Running sum displayed during computation</p> <p>Interactive controls: - Dropdown: Matrix A dimensions (2\u00d72, 2\u00d73, 3\u00d72, 3\u00d73) - Dropdown: Matrix B dimensions (matching first dimension of result) - Button: \"Next Entry\" - advance to next calculation - Button: \"Auto Play\" - animate all calculations - Slider: Animation speed (200ms - 2000ms per step) - Button: Reset - Toggle: Show column interpretation (linear combination view)</p> <p>Default parameters: - Matrix A: 2\u00d73 - Matrix B: 3\u00d72 - Animation speed: 800ms - Values: small integers (1-5)</p> <p>Behavior: - Highlight corresponding row of A and column of B - Show element-wise multiplication with + signs - Display running sum as computation proceeds - Fill in result entry when complete - Move to next entry automatically or on button click - Column interpretation mode shows result as linear combination</p> <p>Implementation: p5.js with animation states here</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-multiplication","title":"Properties of Matrix Multiplication","text":"<p>Matrix multiplication satisfies several important algebraic properties:</p> <ul> <li>Associativity: \\((\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\)</li> <li>Distributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\)</li> <li>Scalar compatibility: \\(k(\\mathbf{A}\\mathbf{B}) = (k\\mathbf{A})\\mathbf{B} = \\mathbf{A}(k\\mathbf{B})\\)</li> <li>Non-commutativity: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\) in general</li> </ul> <p>These properties enable powerful algebraic manipulations while requiring careful attention to the order of operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#the-matrix-transpose","title":"The Matrix Transpose","text":"<p>The matrix transpose operation flips a matrix over its diagonal, converting rows to columns and vice versa. For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-definition","title":"Transpose Definition","text":"<p>\\((\\mathbf{A}^T)_{ij} = a_{ji}\\)</p> <p>where:</p> <ul> <li>The entry in row \\(i\\), column \\(j\\) of \\(\\mathbf{A}^T\\) equals the entry in row \\(j\\), column \\(i\\) of \\(\\mathbf{A}\\)</li> </ul> <p>For example:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix}\\)</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-properties","title":"Transpose Properties","text":"<p>The transpose operation satisfies these properties:</p> <ul> <li>\\((\\mathbf{A}^T)^T = \\mathbf{A}\\) (double transpose returns original)</li> <li>\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\) (transpose distributes over addition)</li> <li>\\((k\\mathbf{A})^T = k\\mathbf{A}^T\\) (scalars pass through)</li> <li>\\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) (order reverses for products!)</li> </ul> <p>The last property is particularly important: when transposing a product, the order of the matrices reverses. This \"reverse order law\" appears frequently in derivations involving neural network backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#special-matrix-types","title":"Special Matrix Types","text":"<p>Many matrices have special structures that simplify computation or carry geometric meaning. Recognizing these types enables algorithmic optimizations and deeper understanding.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#identity-matrix","title":"Identity Matrix","text":"<p>The identity matrix \\(\\mathbf{I}_n\\) is the multiplicative identity for \\(n \\times n\\) matrices. It has ones on the diagonal and zeros elsewhere:</p> <p>\\(\\mathbf{I}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>For any matrix \\(\\mathbf{A}\\) with compatible dimensions:</p> <p>\\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\)</p> <p>The identity matrix represents the \"do nothing\" transformation\u2014it maps every vector to itself.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A diagonal matrix has nonzero entries only on its main diagonal. All off-diagonal entries are zero:</p> <p>\\(\\mathbf{D} = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 \\\\ 0 &amp; d_2 &amp; 0 \\\\ 0 &amp; 0 &amp; d_3 \\end{bmatrix}\\)</p> <p>Diagonal matrices have special computational properties:</p> <ul> <li>Multiplication: Multiplying by a diagonal matrix scales rows or columns</li> <li>Powers: \\(\\mathbf{D}^k\\) has entries \\(d_i^k\\)</li> <li>Inverse: \\(\\mathbf{D}^{-1}\\) has entries \\(1/d_i\\) (if all \\(d_i \\neq 0\\))</li> </ul> <p>In machine learning, diagonal matrices appear in batch normalization and as covariance matrices for independent features.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#triangular-matrices","title":"Triangular Matrices","text":"<p>A triangular matrix has zeros on one side of the diagonal.</p> <p>An upper triangular matrix has zeros below the diagonal:</p> <p>\\(\\mathbf{U} = \\begin{bmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\\\ 0 &amp; u_{22} &amp; u_{23} \\\\ 0 &amp; 0 &amp; u_{33} \\end{bmatrix}\\)</p> <p>A lower triangular matrix has zeros above the diagonal:</p> <p>\\(\\mathbf{L} = \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 \\\\ l_{21} &amp; l_{22} &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; l_{33} \\end{bmatrix}\\)</p> <p>Triangular matrices are computationally advantageous:</p> <ul> <li>Solving \\(\\mathbf{L}\\mathbf{x} = \\mathbf{b}\\) uses forward substitution (start from top)</li> <li>Solving \\(\\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) uses back substitution (start from bottom)</li> <li>Both require only \\(O(n^2)\\) operations instead of \\(O(n^3)\\)</li> </ul> <p>LU decomposition factors any matrix into lower and upper triangular components, enabling efficient system solving.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-special-matrix-types-gallery","title":"Diagram: Special Matrix Types Gallery","text":"Special Matrix Types Interactive Gallery <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize</p> <p>Learning Objective: Help students visually identify and distinguish between different special matrix types (identity, diagonal, upper/lower triangular) by their structural patterns.</p> <p>Layout: Grid of matrix visualizations with interactive selection</p> <p>Visual elements: - 4\u00d74 grid showing four matrix types simultaneously - Each matrix displayed as a colored grid:   - Identity: ones on diagonal (gold), zeros elsewhere (light gray)   - Diagonal: colored diagonal entries, zeros elsewhere   - Upper triangular: colored upper triangle including diagonal, zeros below   - Lower triangular: colored lower triangle including diagonal, zeros above - Structural pattern highlighted with shading - Labels below each matrix type</p> <p>Interactive features: - Click on a matrix type to enlarge and see detailed properties - Hover over entries to see position (i,j) and value - Toggle: Show/hide zero entries - Slider: Matrix size (3\u00d73 to 6\u00d76) - Button: Show random example values</p> <p>Information panels (on click): - Definition of the matrix type - Key properties - Computational advantages - Common applications</p> <p>Color scheme: - Non-zero entries: blue gradient based on value - Zero entries: light gray or hidden - Diagonal: highlighted in gold - Structural regions: subtle background shading</p> <p>Implementation: HTML/CSS/JavaScript with SVG or Canvas</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#symmetric-matrix","title":"Symmetric Matrix","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-symmetric-matrix","title":"Diagram: Symmetric Matrix","text":"<p>A symmetric matrix equals its own transpose: \\(\\mathbf{A} = \\mathbf{A}^T\\). This means \\(a_{ij} = a_{ji}\\) for all entries\u2014the matrix is mirror-symmetric across the diagonal.</p> <p>\\(\\mathbf{S} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{bmatrix}\\)</p> <p>Symmetric matrices arise naturally in many contexts:</p> <ul> <li>Covariance matrices in statistics</li> <li>Adjacency matrices for undirected graphs</li> <li>Hessian matrices (second derivatives) in optimization</li> <li>Gram matrices \\(\\mathbf{A}^T\\mathbf{A}\\) from any matrix \\(\\mathbf{A}\\)</li> </ul> <p>Symmetric matrices have remarkable spectral properties: they always have real eigenvalues and orthogonal eigenvectors, enabling powerful decomposition methods.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>An orthogonal matrix \\(\\mathbf{Q}\\) satisfies \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I}\\). Equivalently, its inverse equals its transpose: \\(\\mathbf{Q}^{-1} = \\mathbf{Q}^T\\).</p> <p>The columns of an orthogonal matrix form an orthonormal set\u2014they are mutually perpendicular unit vectors. Orthogonal matrices represent rotations and reflections that preserve lengths and angles.</p> <p>Properties of orthogonal matrices:</p> <ul> <li>Length preservation: \\(\\|\\mathbf{Q}\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) for all vectors \\(\\mathbf{x}\\)</li> <li>Angle preservation: The angle between \\(\\mathbf{Q}\\mathbf{x}\\) and \\(\\mathbf{Q}\\mathbf{y}\\) equals the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)</li> <li>Determinant: \\(\\det(\\mathbf{Q}) = \\pm 1\\) (rotation if \\(+1\\), reflection if \\(-1\\))</li> <li>Easy inversion: Computing \\(\\mathbf{Q}^{-1}\\) requires only a transpose</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-orthogonal-matrix-transformation","title":"Diagram: Orthogonal Matrix Transformation","text":"Orthogonal Matrix Transformation Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Demonstrate that orthogonal matrices preserve lengths and angles by visualizing how rotation matrices transform shapes without distortion.</p> <p>Canvas layout: - Main area: 2D coordinate plane with original and transformed shapes - Right panel: Matrix display and controls</p> <p>Visual elements: - Coordinate grid with x and y axes - Original shape (unit square or set of vectors) in blue - Transformed shape in red (semi-transparent overlay) - Length indicators showing preservation - Angle arc showing angle preservation between vectors - 2\u00d72 rotation matrix displayed with current angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (0\u00b0 to 360\u00b0) - Checkbox: Show length comparison - Checkbox: Show angle comparison - Dropdown: Shape to transform (square, triangle, circle of vectors) - Button: Add reflection (multiply by reflection matrix) - Display: Matrix values updating with angle</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: unit square - Show lengths: true - Show angles: false</p> <p>Behavior: - Shape smoothly rotates as angle slider changes - Length indicators show |Qx| = |x| - Angle arcs update to show angle preservation - Matrix entries update: cos(\u03b8), -sin(\u03b8), sin(\u03b8), cos(\u03b8) - Reflection button flips across an axis</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse","title":"Matrix Inverse","text":"<p>The matrix inverse generalizes division to matrices. For a square matrix \\(\\mathbf{A}\\), its inverse \\(\\mathbf{A}^{-1}\\) (if it exists) satisfies:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse-definition","title":"Matrix Inverse Definition","text":"<p>\\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(n \\times n\\) square matrix</li> <li>\\(\\mathbf{A}^{-1}\\) is the inverse matrix</li> <li>\\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#invertible-matrices","title":"Invertible Matrices","text":"<p>A matrix is invertible (also called nonsingular or non-degenerate) if its inverse exists. Not all matrices are invertible\u2014those without inverses are called singular.</p> <p>Conditions for invertibility (all equivalent):</p> <ul> <li>\\(\\mathbf{A}^{-1}\\) exists</li> <li>\\(\\det(\\mathbf{A}) \\neq 0\\)</li> <li>The columns of \\(\\mathbf{A}\\) are linearly independent</li> <li>The rows of \\(\\mathbf{A}\\) are linearly independent</li> <li>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\)</li> <li>\\(\\mathbf{A}\\) has full rank</li> </ul> <p>For a 2\u00d72 matrix, the inverse has a closed form:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#22-matrix-inverse-formula","title":"2\u00d72 Matrix Inverse Formula","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\implies \\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(ad - bc\\) is the determinant of \\(\\mathbf{A}\\)</li> <li>The inverse exists only if \\(ad - bc \\neq 0\\)</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-inverse","title":"Properties of Matrix Inverse","text":"<p>When inverses exist, they satisfy these properties:</p> <ul> <li>\\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\)</li> <li>\\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\)</li> <li>\\((k\\mathbf{A})^{-1} = \\frac{1}{k}\\mathbf{A}^{-1}\\) for \\(k \\neq 0\\)</li> <li>\\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) (order reverses!)</li> </ul> <p>The inverse enables solving linear systems: if \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), then \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\).</p> <p>Computational Practice</p> <p>While mathematically elegant, computing \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly is usually avoided in practice. Instead, we solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) directly using methods like LU decomposition, which are more numerically stable and efficient.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-inverse-explorer","title":"Diagram: Matrix Inverse Explorer","text":"Matrix Inverse Interactive Explorer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to explore matrix inversion for 2\u00d72 and 3\u00d73 matrices, verify the inverse property AA\u207b\u00b9 = I, and understand when matrices are singular.</p> <p>Canvas layout: - Left: Input matrix A (editable) - Center: Computed inverse A\u207b\u00b9 (or \"Singular\" warning) - Right: Verification showing AA\u207b\u00b9 = I - Bottom: Controls and determinant display</p> <p>Visual elements: - Editable matrix cells with color-coded entries - Determinant value prominently displayed - Color indicator: green for invertible, red for singular/near-singular - Verification matrix showing identity (or near-identity for numerical precision) - Warning icon when determinant is near zero</p> <p>Interactive controls: - Matrix size toggle: 2\u00d72 / 3\u00d73 - Editable cells for matrix entries - Button: Randomize matrix - Button: Make singular (set det = 0) - Slider: Approach singularity (smoothly varies toward singular) - Toggle: Show calculation steps</p> <p>Default parameters: - Size: 2\u00d72 - Initial matrix: [[2, 1], [1, 1]] (invertible)</p> <p>Behavior: - Real-time inverse computation as entries change - Determinant updates continuously - Verification matrix computes A \u00d7 A\u207b\u00b9 - Near-singular matrices show numerical instability - Step-by-step mode shows cofactor/adjugate method for 2\u00d72</p> <p>Implementation: p5.js with matrix computation library</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#sparse-and-dense-matrices","title":"Sparse and Dense Matrices","text":"<p>Matrices are classified by their distribution of zero and nonzero entries.</p> <p>A dense matrix has few zero entries relative to its total size. Most entries contain meaningful nonzero values. Dense matrices require \\(O(mn)\\) storage and \\(O(mn)\\) operations for most computations.</p> <p>A sparse matrix has many zero entries\u2014typically the number of nonzero entries is \\(O(n)\\) or \\(O(n \\log n)\\) rather than \\(O(n^2)\\) for an \\(n \\times n\\) matrix. Sparse matrices arise naturally in:</p> <ul> <li>Graph adjacency matrices: Most nodes connect to few others</li> <li>Document-term matrices: Each document uses few of all possible words</li> <li>Finite element methods: Local interactions produce banded structures</li> </ul> Property Dense Matrix Sparse Matrix Storage \\(O(mn)\\) \\(O(\\text{nnz})\\) Zero entries Few Many (typically &gt;90%) Storage format 2D array CSR, CSC, COO Multiplication Standard algorithms Specialized sparse algorithms Examples Covariance matrices Adjacency matrices <p>where \\(\\text{nnz}\\) denotes the number of nonzero entries.</p> <p>Sparse matrix formats like Compressed Sparse Row (CSR) store only nonzero values along with their positions, dramatically reducing memory requirements and enabling faster operations by skipping zero computations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-sparse-vs-dense-matrix-visualization","title":"Diagram: Sparse vs Dense Matrix Visualization","text":"Sparse vs Dense Matrix Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Help students understand the structural difference between sparse and dense matrices and appreciate why sparsity enables computational efficiency.</p> <p>Layout: Side-by-side matrix visualizations with statistics</p> <p>Visual elements: - Left panel: Dense matrix (most cells filled with color) - Right panel: Sparse matrix (few colored cells, most white/gray) - Color intensity indicates value magnitude - Zero entries shown as white or light gray - Statistics overlay:   - Total entries   - Nonzero entries   - Sparsity percentage   - Memory comparison (dense vs sparse storage)</p> <p>Interactive features: - Slider: Matrix size (10\u00d710 to 100\u00d7100) - Slider: Sparsity level (10% to 99% zeros) - Dropdown: Sparse pattern (random, diagonal, banded, block) - Toggle: Show storage comparison bar chart - Hover: Show value at position</p> <p>Example data: - Dense: Random values in most cells - Sparse: Graph adjacency pattern or banded structure</p> <p>Statistics display: - Memory ratio: \"Sparse uses X% of dense storage\" - Multiplication speedup estimate</p> <p>Implementation: HTML Canvas or p5.js with efficient rendering</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrices","title":"Block Matrices","text":"<p>A block matrix (or partitioned matrix) is a matrix viewed as an array of smaller matrices called blocks or submatrices. Block structure often reflects natural problem decomposition.</p> <p>\\(\\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), \\(\\mathbf{D}\\) are matrices of compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrix-operations","title":"Block Matrix Operations","text":"<p>Block matrices support \"block-wise\" operations that mirror scalar operations:</p> <p>Block Addition: If two matrices have the same block structure:</p> <p>\\(\\begin{bmatrix} \\mathbf{A}_1 &amp; \\mathbf{B}_1 \\\\ \\mathbf{C}_1 &amp; \\mathbf{D}_1 \\end{bmatrix} + \\begin{bmatrix} \\mathbf{A}_2 &amp; \\mathbf{B}_2 \\\\ \\mathbf{C}_2 &amp; \\mathbf{D}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_1 + \\mathbf{A}_2 &amp; \\mathbf{B}_1 + \\mathbf{B}_2 \\\\ \\mathbf{C}_1 + \\mathbf{C}_2 &amp; \\mathbf{D}_1 + \\mathbf{D}_2 \\end{bmatrix}\\)</p> <p>Block Multiplication: With compatible block dimensions:</p> <p>\\(\\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\begin{bmatrix} \\mathbf{E} \\\\ \\mathbf{F} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}\\mathbf{E} + \\mathbf{B}\\mathbf{F} \\\\ \\mathbf{C}\\mathbf{E} + \\mathbf{D}\\mathbf{F} \\end{bmatrix}\\)</p> <p>Block structure enables:</p> <ul> <li>Parallel computation on independent blocks</li> <li>Efficient algorithms exploiting structure</li> <li>Conceptual clarity in complex systems</li> <li>Memory-efficient storage when blocks have special properties</li> </ul> <p>In deep learning, weight matrices are often organized in blocks corresponding to different layers or attention heads.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-block-matrix-structure","title":"Diagram: Block Matrix Structure","text":"Block Matrix Partitioning Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, classify</p> <p>Learning Objective: Help students understand how matrices can be partitioned into blocks and how block operations mirror element-wise operations at a higher level.</p> <p>Canvas layout: - Main area: Large matrix with adjustable partition lines - Right panel: Block extraction and operation display</p> <p>Visual elements: - 8\u00d78 matrix displayed as a grid - Draggable horizontal and vertical partition lines - Distinct colors for each block region - Block labels (A, B, C, D, etc.) overlaid on regions - Extracted blocks shown separately on right</p> <p>Interactive controls: - Drag partition lines to resize blocks - Dropdown: Example partitioning (2\u00d72 blocks, row partition, column partition) - Toggle: Show block dimensions - Button: Demonstrate block multiplication - Checkbox: Show compatibility requirements</p> <p>Default parameters: - Matrix size: 8\u00d78 - Initial partition: 4\u00d74 blocks (2\u00d72 block structure) - Values: integers 1-9</p> <p>Behavior: - Partition lines snap to integer positions - Block colors update as partitioning changes - Dimension labels update dynamically - Block multiplication demo shows step-by-step block operations</p> <p>Implementation: p5.js with draggable elements</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#applications-in-machine-learning-and-ai","title":"Applications in Machine Learning and AI","text":"<p>The matrix concepts covered in this chapter form the computational foundation of modern machine learning systems.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#data-representation","title":"Data Representation","text":"<p>Machine learning datasets are naturally represented as matrices:</p> <ul> <li>Design matrix \\(\\mathbf{X}\\): Each row is a sample, each column is a feature</li> <li>Weight matrix \\(\\mathbf{W}\\): Neural network parameters connecting layers</li> <li>Embedding matrix \\(\\mathbf{E}\\): Word or token embeddings in NLP</li> </ul> <p>For a dataset with \\(m\\) samples and \\(n\\) features, the design matrix is \\(m \\times n\\). Feature scaling, normalization, and transformation all use matrix operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#neural-network-layers","title":"Neural Network Layers","text":"<p>A fully connected neural network layer performs:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{W}\\) is the weight matrix</li> <li>\\(\\mathbf{x}\\) is the input vector</li> <li>\\(\\mathbf{b}\\) is the bias vector</li> <li>\\(\\sigma\\) is a nonlinear activation function</li> </ul> <p>Batch processing extends this to:</p> <p>\\(\\mathbf{H} = \\sigma(\\mathbf{X}\\mathbf{W}^T + \\mathbf{b})\\)</p> <p>where \\(\\mathbf{X}\\) contains multiple input vectors as rows, enabling parallel computation of many samples.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>Transformer models compute attention using three matrices:</p> <ul> <li>Query matrix \\(\\mathbf{Q}\\): What we're looking for</li> <li>Key matrix \\(\\mathbf{K}\\): What's available to match</li> <li>Value matrix \\(\\mathbf{V}\\): What we retrieve</li> </ul> <p>The attention computation:</p> <p>\\(\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\\)</p> <p>relies entirely on matrix multiplications and transposes covered in this chapter.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-neural-network-layer-matrix-operations","title":"Diagram: Neural Network Layer Matrix Operations","text":"Neural Network Layer Forward Pass Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: implement, demonstrate</p> <p>Learning Objective: Show how matrix-vector multiplication implements a neural network layer, connecting abstract matrix operations to concrete deep learning computations.</p> <p>Canvas layout: - Left: Input vector visualization - Center: Weight matrix with connection lines - Right: Output vector (pre and post activation) - Bottom: Controls and formula display</p> <p>Visual elements: - Input neurons (circles) with values - Weight matrix displayed as a heatmap - Connection lines from inputs to outputs (thickness = weight magnitude) - Output neurons showing weighted sums - Activation function visualization (ReLU, sigmoid curve) - Formula: h = \u03c3(Wx + b) displayed with current values</p> <p>Interactive controls: - Slider: Number of inputs (2-6) - Slider: Number of outputs (2-6) - Dropdown: Activation function (none, ReLU, sigmoid, tanh) - Button: Randomize weights - Button: Randomize input - Toggle: Show bias term - Toggle: Show matrix multiplication step-by-step</p> <p>Default parameters: - Inputs: 3 - Outputs: 2 - Activation: ReLU - Random weights in [-1, 1] - Random inputs in [0, 1]</p> <p>Behavior: - Changing input values updates outputs in real-time - Connection line colors indicate positive (blue) vs negative (red) weights - Step-by-step mode highlights each row-vector dot product - Activation function applied visually to each output</p> <p>Implementation: p5.js with animation</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter introduced matrices as the fundamental data structures of linear algebra with broad applications in AI and machine learning.</p> <p>Core Concepts:</p> <ul> <li>A matrix is a rectangular array of numbers with dimensions \\(m \\times n\\)</li> <li>Matrix entries are indexed by row and column: \\(a_{ij}\\)</li> <li>Row vectors (\\(1 \\times n\\)) and column vectors (\\(m \\times 1\\)) are special cases</li> </ul> <p>Fundamental Operations:</p> <ul> <li>Matrix addition adds corresponding entries (requires same dimensions)</li> <li>Scalar multiplication scales all entries by a constant</li> <li>Matrix-vector product \\(\\mathbf{A}\\mathbf{x}\\) produces a linear combination of columns</li> <li>Matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) requires matching inner dimensions</li> <li>Transpose \\(\\mathbf{A}^T\\) flips rows and columns</li> </ul> <p>Special Matrix Types:</p> <ul> <li>Identity matrix \\(\\mathbf{I}\\): multiplicative identity</li> <li>Diagonal matrix: nonzeros only on diagonal</li> <li>Triangular matrices: zeros above or below diagonal</li> <li>Symmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)</li> <li>Orthogonal matrix: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\) (preserves lengths/angles)</li> </ul> <p>Inverse and Structure:</p> <ul> <li>Matrix inverse \\(\\mathbf{A}^{-1}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\)</li> <li>Invertible matrices have nonzero determinant and linearly independent columns</li> <li>Sparse matrices have few nonzero entries; dense matrices have many</li> <li>Block matrices partition into submatrices for structured computation</li> </ul> <p>Key Properties to Remember:</p> <ul> <li>Matrix multiplication is NOT commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)</li> <li>Product transpose reverses order: \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\)</li> <li>Inverse product reverses order: \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)</li> </ul> <p>These operations form the computational vocabulary of machine learning, from basic linear regression to advanced transformer architectures.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#exercises","title":"Exercises","text":"Exercise 1: Matrix Dimensions <p>Given matrices \\(\\mathbf{A}\\) (3\u00d74), \\(\\mathbf{B}\\) (4\u00d72), and \\(\\mathbf{C}\\) (3\u00d72), which products are defined? What are their dimensions?</p> <ul> <li>\\(\\mathbf{A}\\mathbf{B}\\)</li> <li>\\(\\mathbf{B}\\mathbf{A}\\)</li> <li>\\(\\mathbf{A}\\mathbf{C}\\)</li> <li>\\(\\mathbf{A}\\mathbf{B} + \\mathbf{C}\\)</li> </ul> Exercise 2: Transpose Properties <p>Prove that \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) for 2\u00d72 matrices by computing both sides with generic entries.</p> Exercise 3: Symmetric Matrices <p>Show that for any matrix \\(\\mathbf{A}\\), both \\(\\mathbf{A}^T\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^T\\) are symmetric matrices.</p> Exercise 4: Orthogonal Matrix Verification <p>Verify that the following matrix is orthogonal and determine if it represents a rotation or reflection:</p> <p>\\(\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix}\\)</p> Exercise 5: Block Multiplication <p>Compute the product of the following block matrices:</p> <p>\\(\\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ \\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ -\\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\) is any 2\u00d72 matrix.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/","title":"Quiz: Matrices and Matrix Operations","text":"<p>Test your understanding of matrices, matrix operations, and their properties.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#1-what-is-the-result-of-multiplying-an-m-times-n-matrix-by-an-n-times-p-matrix","title":"1. What is the result of multiplying an \\(m \\times n\\) matrix by an \\(n \\times p\\) matrix?","text":"<ol> <li>An \\(m \\times p\\) matrix</li> <li>An \\(n \\times n\\) matrix</li> <li>An \\(m \\times n\\) matrix</li> <li>A scalar value</li> </ol> Show Answer <p>The correct answer is A. When multiplying an \\(m \\times n\\) matrix by an \\(n \\times p\\) matrix, the result is an \\(m \\times p\\) matrix. The inner dimensions (\\(n\\)) must match, and the outer dimensions (\\(m\\) and \\(p\\)) determine the output size.</p> <p>Concept Tested: Matrix Multiplication</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#2-which-property-does-matrix-multiplication-not-have","title":"2. Which property does matrix multiplication NOT have?","text":"<ol> <li>Associativity: \\((AB)C = A(BC)\\)</li> <li>Distributivity: \\(A(B + C) = AB + AC\\)</li> <li>Commutativity: \\(AB = BA\\)</li> <li>Identity: \\(AI = IA = A\\)</li> </ol> Show Answer <p>The correct answer is C. Matrix multiplication is not commutative in general\u2014\\(AB \\neq BA\\) for most matrices. The order of multiplication matters because each product represents a different sequence of transformations.</p> <p>Concept Tested: Matrix Multiplication Properties</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#3-what-is-the-transpose-of-a-matrix","title":"3. What is the transpose of a matrix?","text":"<ol> <li>The matrix with all elements negated</li> <li>The matrix with rows and columns interchanged</li> <li>The inverse of the matrix</li> <li>The matrix multiplied by itself</li> </ol> Show Answer <p>The correct answer is B. The transpose of a matrix is obtained by interchanging its rows and columns. If \\(A\\) is an \\(m \\times n\\) matrix, then \\(A^T\\) is an \\(n \\times m\\) matrix where \\((A^T)_{ij} = A_{ji}\\).</p> <p>Concept Tested: Matrix Transpose</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#4-a-square-matrix-a-is-invertible-if-and-only-if","title":"4. A square matrix \\(A\\) is invertible if and only if:","text":"<ol> <li>It is symmetric</li> <li>Its determinant is non-zero</li> <li>All diagonal elements are positive</li> <li>It has more rows than columns</li> </ol> Show Answer <p>The correct answer is B. A square matrix is invertible (has an inverse) if and only if its determinant is non-zero. A zero determinant indicates the matrix is singular and maps some non-zero vectors to zero, making the transformation irreversible.</p> <p>Concept Tested: Matrix Inverse</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#5-what-is-the-identity-matrix","title":"5. What is the identity matrix?","text":"<ol> <li>A matrix with all ones</li> <li>A matrix with all zeros</li> <li>A square matrix with ones on the diagonal and zeros elsewhere</li> <li>A matrix that equals its transpose</li> </ol> Show Answer <p>The correct answer is C. The identity matrix \\(I\\) is a square matrix with ones on the main diagonal and zeros elsewhere. It is the multiplicative identity for matrices: \\(AI = IA = A\\) for any compatible matrix \\(A\\).</p> <p>Concept Tested: Identity Matrix</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#6-if-a-is-a-3-times-2-matrix-and-b-is-a-2-times-4-matrix-what-are-the-dimensions-of-ab","title":"6. If \\(A\\) is a \\(3 \\times 2\\) matrix and \\(B\\) is a \\(2 \\times 4\\) matrix, what are the dimensions of \\(AB\\)?","text":"<ol> <li>\\(2 \\times 2\\)</li> <li>\\(3 \\times 4\\)</li> <li>\\(4 \\times 3\\)</li> <li>\\(3 \\times 2\\)</li> </ol> Show Answer <p>The correct answer is B. For matrix multiplication \\(AB\\) where \\(A\\) is \\(3 \\times 2\\) and \\(B\\) is \\(2 \\times 4\\), the inner dimensions (2) match, so multiplication is valid. The result has dimensions from the outer dimensions: \\(3 \\times 4\\).</p> <p>Concept Tested: Matrix Multiplication</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#7-a-symmetric-matrix-satisfies-which-condition","title":"7. A symmetric matrix satisfies which condition?","text":"<ol> <li>\\(A = -A\\)</li> <li>\\(A = A^T\\)</li> <li>\\(A = A^{-1}\\)</li> <li>\\(A = A^2\\)</li> </ol> Show Answer <p>The correct answer is B. A symmetric matrix equals its own transpose: \\(A = A^T\\). This means \\(A_{ij} = A_{ji}\\) for all \\(i, j\\). Symmetric matrices have real eigenvalues and orthogonal eigenvectors.</p> <p>Concept Tested: Symmetric Matrix</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#8-what-does-the-rank-of-a-matrix-represent","title":"8. What does the rank of a matrix represent?","text":"<ol> <li>The number of rows in the matrix</li> <li>The number of non-zero elements</li> <li>The dimension of the column space (number of linearly independent columns)</li> <li>The determinant value</li> </ol> Show Answer <p>The correct answer is C. The rank of a matrix is the dimension of its column space, which equals the number of linearly independent columns (or equivalently, rows). Rank indicates the effective dimensionality of the transformation the matrix represents.</p> <p>Concept Tested: Matrix Rank</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#9-if-a-1-exists-what-is-a-cdot-a-1","title":"9. If \\(A^{-1}\\) exists, what is \\(A \\cdot A^{-1}\\)?","text":"<ol> <li>The zero matrix</li> <li>The identity matrix</li> <li>The transpose of \\(A\\)</li> <li>\\(A\\) squared</li> </ol> Show Answer <p>The correct answer is B. By definition, \\(A \\cdot A^{-1} = A^{-1} \\cdot A = I\\), the identity matrix. The inverse \"undoes\" the transformation performed by the original matrix.</p> <p>Concept Tested: Matrix Inverse</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#10-an-orthogonal-matrix-q-satisfies","title":"10. An orthogonal matrix \\(Q\\) satisfies:","text":"<ol> <li>\\(Q^T = Q\\)</li> <li>\\(Q^T = Q^{-1}\\)</li> <li>\\(Q^2 = I\\)</li> <li>\\(Q + Q^T = I\\)</li> </ol> Show Answer <p>The correct answer is B. An orthogonal matrix satisfies \\(Q^T = Q^{-1}\\), which means \\(Q^TQ = QQ^T = I\\). Orthogonal matrices preserve lengths and angles, representing rotations and reflections.</p> <p>Concept Tested: Orthogonal Matrix</p>"},{"location":"chapters/03-systems-of-linear-equations/","title":"Systems of Linear Equations","text":""},{"location":"chapters/03-systems-of-linear-equations/#summary","title":"Summary","text":"<p>This chapter teaches you to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields. You will master Gaussian elimination, understand row echelon forms, and learn to analyze when solutions exist, are unique, or are infinite. These computational techniques are essential for everything from optimization to neural network training.</p>"},{"location":"chapters/03-systems-of-linear-equations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"chapters/03-systems-of-linear-equations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#introduction","title":"Introduction","text":"<p>Systems of linear equations appear everywhere in science, engineering, and data analysis. Whether you're balancing chemical reactions, analyzing electrical circuits, fitting models to data, or training neural networks, you're solving linear systems. This chapter develops the systematic methods for solving these systems and\u2014equally important\u2014understanding when solutions exist and what form they take.</p> <p>The power of linear algebra lies in its ability to represent complex systems compactly and solve them efficiently. A problem that might involve dozens or thousands of equations becomes a single matrix equation, and the solution emerges through systematic elimination procedures that computers execute with remarkable speed.</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equations-and-systems","title":"Linear Equations and Systems","text":""},{"location":"chapters/03-systems-of-linear-equations/#what-is-a-linear-equation","title":"What is a Linear Equation?","text":"<p>A linear equation in variables \\(x_1, x_2, \\ldots, x_n\\) has the form:</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equation-standard-form","title":"Linear Equation Standard Form","text":"<p>\\(a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\\)</p> <p>where:</p> <ul> <li>\\(a_1, a_2, \\ldots, a_n\\) are the coefficients (constants)</li> <li>\\(x_1, x_2, \\ldots, x_n\\) are the variables (unknowns)</li> <li>\\(b\\) is the constant term (right-hand side)</li> </ul> <p>The equation is \"linear\" because each variable appears only to the first power and is not multiplied by other variables. The graph of a linear equation in two variables is a line; in three variables, it's a plane.</p> <p>Examples of linear equations:</p> <ul> <li>\\(3x + 2y = 7\\)</li> <li>\\(x_1 - 4x_2 + x_3 = 0\\)</li> <li>\\(w + 2x - y + 3z = 5\\)</li> </ul> <p>Non-linear equations (not covered by these methods):</p> <ul> <li>\\(x^2 + y = 5\\) (squared term)</li> <li>\\(xy = 3\\) (product of variables)</li> <li>\\(\\sin(x) + y = 1\\) (nonlinear function)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#systems-of-equations","title":"Systems of Equations","text":"<p>A system of equations is a collection of two or more equations involving the same variables. A solution to the system must satisfy all equations simultaneously.</p> <p>Consider a system of \\(m\\) linear equations in \\(n\\) unknowns:</p> <p>\\(a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1\\)</p> <p>\\(a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2\\)</p> <p>\\(\\vdots\\)</p> <p>\\(a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m\\)</p> <p>Geometrically, each equation defines a hyperplane, and solving the system means finding where all hyperplanes intersect.</p> Equations Variables Geometric Interpretation 2 equations 2 variables Intersection of two lines 3 equations 3 variables Intersection of three planes \\(m\\) equations \\(n\\) variables Intersection of \\(m\\) hyperplanes in \\(\\mathbb{R}^n\\)"},{"location":"chapters/03-systems-of-linear-equations/#diagram-system-of-equations-geometry","title":"Diagram: System of Equations Geometry","text":"System of Equations Geometric Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, explain</p> <p>Learning Objective: Help students visualize how the solution to a system of linear equations corresponds to the geometric intersection of lines (2D) or planes (3D).</p> <p>Canvas layout: - Main area: 2D/3D coordinate system with equations plotted - Right panel: Equation display and controls</p> <p>Visual elements: - 2D mode: Two or three lines with intersection point highlighted - 3D mode: Two or three planes with intersection shown - Solution point marked with a distinct marker - Coordinate axes with labels - Grid for reference</p> <p>Interactive controls: - Toggle: 2D / 3D mode - Sliders: Coefficients for each equation (a, b, c values) - Dropdown: Number of equations (2 or 3) - Checkbox: Show solution coordinates - Button: Generate random system - Dropdown: Solution type (unique, infinite, none)</p> <p>Default parameters: - Mode: 2D - Equations: 2 - Example: x + y = 3, x - y = 1 (solution at (2, 1))</p> <p>Behavior: - Lines/planes update in real-time as coefficients change - Intersection point updates dynamically - When lines are parallel (no solution), display message - When lines are coincident (infinite solutions), highlight entire line - 3D mode allows rotation and zoom</p> <p>Implementation: p5.js with WEBGL for 3D mode</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-representation","title":"Matrix Representation","text":""},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form","title":"Matrix Equation Form","text":"<p>Any system of linear equations can be written compactly as a single matrix equation:</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form_1","title":"Matrix Equation Form","text":"<p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is the \\(m \\times n\\) coefficient matrix</li> <li>\\(\\mathbf{x}\\) is the \\(n \\times 1\\) vector of unknowns</li> <li>\\(\\mathbf{b}\\) is the \\(m \\times 1\\) vector of constants</li> </ul> <p>For the system:</p> <p>\\(2x + 3y = 8\\)</p> <p>\\(x - y = 1\\)</p> <p>The matrix form is:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 1 \\end{bmatrix}\\)</p> <p>This representation reveals the structure: the coefficient matrix \\(\\mathbf{A}\\) encodes how variables combine, while \\(\\mathbf{b}\\) specifies the targets.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-augmented-matrix","title":"The Augmented Matrix","text":"<p>The augmented matrix combines the coefficient matrix and the right-hand side into a single matrix for manipulation:</p> <p>\\([\\mathbf{A} \\mid \\mathbf{b}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{bmatrix}\\)</p> <p>The vertical bar separates coefficients from constants, though it's sometimes omitted. All solution procedures work on the augmented matrix, treating coefficients and constants together.</p> <p>For our example:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; | &amp; 8 \\\\ 1 &amp; -1 &amp; | &amp; 1 \\end{bmatrix}\\)</p> <p>Why Augmented Matrices?</p> <p>The augmented matrix is the standard data structure for solving linear systems. It keeps all relevant information together and allows us to track how row operations affect both coefficients and constants simultaneously.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-operations","title":"Row Operations","text":"<p>The key to solving linear systems is transforming the augmented matrix into a simpler form while preserving the solution set. Three row operations accomplish this.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-three-elementary-row-operations","title":"The Three Elementary Row Operations","text":"<ol> <li>Row Swap (Interchange): Exchange two rows</li> <li>Notation: \\(R_i \\leftrightarrow R_j\\)</li> <li> <p>Swaps row \\(i\\) with row \\(j\\)</p> </li> <li> <p>Row Scaling (Multiplication): Multiply a row by a nonzero constant</p> </li> <li>Notation: \\(kR_i \\rightarrow R_i\\) (where \\(k \\neq 0\\))</li> <li> <p>Multiplies every entry in row \\(i\\) by \\(k\\)</p> </li> <li> <p>Row Addition (Replacement): Add a multiple of one row to another</p> </li> <li>Notation: \\(R_i + kR_j \\rightarrow R_i\\)</li> <li>Adds \\(k\\) times row \\(j\\) to row \\(i\\)</li> </ol> Operation Effect Preserves Solutions? Row Swap Reorders equations Yes Row Scaling Scales an equation Yes (if \\(k \\neq 0\\)) Row Addition Combines equations Yes <p>These operations correspond to valid algebraic manipulations of equations:</p> <ul> <li>Swapping two equations doesn't change solutions</li> <li>Multiplying an equation by a nonzero constant doesn't change its solutions</li> <li>Adding equations produces a valid new equation satisfied by the same solutions</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-row-operations-interactive","title":"Diagram: Row Operations Interactive","text":"Row Operations Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, demonstrate</p> <p>Learning Objective: Enable students to practice applying the three elementary row operations and observe how they transform the augmented matrix while preserving solutions.</p> <p>Canvas layout: - Top: Current augmented matrix (large, clear display) - Middle: Operation selector and parameters - Bottom: History of operations performed</p> <p>Visual elements: - Matrix displayed as a grid with clear row/column separation - Active rows highlighted during operation - Animation showing transformation - Vertical bar separating coefficients from constants - Operation history as a scrollable list</p> <p>Interactive controls: - Dropdown: Select operation (Swap, Scale, Add) - For Swap: Two row selectors - For Scale: Row selector + scalar input (prevent 0) - For Add: Target row + source row + multiplier - Button: Apply operation - Button: Undo last operation - Button: Reset to original - Toggle: Show intermediate steps with animation</p> <p>Default parameters: - Starting matrix: 3\u00d74 augmented matrix - Example system: 2x + y - z = 8, -3x - y + 2z = -11, -2x + y + 2z = -3</p> <p>Behavior: - Selected rows highlight before operation - Animation shows values changing - History updates with notation (e.g., \"R\u2081 \u2194 R\u2082\") - Undo restores previous state - Invalid operations (scaling by 0) show error</p> <p>Implementation: p5.js with animation states</p>"},{"location":"chapters/03-systems-of-linear-equations/#gaussian-elimination","title":"Gaussian Elimination","text":"<p>Gaussian elimination is the systematic procedure for reducing an augmented matrix to row echelon form using row operations. Named after Carl Friedrich Gauss, this algorithm is the workhorse of computational linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-algorithm","title":"The Algorithm","text":"<p>Gaussian elimination proceeds column by column, from left to right:</p> <ol> <li>Find a pivot: Locate the leftmost column with a nonzero entry</li> <li>Position the pivot: Use row swaps to move a nonzero entry to the top of the working submatrix</li> <li>Eliminate below: Use row addition to create zeros below the pivot</li> <li>Move down: Repeat for the next column with the remaining rows</li> </ol> <p>The goal is to create an \"upper triangular\" structure where all entries below the main diagonal are zero.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-echelon-form","title":"Row Echelon Form","text":"<p>A matrix is in row echelon form (REF) if:</p> <ul> <li>All zero rows are at the bottom</li> <li>The leading entry (first nonzero entry) of each row is to the right of the leading entry of the row above</li> <li>All entries below a leading entry are zero</li> </ul> <p>A pivot position is the location of a leading 1 (or leading nonzero entry). A pivot column is a column containing a pivot position.</p> <p>Example of row echelon form:</p> <p>\\(\\begin{bmatrix} \\boxed{2} &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; \\boxed{1} &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; \\boxed{3} &amp; | &amp; 6 \\end{bmatrix}\\)</p> <p>The boxed entries are pivots. Each pivot is to the right of the pivot above it.</p>"},{"location":"chapters/03-systems-of-linear-equations/#back-substitution","title":"Back Substitution","text":"<p>Once a matrix is in row echelon form, we solve for the variables using back substitution\u2014working from the bottom row upward:</p> <ol> <li>Solve the last equation for its variable</li> <li>Substitute into the equation above and solve</li> <li>Continue upward until all variables are found</li> </ol> <p>For the example above:</p> <ul> <li>Row 3: \\(3z = 6 \\Rightarrow z = 2\\)</li> <li>Row 2: \\(y + 4(2) = -2 \\Rightarrow y = -10\\)</li> <li>Row 1: \\(2x + 3(-10) - 2 = 5 \\Rightarrow x = 18.5\\)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-gaussian-elimination-visualizer","title":"Diagram: Gaussian Elimination Visualizer","text":"Gaussian Elimination Step-by-Step Animator <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, implement</p> <p>Learning Objective: Guide students through the complete Gaussian elimination algorithm, showing each row operation and explaining why it's performed.</p> <p>Canvas layout: - Top: Current matrix state with pivot highlighted - Middle: Current operation being performed with explanation - Bottom: Controls and solution display</p> <p>Visual elements: - Matrix with current pivot position highlighted in yellow - Rows being modified highlighted in blue - Zeros created by elimination shown in green (briefly) - Current column being processed indicated - Progress indicator showing algorithm phase - Final solution displayed when complete</p> <p>Interactive controls: - Button: Next Step (advances one row operation) - Button: Auto-solve (animates entire solution) - Slider: Animation speed - Button: Reset - Dropdown: Example system (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show explanatory text for each step - Checkbox: Enable back substitution phase</p> <p>Default parameters: - Matrix size: 3\u00d73 - Animation speed: 1 second per step - Show explanations: true</p> <p>Behavior: - Each step highlights the relevant rows and pivot - Explanation text describes the operation and purpose - Algorithm phases clearly labeled (Forward elimination, Back substitution) - Solution verified by substitution at end - Can step forward/backward through operations</p> <p>Implementation: p5.js with state machine for algorithm steps</p>"},{"location":"chapters/03-systems-of-linear-equations/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>While row echelon form allows back substitution, reduced row echelon form (RREF) goes further to directly reveal solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#definition-and-properties","title":"Definition and Properties","text":"<p>A matrix is in reduced row echelon form if:</p> <ul> <li>It is in row echelon form</li> <li>Each pivot is 1</li> <li>Each pivot is the only nonzero entry in its column</li> </ul> <p>The Gauss-Jordan elimination algorithm extends Gaussian elimination to produce RREF by:</p> <ol> <li>Scaling each pivot row to make the pivot equal to 1</li> <li>Eliminating entries above each pivot (not just below)</li> </ol> <p>Example transformation to RREF:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; 3 &amp; | &amp; 6 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; | &amp; 18.5 \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -10 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>In RREF, the solution is immediately visible: \\(x = 18.5\\), \\(y = -10\\), \\(z = 2\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#comparing-ref-and-rref","title":"Comparing REF and RREF","text":"Property Row Echelon Form Reduced Row Echelon Form Zero rows At bottom At bottom Leading entries Nonzero (any value) Exactly 1 Below pivots All zeros All zeros Above pivots Any value All zeros Solution method Back substitution Direct reading Uniqueness Not unique Unique <p>Computational Efficiency</p> <p>For solving a single system, stopping at REF and using back substitution is often more efficient. RREF requires additional operations. However, RREF is valuable for understanding solution structure and for systems requiring multiple right-hand sides.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-ref-vs-rref-comparison","title":"Diagram: REF vs RREF Comparison","text":"REF vs RREF Side-by-Side Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Help students understand the difference between row echelon form and reduced row echelon form, and when each is preferable.</p> <p>Layout: Side-by-side matrix displays with transformation arrows</p> <p>Visual elements: - Left panel: Original matrix - Center-left: Row echelon form with pivots highlighted - Center-right: Reduced row echelon form with pivots highlighted - Annotations showing key differences - Color coding: pivots (gold), zeros created (green), coefficients (blue)</p> <p>Interactive features: - Button: Generate new random system - Dropdown: Matrix size (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show step count for each form - Toggle: Show solution extraction method - Hover: Show definition of each form</p> <p>Information displayed: - Number of operations to reach each form - Solution method for each (back substitution vs direct) - Uniqueness property highlighted</p> <p>Color scheme: - Pivot positions: gold - Created zeros: light green - Original nonzero entries: blue - Structure indicators: gray outlines</p> <p>Implementation: HTML/CSS/JavaScript with matrix computation</p>"},{"location":"chapters/03-systems-of-linear-equations/#solution-analysis","title":"Solution Analysis","text":"<p>Not every system of linear equations has a solution, and some have infinitely many. Understanding solution existence and uniqueness is as important as computing solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#types-of-solution-sets","title":"Types of Solution Sets","text":"<p>The solution set of a system is the collection of all vectors \\(\\mathbf{x}\\) satisfying \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). Three possibilities exist:</p> <ol> <li>Unique Solution: Exactly one solution exists</li> <li>Infinite Solutions: Infinitely many solutions exist (forming a line, plane, or higher-dimensional subspace)</li> <li>No Solution: No solution exists (the system is inconsistent)</li> </ol> <p>The row echelon form reveals which case applies:</p> <ul> <li>No solution: A row of the form \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) where \\(c \\neq 0\\) (equation \\(0 = c\\))</li> <li>Unique solution: Every column is a pivot column (as many pivots as variables)</li> <li>Infinite solutions: Some columns are not pivot columns (fewer pivots than variables)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#basic-and-free-variables","title":"Basic and Free Variables","text":"<p>In systems with infinitely many solutions, variables split into two types:</p> <ul> <li>Basic variables correspond to pivot columns\u2014they can be expressed in terms of other variables</li> <li>Free variables correspond to non-pivot columns\u2014they can take any value</li> </ul> <p>The number of free variables determines the \"dimension\" of the solution set:</p> Free Variables Solution Set 0 Unique point 1 Line 2 Plane \\(k\\) \\(k\\)-dimensional subspace <p>Example with a free variable:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>Here \\(x_1\\) and \\(x_3\\) are basic variables (columns 1 and 3 are pivot columns), while \\(x_2\\) is free. The general solution is:</p> <p>\\(x_1 = 4 - 2x_2 - 3(2) = -2 - 2x_2\\)</p> <p>\\(x_2 = \\text{free}\\)</p> <p>\\(x_3 = 2\\)</p> <p>Or in vector form: \\(\\mathbf{x} = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\)</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-solution-set-visualizer","title":"Diagram: Solution Set Visualizer","text":"Solution Set Type Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: classify, examine</p> <p>Learning Objective: Enable students to explore how different systems produce unique solutions, infinite solutions (lines/planes), or no solution, and to identify the determining factors.</p> <p>Canvas layout: - Left: Augmented matrix (editable or preset) - Center: 2D/3D geometric visualization - Right: Solution analysis panel</p> <p>Visual elements: - Matrix with pivot positions marked - Geometric view showing lines/planes - Solution point, line, or empty set visualized - Pivot columns highlighted - Free variable columns indicated - Parametric solution displayed (for infinite solutions)</p> <p>Interactive controls: - Dropdown: Preset examples (unique, infinite, none) - Editable matrix cells (for custom exploration) - Toggle: 2D / 3D visualization - Checkbox: Show row echelon form - Checkbox: Show reduced row echelon form - Button: Random consistent system - Button: Random inconsistent system</p> <p>Default parameters: - Mode: 2D - Example: unique solution case</p> <p>Behavior: - Matrix edits update visualization in real-time - Inconsistent systems show parallel lines/planes (no intersection) - Infinite solutions show line or plane of solutions - Solution type automatically detected and labeled - Free variables identified and highlighted</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#homogeneous-systems","title":"Homogeneous Systems","text":"<p>A homogeneous system has all zero constants on the right-hand side:</p> <p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)</p> <p>Homogeneous systems have special properties that make them particularly important in linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-trivial-solution","title":"The Trivial Solution","text":"<p>Every homogeneous system has at least one solution: \\(\\mathbf{x} = \\mathbf{0}\\). This is called the trivial solution because it's obvious\u2014all zeros satisfy any homogeneous equation.</p> <p>The interesting question is whether nontrivial solutions exist.</p>"},{"location":"chapters/03-systems-of-linear-equations/#existence-of-nontrivial-solutions","title":"Existence of Nontrivial Solutions","text":"<p>A homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has nontrivial solutions if and only if it has free variables. This happens when:</p> <ul> <li>The number of variables exceeds the number of pivot columns</li> <li>Equivalently, the number of variables exceeds the rank of \\(\\mathbf{A}\\)</li> </ul> <p>Important consequence: If a homogeneous system has more variables than equations, it always has nontrivial solutions.</p> Variables vs Equations Nontrivial Solutions? Variables &gt; Equations Always Variables = Equations Maybe (depends on matrix) Variables &lt; Equations Maybe (depends on matrix)"},{"location":"chapters/03-systems-of-linear-equations/#solution-space-structure","title":"Solution Space Structure","text":"<p>The solution set of a homogeneous system forms a subspace called the null space of \\(\\mathbf{A}\\). This subspace:</p> <ul> <li>Contains the zero vector</li> <li>Is closed under addition (sum of solutions is a solution)</li> <li>Is closed under scalar multiplication (scalar times solution is a solution)</li> </ul> <p>This structure is fundamental to understanding linear transformations and will be explored further in later chapters.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-homogeneous-system-explorer","title":"Diagram: Homogeneous System Explorer","text":"Homogeneous System Solution Space Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that homogeneous systems always have the trivial solution and may have additional solutions forming a subspace through the origin.</p> <p>Canvas layout: - Left: Coefficient matrix A (editable) - Center: 2D/3D visualization of solution space - Right: Solution analysis</p> <p>Visual elements: - Matrix display with rank calculation - Coordinate axes through origin - Trivial solution (origin) always marked - Nontrivial solution space (line or plane through origin) when present - Basis vectors for solution space shown as arrows - Null space dimension displayed</p> <p>Interactive controls: - Matrix size selector (2\u00d72, 2\u00d73, 3\u00d73, 3\u00d74) - Editable matrix entries - Button: Generate random full-rank matrix (only trivial solution) - Button: Generate random rank-deficient matrix (nontrivial solutions) - Toggle: Show basis vectors for null space - Slider: Rotate 3D view</p> <p>Default parameters: - Size: 3\u00d73 - Initial matrix: rank-deficient (has null space)</p> <p>Behavior: - Real-time rank calculation - Solution space updates as matrix changes - Clear indication of trivial-only vs nontrivial solutions - Null space dimension shown (n - rank) - 3D rotation for spatial understanding</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#numerical-stability","title":"Numerical Stability","text":"<p>In practice, solving linear systems on computers introduces challenges beyond the pure mathematics.</p>"},{"location":"chapters/03-systems-of-linear-equations/#sources-of-numerical-error","title":"Sources of Numerical Error","text":"<p>Numerical stability refers to how errors propagate through a computation. In linear systems, instability can arise from:</p> <ul> <li>Floating-point representation: Real numbers are stored with limited precision</li> <li>Round-off errors: Each arithmetic operation may introduce small errors</li> <li>Ill-conditioned systems: Some systems amplify small input errors into large output errors</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#partial-pivoting","title":"Partial Pivoting","text":"<p>Standard Gaussian elimination can suffer from numerical instability when small pivots lead to division by near-zero values. Partial pivoting addresses this by selecting the largest available pivot:</p> <ol> <li>Before eliminating in a column, scan downward for the entry with largest absolute value</li> <li>Swap rows to bring this entry to the pivot position</li> <li>Proceed with elimination</li> </ol> <p>This strategy prevents division by small numbers and improves numerical stability.</p>"},{"location":"chapters/03-systems-of-linear-equations/#condition-number","title":"Condition Number","text":"<p>The condition number of a matrix quantifies its sensitivity to perturbations. A high condition number indicates an ill-conditioned system where small changes in input cause large changes in output.</p> <ul> <li>Condition number \u2248 1: Well-conditioned (stable)</li> <li>Condition number large (e.g., \\(10^6\\)): Ill-conditioned (unstable)</li> <li>Condition number = \u221e: Singular matrix (no unique solution)</li> </ul> <p>Practical Implications</p> <p>When working with real data, always consider numerical stability. Libraries like NumPy use sophisticated algorithms (LU decomposition with partial pivoting) that are more stable than naive Gaussian elimination. Never compute \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly\u2014use specialized solvers instead.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-numerical-stability-demonstration","title":"Diagram: Numerical Stability Demonstration","text":"Numerical Stability and Condition Number Explorer <p>Type: microsim</p> <p>Bloom Level: Evaluate (L5) Bloom Verb: assess, judge</p> <p>Learning Objective: Demonstrate how small changes in matrix entries can cause large changes in solutions for ill-conditioned systems, and how partial pivoting improves stability.</p> <p>Canvas layout: - Left: Original system and computed solution - Center: Perturbed system and new solution - Right: Analysis (condition number, error magnification)</p> <p>Visual elements: - Two matrices side-by-side (original and perturbed) - Solutions displayed with precision indicators - Error visualization (bar chart showing input vs output error) - Condition number prominently displayed - Color coding: green (well-conditioned) to red (ill-conditioned) - 2D geometric view showing how solution moves</p> <p>Interactive controls: - Dropdown: Example type (well-conditioned, moderately ill-conditioned, severely ill-conditioned) - Slider: Perturbation magnitude (0.0001 to 0.1) - Toggle: Use partial pivoting - Button: Apply random perturbation - Checkbox: Show geometric interpretation - Display: Precision (decimal places shown)</p> <p>Default parameters: - Example: Hilbert matrix 3\u00d73 (ill-conditioned) - Perturbation: 0.001 - Partial pivoting: off</p> <p>Behavior: - Perturbation applied to random entries - Solution error calculated and displayed - Error magnification factor shown (output error / input error) - Comparison with/without partial pivoting - Geometric view shows solution point movement</p> <p>Implementation: p5.js with high-precision arithmetic library</p>"},{"location":"chapters/03-systems-of-linear-equations/#applications","title":"Applications","text":""},{"location":"chapters/03-systems-of-linear-equations/#balancing-chemical-equations","title":"Balancing Chemical Equations","text":"<p>Chemical equations must balance atoms. For the reaction:</p> <p>\\(a \\text{CH}_4 + b \\text{O}_2 \\rightarrow c \\text{CO}_2 + d \\text{H}_2\\text{O}\\)</p> <p>Balancing each element gives a linear system:</p> <ul> <li>Carbon: \\(a = c\\)</li> <li>Hydrogen: \\(4a = 2d\\)</li> <li>Oxygen: \\(2b = 2c + d\\)</li> </ul> <p>This homogeneous system has a one-dimensional solution space. Setting \\(a = 1\\) gives the balanced equation: \\(\\text{CH}_4 + 2\\text{O}_2 \\rightarrow \\text{CO}_2 + 2\\text{H}_2\\text{O}\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#network-flow-analysis","title":"Network Flow Analysis","text":"<p>In electrical circuits or traffic networks, conservation laws produce linear systems. At each node, inflow equals outflow. The resulting system determines currents or traffic flows throughout the network.</p>"},{"location":"chapters/03-systems-of-linear-equations/#machine-learning-linear-regression","title":"Machine Learning: Linear Regression","text":"<p>Fitting a linear model \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}\\) to data leads to the normal equations:</p> <p>\\(\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\)</p> <p>Solving this system gives the least-squares coefficients \\(\\boldsymbol{\\beta}\\) that minimize prediction error.</p>"},{"location":"chapters/03-systems-of-linear-equations/#neural-network-training","title":"Neural Network Training","text":"<p>Training neural networks involves solving systems of equations (approximately) at each optimization step. The gradient computations that guide learning rely on the same matrix operations covered in this chapter.</p>"},{"location":"chapters/03-systems-of-linear-equations/#computational-implementation","title":"Computational Implementation","text":""},{"location":"chapters/03-systems-of-linear-equations/#numpy-example","title":"NumPy Example","text":"<pre><code>import numpy as np\n\n# Define the system Ax = b\nA = np.array([[2, 3, -1],\n              [4, 4, -3],\n              [1, -1, 2]])\nb = np.array([5, 3, 1])\n\n# Solve using NumPy (LU decomposition with pivoting)\nx = np.linalg.solve(A, b)\nprint(f\"Solution: {x}\")\n\n# Verify: check that Ax = b\nprint(f\"Verification (Ax): {A @ x}\")\nprint(f\"Condition number: {np.linalg.cond(A):.2f}\")\n</code></pre>"},{"location":"chapters/03-systems-of-linear-equations/#key-functions","title":"Key Functions","text":"Function Purpose <code>np.linalg.solve(A, b)</code> Solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) <code>np.linalg.matrix_rank(A)</code> Compute rank <code>np.linalg.cond(A)</code> Compute condition number <code>scipy.linalg.lu(A)</code> LU decomposition <code>np.linalg.lstsq(A, b)</code> Least squares solution"},{"location":"chapters/03-systems-of-linear-equations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter developed the theory and practice of solving systems of linear equations.</p> <p>Formulation:</p> <ul> <li>A linear equation has variables appearing to the first power only</li> <li>A system of equations requires simultaneous satisfaction of multiple equations</li> <li>Matrix equation form \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) compactly represents the system</li> <li>The augmented matrix \\([\\mathbf{A} | \\mathbf{b}]\\) combines coefficients and constants</li> </ul> <p>Solution Methods:</p> <ul> <li>Row operations (swap, scale, add) transform systems while preserving solutions</li> <li>Gaussian elimination reduces to row echelon form</li> <li>Back substitution solves from bottom to top</li> <li>Reduced row echelon form allows direct solution reading</li> </ul> <p>Solution Analysis:</p> <ul> <li>Pivot positions and pivot columns determine solution structure</li> <li>Unique solution: all columns are pivot columns</li> <li>Infinite solutions: some columns are free (non-pivot)</li> <li>No solution: inconsistent row \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) with \\(c \\neq 0\\)</li> <li>Free variables can take any value; basic variables are determined by them</li> </ul> <p>Special Systems:</p> <ul> <li>Homogeneous systems (\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)) always have the trivial solution</li> <li>Nontrivial solutions exist when there are free variables</li> <li>The solution set of a homogeneous system is a subspace</li> </ul> <p>Computational Considerations:</p> <ul> <li>Numerical stability matters for practical computation</li> <li>Partial pivoting improves stability</li> <li>Condition number measures sensitivity to perturbation</li> <li>Use library functions rather than explicit inverse computation</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#exercises","title":"Exercises","text":"Exercise 1: Row Echelon Form <p>Reduce the following augmented matrix to row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; | &amp; 3 \\\\ 2 &amp; 5 &amp; 1 &amp; | &amp; 8 \\\\ 3 &amp; 7 &amp; 0 &amp; | &amp; 11 \\end{bmatrix}\\)</p> <p>Then use back substitution to find the solution.</p> Exercise 2: Solution Type Identification <p>For each augmented matrix in row echelon form, determine whether the system has a unique solution, infinite solutions, or no solution:</p> <p>a) \\(\\begin{bmatrix} 1 &amp; 3 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>b) \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>c) \\(\\begin{bmatrix} 1 &amp; 2 &amp; | &amp; 3 \\\\ 0 &amp; 0 &amp; | &amp; 5 \\end{bmatrix}\\)</p> Exercise 3: Free and Basic Variables <p>Given the reduced row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; | &amp; 3 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; | &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; | &amp; 4 \\end{bmatrix}\\)</p> <p>Identify the pivot columns, free variables, and basic variables. Write the general solution in parametric form.</p> Exercise 4: Homogeneous System <p>Consider the homogeneous system with coefficient matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\)</p> <p>Determine whether nontrivial solutions exist. If so, find a basis for the solution space.</p> Exercise 5: Numerical Stability <p>The Hilbert matrix \\(H_n\\) has entries \\(h_{ij} = \\frac{1}{i+j-1}\\). For \\(n = 3\\):</p> <p>\\(H_3 = \\begin{bmatrix} 1 &amp; 1/2 &amp; 1/3 \\\\ 1/2 &amp; 1/3 &amp; 1/4 \\\\ 1/3 &amp; 1/4 &amp; 1/5 \\end{bmatrix}\\)</p> <p>Compute the condition number of \\(H_3\\). What does this tell you about solving systems with this coefficient matrix?</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/","title":"Quiz: Systems of Linear Equations","text":"<p>Test your understanding of solving linear systems and related concepts.</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#1-what-is-the-matrix-equation-form-of-a-system-of-linear-equations","title":"1. What is the matrix equation form of a system of linear equations?","text":"<ol> <li>\\(A + \\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(\\mathbf{x}A = \\mathbf{b}\\)</li> <li>\\(A\\mathbf{b} = \\mathbf{x}\\)</li> </ol> Show Answer <p>The correct answer is B. A system of linear equations can be written as \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(A\\) is the coefficient matrix, \\(\\mathbf{x}\\) is the vector of unknowns, and \\(\\mathbf{b}\\) is the vector of constants.</p> <p>Concept Tested: Matrix Equation</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#2-what-is-the-purpose-of-gaussian-elimination","title":"2. What is the purpose of Gaussian elimination?","text":"<ol> <li>To compute the determinant of a matrix</li> <li>To transform a matrix to row echelon form for solving linear systems</li> <li>To find the eigenvalues of a matrix</li> <li>To compute the transpose of a matrix</li> </ol> Show Answer <p>The correct answer is B. Gaussian elimination transforms a matrix (or augmented matrix) to row echelon form using elementary row operations. This simplifies the system so that solutions can be found through back substitution.</p> <p>Concept Tested: Gaussian Elimination</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#3-a-system-of-linear-equations-has-infinitely-many-solutions-when","title":"3. A system of linear equations has infinitely many solutions when:","text":"<ol> <li>The coefficient matrix is square</li> <li>There are more unknowns than equations and the system is consistent</li> <li>The determinant of the coefficient matrix is non-zero</li> <li>All equations are identical</li> </ol> Show Answer <p>The correct answer is B. A system has infinitely many solutions when it is consistent (at least one solution exists) but has free variables\u2014typically when there are more unknowns than independent equations, or when rows become zero during elimination.</p> <p>Concept Tested: Solution Types</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#4-what-is-row-echelon-form","title":"4. What is row echelon form?","text":"<ol> <li>A matrix where all elements are in ascending order</li> <li>A matrix with a staircase pattern of leading ones and zeros below them</li> <li>A diagonal matrix</li> <li>A matrix equal to its transpose</li> </ol> Show Answer <p>The correct answer is B. Row echelon form has a staircase pattern: each row's leading entry (pivot) is to the right of the row above it, and all entries below each pivot are zero. This form enables back substitution.</p> <p>Concept Tested: Row Echelon Form</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#5-what-does-it-mean-for-a-system-to-be-inconsistent","title":"5. What does it mean for a system to be inconsistent?","text":"<ol> <li>The system has exactly one solution</li> <li>The system has infinitely many solutions</li> <li>The system has no solution</li> <li>The system has negative solutions</li> </ol> Show Answer <p>The correct answer is C. An inconsistent system has no solution\u2014the equations are contradictory. This occurs when row reduction produces a row like \\([0, 0, \\ldots, 0 | c]\\) where \\(c \\neq 0\\), representing \\(0 = c\\).</p> <p>Concept Tested: Consistent vs Inconsistent Systems</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#6-lu-decomposition-factors-a-matrix-a-as","title":"6. LU decomposition factors a matrix \\(A\\) as:","text":"<ol> <li>\\(A = L + U\\)</li> <li>\\(A = LU\\) where \\(L\\) is lower triangular and \\(U\\) is upper triangular</li> <li>\\(A = L^TU^T\\)</li> <li>\\(A = U^{-1}L^{-1}\\)</li> </ol> Show Answer <p>The correct answer is B. LU decomposition factors a matrix as \\(A = LU\\), where \\(L\\) is a lower triangular matrix (with ones on the diagonal in Doolittle form) and \\(U\\) is an upper triangular matrix. This is useful for efficiently solving multiple systems with the same coefficient matrix.</p> <p>Concept Tested: LU Decomposition</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#7-what-is-back-substitution-used-for","title":"7. What is back substitution used for?","text":"<ol> <li>Converting a matrix to row echelon form</li> <li>Solving a triangular system by working from the last equation upward</li> <li>Computing the inverse of a matrix</li> <li>Finding the determinant</li> </ol> Show Answer <p>The correct answer is B. Back substitution solves an upper triangular system by starting with the last equation (which has only one unknown), solving for that variable, then substituting back into previous equations to find remaining unknowns.</p> <p>Concept Tested: Back Substitution</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#8-the-least-squares-solution-minimizes","title":"8. The least squares solution minimizes:","text":"<ol> <li>The number of variables</li> <li>The sum of squared residuals \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\)</li> <li>The determinant of the coefficient matrix</li> <li>The condition number</li> </ol> Show Answer <p>The correct answer is B. The least squares solution finds the \\(\\mathbf{x}\\) that minimizes the squared error \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\). This is essential for overdetermined systems (more equations than unknowns) where an exact solution may not exist.</p> <p>Concept Tested: Least Squares</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#9-which-matrix-operation-is-not-an-elementary-row-operation","title":"9. Which matrix operation is NOT an elementary row operation?","text":"<ol> <li>Swapping two rows</li> <li>Multiplying a row by a non-zero scalar</li> <li>Adding a multiple of one row to another</li> <li>Taking the transpose of a row</li> </ol> Show Answer <p>The correct answer is D. The three elementary row operations are: (1) swapping two rows, (2) multiplying a row by a non-zero scalar, and (3) adding a multiple of one row to another. Transposing is not an elementary row operation.</p> <p>Concept Tested: Elementary Row Operations</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#10-the-normal-equations-for-least-squares-are","title":"10. The normal equations for least squares are:","text":"<ol> <li>\\(A\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\)</li> <li>\\(AA^T\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A^{-1}\\mathbf{b} = \\mathbf{x}\\)</li> </ol> Show Answer <p>The correct answer is B. The normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) are derived by setting the gradient of the squared error to zero. They provide the least squares solution when \\(A^TA\\) is invertible.</p> <p>Concept Tested: Normal Equations</p>"},{"location":"chapters/04-linear-transformations/","title":"Linear Transformations","text":""},{"location":"chapters/04-linear-transformations/#summary","title":"Summary","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition. You will learn about rotation, scaling, shearing, reflection, and projection transformations, and understand abstract concepts like kernel, range, and change of basis. These ideas are fundamental to computer graphics, robotics, and understanding how neural networks transform data.</p>"},{"location":"chapters/04-linear-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"chapters/04-linear-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> </ul>"},{"location":"chapters/04-linear-transformations/#introduction","title":"Introduction","text":"<p>Every matrix multiplication \\(\\mathbf{A}\\mathbf{x}\\) can be viewed as a transformation\u2014taking an input vector \\(\\mathbf{x}\\) and producing an output vector \\(\\mathbf{y}\\). This perspective transforms matrices from static arrays of numbers into dynamic operators that rotate, scale, shear, project, and transform geometric objects.</p> <p>Understanding transformations is essential for computer graphics, where every rotation, scaling, and perspective projection is a matrix multiplication. It's equally crucial for machine learning, where neural networks apply layer after layer of linear transformations (with nonlinearities between them) to map inputs to outputs. This chapter develops the mathematical framework for understanding these transformations and their properties.</p>"},{"location":"chapters/04-linear-transformations/#functions-and-transformations","title":"Functions and Transformations","text":""},{"location":"chapters/04-linear-transformations/#functions-between-vector-spaces","title":"Functions Between Vector Spaces","text":"<p>A function \\(T\\) from a set \\(V\\) to a set \\(W\\), written \\(T: V \\rightarrow W\\), is a rule that assigns to each element \\(\\mathbf{v}\\) in \\(V\\) exactly one element \\(T(\\mathbf{v})\\) in \\(W\\).</p> <p>Key terminology:</p> <ul> <li>The domain of \\(T\\) is the set \\(V\\) of all possible inputs</li> <li>The codomain of \\(T\\) is the set \\(W\\) of all possible outputs</li> <li>The image of a vector \\(\\mathbf{v}\\) is its output \\(T(\\mathbf{v})\\)</li> <li>The range (or image of \\(T\\)) is the set of all outputs: \\(\\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</li> </ul> <p>Range vs Codomain</p> <p>The codomain is the set where outputs could live. The range is where outputs actually live. For example, \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) has codomain \\(\\mathbb{R}^3\\), but its range might be a plane within \\(\\mathbb{R}^3\\).</p>"},{"location":"chapters/04-linear-transformations/#linear-transformations_1","title":"Linear Transformations","text":"<p>A linear transformation (or linear map) is a function \\(T: V \\rightarrow W\\) between vector spaces that preserves vector addition and scalar multiplication:</p>"},{"location":"chapters/04-linear-transformations/#linearity-conditions","title":"Linearity Conditions","text":"<ol> <li> <p>\\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in V\\)</p> </li> <li> <p>\\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) for all \\(\\mathbf{v} \\in V\\) and scalars \\(c\\)</p> </li> </ol> <p>These two conditions can be combined into one:</p> <p>\\(T(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2) = c_1 T(\\mathbf{v}_1) + c_2 T(\\mathbf{v}_2)\\)</p> <p>Linear transformations preserve linear combinations. They map lines to lines (or points), planes to planes (or lines or points), and the origin to the origin.</p> Property Linear Non-Linear Origin maps to origin Always Not necessarily Lines map to... Lines or points Curves possible Parallelism preserved Yes No Grid structure Preserved Distorted"},{"location":"chapters/04-linear-transformations/#the-transformation-matrix","title":"The Transformation Matrix","text":"<p>Every linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) can be represented by an \\(m \\times n\\) matrix \\(\\mathbf{A}\\). The transformation is then:</p> <p>\\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\)</p> <p>To find the transformation matrix, apply \\(T\\) to each standard basis vector:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} T(\\mathbf{e}_1) &amp; T(\\mathbf{e}_2) &amp; \\cdots &amp; T(\\mathbf{e}_n) \\end{bmatrix}\\)</p> <p>The columns of \\(\\mathbf{A}\\) are the images of the standard basis vectors. This fundamental observation connects abstract transformations to concrete matrix computations.</p>"},{"location":"chapters/04-linear-transformations/#diagram-linear-transformation-visualizer","title":"Diagram: Linear Transformation Visualizer","text":"Linear Transformation Fundamentals Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that linear transformations preserve the grid structure and that the transformation is completely determined by where basis vectors map.</p> <p>Canvas layout: - Left: Original coordinate plane with basis vectors and grid - Right: Transformed plane showing result of applying T - Bottom: Matrix display and controls</p> <p>Visual elements: - Standard basis vectors e\u2081 (red) and e\u2082 (blue) in original space - Transformed basis vectors T(e\u2081) and T(e\u2082) in target space - Grid of points in original space - Corresponding grid in transformed space - Sample vector (user-controlled) showing before/after - 2\u00d72 transformation matrix displayed</p> <p>Interactive controls: - Draggable endpoints for T(e\u2081) and T(e\u2082) to define transformation - Slider: Animation between original and transformed states - Button: Reset to identity - Dropdown: Preset transformations (rotation, scaling, shear, reflection) - Checkbox: Show grid lines - Checkbox: Show sample vector path</p> <p>Default parameters: - Initial transformation: identity - Grid: 5\u00d75 - Sample vector: (1, 1)</p> <p>Behavior: - Dragging basis vector endpoints updates matrix in real-time - Grid morphs smoothly during animation - Matrix entries update as endpoints move - Preset dropdown smoothly transitions to new transformation - Sample vector shows how arbitrary points transform</p> <p>Implementation: p5.js with smooth animation</p>"},{"location":"chapters/04-linear-transformations/#geometric-transformations-in-2d","title":"Geometric Transformations in 2D","text":"<p>The power of linear transformations becomes vivid when we visualize their geometric effects. Each type of transformation has a characteristic matrix structure.</p>"},{"location":"chapters/04-linear-transformations/#rotation-matrix","title":"Rotation Matrix","text":"<p>A rotation matrix rotates vectors around the origin by a fixed angle. For 2D rotation by angle \\(\\theta\\) counterclockwise:</p>"},{"location":"chapters/04-linear-transformations/#2d-rotation-matrix","title":"2D Rotation Matrix","text":"<p>\\(\\mathbf{R}(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the rotation angle (positive = counterclockwise)</li> <li>The columns are the rotated basis vectors</li> </ul> <p>Properties of 2D rotation matrices:</p> <ul> <li>\\(\\mathbf{R}(\\theta)^T = \\mathbf{R}(-\\theta) = \\mathbf{R}(\\theta)^{-1}\\) (orthogonal matrix)</li> <li>\\(\\det(\\mathbf{R}(\\theta)) = 1\\) (preserves area and orientation)</li> <li>\\(\\mathbf{R}(\\alpha)\\mathbf{R}(\\beta) = \\mathbf{R}(\\alpha + \\beta)\\) (rotations compose by adding angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-2d-rotation-interactive","title":"Diagram: 2D Rotation Interactive","text":"2D Rotation Matrix Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: demonstrate, calculate</p> <p>Learning Objective: Enable students to see how the rotation matrix transforms vectors and shapes, and verify the relationship between angle and matrix entries.</p> <p>Canvas layout: - Main area: Coordinate plane with original and rotated shapes - Right panel: Matrix display with cos/sin values - Bottom: Angle control</p> <p>Visual elements: - Unit circle for reference - Original shape (arrow, square, or F-shape) in blue - Rotated shape in red (semi-transparent) - Angle arc showing rotation amount - Basis vectors before and after rotation - Matrix entries updating with angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (-360\u00b0 to 360\u00b0) - Dropdown: Shape to rotate (arrow, square, triangle, F-shape) - Checkbox: Show unit circle - Checkbox: Show angle arc - Button: Animate full rotation - Input: Enter specific angle in degrees</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: F-shape (to show orientation) - Unit circle: visible</p> <p>Behavior: - Shape rotates smoothly as angle slider moves - Matrix entries display cos(\u03b8) and sin(\u03b8) values - Animation shows continuous rotation - F-shape clearly shows orientation preservation</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/04-linear-transformations/#3d-rotation","title":"3D Rotation","text":"<p>3D rotation is more complex because we must specify an axis of rotation. The three fundamental rotation matrices rotate around the coordinate axes:</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-x-axis","title":"Rotation Around X-Axis","text":"<p>\\(\\mathbf{R}_x(\\theta) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\theta &amp; -\\sin\\theta \\\\ 0 &amp; \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-y-axis","title":"Rotation Around Y-Axis","text":"<p>\\(\\mathbf{R}_y(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; 0 &amp; \\sin\\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\theta &amp; 0 &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-z-axis","title":"Rotation Around Z-Axis","text":"<p>\\(\\mathbf{R}_z(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>General 3D rotations can be composed from these basic rotations, though the order matters (rotations don't commute in 3D).</p> <p>Gimbal Lock</p> <p>When composing rotations using Euler angles, certain orientations cause \"gimbal lock\" where a degree of freedom is lost. Quaternions provide an alternative representation that avoids this problem, used extensively in robotics and game development.</p>"},{"location":"chapters/04-linear-transformations/#scaling-matrix","title":"Scaling Matrix","text":"<p>A scaling matrix stretches or compresses vectors along the coordinate axes.</p> <p>Uniform scaling scales equally in all directions:</p> <p>\\(\\mathbf{S}_{\\text{uniform}}(k) = \\begin{bmatrix} k &amp; 0 \\\\ 0 &amp; k \\end{bmatrix}\\)</p> <p>Non-uniform scaling scales differently along each axis:</p> <p>\\(\\mathbf{S}(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(s_x\\) is the scaling factor along the x-axis</li> <li>\\(s_y\\) is the scaling factor along the y-axis</li> <li>\\(|s| &gt; 1\\) stretches; \\(|s| &lt; 1\\) compresses</li> <li>\\(s &lt; 0\\) also reflects</li> </ul> Scaling Type Effect Determinant Uniform \\(k &gt; 1\\) Enlarges \\(k^2\\) Uniform \\(0 &lt; k &lt; 1\\) Shrinks \\(k^2\\) Non-uniform Stretches/compresses differently \\(s_x \\cdot s_y\\) Negative factor Also reflects Negative"},{"location":"chapters/04-linear-transformations/#shear-matrix","title":"Shear Matrix","text":"<p>A shear matrix skews shapes by shifting points parallel to an axis, proportional to their distance from that axis.</p> <p>Horizontal shear (shifts x based on y):</p> <p>\\(\\mathbf{H}_x(k) = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Vertical shear (shifts y based on x):</p> <p>\\(\\mathbf{H}_y(k) = \\begin{bmatrix} 1 &amp; 0 \\\\ k &amp; 1 \\end{bmatrix}\\)</p> <p>Shear transformations:</p> <ul> <li>Turn rectangles into parallelograms</li> <li>Preserve area (\\(\\det = 1\\))</li> <li>Are not orthogonal (don't preserve angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#reflection-matrix","title":"Reflection Matrix","text":"<p>A reflection matrix mirrors points across a line (in 2D) or plane (in 3D).</p> <p>Reflection across the x-axis:</p> <p>\\(\\mathbf{F}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</p> <p>Reflection across the y-axis:</p> <p>\\(\\mathbf{F}_y = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Reflection across a line through the origin at angle \\(\\theta\\):</p> <p>\\(\\mathbf{F}(\\theta) = \\begin{bmatrix} \\cos 2\\theta &amp; \\sin 2\\theta \\\\ \\sin 2\\theta &amp; -\\cos 2\\theta \\end{bmatrix}\\)</p> <p>Reflections have determinant \\(-1\\), indicating they reverse orientation (turning clockwise into counterclockwise).</p>"},{"location":"chapters/04-linear-transformations/#diagram-geometric-transformations-gallery","title":"Diagram: Geometric Transformations Gallery","text":"Geometric Transformations Interactive Gallery <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Enable students to compare and contrast different geometric transformations, understanding their visual effects and matrix structures.</p> <p>Canvas layout: - Main area: Split view showing original and transformed shapes - Right panel: Transformation type selector and parameters - Bottom: Matrix display</p> <p>Visual elements: - Original shape (configurable) in blue - Transformed shape in red - Grid showing deformation - Transformation type label - Matrix with current values - Key properties (determinant, orthogonality)</p> <p>Interactive controls: - Tabs: Rotation | Scaling | Shear | Reflection - For Rotation: Angle slider - For Scaling: sx and sy sliders (or single k for uniform) - For Shear: k slider, direction toggle (horizontal/vertical) - For Reflection: Angle slider for reflection line - Dropdown: Shape (square, circle of points, F-shape, arrow) - Toggle: Show grid deformation - Button: Animate transformation</p> <p>Default parameters: - Transformation: Rotation - Shape: F-shape - Show grid: true</p> <p>Behavior: - Smooth animation between original and transformed states - Matrix updates in real-time with parameter changes - Properties panel shows det, orthogonality, etc. - Grid clearly shows how space is warped - Comparisons possible by switching between tabs</p> <p>Implementation: p5.js with tabbed interface</p>"},{"location":"chapters/04-linear-transformations/#projection","title":"Projection","text":"<p>A projection maps vectors onto a subspace (line, plane, etc.). Unlike the transformations above, projections typically reduce dimension and are not invertible.</p>"},{"location":"chapters/04-linear-transformations/#orthogonal-projection","title":"Orthogonal Projection","text":"<p>An orthogonal projection projects vectors perpendicularly onto a subspace. For projection onto a line through the origin with direction \\(\\mathbf{u}\\):</p>"},{"location":"chapters/04-linear-transformations/#projection-onto-a-line","title":"Projection onto a Line","text":"<p>\\(\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}\\)</p> <p>The projection matrix onto the line spanned by unit vector \\(\\hat{\\mathbf{u}}\\) is:</p> <p>\\(\\mathbf{P} = \\hat{\\mathbf{u}} \\hat{\\mathbf{u}}^T\\)</p> <p>For projection onto the x-axis:</p> <p>\\(\\mathbf{P}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>Properties of orthogonal projection matrices:</p> <ul> <li>\\(\\mathbf{P}^2 = \\mathbf{P}\\) (applying twice gives same result)</li> <li>\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)</li> <li>Eigenvalues are 0 and 1</li> <li>Not invertible (determinant = 0)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-orthogonal-projection-visualizer","title":"Diagram: Orthogonal Projection Visualizer","text":"Orthogonal Projection Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students visualize how orthogonal projection maps vectors onto lines or planes, showing the perpendicular relationship between the original vector, its projection, and the error component.</p> <p>Canvas layout: - Main area: 2D/3D coordinate space - Right panel: Vector components and projection formula</p> <p>Visual elements: - Original vector v (blue arrow) - Projection line/plane (gray) - Projected vector proj(v) (red arrow on line) - Error vector (v - proj(v)) shown as dashed green arrow - Right angle indicator showing orthogonality - Unit direction vector u</p> <p>Interactive controls: - Draggable vector v endpoint - Slider: Direction of projection line (angle) - Toggle: 2D / 3D mode - Checkbox: Show error vector - Checkbox: Show right angle indicator - Checkbox: Show projection formula with values - Button: Animate projection process</p> <p>Default parameters: - Mode: 2D - Projection line: 30\u00b0 from x-axis - Vector v: (3, 2)</p> <p>Behavior: - Vector v can be dragged to any position - Projection updates in real-time - Error vector clearly perpendicular to projection line - Formula shows computed values - 3D mode allows projection onto plane</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/04-linear-transformations/#composition-of-transformations","title":"Composition of Transformations","text":"<p>Applying one transformation after another is called composition. If \\(S\\) and \\(T\\) are linear transformations, the composition \\(S \\circ T\\) applies \\(T\\) first, then \\(S\\):</p> <p>\\((S \\circ T)(\\mathbf{x}) = S(T(\\mathbf{x}))\\)</p> <p>For matrix representations, composition corresponds to matrix multiplication:</p> <p>\\(\\mathbf{A}_{S \\circ T} = \\mathbf{A}_S \\mathbf{A}_T\\)</p> <p>Order Matters</p> <p>Matrix multiplication is not commutative, so \\(\\mathbf{A}_S \\mathbf{A}_T \\neq \\mathbf{A}_T \\mathbf{A}_S\\) in general. Rotating then scaling gives different results than scaling then rotating.</p> <p>Common composition examples:</p> <ul> <li>Rotation around a point: Translate to origin, rotate, translate back</li> <li>Scaling about a point: Translate to origin, scale, translate back</li> <li>Euler angles: Compose three axis-aligned rotations</li> </ul> Composition Matrix Product Application Rotate then scale \\(\\mathbf{S}\\mathbf{R}\\) Graphics, robotics Scale then rotate \\(\\mathbf{R}\\mathbf{S}\\) Different result! Multiple rotations \\(\\mathbf{R}_3 \\mathbf{R}_2 \\mathbf{R}_1\\) 3D orientation"},{"location":"chapters/04-linear-transformations/#diagram-transformation-composition","title":"Diagram: Transformation Composition","text":"Transformation Composition Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare</p> <p>Learning Objective: Demonstrate that the order of transformations matters and show how composed transformations combine into a single matrix product.</p> <p>Canvas layout: - Top: Two transformation pipelines side by side (T then S vs S then T) - Bottom: Resulting shapes and combined matrices</p> <p>Visual elements: - Original shape in center - Pipeline 1: Original \u2192 T \u2192 S (with intermediate state shown) - Pipeline 2: Original \u2192 S \u2192 T (with intermediate state shown) - Final shapes for each pipeline (different unless transformations commute) - Matrices for T, S, and their products ST and TS</p> <p>Interactive controls: - Dropdown: First transformation type (rotation, scaling, shear) - Parameters for first transformation - Dropdown: Second transformation type - Parameters for second transformation - Checkbox: Show intermediate states - Button: Animate both pipelines - Toggle: Show matrix products</p> <p>Default parameters: - T: Rotation 45\u00b0 - S: Scaling (2, 1) - Shape: unit square</p> <p>Behavior: - Both pipelines animate simultaneously for comparison - Intermediate shapes visible between transformations - Matrix products computed and displayed - Clear visual demonstration that order matters - Commutative cases (e.g., two rotations) show same result</p> <p>Implementation: p5.js with parallel animation</p>"},{"location":"chapters/04-linear-transformations/#kernel-and-range","title":"Kernel and Range","text":"<p>Every linear transformation has two fundamental subspaces that reveal its structure.</p>"},{"location":"chapters/04-linear-transformations/#kernel-null-space","title":"Kernel (Null Space)","text":"<p>The kernel of a linear transformation \\(T: V \\rightarrow W\\) is the set of all vectors that map to zero:</p> <p>\\(\\ker(T) = \\{\\mathbf{v} \\in V : T(\\mathbf{v}) = \\mathbf{0}\\}\\)</p> <p>For a matrix transformation \\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the kernel equals the null space of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Null}(\\mathbf{A}) = \\{\\mathbf{x} : \\mathbf{A}\\mathbf{x} = \\mathbf{0}\\}\\)</p> <p>The kernel is always a subspace of the domain. Its dimension is called the nullity of \\(T\\).</p>"},{"location":"chapters/04-linear-transformations/#range-column-space","title":"Range (Column Space)","text":"<p>The range of \\(T\\) is the set of all possible outputs:</p> <p>\\(\\text{Range}(T) = \\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</p> <p>For a matrix \\(\\mathbf{A}\\), the range equals the column space\u2014the span of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Col}(\\mathbf{A}) = \\text{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\\)</p> <p>The range is always a subspace of the codomain. Its dimension is the rank of \\(T\\).</p> Subspace Definition Dimension Name Kernel / Null Space Vectors mapping to zero Nullity Range / Column Space All possible outputs Rank"},{"location":"chapters/04-linear-transformations/#diagram-kernel-and-range-visualizer","title":"Diagram: Kernel and Range Visualizer","text":"Kernel and Range Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: differentiate, examine</p> <p>Learning Objective: Help students visualize the kernel (what maps to zero) and range (what outputs are possible) of a linear transformation, understanding their relationship to the matrix structure.</p> <p>Canvas layout: - Left: Domain space showing kernel - Right: Codomain space showing range - Center: Transformation arrow - Bottom: Matrix and dimension information</p> <p>Visual elements: - Domain with kernel subspace highlighted (line or plane in gray) - Arrows from domain to codomain - Codomain with range subspace highlighted (line or plane in color) - Zero vector in codomain marked - Vectors in kernel shown collapsing to zero - Sample vectors outside kernel shown mapping to range</p> <p>Interactive controls: - Matrix editor (2\u00d72 or 3\u00d72 or 2\u00d73) - Button: Random full-rank matrix - Button: Random rank-deficient matrix - Checkbox: Show kernel vectors - Checkbox: Show how kernel maps to zero - Checkbox: Animate transformation - Display: Rank and nullity values</p> <p>Default parameters: - Matrix: 2\u00d73 with rank 2 (nontrivial kernel)</p> <p>Behavior: - Kernel automatically computed and displayed - Range computed as column space - Arrows show transformation action - Rank and nullity update with matrix changes - Animation shows vectors transforming</p> <p>Implementation: p5.js with linear algebra computations</p>"},{"location":"chapters/04-linear-transformations/#the-rank-nullity-theorem","title":"The Rank-Nullity Theorem","text":"<p>One of the most important theorems in linear algebra connects the dimensions of the kernel and range.</p>"},{"location":"chapters/04-linear-transformations/#statement-of-the-theorem","title":"Statement of the Theorem","text":"<p>For a linear transformation \\(T: V \\rightarrow W\\) where \\(V\\) is finite-dimensional:</p>"},{"location":"chapters/04-linear-transformations/#rank-nullity-theorem","title":"Rank-Nullity Theorem","text":"<p>\\(\\dim(V) = \\text{rank}(T) + \\text{nullity}(T)\\)</p> <p>where:</p> <ul> <li>\\(\\dim(V)\\) is the dimension of the domain (number of columns of \\(\\mathbf{A}\\))</li> <li>\\(\\text{rank}(T)\\) is the dimension of the range</li> <li>\\(\\text{nullity}(T)\\) is the dimension of the kernel</li> </ul> <p>For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\):</p> <p>\\(n = \\text{rank}(\\mathbf{A}) + \\text{nullity}(\\mathbf{A})\\)</p>"},{"location":"chapters/04-linear-transformations/#intuition","title":"Intuition","text":"<p>The theorem says that dimension is conserved: what doesn't go into the kernel must go somewhere (the range). If more vectors collapse to zero (higher nullity), fewer independent output directions remain (lower rank).</p>"},{"location":"chapters/04-linear-transformations/#consequences","title":"Consequences","text":"<p>The Rank-Nullity Theorem has powerful implications:</p> <ul> <li>If \\(\\text{rank}(\\mathbf{A}) = n\\) (full column rank), then \\(\\text{nullity} = 0\\), so \\(T\\) is injective (one-to-one)</li> <li>If \\(\\text{rank}(\\mathbf{A}) = m\\) (full row rank), then \\(T\\) is surjective (onto)</li> <li>If both, \\(T\\) is bijective (invertible) and \\(m = n\\)</li> </ul> Matrix Size Rank Nullity Properties \\(3 \\times 3\\), rank 3 3 0 Invertible \\(3 \\times 3\\), rank 2 2 1 Kernel is a line \\(3 \\times 4\\), rank 3 3 1 Kernel is a line \\(4 \\times 3\\), rank 3 3 0 One-to-one"},{"location":"chapters/04-linear-transformations/#invertible-transformations","title":"Invertible Transformations","text":"<p>An invertible transformation is a linear transformation with an inverse\u2014a transformation that \"undoes\" the original.</p>"},{"location":"chapters/04-linear-transformations/#conditions-for-invertibility","title":"Conditions for Invertibility","text":"<p>A linear transformation \\(T: V \\rightarrow W\\) is invertible if and only if:</p> <ul> <li>\\(T\\) is one-to-one (injective): different inputs give different outputs</li> <li>\\(T\\) is onto (surjective): every output is achieved</li> <li>Equivalently: \\(\\ker(T) = \\{\\mathbf{0}\\}\\) and \\(\\text{Range}(T) = W\\)</li> </ul> <p>For a matrix \\(\\mathbf{A}\\):</p> <ul> <li>Must be square (\\(m = n\\))</li> <li>Must have full rank (\\(\\text{rank}(\\mathbf{A}) = n\\))</li> <li>Must have nullity zero</li> <li>Must have nonzero determinant</li> </ul>"},{"location":"chapters/04-linear-transformations/#inverse-of-geometric-transformations","title":"Inverse of Geometric Transformations","text":"Transformation Matrix Inverse Rotation by \\(\\theta\\) \\(\\mathbf{R}(\\theta)\\) \\(\\mathbf{R}(-\\theta) = \\mathbf{R}^T\\) Scaling by \\((s_x, s_y)\\) \\(\\text{diag}(s_x, s_y)\\) \\(\\text{diag}(1/s_x, 1/s_y)\\) Shear by \\(k\\) \\(\\mathbf{H}(k)\\) \\(\\mathbf{H}(-k)\\) Reflection \\(\\mathbf{F}\\) \\(\\mathbf{F}\\) (self-inverse) Projection \\(\\mathbf{P}\\) Not invertible <p>Projections are never invertible because they collapse dimension\u2014once information is lost, it cannot be recovered.</p>"},{"location":"chapters/04-linear-transformations/#change-of-basis","title":"Change of Basis","text":"<p>Different bases provide different \"viewpoints\" on the same vector space. Change of basis allows us to translate between these viewpoints.</p>"},{"location":"chapters/04-linear-transformations/#basis-transition-matrix","title":"Basis Transition Matrix","text":"<p>Given two bases \\(\\mathcal{B} = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) and \\(\\mathcal{C} = \\{\\mathbf{c}_1, \\ldots, \\mathbf{c}_n\\}\\), the basis transition matrix \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) converts coordinates from \\(\\mathcal{C}\\) to \\(\\mathcal{B}\\).</p> <p>If \\([\\mathbf{v}]_{\\mathcal{C}}\\) are the coordinates of \\(\\mathbf{v}\\) in basis \\(\\mathcal{C}\\), then:</p> <p>\\([\\mathbf{v}]_{\\mathcal{B}} = \\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}} [\\mathbf{v}]_{\\mathcal{C}}\\)</p> <p>The columns of \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) are the \\(\\mathcal{C}\\) basis vectors expressed in \\(\\mathcal{B}\\) coordinates.</p>"},{"location":"chapters/04-linear-transformations/#similar-matrices","title":"Similar Matrices","text":"<p>If \\(\\mathbf{A}\\) represents a transformation in the standard basis and \\(\\mathbf{P}\\) is the change of basis matrix, then:</p> <p>\\(\\mathbf{A}' = \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\)</p> <p>represents the same transformation in the new basis. Matrices related this way are called similar matrices\u2014they represent the same transformation in different coordinate systems.</p> <p>Similar matrices have the same:</p> <ul> <li>Determinant</li> <li>Trace</li> <li>Eigenvalues</li> <li>Rank</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-change-of-basis-visualizer","title":"Diagram: Change of Basis Visualizer","text":"Change of Basis Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that the same vector has different coordinate representations in different bases, and how the transition matrix converts between them.</p> <p>Canvas layout: - Left: Standard basis view with vector - Right: Custom basis view with same vector - Bottom: Coordinate displays and transition matrix</p> <p>Visual elements: - Standard basis vectors (e\u2081, e\u2082) in black - Custom basis vectors (b\u2081, b\u2082) in purple - Same geometric vector shown in both views - Coordinates displayed in each basis - Transition matrix P - Grid lines for each basis</p> <p>Interactive controls: - Draggable endpoints for custom basis vectors - Draggable vector to transform - Button: Reset to standard basis - Checkbox: Show both bases overlaid - Checkbox: Show transition matrix calculation - Dropdown: Preset bases (standard, rotated, skewed)</p> <p>Default parameters: - Custom basis: rotated 30\u00b0 from standard - Vector: (2, 1) in standard coordinates</p> <p>Behavior: - Vector stays fixed geometrically as basis changes - Coordinates update to reflect new basis - Transition matrix updates with basis - Overlay mode shows both grids simultaneously - Clear visualization that vector is unchanged, only representation</p> <p>Implementation: p5.js with coordinate transformation</p>"},{"location":"chapters/04-linear-transformations/#applications","title":"Applications","text":""},{"location":"chapters/04-linear-transformations/#computer-graphics","title":"Computer Graphics","text":"<p>Every transformation in computer graphics\u2014modeling, viewing, projection\u2014is a linear (or affine) transformation represented by matrices. The graphics pipeline applies a sequence of transformations:</p> <ol> <li>Model matrix: Object space \u2192 World space</li> <li>View matrix: World space \u2192 Camera space</li> <li>Projection matrix: Camera space \u2192 Clip space</li> </ol> <p>GPUs are optimized for these matrix multiplications, processing millions of vertices per second.</p>"},{"location":"chapters/04-linear-transformations/#robotics","title":"Robotics","text":"<p>Robot arm kinematics uses transformation matrices to track how joints connect. Each joint applies a rotation or translation, and composing these gives the end-effector position:</p> <p>\\(\\mathbf{T}_{\\text{total}} = \\mathbf{T}_1 \\mathbf{T}_2 \\cdots \\mathbf{T}_n\\)</p>"},{"location":"chapters/04-linear-transformations/#neural-networks","title":"Neural Networks","text":"<p>Each layer of a neural network applies a linear transformation (weights) followed by a nonlinearity:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>Understanding transformations helps interpret:</p> <ul> <li>What the network \"sees\" at each layer</li> <li>How information flows and transforms</li> <li>Why deep networks can learn complex mappings</li> </ul>"},{"location":"chapters/04-linear-transformations/#principal-component-analysis","title":"Principal Component Analysis","text":"<p>PCA finds a change of basis that:</p> <ul> <li>Aligns axes with directions of maximum variance</li> <li>Decorrelates the data</li> <li>Enables dimensionality reduction by projecting onto top components</li> </ul> <p>This is a change of basis to the eigenvector basis of the covariance matrix.</p>"},{"location":"chapters/04-linear-transformations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter connected matrix algebra to geometric transformation.</p> <p>Foundations:</p> <ul> <li>A function maps inputs from a domain to outputs in a codomain</li> <li>A linear transformation preserves addition and scalar multiplication</li> <li>Every linear transformation has a transformation matrix whose columns are images of basis vectors</li> </ul> <p>Geometric Transformations:</p> <ul> <li>Rotation matrices rotate while preserving lengths and angles</li> <li>Scaling matrices stretch or compress along coordinate axes</li> <li>Shear matrices skew shapes by sliding parallel to an axis</li> <li>Reflection matrices mirror across a line or plane</li> <li>Projection matrices map onto lower-dimensional subspaces</li> </ul> <p>Composition and Structure:</p> <ul> <li>Composition of transformations corresponds to matrix multiplication</li> <li>Order matters: \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) in general</li> <li>The kernel contains vectors mapping to zero; its dimension is nullity</li> <li>The range is the set of all outputs; its dimension is rank</li> <li>Rank-Nullity Theorem: \\(n = \\text{rank} + \\text{nullity}\\)</li> </ul> <p>Invertibility and Basis:</p> <ul> <li>Invertible transformations have trivial kernel and full range</li> <li>Change of basis provides different coordinate views of the same transformation</li> <li>Similar matrices represent the same transformation in different bases</li> </ul> <p>Key Properties:</p> Transformation Preserves Lengths? Preserves Angles? Invertible? Determinant Rotation Yes Yes Yes 1 Uniform Scaling No Yes Yes (if \\(k \\neq 0\\)) \\(k^n\\) Shear No No Yes 1 Reflection Yes Yes Yes \\(-1\\) Projection No No No 0"},{"location":"chapters/04-linear-transformations/#exercises","title":"Exercises","text":"Exercise 1: Finding Transformation Matrices <p>Find the 2\u00d72 matrix for the linear transformation that:</p> <p>a) Reflects across the line \\(y = x\\)</p> <p>b) Rotates by 90\u00b0 counterclockwise, then scales by factor 2</p> <p>c) Projects onto the line \\(y = 2x\\)</p> Exercise 2: Composition Order <p>Let \\(R\\) be rotation by 45\u00b0 and \\(S\\) be scaling by \\((2, 1)\\). Compute both \\(RS\\) and \\(SR\\), and describe geometrically why they differ.</p> Exercise 3: Kernel and Range <p>For the matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix}\\):</p> <p>a) Find a basis for the kernel (null space)</p> <p>b) Find a basis for the range (column space)</p> <p>c) Verify the Rank-Nullity Theorem</p> Exercise 4: Invertibility <p>Determine which transformations are invertible and find the inverse if it exists:</p> <p>a) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{bmatrix}\\mathbf{x}\\)</p> <p>b) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 3 \\end{bmatrix}\\mathbf{x}\\)</p> <p>c) Projection onto the x-axis</p> Exercise 5: Change of Basis <p>Let \\(\\mathcal{B} = \\{(1, 1), (1, -1)\\}\\) be a basis for \\(\\mathbb{R}^2\\).</p> <p>a) Find the change of basis matrix from standard coordinates to \\(\\mathcal{B}\\)-coordinates</p> <p>b) Express the vector \\((3, 1)\\) in \\(\\mathcal{B}\\)-coordinates</p> <p>c) If a transformation is rotation by 90\u00b0 in standard coordinates, what matrix represents it in \\(\\mathcal{B}\\)-coordinates?</p>"},{"location":"chapters/04-linear-transformations/quiz/","title":"Quiz: Linear Transformations","text":"<p>Test your understanding of linear transformations and their properties.</p>"},{"location":"chapters/04-linear-transformations/quiz/#1-a-transformation-t-is-linear-if-it-satisfies","title":"1. A transformation \\(T\\) is linear if it satisfies:","text":"<ol> <li>\\(T(\\mathbf{x} + \\mathbf{y}) = T(\\mathbf{x}) + T(\\mathbf{y})\\) and \\(T(c\\mathbf{x}) = cT(\\mathbf{x})\\)</li> <li>\\(T(\\mathbf{x}) = \\mathbf{x}\\) for all \\(\\mathbf{x}\\)</li> <li>\\(T(\\mathbf{x}) \\cdot T(\\mathbf{y}) = T(\\mathbf{x} \\cdot \\mathbf{y})\\)</li> <li>\\(T(\\mathbf{0}) \\neq \\mathbf{0}\\)</li> </ol> Show Answer <p>The correct answer is A. A linear transformation satisfies two properties: additivity \\(T(\\mathbf{x} + \\mathbf{y}) = T(\\mathbf{x}) + T(\\mathbf{y})\\) and homogeneity \\(T(c\\mathbf{x}) = cT(\\mathbf{x})\\). These can be combined as \\(T(a\\mathbf{x} + b\\mathbf{y}) = aT(\\mathbf{x}) + bT(\\mathbf{y})\\).</p> <p>Concept Tested: Linear Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#2-every-linear-transformation-can-be-represented-by","title":"2. Every linear transformation can be represented by:","text":"<ol> <li>A scalar</li> <li>A vector</li> <li>A matrix</li> <li>A polynomial</li> </ol> Show Answer <p>The correct answer is C. Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. For \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\), there exists a unique \\(m \\times n\\) matrix \\(A\\) such that \\(T(\\mathbf{x}) = A\\mathbf{x}\\).</p> <p>Concept Tested: Matrix Representation</p>"},{"location":"chapters/04-linear-transformations/quiz/#3-what-is-the-kernel-null-space-of-a-linear-transformation","title":"3. What is the kernel (null space) of a linear transformation?","text":"<ol> <li>The set of all possible outputs</li> <li>The set of all inputs that map to the zero vector</li> <li>The identity transformation</li> <li>The inverse transformation</li> </ol> Show Answer <p>The correct answer is B. The kernel (or null space) of a linear transformation \\(T\\) is the set of all vectors \\(\\mathbf{x}\\) such that \\(T(\\mathbf{x}) = \\mathbf{0}\\). It measures the \"collapse\" in the transformation.</p> <p>Concept Tested: Kernel</p>"},{"location":"chapters/04-linear-transformations/quiz/#4-the-image-range-of-a-linear-transformation-is","title":"4. The image (range) of a linear transformation is:","text":"<ol> <li>The set of all inputs</li> <li>The set of all possible outputs</li> <li>The zero vector</li> <li>The inverse of the kernel</li> </ol> Show Answer <p>The correct answer is B. The image (or range) of a linear transformation is the set of all possible outputs\u2014all vectors that can be reached by applying the transformation to some input. It equals the column space of the matrix representation.</p> <p>Concept Tested: Image</p>"},{"location":"chapters/04-linear-transformations/quiz/#5-a-rotation-in-2d-by-angle-theta-counterclockwise-is-represented-by","title":"5. A rotation in 2D by angle \\(\\theta\\) counterclockwise is represented by:","text":"<ol> <li>\\(\\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\sin\\theta &amp; \\cos\\theta \\\\ -\\cos\\theta &amp; \\sin\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\theta &amp; 0 \\\\ 0 &amp; \\theta \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is B. The standard 2D rotation matrix for counterclockwise rotation by angle \\(\\theta\\) is \\(\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\). This preserves lengths and rotates vectors by the specified angle.</p> <p>Concept Tested: Rotation Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#6-a-linear-transformation-is-injective-one-to-one-if-and-only-if","title":"6. A linear transformation is injective (one-to-one) if and only if:","text":"<ol> <li>Its image is the entire codomain</li> <li>Its kernel contains only the zero vector</li> <li>It is represented by a square matrix</li> <li>It preserves the dot product</li> </ol> Show Answer <p>The correct answer is B. A linear transformation is injective if and only if its kernel is trivial (contains only \\(\\mathbf{0}\\)). This means different inputs always produce different outputs\u2014no two distinct vectors map to the same output.</p> <p>Concept Tested: Injective Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#7-which-transformation-scales-a-vector-by-2-in-the-x-direction-only","title":"7. Which transformation scales a vector by 2 in the x-direction only?","text":"<ol> <li>\\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 0 &amp; 2 \\\\ 2 &amp; 0 \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is B. The matrix \\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\) scales the x-component by 2 while leaving the y-component unchanged. This is a non-uniform scaling (stretching) transformation.</p> <p>Concept Tested: Scaling Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#8-the-composition-of-two-linear-transformations-t_1-and-t_2-corresponds-to","title":"8. The composition of two linear transformations \\(T_1\\) and \\(T_2\\) corresponds to:","text":"<ol> <li>Adding their matrices</li> <li>Multiplying their matrices</li> <li>Finding the inverse of their matrices</li> <li>Taking the transpose of their matrices</li> </ol> Show Answer <p>The correct answer is B. The composition of linear transformations corresponds to matrix multiplication. If \\(T_1\\) is represented by \\(A\\) and \\(T_2\\) by \\(B\\), then \\(T_2 \\circ T_1\\) (apply \\(T_1\\) first, then \\(T_2\\)) is represented by \\(BA\\).</p> <p>Concept Tested: Composition of Transformations</p>"},{"location":"chapters/04-linear-transformations/quiz/#9-a-reflection-across-the-x-axis-is-represented-by","title":"9. A reflection across the x-axis is represented by:","text":"<ol> <li>\\(\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is A. The matrix \\(\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\) reflects vectors across the x-axis by negating the y-component while preserving the x-component.</p> <p>Concept Tested: Reflection Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#10-a-shear-transformation-in-the-x-direction","title":"10. A shear transformation in the x-direction:","text":"<ol> <li>Rotates vectors around the origin</li> <li>Scales all vectors uniformly</li> <li>Shifts x-coordinates proportionally to y-coordinates</li> <li>Projects vectors onto the x-axis</li> </ol> Show Answer <p>The correct answer is C. A shear in the x-direction shifts x-coordinates by an amount proportional to the y-coordinate, represented by \\(\\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\). This slants rectangles into parallelograms while preserving area.</p> <p>Concept Tested: Shear Transformation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/","title":"Determinants and Matrix Properties","text":""},{"location":"chapters/05-determinants-and-matrix-properties/#summary","title":"Summary","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes. This chapter covers determinant computation methods, their properties, and geometric interpretation as the volume scaling factor of a transformation. You will also learn Cramer's rule and understand the relationship between determinants and matrix invertibility.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 13 concepts from the learning graph:</p> <ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"chapters/05-determinants-and-matrix-properties/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/","title":"Quiz: Determinants and Matrix Properties","text":"<p>Test your understanding of determinants and fundamental matrix properties.</p> <p>Note: This quiz covers key concepts from the chapter outline. Full chapter content is under development.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#1-what-does-the-determinant-of-a-matrix-represent-geometrically","title":"1. What does the determinant of a matrix represent geometrically?","text":"<ol> <li>The sum of diagonal elements</li> <li>The signed volume scaling factor of the transformation</li> <li>The number of pivots in row echelon form</li> <li>The trace of the matrix</li> </ol> Show Answer <p>The correct answer is B. The determinant represents the signed scaling factor for volumes (areas in 2D). A determinant of 2 means the transformation doubles volumes; a negative determinant indicates the transformation reverses orientation.</p> <p>Concept Tested: Determinant</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#2-if-deta-0-then-the-matrix-a-is","title":"2. If \\(\\det(A) = 0\\), then the matrix \\(A\\) is:","text":"<ol> <li>Orthogonal</li> <li>Symmetric</li> <li>Singular (non-invertible)</li> <li>Positive definite</li> </ol> Show Answer <p>The correct answer is C. A matrix with zero determinant is singular, meaning it has no inverse. The transformation collapses space in at least one dimension, making the operation irreversible.</p> <p>Concept Tested: Singular Matrix</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#3-for-any-square-matrices-a-and-b-which-property-holds","title":"3. For any square matrices \\(A\\) and \\(B\\), which property holds?","text":"<ol> <li>\\(\\det(A + B) = \\det(A) + \\det(B)\\)</li> <li>\\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</li> <li>\\(\\det(AB) = \\det(A) + \\det(B)\\)</li> <li>\\(\\det(A^{-1}) = \\det(A)\\)</li> </ol> Show Answer <p>The correct answer is B. The determinant is multiplicative: \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\). This reflects that composing transformations multiplies their volume scaling factors.</p> <p>Concept Tested: Determinant Properties</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#4-the-trace-of-a-matrix-is","title":"4. The trace of a matrix is:","text":"<ol> <li>The product of diagonal elements</li> <li>The sum of diagonal elements</li> <li>The sum of all elements</li> <li>The determinant divided by the dimension</li> </ol> Show Answer <p>The correct answer is B. The trace of a square matrix is the sum of its diagonal elements: \\(\\text{tr}(A) = \\sum_{i=1}^n A_{ii}\\). The trace equals the sum of eigenvalues.</p> <p>Concept Tested: Trace</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#5-what-is-detat-in-terms-of-deta","title":"5. What is \\(\\det(A^T)\\) in terms of \\(\\det(A)\\)?","text":"<ol> <li>\\(-\\det(A)\\)</li> <li>\\(\\det(A)\\)</li> <li>\\(1/\\det(A)\\)</li> <li>\\(\\det(A)^2\\)</li> </ol> Show Answer <p>The correct answer is B. The determinant of a transpose equals the original determinant: \\(\\det(A^T) = \\det(A)\\). Transposing swaps rows and columns but doesn't change the volume scaling factor.</p> <p>Concept Tested: Determinant of Transpose</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#6-if-matrix-a-has-determinant-5-what-is-det2a-for-a-3-times-3-matrix","title":"6. If matrix \\(A\\) has determinant 5, what is \\(\\det(2A)\\) for a \\(3 \\times 3\\) matrix?","text":"<ol> <li>10</li> <li>25</li> <li>40</li> <li>80</li> </ol> Show Answer <p>The correct answer is C. For an \\(n \\times n\\) matrix, \\(\\det(cA) = c^n \\det(A)\\). For a \\(3 \\times 3\\) matrix with \\(\\det(A) = 5\\): \\(\\det(2A) = 2^3 \\cdot 5 = 8 \\cdot 5 = 40\\).</p> <p>Concept Tested: Scalar Multiplication and Determinants</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#7-a-positive-definite-matrix-has","title":"7. A positive definite matrix has:","text":"<ol> <li>All positive entries</li> <li>All positive eigenvalues</li> <li>Positive determinant only</li> <li>Positive trace only</li> </ol> Show Answer <p>The correct answer is B. A positive definite matrix has all positive eigenvalues. Equivalently, \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all non-zero vectors \\(\\mathbf{x}\\). This implies the matrix is invertible.</p> <p>Concept Tested: Positive Definite</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#8-which-row-operation-does-not-change-the-determinant","title":"8. Which row operation does NOT change the determinant?","text":"<ol> <li>Adding a multiple of one row to another</li> <li>Multiplying a row by a non-zero scalar</li> <li>Swapping two rows</li> <li>All row operations change the determinant</li> </ol> Show Answer <p>The correct answer is A. Adding a multiple of one row to another does not change the determinant. Swapping rows multiplies the determinant by \\(-1\\), and multiplying a row by scalar \\(c\\) multiplies the determinant by \\(c\\).</p> <p>Concept Tested: Determinant and Row Operations</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#9-what-is-deta-1-in-terms-of-deta","title":"9. What is \\(\\det(A^{-1})\\) in terms of \\(\\det(A)\\)?","text":"<ol> <li>\\(\\det(A)\\)</li> <li>\\(-\\det(A)\\)</li> <li>\\(1/\\det(A)\\)</li> <li>\\(\\det(A)^2\\)</li> </ol> Show Answer <p>The correct answer is C. Since \\(AA^{-1} = I\\) and \\(\\det(I) = 1\\), we have \\(\\det(A) \\cdot \\det(A^{-1}) = 1\\), so \\(\\det(A^{-1}) = 1/\\det(A)\\).</p> <p>Concept Tested: Determinant of Inverse</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#10-a-matrix-is-positive-semidefinite-if","title":"10. A matrix is positive semidefinite if:","text":"<ol> <li>All eigenvalues are strictly positive</li> <li>All eigenvalues are non-negative (zero or positive)</li> <li>The determinant is positive</li> <li>All entries are non-negative</li> </ol> Show Answer <p>The correct answer is B. A positive semidefinite matrix has all non-negative eigenvalues (zero or positive). Equivalently, \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) for all vectors \\(\\mathbf{x}\\).</p> <p>Concept Tested: Positive Semidefinite</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/","title":"Eigenvalues and Eigenvectors","text":""},{"location":"chapters/06-eigenvalues-and-eigenvectors/#summary","title":"Summary","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations. This chapter covers eigenvalues, eigenvectors, characteristic polynomials, and diagonalization. You will learn the spectral theorem for symmetric matrices and the power iteration method. These concepts are essential for PCA, stability analysis, and understanding how neural networks learn.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 5: Determinants and Matrix Properties</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#introduction","title":"Introduction","text":"<p>When a linear transformation acts on a vector, it typically changes both the direction and magnitude of that vector. However, certain special vectors maintain their direction under transformation\u2014they may stretch, shrink, or flip, but they remain on the same line through the origin. These exceptional vectors, called eigenvectors, and their associated scaling factors, called eigenvalues, reveal the fundamental structure of linear transformations.</p> <p>Understanding eigenanalysis is crucial for modern AI and machine learning applications. Principal Component Analysis (PCA) uses eigenvectors to find the directions of maximum variance in data. Google's PageRank algorithm models web importance as an eigenvector problem. Neural networks converge based on the eigenvalues of their weight matrices. Stability analysis of dynamical systems depends entirely on eigenvalue properties.</p> <p>This chapter develops eigenanalysis from first principles, building intuition through visualizations before presenting the computational techniques used in practice.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigenvalues-and-eigenvectors-the-core-concepts","title":"Eigenvalues and Eigenvectors: The Core Concepts","text":"<p>Consider a linear transformation represented by a square matrix \\(A\\). When we apply \\(A\\) to most vectors, both the direction and magnitude change. But for special vectors, the transformation only scales the vector without changing its direction.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-eigen-equation","title":"The Eigen Equation","text":"<p>The relationship between a matrix \\(A\\), an eigenvector \\(\\mathbf{v}\\), and its eigenvalue \\(\\lambda\\) is captured by the eigen equation:</p> <p>\\(A\\mathbf{v} = \\lambda\\mathbf{v}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\(n \\times n\\) square matrix</li> <li>\\(\\mathbf{v}\\) is a non-zero vector (the eigenvector)</li> <li>\\(\\lambda\\) is a scalar (the eigenvalue)</li> </ul> <p>This equation states that applying the transformation \\(A\\) to the eigenvector \\(\\mathbf{v}\\) produces the same result as simply scaling \\(\\mathbf{v}\\) by the factor \\(\\lambda\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenvector-transformation-visualization","title":"Diagram: Eigenvector Transformation Visualization","text":"Eigenvector Transformation Visualization <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Demonstrate visually how eigenvectors maintain their direction under linear transformation while other vectors change direction</p> <p>Visual elements: - 2D coordinate plane with grid lines - A unit circle showing sample vectors - Original vector (blue arrow) that user can drag to different positions - Transformed vector (red arrow) showing result of A*v - Eigenvector directions displayed as dashed lines through origin - Matrix A displayed in corner with editable values</p> <p>Interactive controls: - Drag handle on the blue vector to change its direction and magnitude - 2x2 matrix input fields for A (pre-populated with example: [[2, 1], [1, 2]]) - \"Show Eigenvectors\" toggle button - \"Animate Transformation\" button that smoothly morphs vector to its transformed position - Reset button</p> <p>Default parameters: - Matrix A = [[2, 1], [1, 2]] (eigenvalues 3 and 1) - Initial vector at (1, 0) - Canvas size: responsive, minimum 600x500px</p> <p>Behavior: - As user drags the vector, show both original and transformed positions - When vector aligns with an eigenvector direction, highlight this with a glow effect - Display \"Eigenvector detected!\" message when alignment occurs - Show eigenvalue as the ratio of transformed to original length - Color code: vectors along eigenvectors glow green, others remain blue/red</p> <p>Implementation: p5.js with responsive canvas design</p> <p>The key insight is that eigenvectors define the \"natural axes\" of a linear transformation. Along these axes, the transformation acts as pure scaling\u2014no rotation or shearing occurs.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>Different eigenvalue values produce different geometric behaviors:</p> Eigenvalue Geometric Effect \\(\\lambda &gt; 1\\) Stretches the eigenvector away from origin \\(0 &lt; \\lambda &lt; 1\\) Compresses the eigenvector toward origin \\(\\lambda = 1\\) Leaves the eigenvector unchanged \\(\\lambda = 0\\) Collapses the eigenvector to the origin \\(\\lambda &lt; 0\\) Flips and scales the eigenvector <p>Eigenvectors Are Not Unique in Scale</p> <p>If \\(\\mathbf{v}\\) is an eigenvector with eigenvalue \\(\\lambda\\), then any non-zero scalar multiple \\(c\\mathbf{v}\\) is also an eigenvector with the same eigenvalue. We often normalize eigenvectors to have unit length for convenience.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#finding-eigenvalues-the-characteristic-polynomial","title":"Finding Eigenvalues: The Characteristic Polynomial","text":"<p>To find eigenvalues, we need to solve the eigen equation systematically. Rearranging \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) gives us:</p> <p>\\(A\\mathbf{v} - \\lambda\\mathbf{v} = \\mathbf{0}\\)</p> <p>\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the identity matrix of the same size as \\(A\\)</li> <li>\\(\\mathbf{0}\\) is the zero vector</li> </ul> <p>For a non-zero solution \\(\\mathbf{v}\\) to exist, the matrix \\((A - \\lambda I)\\) must be singular (non-invertible). This happens precisely when its determinant equals zero.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-characteristic-equation","title":"The Characteristic Equation","text":"<p>The characteristic equation determines all eigenvalues of a matrix:</p> <p>\\(\\det(A - \\lambda I) = 0\\)</p> <p>where:</p> <ul> <li>\\(\\det\\) denotes the determinant</li> <li>\\(A\\) is the square matrix</li> <li>\\(\\lambda\\) represents the unknown eigenvalue</li> <li>\\(I\\) is the identity matrix</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-characteristic-polynomial","title":"The Characteristic Polynomial","text":"<p>Expanding the determinant \\(\\det(A - \\lambda I)\\) produces a polynomial in \\(\\lambda\\) called the characteristic polynomial. For an \\(n \\times n\\) matrix, this polynomial has degree \\(n\\):</p> <p>\\(p(\\lambda) = \\det(A - \\lambda I) = (-1)^n \\lambda^n + c_{n-1}\\lambda^{n-1} + \\cdots + c_1\\lambda + c_0\\)</p> <p>The eigenvalues are the roots of this polynomial.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-finding-eigenvalues-of-a-22-matrix","title":"Example: Finding Eigenvalues of a 2\u00d72 Matrix","text":"<p>Consider the matrix:</p> <p>\\(A = \\begin{bmatrix} 4 &amp; 2 \\\\ 1 &amp; 3 \\end{bmatrix}\\)</p> <p>Step 1: Form \\((A - \\lambda I)\\):</p> <p>\\(A - \\lambda I = \\begin{bmatrix} 4-\\lambda &amp; 2 \\\\ 1 &amp; 3-\\lambda \\end{bmatrix}\\)</p> <p>Step 2: Compute the determinant:</p> <p>\\(\\det(A - \\lambda I) = (4-\\lambda)(3-\\lambda) - (2)(1)\\) \\(= 12 - 4\\lambda - 3\\lambda + \\lambda^2 - 2\\) \\(= \\lambda^2 - 7\\lambda + 10\\)</p> <p>Step 3: Solve the characteristic equation:</p> <p>\\(\\lambda^2 - 7\\lambda + 10 = 0\\) \\((\\lambda - 5)(\\lambda - 2) = 0\\)</p> <p>The eigenvalues are \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-characteristic-polynomial-explorer","title":"Diagram: Characteristic Polynomial Explorer","text":"Characteristic Polynomial Explorer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Enable students to interactively compute characteristic polynomials and find eigenvalues for 2x2 and 3x3 matrices</p> <p>Visual elements: - Left panel: Matrix input grid (2x2 or 3x3) - Center panel: Step-by-step calculation display showing:   - The (A - \u03bbI) matrix with \u03bb as variable   - Determinant expansion   - Resulting polynomial in standard form - Right panel: Graph of the characteristic polynomial with x-axis as \u03bb - Eigenvalues marked as points where curve crosses x-axis - Vertical dashed lines from roots to x-axis</p> <p>Interactive controls: - Matrix size toggle (2x2 / 3x3) - Numeric input fields for matrix entries - \"Calculate\" button to compute polynomial - Slider to trace along the polynomial curve - Pre-set example matrices dropdown (identity, rotation, symmetric, defective)</p> <p>Default parameters: - 2x2 matrix mode - Matrix A = [[4, 2], [1, 3]] - Polynomial graph range: \u03bb from -2 to 8 - Canvas size: responsive, minimum 900x500px</p> <p>Behavior: - Real-time polynomial graph update as matrix values change - Highlight eigenvalues on graph with dots and labels - Show factored form when roots are nice numbers - Display \"Complex roots\" indicator when polynomial has no real zeros - Step-through animation of determinant calculation</p> <p>Implementation: p5.js with math expression rendering</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#finding-eigenvectors","title":"Finding Eigenvectors","text":"<p>Once we have an eigenvalue \\(\\lambda\\), we find its corresponding eigenvector(s) by solving:</p> <p>\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)</p> <p>This is a homogeneous system of linear equations. We use row reduction to find the null space of \\((A - \\lambda I)\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-finding-eigenvectors","title":"Example: Finding Eigenvectors","text":"<p>Continuing with our matrix \\(A = \\begin{bmatrix} 4 &amp; 2 \\\\ 1 &amp; 3 \\end{bmatrix}\\):</p> <p>For \\(\\lambda_1 = 5\\):</p> <p>\\(A - 5I = \\begin{bmatrix} -1 &amp; 2 \\\\ 1 &amp; -2 \\end{bmatrix}\\)</p> <p>Row reduce to find the null space:</p> <p>\\(\\begin{bmatrix} -1 &amp; 2 \\\\ 1 &amp; -2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; -2 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>From \\(x_1 - 2x_2 = 0\\), we get \\(x_1 = 2x_2\\). Setting \\(x_2 = 1\\):</p> <p>\\(\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)</p> <p>For \\(\\lambda_2 = 2\\):</p> <p>\\(A - 2I = \\begin{bmatrix} 2 &amp; 2 \\\\ 1 &amp; 1 \\end{bmatrix}\\)</p> <p>Row reduce:</p> <p>\\(\\begin{bmatrix} 2 &amp; 2 \\\\ 1 &amp; 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>From \\(x_1 + x_2 = 0\\), we get \\(x_1 = -x_2\\). Setting \\(x_2 = 1\\):</p> <p>\\(\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigenspace","title":"Eigenspace","text":"<p>The eigenspace corresponding to an eigenvalue \\(\\lambda\\) is the set of all eigenvectors with that eigenvalue, together with the zero vector. Formally:</p> <p>\\(E_\\lambda = \\text{null}(A - \\lambda I) = \\{\\mathbf{v} \\in \\mathbb{R}^n : A\\mathbf{v} = \\lambda\\mathbf{v}\\}\\)</p> <p>The eigenspace is a vector subspace of \\(\\mathbb{R}^n\\). Its dimension is called the geometric multiplicity of the eigenvalue.</p> <p>Key properties of eigenspaces:</p> <ul> <li>Every eigenspace contains the zero vector</li> <li>Every eigenspace is closed under addition and scalar multiplication</li> <li>The dimension of an eigenspace is at least 1</li> <li>Eigenvectors from different eigenspaces are linearly independent</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenspace-visualization","title":"Diagram: Eigenspace Visualization","text":"Eigenspace Visualization <p>Type: diagram</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize eigenspaces as subspaces and understand how their dimension relates to geometric multiplicity</p> <p>Components to show: - 3D coordinate system with translucent planes/lines representing eigenspaces - For a 3x3 matrix with three distinct eigenvalues: three lines through origin - For a 3x3 matrix with a repeated eigenvalue (geometric multiplicity 2): one line and one plane - Original vectors and their transformed counterparts - Color coding by eigenvalue</p> <p>Layout: - Main 3D view showing eigenspaces - Rotation controls to view from different angles - Matrix display in corner - Legend showing eigenvalue-color correspondence</p> <p>Visual style: - Eigenspaces rendered as semi-transparent colored surfaces - Lines rendered as tubes for visibility - Sample vectors as arrows with different opacities</p> <p>Color scheme: - Eigenspace 1: Blue (line or plane) - Eigenspace 2: Orange (line or plane) - Eigenspace 3: Green (line) - Background grid: Light gray</p> <p>Implementation: Three.js or p5.js with WEBGL mode for 3D rendering</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#algebraic-and-geometric-multiplicity","title":"Algebraic and Geometric Multiplicity","text":"<p>The relationship between algebraic and geometric multiplicity is fundamental to understanding when matrices can be diagonalized.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#algebraic-multiplicity","title":"Algebraic Multiplicity","text":"<p>The algebraic multiplicity of an eigenvalue is the number of times it appears as a root of the characteristic polynomial. If the characteristic polynomial factors as:</p> <p>\\(p(\\lambda) = (\\lambda - \\lambda_1)^{m_1}(\\lambda - \\lambda_2)^{m_2} \\cdots (\\lambda - \\lambda_k)^{m_k}\\)</p> <p>then the algebraic multiplicity of \\(\\lambda_i\\) is \\(m_i\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-multiplicity","title":"Geometric Multiplicity","text":"<p>The geometric multiplicity of an eigenvalue is the dimension of its eigenspace:</p> <p>\\(g_i = \\dim(E_{\\lambda_i}) = \\dim(\\text{null}(A - \\lambda_i I))\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-multiplicity-inequality","title":"The Multiplicity Inequality","text":"<p>A fundamental theorem states that for any eigenvalue:</p> <p>\\(1 \\leq \\text{geometric multiplicity} \\leq \\text{algebraic multiplicity}\\)</p> <p>This inequality has profound implications for diagonalization.</p> Scenario Algebraic Geometric Diagonalizable? All eigenvalues distinct 1 each 1 each Yes Repeated eigenvalue, full eigenspace \\(m\\) \\(m\\) Yes Repeated eigenvalue, deficient eigenspace \\(m\\) \\(&lt; m\\) No"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-multiplicity-comparison-chart","title":"Diagram: Multiplicity Comparison Chart","text":"Multiplicity Comparison Chart <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare algebraic and geometric multiplicity across different matrix types and understand implications for diagonalization</p> <p>Layout: Three-column comparison with expandable examples</p> <p>Sections: 1. \"Distinct Eigenvalues\" column    - Example: A = [[2, 0], [0, 3]]    - Characteristic polynomial: (\u03bb-2)(\u03bb-3)    - Each eigenvalue has alg. mult. = geo. mult. = 1    - Status: Diagonalizable \u2713</p> <ol> <li>\"Repeated with Full Eigenspace\" column</li> <li>Example: A = [[2, 0], [0, 2]]</li> <li>Characteristic polynomial: (\u03bb-2)\u00b2</li> <li>Eigenvalue 2: alg. mult. = 2, geo. mult. = 2</li> <li> <p>Status: Diagonalizable \u2713</p> </li> <li> <p>\"Defective Matrix\" column</p> </li> <li>Example: A = [[2, 1], [0, 2]]</li> <li>Characteristic polynomial: (\u03bb-2)\u00b2</li> <li>Eigenvalue 2: alg. mult. = 2, geo. mult. = 1</li> <li>Status: Not Diagonalizable \u2717</li> </ol> <p>Interactive elements: - Hover over each example to see eigenspace visualization - Click to expand full calculation - Toggle between 2x2 and 3x3 examples</p> <p>Visual style: - Clean cards with matrix notation - Color indicators: green for diagonalizable, red for defective - Progress bars showing geometric/algebraic ratio</p> <p>Implementation: HTML/CSS/JavaScript with SVG visualizations</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#similar-matrices-and-diagonalization","title":"Similar Matrices and Diagonalization","text":"<p>Two matrices \\(A\\) and \\(B\\) are similar if there exists an invertible matrix \\(P\\) such that:</p> <p>\\(B = P^{-1}AP\\)</p> <p>Similar matrices share important properties:</p> <ul> <li>Same eigenvalues (with same algebraic multiplicities)</li> <li>Same determinant</li> <li>Same trace</li> <li>Same rank</li> <li>Same characteristic polynomial</li> </ul> <p>The geometric interpretation is that similar matrices represent the same linear transformation in different bases.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagonalization","title":"Diagonalization","text":"<p>A matrix \\(A\\) is diagonalizable if it is similar to a diagonal matrix. This means we can write:</p> <p>\\(A = PDP^{-1}\\)</p> <p>where:</p> <ul> <li>\\(D\\) is a diagonal matrix containing the eigenvalues</li> <li>\\(P\\) is a matrix whose columns are the corresponding eigenvectors</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-diagonal-form","title":"The Diagonal Form","text":"<p>The diagonal form \\(D\\) of a diagonalizable matrix contains eigenvalues on its main diagonal:</p> <p>\\(D = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix}\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#conditions-for-diagonalizability","title":"Conditions for Diagonalizability","text":"<p>A matrix \\(A\\) is diagonalizable if and only if:</p> <ol> <li>The sum of geometric multiplicities equals \\(n\\) (the matrix dimension), OR</li> <li>For each eigenvalue, geometric multiplicity equals algebraic multiplicity, OR</li> <li>\\(A\\) has \\(n\\) linearly independent eigenvectors</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-diagonalization-process-workflow","title":"Diagram: Diagonalization Process Workflow","text":"Diagonalization Process Workflow <p>Type: workflow</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Guide students through the step-by-step process of diagonalizing a matrix</p> <p>Visual style: Flowchart with decision diamonds and process rectangles</p> <p>Steps: 1. Start: \"Given matrix A\"    Hover text: \"Begin with an n\u00d7n matrix A\"</p> <ol> <li> <p>Process: \"Find characteristic polynomial det(A - \u03bbI)\"    Hover text: \"Expand determinant to get polynomial in \u03bb\"</p> </li> <li> <p>Process: \"Solve characteristic equation for eigenvalues\"    Hover text: \"Find all roots \u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2096\"</p> </li> <li> <p>Decision: \"All n eigenvalues found (counting multiplicity)?\"    Hover text: \"Complex eigenvalues count too\"</p> </li> <li> <p>No \u2192 End: \"Check for complex eigenvalues\"</p> </li> <li> <p>Process: \"For each eigenvalue, find eigenvectors\"    Hover text: \"Solve (A - \u03bbI)v = 0 for each \u03bb\"</p> </li> <li> <p>Process: \"Determine geometric multiplicity of each eigenvalue\"    Hover text: \"Count linearly independent eigenvectors\"</p> </li> <li> <p>Decision: \"Geometric mult. = Algebraic mult. for all eigenvalues?\"</p> </li> <li>No \u2192 End: \"Matrix is NOT diagonalizable\"</li> <li> <p>Yes \u2192 Continue</p> </li> <li> <p>Process: \"Form P from eigenvector columns\"    Hover text: \"P = [v\u2081 | v\u2082 | ... | v\u2099]\"</p> </li> <li> <p>Process: \"Form D from eigenvalues\"    Hover text: \"D = diag(\u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2099)\"</p> </li> <li> <p>End: \"A = PDP\u207b\u00b9\"     Hover text: \"Diagonalization complete!\"</p> </li> </ol> <p>Color coding: - Blue: Computation steps - Yellow: Decision points - Green: Success outcomes - Red: Failure outcomes</p> <p>Implementation: Mermaid.js or custom SVG with hover interactions</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#why-diagonalization-matters","title":"Why Diagonalization Matters","text":"<p>Diagonalization simplifies many computations:</p> <p>Matrix Powers: Computing \\(A^k\\) becomes trivial:</p> <p>\\(A^k = PD^kP^{-1}\\)</p> <p>where \\(D^k = \\begin{bmatrix} \\lambda_1^k &amp; 0 &amp; \\cdots \\\\ 0 &amp; \\lambda_2^k &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\)</p> <p>Exponentials: The matrix exponential \\(e^A\\) is essential for solving differential equations:</p> <p>\\(e^A = Pe^DP^{-1}\\)</p> <p>where \\(e^D = \\begin{bmatrix} e^{\\lambda_1} &amp; 0 &amp; \\cdots \\\\ 0 &amp; e^{\\lambda_2} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\)</p> <p>Systems of Differential Equations: The system \\(\\frac{d\\mathbf{x}}{dt} = A\\mathbf{x}\\) has solution:</p> <p>\\(\\mathbf{x}(t) = c_1e^{\\lambda_1 t}\\mathbf{v}_1 + c_2e^{\\lambda_2 t}\\mathbf{v}_2 + \\cdots\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-matrix-power-calculator","title":"Diagram: Matrix Power Calculator","text":"Matrix Power Calculator MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Demonstrate how diagonalization simplifies computing matrix powers by comparing direct multiplication vs. the eigenvalue approach</p> <p>Visual elements: - Left panel: Input matrix A (2x2 or 3x3) - Center panel: Diagonalization display showing P, D, P\u207b\u00b9 - Right panel: Result matrix A\u1d4f - Bottom: Step-by-step calculation toggle</p> <p>Interactive controls: - Matrix entry fields for A - Power k slider (1 to 20) - \"Compute Direct\" button (shows A\u00d7A\u00d7...\u00d7A method) - \"Compute via Diagonalization\" button (shows PD^kP^{-1} method) - Speed comparison display - Animation speed slider</p> <p>Default parameters: - Matrix A = [[2, 1], [0, 3]] - Power k = 5 - Canvas: responsive layout</p> <p>Behavior: - Show step-by-step computation for both methods - Highlight the efficiency of eigenvalue method for large k - Display operation count for each method - Warning message if matrix is not diagonalizable - Show numerical error comparison for high powers</p> <p>Implementation: p5.js with matrix computation library</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigendecomposition","title":"Eigendecomposition","text":"<p>The eigendecomposition (also called spectral decomposition for symmetric matrices) expresses a diagonalizable matrix as a product of its eigenvectors and eigenvalues:</p> <p>\\(A = PDP^{-1} = \\sum_{i=1}^{n} \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) (for symmetric matrices with orthonormal eigenvectors)</p> <p>More generally, for any diagonalizable matrix:</p> <p>\\(A = \\sum_{i=1}^{n} \\lambda_i \\mathbf{v}_i \\mathbf{w}_i^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_i\\) are the right eigenvectors (columns of \\(P\\))</li> <li>\\(\\mathbf{w}_i\\) are the left eigenvectors (rows of \\(P^{-1}\\))</li> <li>\\(\\lambda_i\\) are the eigenvalues</li> </ul> <p>This representation reveals that a matrix can be decomposed into a sum of rank-1 matrices, each scaled by an eigenvalue.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#complex-eigenvalues","title":"Complex Eigenvalues","text":"<p>Real matrices can have complex eigenvalues. When they occur, complex eigenvalues always appear in conjugate pairs: if \\(\\lambda = a + bi\\) is an eigenvalue, so is \\(\\bar{\\lambda} = a - bi\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-interpretation_1","title":"Geometric Interpretation","text":"<p>Complex eigenvalues indicate rotation in the transformation. For a 2\u00d72 real matrix with eigenvalues \\(\\lambda = a \\pm bi\\):</p> <ul> <li>\\(|{\\lambda}| = \\sqrt{a^2 + b^2}\\) gives the scaling factor</li> <li>\\(\\theta = \\arctan(b/a)\\) gives the rotation angle</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-rotation-matrix","title":"Example: Rotation Matrix","text":"<p>The rotation matrix by angle \\(\\theta\\):</p> <p>\\(R_\\theta = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>has eigenvalues \\(\\lambda = \\cos\\theta \\pm i\\sin\\theta = e^{\\pm i\\theta}\\).</p> <p>For a 90\u00b0 rotation:</p> <p>\\(R_{90\u00b0} = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}\\)</p> <p>The eigenvalues are \\(\\lambda = \\pm i\\), which are purely imaginary\u2014reflecting pure rotation with no scaling.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-complex-eigenvalue-visualizer","title":"Diagram: Complex Eigenvalue Visualizer","text":"Complex Eigenvalue Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how complex eigenvalues correspond to rotation-scaling transformations in 2D</p> <p>Visual elements: - Left panel: 2D plane showing transformation of a unit square - Right panel: Complex plane showing eigenvalue locations - Unit circle on complex plane for reference - Spiral path showing repeated application of transformation - Angle arc showing rotation per step</p> <p>Interactive controls: - Slider for real part a of eigenvalue (range: -2 to 2) - Slider for imaginary part b of eigenvalue (range: -2 to 2) - \"Animate\" button to show repeated transformation - Step counter display - \"Show Conjugate Pair\" toggle - Reset button</p> <p>Default parameters: - a = 0.9 (slight contraction) - b = 0.4 (rotation component) - Canvas: 800x400px responsive</p> <p>Behavior: - As sliders adjust, show corresponding matrix A - Animate unit square through multiple transformation steps - Plot trajectory of corner point as spiral - Display eigenvalue magnitude and angle - Show connection between eigenvalue position and transformation behavior:   - |\u03bb| &gt; 1: spiral outward   - |\u03bb| &lt; 1: spiral inward   - |\u03bb| = 1: pure rotation (circle) - Highlight conjugate pair relationship</p> <p>Implementation: p5.js with complex number support</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#symmetric-matrices-and-the-spectral-theorem","title":"Symmetric Matrices and the Spectral Theorem","text":"<p>Symmetric matrices (\\(A = A^T\\)) have particularly nice eigenvalue properties that make them central to applications in machine learning, physics, and engineering.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#symmetric-eigenvalues","title":"Symmetric Eigenvalues","text":"<p>For a symmetric matrix \\(A\\):</p> <ol> <li>All eigenvalues are real (no complex eigenvalues)</li> <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal</li> <li>The matrix is always diagonalizable</li> </ol> <p>These properties follow from the fact that symmetric matrices equal their own transposes, constraining the characteristic polynomial to have only real roots.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-spectral-theorem","title":"The Spectral Theorem","text":"<p>The Spectral Theorem provides a complete characterization of symmetric matrices:</p> <p>The Spectral Theorem for Real Symmetric Matrices</p> <p>A real matrix \\(A\\) is symmetric if and only if it can be orthogonally diagonalized:</p> <p>\\(A = Q\\Lambda Q^T\\)</p> <p>where \\(Q\\) is an orthogonal matrix (\\(Q^TQ = I\\)) whose columns are orthonormal eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of real eigenvalues.</p> <p>The beauty of orthogonal diagonalization is that \\(Q^{-1} = Q^T\\), which is computationally simple to obtain.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#spectral-decomposition-form","title":"Spectral Decomposition Form","text":"<p>For a symmetric matrix, the eigendecomposition takes the elegant form:</p> <p>\\(A = \\sum_{i=1}^{n} \\lambda_i \\mathbf{q}_i \\mathbf{q}_i^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{q}_i\\) are orthonormal eigenvectors</li> <li>\\(\\lambda_i\\) are real eigenvalues</li> <li>Each \\(\\mathbf{q}_i \\mathbf{q}_i^T\\) is a projection matrix onto the eigenspace</li> </ul> <p>This decomposition is the foundation of Principal Component Analysis (PCA).</p> Property General Matrix Symmetric Matrix Eigenvalues May be complex Always real Eigenvectors Generally not orthogonal Orthogonal (for distinct \u03bb) Diagonalization Not guaranteed Always possible Inverse of P Must compute P\u207b\u00b9 Simply P^T Numerical stability May have issues Highly stable"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-spectral-theorem-for-symmetric-matrices","title":"Diagram: Spectral Theorem for Symmetric Matrices","text":"Spectral Theorem Interactive Demonstration <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Demonstrate the spectral theorem by showing how symmetric matrices decompose into orthogonal eigenvectors and real eigenvalues</p> <p>Visual elements: - Left panel: 2D/3D visualization of transformation - Center panel: Matrix equation display A = Q\u039bQ\u1d40 - Right panel: Individual rank-1 components \u03bb\u1d62q\u1d62q\u1d62\u1d40 - Orthogonality indicator showing q\u1d62\u00b7q\u2c7c = 0 for i\u2260j - Unit sphere showing eigenvector directions</p> <p>Interactive controls: - Symmetric matrix input (auto-enforced: entering a\u1d62\u2c7c sets a\u2c7c\u1d62) - Slider to blend between original matrix and diagonal form - Component selector to highlight individual \u03bb\u1d62q\u1d62q\u1d62\u1d40 terms - \"Verify Orthogonality\" button - 2D/3D toggle (for 2x2 and 3x3 matrices)</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 3]] (symmetric) - Canvas: responsive</p> <p>Behavior: - Real-time eigenvalue/eigenvector computation - Show eigenvectors as perpendicular on unit circle/sphere - Animate decomposition into sum of outer products - Verify QQ\u1d40 = I visually - Display reconstruction error when summing components</p> <p>Implementation: p5.js with linear algebra computations</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#power-iteration-computing-the-dominant-eigenvalue","title":"Power Iteration: Computing the Dominant Eigenvalue","text":"<p>For large matrices, computing eigenvalues through the characteristic polynomial is impractical. Power iteration is a simple iterative algorithm for finding the largest eigenvalue and its eigenvector.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-dominant-eigenvalue","title":"The Dominant Eigenvalue","text":"<p>The dominant eigenvalue is the eigenvalue with the largest absolute value:</p> <p>\\(|\\lambda_1| &gt; |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|\\)</p> <p>The corresponding eigenvector is called the dominant eigenvector.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#power-iteration-algorithm","title":"Power Iteration Algorithm","text":"<p>The algorithm works by repeatedly multiplying a random vector by the matrix:</p> <ol> <li>Start with a random non-zero vector \\(\\mathbf{x}_0\\)</li> <li>Compute \\(\\mathbf{y}_{k+1} = A\\mathbf{x}_k\\)</li> <li>Normalize: \\(\\mathbf{x}_{k+1} = \\mathbf{y}_{k+1} / \\|\\mathbf{y}_{k+1}\\|\\)</li> <li>Repeat until convergence</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#convergence-rate","title":"Convergence Rate","text":"<p>The convergence rate depends on the ratio of the two largest eigenvalues:</p> <p>\\(\\text{error after } k \\text{ iterations} \\approx O\\left(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k\\right)\\)</p> <p>If \\(|\\lambda_1| \\gg |\\lambda_2|\\), convergence is fast. If \\(|\\lambda_1| \\approx |\\lambda_2|\\), convergence is slow.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#rayleigh-quotient","title":"Rayleigh Quotient","text":"<p>After obtaining an approximate eigenvector \\(\\mathbf{x}\\), we can estimate the eigenvalue using the Rayleigh quotient:</p> <p>\\(\\lambda \\approx R(\\mathbf{x}) = \\frac{\\mathbf{x}^T A \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}\\)</p> <p>The Rayleigh quotient gives a more accurate eigenvalue estimate than examining vector scaling alone.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-power-iteration-microsim","title":"Diagram: Power Iteration MicroSim","text":"Power Iteration Algorithm Visualization <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how power iteration converges to the dominant eigenvector and how convergence rate depends on the eigenvalue ratio</p> <p>Visual elements: - Left panel: 2D plane showing vector iterations   - Current vector (solid arrow)   - Previous vectors (faded arrows showing history)   - True dominant eigenvector direction (dashed line)   - Angle error indicator - Right panel: Convergence plot   - X-axis: iteration number   - Y-axis: log(error) where error = angle to true eigenvector   - Theoretical convergence rate line for comparison - Bottom panel: Matrix and current eigenvalue estimate</p> <p>Interactive controls: - 2x2 matrix input - \"Step\" button for single iteration - \"Run\" button for continuous animation - Speed slider for animation - \"Reset with Random Vector\" button - Convergence threshold input - Display of \u03bb\u2082/\u03bb\u2081 ratio</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 2]] (eigenvalues \u2248 3.62, 1.38) - Initial vector: random unit vector - Canvas: 800x600px responsive</p> <p>Behavior: - Show each iteration step clearly - Highlight when convergence criterion met - Display iteration count and current eigenvalue estimate - Show Rayleigh quotient computation - Compare to true eigenvalue (computed analytically for 2x2) - Demonstrate slow convergence when eigenvalue ratio near 1 - Warning if matrix has complex dominant eigenvalue</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#variants-of-power-iteration","title":"Variants of Power Iteration","text":"<p>Several important algorithms extend the basic power iteration:</p> <ul> <li>Inverse Power Iteration: Find the smallest eigenvalue by applying power iteration to \\(A^{-1}\\)</li> <li>Shifted Inverse Iteration: Find eigenvalue closest to a given shift \\(\\sigma\\) using \\((A - \\sigma I)^{-1}\\)</li> <li>QR Algorithm: Industry-standard method that finds all eigenvalues simultaneously</li> <li>Lanczos Algorithm: Efficient for large sparse symmetric matrices</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#applications-in-machine-learning-and-ai","title":"Applications in Machine Learning and AI","text":"<p>Eigenanalysis is fundamental to numerous AI and machine learning algorithms:</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA finds the directions of maximum variance in data by computing eigenvectors of the covariance matrix:</p> <ol> <li>Center the data: \\(\\bar{X} = X - \\mu\\)</li> <li>Compute covariance matrix: \\(C = \\frac{1}{n-1}\\bar{X}^T\\bar{X}\\)</li> <li>Find eigenvalues and eigenvectors of \\(C\\)</li> <li>Project data onto top \\(k\\) eigenvectors</li> </ol> <p>The eigenvectors are the principal components, and eigenvalues indicate variance explained.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#pagerank-algorithm","title":"PageRank Algorithm","text":"<p>Google's PageRank models web page importance as the dominant eigenvector of a modified adjacency matrix:</p> <p>\\(\\mathbf{r} = M\\mathbf{r}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{r}\\) is the rank vector (eigenvector)</li> <li>\\(M\\) is the transition probability matrix</li> <li>PageRank is the eigenvector with eigenvalue 1</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#stability-analysis","title":"Stability Analysis","text":"<p>In dynamical systems and neural network training, eigenvalues determine stability:</p> <ul> <li>\\(|\\lambda| &lt; 1\\) for all eigenvalues: system is stable (converges)</li> <li>\\(|\\lambda| = 1\\): system is marginally stable (oscillates)</li> <li>\\(|\\lambda| &gt; 1\\): system is unstable (explodes)</li> </ul> <p>Neural networks with weight matrices having eigenvalues far from 1 suffer from vanishing or exploding gradients.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenvalue-applications-map","title":"Diagram: Eigenvalue Applications Map","text":"Eigenvalue Applications in ML and AI <p>Type: infographic</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Connect eigenanalysis concepts to real-world machine learning applications and understand when to apply each technique</p> <p>Layout: Central hub-and-spoke diagram with clickable nodes</p> <p>Central concept: \"Eigenanalysis\"</p> <p>Spokes (applications): 1. PCA / Dimensionality Reduction    - Uses: Covariance matrix eigenvectors    - Key insight: Eigenvectors = directions of max variance    - Example: Face recognition (Eigenfaces)</p> <ol> <li>Spectral Clustering</li> <li>Uses: Graph Laplacian eigenvectors</li> <li>Key insight: Second eigenvector separates clusters</li> <li> <p>Example: Image segmentation</p> </li> <li> <p>Google PageRank</p> </li> <li>Uses: Dominant eigenvector</li> <li>Key insight: Power iteration at web scale</li> <li> <p>Example: Web page ranking</p> </li> <li> <p>Neural Network Stability</p> </li> <li>Uses: Weight matrix eigenvalues</li> <li>Key insight: |\u03bb| controls gradient flow</li> <li> <p>Example: RNN vanishing gradients</p> </li> <li> <p>Recommender Systems</p> </li> <li>Uses: Matrix factorization (related to eigendecomposition)</li> <li>Key insight: Low-rank approximation</li> <li> <p>Example: Netflix recommendations</p> </li> <li> <p>Quantum Computing</p> </li> <li>Uses: Eigenvalues as measurement outcomes</li> <li>Key insight: Observables are Hermitian operators</li> <li>Example: Quantum simulation</li> </ol> <p>Interactive elements: - Click each spoke to expand details - Hover for quick summary - Links to related concepts in other chapters</p> <p>Visual style: - Modern flat design with icons for each application - Color coding by application domain (ML, physics, web, etc.) - Connecting lines showing concept flow</p> <p>Implementation: D3.js or custom SVG with interaction handlers</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#computational-considerations","title":"Computational Considerations","text":"<p>When working with eigenproblems in practice, several computational issues arise:</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#numerical-stability","title":"Numerical Stability","text":"<ul> <li>Direct polynomial root-finding is numerically unstable for large matrices</li> <li>The QR algorithm is the standard stable method</li> <li>Symmetric matrices have more stable algorithms (divide-and-conquer, MRRR)</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#computational-complexity","title":"Computational Complexity","text":"Method Complexity Use Case Characteristic polynomial \\(O(n^3)\\) for determinant Theoretical, small matrices Power iteration \\(O(n^2)\\) per iteration Dominant eigenvalue only QR algorithm \\(O(n^3)\\) total All eigenvalues, dense matrices Lanczos/Arnoldi \\(O(kn^2)\\) Top \\(k\\) eigenvalues, sparse matrices"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#software-libraries","title":"Software Libraries","text":"<p>In practice, use optimized libraries:</p> <ul> <li>NumPy/SciPy: <code>np.linalg.eig()</code>, <code>np.linalg.eigh()</code> for symmetric</li> <li>PyTorch: <code>torch.linalg.eig()</code> for GPU acceleration</li> <li>LAPACK: Industry-standard Fortran library underlying most implementations</li> </ul> <pre><code>import numpy as np\n\n# General eigenvalue problem\nA = np.array([[4, 2], [1, 3]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Symmetric matrix (more stable)\nS = np.array([[3, 1], [1, 3]])\neigenvalues, eigenvectors = np.linalg.eigh(S)\n</code></pre>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#summary_1","title":"Summary","text":"<p>This chapter introduced eigenanalysis as a fundamental tool for understanding linear transformations:</p> <p>Core Concepts:</p> <ul> <li>Eigenvalues and eigenvectors satisfy \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), revealing directions preserved by transformations</li> <li>The characteristic polynomial \\(\\det(A - \\lambda I) = 0\\) yields eigenvalues as its roots</li> <li>Eigenspaces are subspaces containing all eigenvectors for a given eigenvalue</li> </ul> <p>Multiplicities and Diagonalization:</p> <ul> <li>Algebraic multiplicity counts eigenvalue repetition in the characteristic polynomial</li> <li>Geometric multiplicity measures eigenspace dimension</li> <li>A matrix is diagonalizable when geometric equals algebraic multiplicity for all eigenvalues</li> <li>Similar matrices share eigenvalues and represent the same transformation in different bases</li> </ul> <p>Special Cases:</p> <ul> <li>Complex eigenvalues indicate rotation and appear in conjugate pairs for real matrices</li> <li>Symmetric matrices have real eigenvalues and orthogonal eigenvectors (Spectral Theorem)</li> </ul> <p>Computation:</p> <ul> <li>Power iteration finds the dominant eigenvalue through repeated matrix-vector multiplication</li> <li>The Rayleigh quotient provides eigenvalue estimates from approximate eigenvectors</li> <li>Eigendecomposition \\(A = PDP^{-1}\\) enables efficient computation of matrix powers and exponentials</li> </ul> <p>Key Takeaways for AI/ML:</p> <ol> <li>PCA reduces dimensionality using covariance matrix eigenvectors</li> <li>PageRank is an eigenvector problem solved by power iteration</li> <li>Neural network stability depends on weight matrix eigenvalues</li> <li>The spectral theorem guarantees nice properties for symmetric matrices (common in ML)</li> </ol> Self-Check: Can you identify which eigenvalue property determines whether a neural network's gradients will vanish or explode? <p>The magnitude of the eigenvalues of the weight matrices determines gradient behavior. If \\(|\\lambda| &lt; 1\\) for all eigenvalues, gradients shrink exponentially (vanishing). If \\(|\\lambda| &gt; 1\\), gradients grow exponentially (exploding). Stable training requires eigenvalues near \\(|\\lambda| = 1\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/","title":"Quiz: Eigenvalues and Eigenvectors","text":"<p>Test your understanding of eigenvalues, eigenvectors, and their applications.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#1-an-eigenvector-mathbfv-of-matrix-a-satisfies","title":"1. An eigenvector \\(\\mathbf{v}\\) of matrix \\(A\\) satisfies:","text":"<ol> <li>\\(A\\mathbf{v} = \\mathbf{0}\\)</li> <li>\\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) for some scalar \\(\\lambda\\)</li> <li>\\(A + \\mathbf{v} = \\lambda\\)</li> <li>\\(\\mathbf{v}^T A = \\lambda\\)</li> </ol> Show Answer <p>The correct answer is B. An eigenvector \\(\\mathbf{v}\\) of matrix \\(A\\) satisfies \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), where \\(\\lambda\\) is the corresponding eigenvalue. The matrix simply scales the eigenvector rather than changing its direction.</p> <p>Concept Tested: Eigenvector</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#2-the-characteristic-polynomial-of-matrix-a-is","title":"2. The characteristic polynomial of matrix \\(A\\) is:","text":"<ol> <li>\\(\\det(A)\\)</li> <li>\\(\\det(A - \\lambda I)\\)</li> <li>\\(\\det(A + \\lambda I)\\)</li> <li>\\(\\text{tr}(A) - \\lambda\\)</li> </ol> Show Answer <p>The correct answer is B. The characteristic polynomial is \\(p(\\lambda) = \\det(A - \\lambda I)\\). Setting this equal to zero gives the characteristic equation, whose roots are the eigenvalues.</p> <p>Concept Tested: Characteristic Polynomial</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#3-the-eigenvalues-of-a-symmetric-matrix-are-always","title":"3. The eigenvalues of a symmetric matrix are always:","text":"<ol> <li>Complex numbers</li> <li>Real numbers</li> <li>Positive numbers</li> <li>Zero</li> </ol> Show Answer <p>The correct answer is B. A key property of symmetric matrices is that all eigenvalues are real numbers. Additionally, eigenvectors corresponding to distinct eigenvalues are orthogonal.</p> <p>Concept Tested: Eigenvalues of Symmetric Matrices</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#4-the-sum-of-all-eigenvalues-of-a-matrix-equals","title":"4. The sum of all eigenvalues of a matrix equals:","text":"<ol> <li>The determinant</li> <li>The trace</li> <li>The rank</li> <li>Zero</li> </ol> Show Answer <p>The correct answer is B. The sum of all eigenvalues (counting multiplicities) equals the trace of the matrix. Similarly, the product of all eigenvalues equals the determinant.</p> <p>Concept Tested: Eigenvalue Properties</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#5-if-lambda-is-an-eigenvalue-of-a-then-the-eigenvalue-of-a2-for-the-same-eigenvector-is","title":"5. If \\(\\lambda\\) is an eigenvalue of \\(A\\), then the eigenvalue of \\(A^2\\) for the same eigenvector is:","text":"<ol> <li>\\(\\lambda\\)</li> <li>\\(2\\lambda\\)</li> <li>\\(\\lambda^2\\)</li> <li>\\(\\sqrt{\\lambda}\\)</li> </ol> Show Answer <p>The correct answer is C. If \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), then \\(A^2\\mathbf{v} = A(A\\mathbf{v}) = A(\\lambda\\mathbf{v}) = \\lambda(A\\mathbf{v}) = \\lambda^2\\mathbf{v}\\). Raising a matrix to a power raises its eigenvalues to the same power.</p> <p>Concept Tested: Powers of Matrices</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#6-eigendecomposition-of-matrix-a-when-possible-is","title":"6. Eigendecomposition of matrix \\(A\\) (when possible) is:","text":"<ol> <li>\\(A = V + D\\)</li> <li>\\(A = VDV^{-1}\\)</li> <li>\\(A = V^T D V\\)</li> <li>\\(A = D - V\\)</li> </ol> Show Answer <p>The correct answer is B. Eigendecomposition expresses a matrix as \\(A = VDV^{-1}\\), where \\(V\\) is the matrix of eigenvectors and \\(D\\) is a diagonal matrix of eigenvalues. For symmetric matrices, \\(V\\) is orthogonal so \\(A = VDV^T\\).</p> <p>Concept Tested: Eigendecomposition</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#7-a-matrix-is-diagonalizable-if","title":"7. A matrix is diagonalizable if:","text":"<ol> <li>It is symmetric</li> <li>It has \\(n\\) linearly independent eigenvectors</li> <li>All eigenvalues are positive</li> <li>It is invertible</li> </ol> Show Answer <p>The correct answer is B. A matrix is diagonalizable if and only if it has \\(n\\) linearly independent eigenvectors, where \\(n\\) is the matrix dimension. Symmetric matrices always satisfy this condition.</p> <p>Concept Tested: Diagonalizability</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#8-the-spectral-theorem-states-that-for-a-real-symmetric-matrix","title":"8. The spectral theorem states that for a real symmetric matrix:","text":"<ol> <li>All eigenvalues are complex</li> <li>The matrix has an orthonormal basis of eigenvectors</li> <li>The matrix is not diagonalizable</li> <li>Eigenvalues form a spectrum of colors</li> </ol> Show Answer <p>The correct answer is B. The spectral theorem guarantees that real symmetric matrices have real eigenvalues and an orthonormal basis of eigenvectors. This enables the decomposition \\(A = Q\\Lambda Q^T\\) where \\(Q\\) is orthogonal.</p> <p>Concept Tested: Spectral Theorem</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#9-why-are-eigenvalues-important-in-stability-analysis","title":"9. Why are eigenvalues important in stability analysis?","text":"<ol> <li>They determine the color of the system</li> <li>Negative real parts indicate stable equilibria</li> <li>They measure the matrix size</li> <li>They count the number of solutions</li> </ol> Show Answer <p>The correct answer is B. In dynamical systems \\(\\dot{\\mathbf{x}} = A\\mathbf{x}\\), eigenvalues determine stability. If all eigenvalues have negative real parts, perturbations decay over time and the system is stable. Positive real parts indicate instability.</p> <p>Concept Tested: Stability Analysis</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#10-in-principal-component-analysis-pca-eigenvectors-of-the-covariance-matrix-represent","title":"10. In Principal Component Analysis (PCA), eigenvectors of the covariance matrix represent:","text":"<ol> <li>Random noise directions</li> <li>Directions of maximum variance in the data</li> <li>The mean of the data</li> <li>Outlier locations</li> </ol> Show Answer <p>The correct answer is B. In PCA, eigenvectors of the covariance matrix (principal components) point in directions of maximum variance. The corresponding eigenvalues indicate how much variance is captured by each direction.</p> <p>Concept Tested: Principal Component Analysis</p>"},{"location":"chapters/07-matrix-decompositions/","title":"Matrix Decompositions","text":""},{"location":"chapters/07-matrix-decompositions/#summary","title":"Summary","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction. This chapter covers LU, QR, Cholesky, and Singular Value Decomposition (SVD). Each decomposition has specific use cases: LU for solving systems efficiently, QR for least squares problems, Cholesky for symmetric positive definite matrices, and SVD for low-rank approximations and recommender systems.</p>"},{"location":"chapters/07-matrix-decompositions/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"chapters/07-matrix-decompositions/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 8: Vector Spaces and Inner Products (for Gram-Schmidt)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#introduction","title":"Introduction","text":"<p>Just as integers can be factored into primes to reveal their structure, matrices can be decomposed into products of simpler matrices. These matrix factorizations expose the underlying structure of linear transformations, enable efficient computation, and provide geometric insight into how matrices act on vectors.</p> <p>Matrix decompositions are the workhorses of numerical linear algebra. When you solve a system of linear equations, compute a least-squares fit, reduce dimensionality with PCA, or build a recommender system, you are using matrix decompositions behind the scenes. Understanding these factorizations\u2014when to use each one and what makes them numerically stable\u2014is essential for any practitioner working with data.</p> <p>This chapter covers four major decompositions, each with distinct purposes:</p> Decomposition Form Primary Use Cases LU \\(A = LU\\) Solving systems, computing determinants QR \\(A = QR\\) Least squares, eigenvalue algorithms Cholesky \\(A = LL^T\\) Symmetric positive definite systems SVD \\(A = U\\Sigma V^T\\) Low-rank approximation, dimensionality reduction"},{"location":"chapters/07-matrix-decompositions/#matrix-rank-a-foundation-for-decompositions","title":"Matrix Rank: A Foundation for Decompositions","text":"<p>Before diving into specific decompositions, we need to understand matrix rank, which fundamentally determines what decompositions are possible and their properties.</p>"},{"location":"chapters/07-matrix-decompositions/#matrix-rank","title":"Matrix Rank","text":"<p>The rank of a matrix \\(A\\) is the dimension of its column space (equivalently, its row space):</p> <p>\\(\\text{rank}(A) = \\dim(\\text{col}(A)) = \\dim(\\text{row}(A))\\)</p> <p>For an \\(m \\times n\\) matrix:</p> <ul> <li>\\(\\text{rank}(A) \\leq \\min(m, n)\\)</li> <li>If \\(\\text{rank}(A) = \\min(m, n)\\), the matrix has full rank</li> <li>If \\(\\text{rank}(A) &lt; \\min(m, n)\\), the matrix is rank-deficient</li> </ul> <p>The rank tells us how many linearly independent columns (or rows) the matrix contains, which directly impacts the uniqueness of solutions to linear systems and the structure of decompositions.</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-matrix-rank-visualizer","title":"Diagram: Matrix Rank Visualizer","text":"Matrix Rank Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how matrix rank relates to the column space and understand rank-deficient matrices geometrically</p> <p>Visual elements: - Left panel: 3x3 matrix input with editable values - Center panel: 3D visualization showing column vectors as arrows from origin - Column space displayed as a plane (rank 2) or line (rank 1) or point (rank 0) - Right panel: Row echelon form showing pivot positions</p> <p>Interactive controls: - Matrix entry fields (3x3) - \"Compute Rank\" button - Toggle to show/hide individual column vectors - Toggle to show column space as shaded region - Preset examples dropdown (full rank, rank 2, rank 1) - Animation to show column reduction</p> <p>Default parameters: - Matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] (rank 2) - Canvas: responsive, minimum 900x500px</p> <p>Behavior: - Real-time rank computation as matrix values change - Highlight linearly dependent columns - Show column space dimension visually - Display row echelon form with pivot columns highlighted - Animate transition when rank changes</p> <p>Implementation: p5.js with WEBGL for 3D visualization</p>"},{"location":"chapters/07-matrix-decompositions/#lu-decomposition","title":"LU Decomposition","text":"<p>LU Decomposition factors a square matrix into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\):</p> <p>\\(A = LU\\)</p> <p>where:</p> <ul> <li>\\(L\\) is lower triangular with 1s on the diagonal</li> <li>\\(U\\) is upper triangular</li> </ul> <p>This decomposition essentially records the steps of Gaussian elimination in matrix form.</p>"},{"location":"chapters/07-matrix-decompositions/#why-lu-decomposition-matters","title":"Why LU Decomposition Matters","text":"<p>LU decomposition transforms the problem of solving \\(A\\mathbf{x} = \\mathbf{b}\\) into two simpler triangular systems:</p> <ol> <li>Solve \\(L\\mathbf{y} = \\mathbf{b}\\) for \\(\\mathbf{y}\\) (forward substitution)</li> <li>Solve \\(U\\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) (back substitution)</li> </ol> <p>Each triangular solve takes only \\(O(n^2)\\) operations, compared to \\(O(n^3)\\) for the original system. The key advantage is that once we have computed the LU factorization (which costs \\(O(n^3)\\)), we can solve for multiple right-hand sides \\(\\mathbf{b}_1, \\mathbf{b}_2, \\ldots\\) with only \\(O(n^2)\\) additional work each.</p>"},{"location":"chapters/07-matrix-decompositions/#computing-lu-decomposition","title":"Computing LU Decomposition","text":"<p>The algorithm follows Gaussian elimination, but instead of modifying the right-hand side, we store the multipliers:</p> <ol> <li>For each column \\(k = 1, \\ldots, n-1\\):</li> <li> <p>For each row \\(i = k+1, \\ldots, n\\):</p> <ul> <li>Compute multiplier: \\(l_{ik} = a_{ik}/a_{kk}\\)</li> <li>Eliminate: \\(a_{ij} \\leftarrow a_{ij} - l_{ik} \\cdot a_{kj}\\) for \\(j = k, \\ldots, n\\)</li> </ul> </li> <li> <p>The multipliers form \\(L\\), and the reduced matrix becomes \\(U\\)</p> </li> </ol>"},{"location":"chapters/07-matrix-decompositions/#example-lu-decomposition","title":"Example: LU Decomposition","text":"<p>Consider the matrix:</p> <p>\\(A = \\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 4 &amp; 3 &amp; 3 \\\\ 8 &amp; 7 &amp; 9 \\end{bmatrix}\\)</p> <p>Step 1: Eliminate below the first pivot (2):</p> <ul> <li>Multiplier for row 2: \\(l_{21} = 4/2 = 2\\)</li> <li>Multiplier for row 3: \\(l_{31} = 8/2 = 4\\)</li> </ul> <p>After elimination:</p> <p>\\(\\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 3 &amp; 5 \\end{bmatrix}\\)</p> <p>Step 2: Eliminate below the second pivot (1):</p> <ul> <li>Multiplier for row 3: \\(l_{32} = 3/1 = 3\\)</li> </ul> <p>After elimination:</p> <p>\\(U = \\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 2 \\end{bmatrix}\\)</p> <p>The multipliers form \\(L\\):</p> <p>\\(L = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 4 &amp; 3 &amp; 1 \\end{bmatrix}\\)</p> <p>You can verify: \\(LU = A\\).</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-lu-decomposition-step-by-step","title":"Diagram: LU Decomposition Step-by-Step","text":"LU Decomposition Algorithm Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand the LU decomposition algorithm by watching elimination steps and multiplier storage</p> <p>Visual elements: - Left panel: Original matrix A with current state - Center panel: L matrix being built (showing multipliers) - Right panel: U matrix being formed (showing elimination) - Highlight current pivot element - Arrows showing elimination operations - Step counter and description</p> <p>Interactive controls: - Matrix size selector (2x2, 3x3, 4x4) - Matrix entry fields - \"Next Step\" button for manual stepping - \"Auto Run\" button with speed slider - \"Reset\" button - \"Verify LU = A\" button</p> <p>Default parameters: - 3x3 matrix mode - Matrix A = [[2, 1, 1], [4, 3, 3], [8, 7, 9]] - Canvas: responsive</p> <p>Behavior: - Highlight current pivot in yellow - Highlight elements being eliminated in red - Show multiplier calculation - Animate row operations - Display running product L\u00d7U for verification - Warning if zero pivot encountered (needs pivoting)</p> <p>Implementation: p5.js with matrix animation</p>"},{"location":"chapters/07-matrix-decompositions/#partial-pivoting","title":"Partial Pivoting","text":"<p>The basic LU decomposition fails when a pivot element is zero, and becomes numerically unstable when pivots are small. Partial pivoting addresses this by swapping rows to bring the largest element in the column to the pivot position.</p> <p>With partial pivoting, we compute:</p> <p>\\(PA = LU\\)</p> <p>where:</p> <ul> <li>\\(P\\) is a permutation matrix recording the row swaps</li> <li>\\(L\\) is lower triangular with entries \\(|l_{ij}| \\leq 1\\)</li> <li>\\(U\\) is upper triangular</li> </ul> <p>The permutation matrix \\(P\\) is orthogonal (\\(P^{-1} = P^T\\)), so solving \\(A\\mathbf{x} = \\mathbf{b}\\) becomes:</p> <ol> <li>Compute \\(P\\mathbf{b}\\) (apply row permutations)</li> <li>Solve \\(L\\mathbf{y} = P\\mathbf{b}\\)</li> <li>Solve \\(U\\mathbf{x} = \\mathbf{y}\\)</li> </ol> <p>Numerical Stability</p> <p>Always use partial pivoting in practice. Without it, even small rounding errors can be amplified catastrophically. Most numerical libraries (LAPACK, NumPy) use partial pivoting by default.</p>"},{"location":"chapters/07-matrix-decompositions/#computational-cost","title":"Computational Cost","text":"Operation Cost LU factorization \\(\\frac{2}{3}n^3\\) flops Forward substitution \\(n^2\\) flops Back substitution \\(n^2\\) flops Total for one solve \\(\\frac{2}{3}n^3 + 2n^2\\) flops Additional solves \\(2n^2\\) flops each"},{"location":"chapters/07-matrix-decompositions/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>For symmetric positive definite matrices, we can use a more efficient and stable decomposition called Cholesky decomposition:</p> <p>\\(A = LL^T\\)</p> <p>where:</p> <ul> <li>\\(A\\) is symmetric (\\(A = A^T\\)) and positive definite</li> <li>\\(L\\) is lower triangular with positive diagonal entries</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#positive-definite-matrix","title":"Positive Definite Matrix","text":"<p>A symmetric matrix \\(A\\) is positive definite if:</p> <p>\\(\\mathbf{x}^T A \\mathbf{x} &gt; 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}\\)</p> <p>Equivalent characterizations:</p> <ul> <li>All eigenvalues of \\(A\\) are positive</li> <li>All leading principal minors are positive</li> <li>\\(A\\) can be written as \\(A = B^TB\\) for some matrix \\(B\\) with full column rank</li> <li>The Cholesky decomposition exists</li> </ul> <p>Positive definite matrices arise frequently in applications:</p> <ul> <li>Covariance matrices in statistics</li> <li>Gram matrices \\(X^TX\\) in machine learning</li> <li>Hessians of convex functions at minima</li> <li>Stiffness matrices in finite element analysis</li> </ul> Property Positive Definite Positive Semi-Definite Eigenvalues All \\(&gt; 0\\) All \\(\\geq 0\\) Quadratic form \\(\\mathbf{x}^TA\\mathbf{x} &gt; 0\\) for \\(\\mathbf{x} \\neq 0\\) \\(\\mathbf{x}^TA\\mathbf{x} \\geq 0\\) for all \\(\\mathbf{x}\\) Invertibility Always invertible May be singular Cholesky Unique \\(LL^T\\) exists \\(LL^T\\) exists but \\(L\\) may have zeros"},{"location":"chapters/07-matrix-decompositions/#diagram-positive-definiteness-visualizer","title":"Diagram: Positive Definiteness Visualizer","text":"Positive Definiteness Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize the quadratic form \\(\\mathbf{x}^T A \\mathbf{x}\\) and understand how positive definiteness relates to the shape of the surface</p> <p>Visual elements: - Left panel: 2x2 symmetric matrix input - Center panel: 3D surface plot of \\(f(x,y) = [x, y] A [x, y]^T\\) - Eigenvalue display with color coding (positive=green, negative=red) - Level curves (contour plot) below 3D surface - Classification label: \"Positive Definite\", \"Negative Definite\", \"Indefinite\", \"Positive Semi-Definite\"</p> <p>Interactive controls: - Matrix entry fields (symmetric: entering a\u2081\u2082 sets a\u2082\u2081) - Rotation controls for 3D view - Toggle contour lines - Preset examples: positive definite, negative definite, indefinite, semi-definite</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 2]] (positive definite) - Surface plot range: x, y \u2208 [-2, 2] - Canvas: responsive</p> <p>Behavior: - Real-time surface update as matrix changes - Color surface by height (red for negative, green for positive) - Show eigenvalues and eigenvector directions on contour plot - Highlight minimum point for positive definite matrices - Saddle point visualization for indefinite matrices</p> <p>Implementation: p5.js with WEBGL for 3D surface rendering</p>"},{"location":"chapters/07-matrix-decompositions/#computing-cholesky-decomposition","title":"Computing Cholesky Decomposition","text":"<p>The Cholesky algorithm computes \\(L\\) column by column:</p> <p>For \\(j = 1, \\ldots, n\\):</p> <p>\\(l_{jj} = \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} l_{jk}^2}\\)</p> <p>For \\(i = j+1, \\ldots, n\\):</p> <p>\\(l_{ij} = \\frac{1}{l_{jj}}\\left(a_{ij} - \\sum_{k=1}^{j-1} l_{ik}l_{jk}\\right)\\)</p>"},{"location":"chapters/07-matrix-decompositions/#example-cholesky-decomposition","title":"Example: Cholesky Decomposition","text":"<p>Consider the positive definite matrix:</p> <p>\\(A = \\begin{bmatrix} 4 &amp; 2 &amp; 2 \\\\ 2 &amp; 5 &amp; 1 \\\\ 2 &amp; 1 &amp; 6 \\end{bmatrix}\\)</p> <p>Column 1:</p> <ul> <li>\\(l_{11} = \\sqrt{4} = 2\\)</li> <li>\\(l_{21} = 2/2 = 1\\)</li> <li>\\(l_{31} = 2/2 = 1\\)</li> </ul> <p>Column 2:</p> <ul> <li>\\(l_{22} = \\sqrt{5 - 1^2} = \\sqrt{4} = 2\\)</li> <li>\\(l_{32} = (1 - 1 \\cdot 1)/2 = 0\\)</li> </ul> <p>Column 3:</p> <ul> <li>\\(l_{33} = \\sqrt{6 - 1^2 - 0^2} = \\sqrt{5}\\)</li> </ul> <p>Result:</p> <p>\\(L = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; 0 &amp; \\sqrt{5} \\end{bmatrix}\\)</p>"},{"location":"chapters/07-matrix-decompositions/#advantages-of-cholesky","title":"Advantages of Cholesky","text":"<ul> <li>Half the work: Cholesky requires \\(\\frac{1}{3}n^3\\) flops vs. \\(\\frac{2}{3}n^3\\) for LU</li> <li>No pivoting needed: Positive definiteness guarantees numerical stability</li> <li>Natural for applications: Covariance matrices and Gram matrices are positive semi-definite</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#qr-decomposition","title":"QR Decomposition","text":"<p>QR Decomposition factors a matrix into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\):</p> <p>\\(A = QR\\)</p> <p>where:</p> <ul> <li>\\(Q\\) is orthogonal (\\(Q^TQ = I\\), columns form orthonormal basis)</li> <li>\\(R\\) is upper triangular</li> </ul> <p>For an \\(m \\times n\\) matrix with \\(m \\geq n\\): - Full QR: \\(Q\\) is \\(m \\times m\\), \\(R\\) is \\(m \\times n\\) - Reduced QR: \\(Q\\) is \\(m \\times n\\), \\(R\\) is \\(n \\times n\\)</p>"},{"location":"chapters/07-matrix-decompositions/#why-qr-decomposition-matters","title":"Why QR Decomposition Matters","text":"<p>QR decomposition is the foundation of:</p> <ol> <li>Least squares problems: Solving \\(A\\mathbf{x} \\approx \\mathbf{b}\\) in the overdetermined case</li> <li>QR algorithm: The standard method for computing eigenvalues</li> <li>Orthogonalization: Creating orthonormal bases from arbitrary vectors</li> </ol> <p>For least squares, the normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) can have numerical issues. Using QR, we instead solve:</p> <p>\\(R\\mathbf{x} = Q^T\\mathbf{b}\\)</p> <p>which is numerically stable because \\(Q\\) preserves norms.</p>"},{"location":"chapters/07-matrix-decompositions/#gram-schmidt-qr","title":"Gram-Schmidt QR","text":"<p>The Gram-Schmidt process constructs \\(Q\\) by orthonormalizing the columns of \\(A\\) one at a time:</p> <p>For \\(j = 1, \\ldots, n\\):</p> <ol> <li>Start with column \\(\\mathbf{a}_j\\)</li> <li>Subtract projections onto previous \\(\\mathbf{q}\\) vectors:    \\(\\mathbf{v}_j = \\mathbf{a}_j - \\sum_{i=1}^{j-1} (\\mathbf{q}_i^T \\mathbf{a}_j) \\mathbf{q}_i\\)</li> <li>Normalize: \\(\\mathbf{q}_j = \\mathbf{v}_j / \\|\\mathbf{v}_j\\|\\)</li> </ol> <p>The coefficients form the upper triangular matrix \\(R\\):</p> <ul> <li>\\(r_{ij} = \\mathbf{q}_i^T \\mathbf{a}_j\\) for \\(i &lt; j\\)</li> <li>\\(r_{jj} = \\|\\mathbf{v}_j\\|\\)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#diagram-gram-schmidt-orthogonalization","title":"Diagram: Gram-Schmidt Orthogonalization","text":"Gram-Schmidt Process Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how Gram-Schmidt creates orthonormal vectors by visualizing the projection and subtraction steps</p> <p>Visual elements: - 3D coordinate system - Original vectors a\u2081, a\u2082, a\u2083 as colored arrows - Orthonormal vectors q\u2081, q\u2082, q\u2083 as they are computed - Projection vectors showing what is subtracted - Right-angle indicators between orthogonal vectors</p> <p>Interactive controls: - Input matrix A (3 columns, 3 rows for 3D visualization) - \"Step\" button to advance one orthonormalization step - \"Run All\" button for complete animation - \"Reset\" button - Speed slider - Toggle to show/hide projection components</p> <p>Default parameters: - Matrix A with 3 linearly independent columns - Canvas: responsive 3D view</p> <p>Behavior: - Show each projection step clearly - Animate subtraction of projection - Show normalization step - Display current q vector and r values - Highlight orthogonality with right-angle symbols - Warning if vectors are nearly linearly dependent</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p> <p>Classical vs. Modified Gram-Schmidt</p> <p>The classical Gram-Schmidt algorithm described above can lose orthogonality due to rounding errors. The modified Gram-Schmidt algorithm recomputes projections against the updated vectors rather than original vectors, providing better numerical stability.</p>"},{"location":"chapters/07-matrix-decompositions/#householder-qr","title":"Householder QR","text":"<p>Householder QR uses orthogonal reflections (Householder transformations) to zero out elements below the diagonal. Each Householder matrix has the form:</p> <p>\\(H = I - 2\\mathbf{v}\\mathbf{v}^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}\\) is a unit vector defining the reflection plane</li> <li>\\(H\\) is orthogonal and symmetric (\\(H = H^T = H^{-1}\\))</li> </ul> <p>The algorithm applies successive Householder reflections:</p> <p>\\(H_n \\cdots H_2 H_1 A = R\\)</p> <p>Therefore:</p> <p>\\(Q = H_1 H_2 \\cdots H_n\\)</p>"},{"location":"chapters/07-matrix-decompositions/#advantages-of-householder-qr","title":"Advantages of Householder QR","text":"<ul> <li>Numerically stable: Each transformation is exactly orthogonal</li> <li>Efficient storage: Only need to store reflection vectors \\(\\mathbf{v}\\)</li> <li>Standard choice: Used by LAPACK and all major numerical libraries</li> </ul> Method Stability Flops Storage Classical Gram-Schmidt Poor \\(2mn^2\\) \\(mn + n^2\\) Modified Gram-Schmidt Good \\(2mn^2\\) \\(mn + n^2\\) Householder Excellent \\(2mn^2 - \\frac{2}{3}n^3\\) \\(mn\\) (compact)"},{"location":"chapters/07-matrix-decompositions/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>The Singular Value Decomposition is perhaps the most important and versatile matrix decomposition. It applies to any \\(m \\times n\\) matrix (not just square matrices):</p> <p>\\(A = U\\Sigma V^T\\)</p> <p>where:</p> <ul> <li>\\(U\\) is an \\(m \\times m\\) orthogonal matrix (left singular vectors)</li> <li>\\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix (singular values)</li> <li>\\(V\\) is an \\(n \\times n\\) orthogonal matrix (right singular vectors)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#understanding-svd-components","title":"Understanding SVD Components","text":""},{"location":"chapters/07-matrix-decompositions/#singular-values","title":"Singular Values","text":"<p>The singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\) are the diagonal entries of \\(\\Sigma\\), where \\(r = \\text{rank}(A)\\). They measure how much the matrix stretches vectors in each principal direction.</p> <p>Key relationships:</p> <ul> <li>\\(\\sigma_i = \\sqrt{\\lambda_i(A^TA)} = \\sqrt{\\lambda_i(AA^T)}\\) (singular values are square roots of eigenvalues)</li> <li>\\(\\|A\\|_2 = \\sigma_1\\) (spectral norm is largest singular value)</li> <li>\\(\\|A\\|_F = \\sqrt{\\sum_i \\sigma_i^2}\\) (Frobenius norm)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#left-singular-vectors","title":"Left Singular Vectors","text":"<p>The columns of \\(U\\) are left singular vectors. They form an orthonormal basis for the column space (first \\(r\\) vectors) and left null space (remaining vectors) of \\(A\\).</p> <p>\\(A\\mathbf{v}_i = \\sigma_i \\mathbf{u}_i\\)</p>"},{"location":"chapters/07-matrix-decompositions/#right-singular-vectors","title":"Right Singular Vectors","text":"<p>The columns of \\(V\\) are right singular vectors. They form an orthonormal basis for the row space (first \\(r\\) vectors) and null space (remaining vectors) of \\(A\\).</p> <p>\\(A^T\\mathbf{u}_i = \\sigma_i \\mathbf{v}_i\\)</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-svd-geometry","title":"Diagram: SVD Geometry","text":"SVD Geometric Interpretation <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize SVD as a sequence of rotation, scaling, and rotation operations on the unit sphere</p> <p>Visual elements: - Four panels showing transformation stages:   1. Original unit circle/sphere   2. After V^T rotation (aligns with principal axes)   3. After \u03a3 scaling (ellipse with \u03c3\u2081, \u03c3\u2082 semi-axes)   4. After U rotation (final orientation) - Matrix display showing A = U\u03a3V^T - Singular values displayed as axis lengths</p> <p>Interactive controls: - 2x2 or 2x3 matrix input for A - \"Animate Transformation\" button showing step-by-step - Slider to interpolate between stages - Toggle to show/hide singular vectors - Toggle 2D (circle\u2192ellipse) vs 3D (sphere\u2192ellipsoid)</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 3]] - Animation duration: 2 seconds per stage - Canvas: responsive</p> <p>Behavior: - Show unit circle transforming to ellipse - Label semi-axes with singular values - Show right singular vectors as directions on original circle - Show left singular vectors as directions on transformed ellipse - Display matrix factorization alongside visualization</p> <p>Implementation: p5.js with smooth animation transitions</p>"},{"location":"chapters/07-matrix-decompositions/#the-fundamental-picture","title":"The Fundamental Picture","text":"<p>The SVD reveals the geometry of any linear transformation:</p> <ol> <li>\\(V^T\\) rotates the input space to align with the principal axes</li> <li>\\(\\Sigma\\) stretches/compresses along each axis by the singular values</li> <li>\\(U\\) rotates the output space to the final orientation</li> </ol> <p>This decomposition exposes the four fundamental subspaces:</p> Subspace Dimension Basis from SVD Column space \\(r\\) First \\(r\\) columns of \\(U\\) Left null space \\(m - r\\) Last \\(m - r\\) columns of \\(U\\) Row space \\(r\\) First \\(r\\) columns of \\(V\\) Null space \\(n - r\\) Last \\(n - r\\) columns of \\(V\\)"},{"location":"chapters/07-matrix-decompositions/#full-svd-vs-compact-svd-vs-truncated-svd","title":"Full SVD vs. Compact SVD vs. Truncated SVD","text":"<p>There are three forms of SVD depending on how we handle zero singular values:</p>"},{"location":"chapters/07-matrix-decompositions/#full-svd","title":"Full SVD","text":"<p>The full SVD includes all \\(m\\) left singular vectors and all \\(n\\) right singular vectors:</p> <p>\\(A = U_{m \\times m} \\Sigma_{m \\times n} V^T_{n \\times n}\\)</p>"},{"location":"chapters/07-matrix-decompositions/#compact-svd","title":"Compact SVD","text":"<p>The compact (reduced) SVD keeps only the \\(r\\) non-zero singular values and their vectors:</p> <p>\\(A = U_{m \\times r} \\Sigma_{r \\times r} V^T_{r \\times n}\\)</p> <p>This is more memory-efficient and is what NumPy's <code>np.linalg.svd(full_matrices=False)</code> returns.</p>"},{"location":"chapters/07-matrix-decompositions/#truncated-svd","title":"Truncated SVD","text":"<p>The truncated SVD keeps only the \\(k\\) largest singular values (where \\(k &lt; r\\)):</p> <p>\\(A_k = U_{m \\times k} \\Sigma_{k \\times k} V^T_{k \\times n}\\)</p> <p>This gives the best rank-\\(k\\) approximation to \\(A\\) (Eckart-Young theorem).</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-svd-forms-comparison","title":"Diagram: SVD Forms Comparison","text":"SVD Forms Comparison <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare full, compact, and truncated SVD visually and understand when to use each form</p> <p>Layout: Three columns showing matrix dimensions for each SVD form</p> <p>Sections: 1. \"Full SVD\" column    - Matrix dimensions: U(m\u00d7m), \u03a3(m\u00d7n), V^T(n\u00d7n)    - Visual: Full-size matrices with zero padding shown    - Use case: Complete basis for all four fundamental subspaces    - Memory: O(m\u00b2 + mn + n\u00b2)</p> <ol> <li>\"Compact SVD\" column</li> <li>Matrix dimensions: U(m\u00d7r), \u03a3(r\u00d7r), V^T(r\u00d7n)</li> <li>Visual: Trimmed matrices, only rank-r portion</li> <li>Use case: Efficient storage, exact reconstruction</li> <li> <p>Memory: O(mr + r\u00b2 + rn)</p> </li> <li> <p>\"Truncated SVD\" column</p> </li> <li>Matrix dimensions: U(m\u00d7k), \u03a3(k\u00d7k), V^T(k\u00d7n)</li> <li>Visual: Further trimmed to k components</li> <li>Use case: Low-rank approximation, denoising</li> <li>Memory: O(mk + k\u00b2 + kn)</li> <li>Note: \"k &lt; r, approximate reconstruction\"</li> </ol> <p>Interactive elements: - Hover to see example with specific dimensions - Click to see Python code for each form - Toggle to show reconstruction error for truncated form</p> <p>Visual style: - Matrix blocks with dimension labels - Color coding: kept components in blue, discarded in gray - Singular values shown as bar chart below</p> <p>Implementation: HTML/CSS/JavaScript with SVG matrices</p>"},{"location":"chapters/07-matrix-decompositions/#computing-svd","title":"Computing SVD","text":"<p>SVD is typically computed in two stages:</p> <ol> <li>Bidiagonalization: Transform \\(A\\) to bidiagonal form using Householder reflections</li> <li>Diagonalization: Apply the QR algorithm (or divide-and-conquer) to find singular values</li> </ol> <p>For an \\(m \\times n\\) matrix with \\(m \\geq n\\):</p> Operation Flops Full SVD \\(2mn^2 + 11n^3\\) Compact SVD \\(2mn^2 + 11n^3\\) Truncated SVD (randomized) \\(O(mnk + (m+n)k^2)\\) <p>Randomized SVD for Large Matrices</p> <p>For very large matrices where only the top \\(k\\) singular values are needed, randomized algorithms provide significant speedups. Libraries like <code>sklearn.decomposition.TruncatedSVD</code> implement these efficient methods.</p>"},{"location":"chapters/07-matrix-decompositions/#low-rank-approximation","title":"Low-Rank Approximation","text":"<p>One of the most powerful applications of SVD is low-rank approximation. Given a matrix \\(A\\) with rank \\(r\\), the best rank-\\(k\\) approximation (for \\(k &lt; r\\)) is:</p> <p>\\(A_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\)</p>"},{"location":"chapters/07-matrix-decompositions/#the-eckart-young-theorem","title":"The Eckart-Young Theorem","text":"<p>The truncated SVD provides the optimal low-rank approximation:</p> <p>\\(A_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|_F\\)</p> <p>The approximation error is:</p> <p>\\(\\|A - A_k\\|_F = \\sqrt{\\sigma_{k+1}^2 + \\sigma_{k+2}^2 + \\cdots + \\sigma_r^2}\\)</p> <p>\\(\\|A - A_k\\|_2 = \\sigma_{k+1}\\)</p> <p>This theorem justifies using truncated SVD for:</p> <ul> <li>Image compression: Store only top \\(k\\) singular components</li> <li>Noise reduction: Small singular values often correspond to noise</li> <li>Dimensionality reduction: Project data onto top \\(k\\) principal directions</li> <li>Recommender systems: Approximate user-item matrices</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#diagram-image-compression-with-svd","title":"Diagram: Image Compression with SVD","text":"Image Compression MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how truncated SVD compresses images by observing the quality-storage tradeoff as k varies</p> <p>Visual elements: - Left panel: Original grayscale image (as matrix) - Center panel: Reconstructed image using rank-k approximation - Right panel: Difference image (error visualization) - Bottom: Singular value spectrum (bar chart or line plot) - Statistics display: compression ratio, PSNR, % variance captured</p> <p>Interactive controls: - Image selector (sample images or upload) - Slider for rank k (1 to full rank) - \"Show Singular Values\" toggle - \"Show Error Image\" toggle - \"Compare Side-by-Side\" view option</p> <p>Default parameters: - Sample grayscale image (256\u00d7256) - Initial k = 50 - Canvas: responsive</p> <p>Behavior: - Real-time reconstruction as k changes - Show compression ratio: k(m+n+1) / (m\u00d7n) - Display percentage of Frobenius norm captured - Highlight \"elbow\" in singular value plot - Show time/memory comparison</p> <p>Implementation: p5.js with image processing</p>"},{"location":"chapters/07-matrix-decompositions/#applications-of-low-rank-approximation","title":"Applications of Low-Rank Approximation","text":"<ol> <li>Recommender Systems (Netflix Problem)</li> <li>User-movie rating matrix is approximately low-rank</li> <li>Missing entries predicted from low-rank factors</li> <li> <p>\\(R \\approx UV^T\\) where \\(U\\) = user factors, \\(V\\) = item factors</p> </li> <li> <p>Latent Semantic Analysis (LSA)</p> </li> <li>Term-document matrix decomposed via SVD</li> <li>Captures semantic relationships between words</li> <li> <p>Precursor to modern word embeddings</p> </li> <li> <p>Principal Component Analysis (PCA)</p> </li> <li>Centered data matrix \\(X\\) decomposed as \\(X = U\\Sigma V^T\\)</li> <li>Principal components are columns of \\(V\\)</li> <li>Variance along each PC is \\(\\sigma_i^2/(n-1)\\)</li> </ol>"},{"location":"chapters/07-matrix-decompositions/#numerical-rank-and-condition-number","title":"Numerical Rank and Condition Number","text":"<p>In exact arithmetic, rank is well-defined. In floating-point computation, small singular values may arise from noise rather than true rank deficiency.</p>"},{"location":"chapters/07-matrix-decompositions/#numerical-rank","title":"Numerical Rank","text":"<p>The numerical rank is the number of singular values larger than a threshold \\(\\epsilon\\):</p> <p>\\(\\text{rank}_\\epsilon(A) = |\\{i : \\sigma_i &gt; \\epsilon\\}|\\)</p> <p>Common choices for \\(\\epsilon\\):</p> <ul> <li>\\(\\epsilon = \\max(m,n) \\cdot \\epsilon_{\\text{machine}} \\cdot \\sigma_1\\)</li> <li>\\(\\epsilon = \\sqrt{\\epsilon_{\\text{machine}}} \\cdot \\sigma_1\\)</li> </ul> <p>where \\(\\epsilon_{\\text{machine}} \\approx 2.2 \\times 10^{-16}\\) for double precision.</p>"},{"location":"chapters/07-matrix-decompositions/#condition-number","title":"Condition Number","text":"<p>The condition number measures how sensitive a matrix computation is to perturbations:</p> <p>\\(\\kappa(A) = \\frac{\\sigma_1}{\\sigma_r} = \\|A\\| \\cdot \\|A^{-1}\\|\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_1\\) is the largest singular value</li> <li>\\(\\sigma_r\\) is the smallest non-zero singular value</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#interpretation","title":"Interpretation","text":"Condition Number Interpretation \\(\\kappa \\approx 1\\) Well-conditioned (stable computation) \\(\\kappa \\approx 10^k\\) Lose ~\\(k\\) digits of accuracy \\(\\kappa = \\infty\\) Singular matrix (not invertible) <p>For solving \\(A\\mathbf{x} = \\mathbf{b}\\), the relative error in the solution satisfies:</p> <p>\\(\\frac{\\|\\delta \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leq \\kappa(A) \\cdot \\frac{\\|\\delta \\mathbf{b}\\|}{\\|\\mathbf{b}\\|}\\)</p> <p>A small perturbation \\(\\delta \\mathbf{b}\\) in the right-hand side can be amplified by \\(\\kappa(A)\\) in the solution.</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-condition-number-visualizer","title":"Diagram: Condition Number Visualizer","text":"Condition Number and Sensitivity Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how condition number affects solution sensitivity by perturbing linear systems</p> <p>Visual elements: - Left panel: 2D visualization of linear system Ax = b   - Two lines representing equations   - Intersection point (solution)   - Perturbation region around b   - Resulting uncertainty region around x - Right panel: Singular value bar chart   - \u03c3\u2081 and \u03c3\u2082 (or more) as bars   - Condition number \u03ba = \u03c3\u2081/\u03c3\u2082 displayed - Matrix A and vectors b, x displayed</p> <p>Interactive controls: - 2x2 matrix input - Slider to control perturbation magnitude \u03b5 - \"Add Random Perturbation\" button - Toggle between well-conditioned and ill-conditioned examples - Show/hide uncertainty ellipse</p> <p>Default parameters: - Well-conditioned example: A = [[2, 0], [0, 2]] (\u03ba = 1) - Ill-conditioned example: A = [[1, 1], [1, 1.0001]] (\u03ba \u2248 10000) - Canvas: responsive</p> <p>Behavior: - Show how nearly parallel lines (ill-conditioned) create large uncertainty - Animate perturbations and show solution movement - Display digits of accuracy lost - Compare \u03ba calculation methods</p> <p>Implementation: p5.js with geometric visualization</p>"},{"location":"chapters/07-matrix-decompositions/#improving-conditioning","title":"Improving Conditioning","text":"<p>Several techniques can improve numerical conditioning:</p> <ul> <li>Preconditioning: Transform \\(A\\mathbf{x} = \\mathbf{b}\\) to \\(M^{-1}A\\mathbf{x} = M^{-1}\\mathbf{b}\\) where \\(M \\approx A\\)</li> <li>Regularization: Replace \\(A^TA\\) with \\(A^TA + \\lambda I\\) (ridge regression)</li> <li>Scaling: Equilibrate row and column norms of \\(A\\)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#choosing-the-right-decomposition","title":"Choosing the Right Decomposition","text":"<p>The choice of decomposition depends on the matrix structure and application:</p> <pre><code>Is A square?\n\u251c\u2500\u2500 Yes: Is A symmetric positive definite?\n\u2502   \u251c\u2500\u2500 Yes: Use Cholesky (fastest, most stable)\n\u2502   \u2514\u2500\u2500 No: Use LU with partial pivoting\n\u2502\n\u2514\u2500\u2500 No (rectangular): What's the goal?\n    \u251c\u2500\u2500 Least squares: Use QR\n    \u251c\u2500\u2500 Low-rank approximation: Use SVD\n    \u2514\u2500\u2500 Eigenvalue-like analysis: Use SVD\n</code></pre> Application Recommended Decomposition Why Solve \\(A\\mathbf{x} = \\mathbf{b}\\) (multiple \\(\\mathbf{b}\\)) LU Factor once, solve many Solve \\(A\\mathbf{x} = \\mathbf{b}\\) (\\(A\\) SPD) Cholesky Half the work, more stable Least squares QR Numerically stable Eigenvalues (symmetric) QR algorithm on tridiagonal form Standard method Singular values SVD via bidiagonalization Definitive answer Low-rank approximation Truncated SVD Optimal by Eckart-Young Matrix rank SVD Count significant \\(\\sigma_i\\)"},{"location":"chapters/07-matrix-decompositions/#diagram-decomposition-decision-tree","title":"Diagram: Decomposition Decision Tree","text":"Matrix Decomposition Selection Guide <p>Type: workflow</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Guide students to select the appropriate matrix decomposition based on matrix properties and application needs</p> <p>Visual style: Interactive decision tree flowchart</p> <p>Steps: 1. Start: \"Given matrix A, what do you need?\"    Hover text: \"First consider your goal and matrix properties\"</p> <ol> <li>Decision: \"Goal?\"</li> <li>\"Solve linear system\" \u2192 Branch A</li> <li>\"Least squares / overdetermined\" \u2192 Branch B</li> <li>\"Low-rank approximation\" \u2192 Branch C</li> <li>\"Eigenvalues\" \u2192 Branch D</li> </ol> <p>3a. Decision: \"Is A symmetric positive definite?\"     - Yes \u2192 \"Use Cholesky LL^T\"       Hover text: \"Fastest, half the flops of LU\"     - No \u2192 \"Use LU with pivoting\"       Hover text: \"Works for any invertible matrix\"</p> <p>3b. Process: \"Use QR Decomposition\"     Hover text: \"More stable than normal equations\"</p> <p>3c. Process: \"Use Truncated SVD\"     Hover text: \"Optimal rank-k approximation by Eckart-Young\"</p> <p>3d. Decision: \"Is A symmetric?\"     - Yes \u2192 \"Eigendecomposition via QR algorithm\"     - No \u2192 \"Consider SVD for singular values\"</p> <p>Color coding: - Blue: Decision nodes - Green: Recommended decompositions - Orange: Computation nodes - Gray: Information nodes</p> <p>Interactive: - Click nodes to see code examples - Hover for complexity information - Links to relevant sections</p> <p>Implementation: D3.js or Mermaid.js with interaction handlers</p>"},{"location":"chapters/07-matrix-decompositions/#practical-implementation","title":"Practical Implementation","text":"<p>Here is how to use these decompositions in Python:</p> <pre><code>import numpy as np\nfrom scipy import linalg\n\n# Sample matrices\nA = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]], dtype=float)\nb = np.array([1, 2, 3], dtype=float)\n\n# LU Decomposition (with pivoting)\nP, L, U = linalg.lu(A)\nx_lu = linalg.solve_triangular(U,\n       linalg.solve_triangular(L, P @ b, lower=True))\n\n# Cholesky (if A is positive definite)\nL_chol = linalg.cholesky(A, lower=True)\nx_chol = linalg.cho_solve((L_chol, True), b)\n\n# QR Decomposition\nQ, R = linalg.qr(A)\nx_qr = linalg.solve_triangular(R, Q.T @ b)\n\n# SVD\nU, s, Vh = linalg.svd(A)\n# Solve via pseudoinverse\nx_svd = Vh.T @ np.diag(1/s) @ U.T @ b\n\n# Condition number\ncond = np.linalg.cond(A)\nprint(f\"Condition number: {cond:.2f}\")\n\n# Low-rank approximation\nk = 2  # rank of approximation\nA_k = U[:, :k] @ np.diag(s[:k]) @ Vh[:k, :]\nerror = np.linalg.norm(A - A_k, 'fro')\nprint(f\"Rank-{k} approximation error: {error:.4f}\")\n</code></pre>"},{"location":"chapters/07-matrix-decompositions/#summary_1","title":"Summary","text":"<p>This chapter covered the four essential matrix decompositions:</p> <p>LU Decomposition:</p> <ul> <li>Factors \\(A = LU\\) (with pivoting: \\(PA = LU\\))</li> <li>Used for solving linear systems efficiently</li> <li>Cost: \\(\\frac{2}{3}n^3\\) flops to factor, \\(2n^2\\) per solve</li> </ul> <p>Cholesky Decomposition:</p> <ul> <li>Factors \\(A = LL^T\\) for symmetric positive definite matrices</li> <li>Half the cost of LU, no pivoting needed</li> <li>Positive definite means all eigenvalues positive, \\(\\mathbf{x}^TA\\mathbf{x} &gt; 0\\)</li> </ul> <p>QR Decomposition:</p> <ul> <li>Factors \\(A = QR\\) with orthogonal \\(Q\\)</li> <li>Foundation for least squares and eigenvalue algorithms</li> <li>Gram-Schmidt (intuitive) vs. Householder (stable)</li> </ul> <p>Singular Value Decomposition:</p> <ul> <li>Factors \\(A = U\\Sigma V^T\\) for any matrix</li> <li>Singular values reveal matrix structure and rank</li> <li>Truncated SVD gives optimal low-rank approximation</li> </ul> <p>Key Concepts:</p> <ul> <li>Matrix rank determines decomposition structure</li> <li>Numerical rank accounts for floating-point limitations</li> <li>Condition number \\(\\kappa = \\sigma_1/\\sigma_r\\) measures sensitivity</li> </ul> <p>Practical Guidelines:</p> <ol> <li>Use Cholesky for symmetric positive definite systems</li> <li>Use LU for general square systems (with pivoting!)</li> <li>Use QR for least squares problems</li> <li>Use SVD for low-rank approximation and dimensionality reduction</li> <li>Always check condition number before trusting numerical results</li> </ol> Self-Check: When would you choose SVD over QR for a least squares problem? <p>SVD is preferred when the matrix is rank-deficient or nearly rank-deficient (ill-conditioned). QR can fail or give unstable results when columns are nearly linearly dependent, while SVD explicitly reveals the rank through singular values and handles rank deficiency gracefully via the pseudoinverse. SVD also provides the minimum-norm solution when multiple solutions exist.</p>"},{"location":"chapters/07-matrix-decompositions/quiz/","title":"Quiz: Matrix Decompositions","text":"<p>Test your understanding of SVD, QR, and other matrix decompositions.</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#1-the-singular-value-decomposition-svd-factors-a-matrix-a-as","title":"1. The Singular Value Decomposition (SVD) factors a matrix \\(A\\) as:","text":"<ol> <li>\\(A = U\\Sigma V\\)</li> <li>\\(A = U\\Sigma V^T\\)</li> <li>\\(A = \\Sigma UV\\)</li> <li>\\(A = U + \\Sigma + V\\)</li> </ol> Show Answer <p>The correct answer is B. The SVD decomposes any \\(m \\times n\\) matrix as \\(A = U\\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is a diagonal matrix of singular values.</p> <p>Concept Tested: Singular Value Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#2-singular-values-of-a-matrix-are-always","title":"2. Singular values of a matrix are always:","text":"<ol> <li>Complex numbers</li> <li>Negative or zero</li> <li>Non-negative real numbers</li> <li>Equal to the eigenvalues</li> </ol> Show Answer <p>The correct answer is C. Singular values are always non-negative real numbers, typically arranged in decreasing order: \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\). They are the square roots of eigenvalues of \\(A^TA\\).</p> <p>Concept Tested: Singular Values</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#3-the-rank-of-a-matrix-equals","title":"3. The rank of a matrix equals:","text":"<ol> <li>The number of rows</li> <li>The number of non-zero singular values</li> <li>The largest singular value</li> <li>The trace</li> </ol> Show Answer <p>The correct answer is B. The rank of a matrix equals the number of non-zero singular values. This provides a robust numerical way to determine rank, especially when using a tolerance for \"effectively zero\" values.</p> <p>Concept Tested: Rank and SVD</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#4-in-the-low-rank-approximation-via-svd-keeping-only-the-top-k-singular-values-minimizes","title":"4. In the low-rank approximation via SVD, keeping only the top \\(k\\) singular values minimizes:","text":"<ol> <li>The Frobenius norm of the error</li> <li>The rank of the matrix</li> <li>The number of computations</li> <li>The trace of the matrix</li> </ol> Show Answer <p>The correct answer is A. The truncated SVD gives the best rank-\\(k\\) approximation in both the Frobenius norm and the spectral norm. This is the Eckart-Young-Mirsky theorem.</p> <p>Concept Tested: Low-Rank Approximation</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#5-qr-decomposition-factors-a-matrix-as","title":"5. QR decomposition factors a matrix as:","text":"<ol> <li>\\(A = QR\\) where \\(Q\\) is orthogonal and \\(R\\) is upper triangular</li> <li>\\(A = QR\\) where \\(Q\\) is diagonal and \\(R\\) is symmetric</li> <li>\\(A = R^TQ\\)</li> <li>\\(A = Q + R\\)</li> </ol> Show Answer <p>The correct answer is A. QR decomposition expresses a matrix as \\(A = QR\\), where \\(Q\\) is an orthogonal matrix (orthonormal columns) and \\(R\\) is upper triangular. It is used in solving least squares and computing eigenvalues.</p> <p>Concept Tested: QR Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#6-the-gram-schmidt-process-produces","title":"6. The Gram-Schmidt process produces:","text":"<ol> <li>A diagonal matrix</li> <li>An orthonormal basis from a set of vectors</li> <li>The inverse of a matrix</li> <li>The determinant</li> </ol> Show Answer <p>The correct answer is B. The Gram-Schmidt process takes a set of linearly independent vectors and produces an orthonormal set spanning the same space. It is the foundation of QR decomposition.</p> <p>Concept Tested: Gram-Schmidt Process</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#7-cholesky-decomposition-applies-to","title":"7. Cholesky decomposition applies to:","text":"<ol> <li>Any square matrix</li> <li>Positive definite symmetric matrices</li> <li>Singular matrices only</li> <li>Rectangular matrices</li> </ol> Show Answer <p>The correct answer is B. Cholesky decomposition factors a positive definite symmetric matrix as \\(A = LL^T\\), where \\(L\\) is lower triangular. It is twice as efficient as LU decomposition for applicable matrices.</p> <p>Concept Tested: Cholesky Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#8-the-left-singular-vectors-columns-of-u-in-svd-are-eigenvectors-of","title":"8. The left singular vectors (columns of \\(U\\) in SVD) are eigenvectors of:","text":"<ol> <li>\\(A\\)</li> <li>\\(A^TA\\)</li> <li>\\(AA^T\\)</li> <li>\\(A + A^T\\)</li> </ol> Show Answer <p>The correct answer is C. The left singular vectors (columns of \\(U\\)) are eigenvectors of \\(AA^T\\), while the right singular vectors (columns of \\(V\\)) are eigenvectors of \\(A^TA\\). The squared singular values are the eigenvalues.</p> <p>Concept Tested: SVD and Eigenvalues</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#9-the-pseudoinverse-a-computed-via-svd-satisfies","title":"9. The pseudoinverse \\(A^+\\) computed via SVD satisfies:","text":"<ol> <li>\\(AA^+ = I\\) always</li> <li>\\(AA^+A = A\\)</li> <li>\\(A^+ = A^T\\)</li> <li>\\(A^+ = A^{-1}\\) always</li> </ol> Show Answer <p>The correct answer is B. The Moore-Penrose pseudoinverse satisfies \\(AA^+A = A\\) (among other conditions). It generalizes the inverse to non-square and singular matrices, providing least-squares solutions.</p> <p>Concept Tested: Pseudoinverse</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#10-which-decomposition-is-most-useful-for-image-compression","title":"10. Which decomposition is most useful for image compression?","text":"<ol> <li>LU decomposition</li> <li>QR decomposition</li> <li>SVD with truncation</li> <li>Cholesky decomposition</li> </ol> Show Answer <p>The correct answer is C. Truncated SVD is ideal for image compression because it provides the optimal low-rank approximation. Keeping only the largest singular values captures the most important image features while reducing storage.</p> <p>Concept Tested: SVD Applications</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/","title":"Vector Spaces and Inner Products","text":""},{"location":"chapters/08-vector-spaces-and-inner-products/#summary","title":"Summary","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications. You will learn about abstract vector spaces, inner products, orthogonality, the Gram-Schmidt orthogonalization process, and projections. This chapter also covers the four fundamental subspaces of a matrix and the pseudoinverse, which are essential for least squares and machine learning.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"chapters/08-vector-spaces-and-inner-products/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#introduction","title":"Introduction","text":"<p>So far, we have worked with vectors as arrows in \\(\\mathbb{R}^n\\)\u2014ordered lists of real numbers that we can add and scale. But the power of linear algebra extends far beyond number arrays. Functions, matrices, polynomials, and even quantum states can all be treated as \"vectors\" in appropriately defined spaces.</p> <p>This chapter develops the abstract framework that unifies these diverse applications. By identifying the essential properties that make \\(\\mathbb{R}^n\\) useful\u2014the ability to add vectors, scale them, measure lengths, and find angles\u2014we can extend linear algebra to any mathematical structure satisfying these properties.</p> <p>The payoff is enormous. The same techniques that solve systems of equations in \\(\\mathbb{R}^n\\) can approximate functions with polynomials, denoise signals, and find optimal solutions in infinite-dimensional spaces. The abstract perspective reveals that linear algebra is not just about numbers\u2014it's about structure.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#abstract-vector-spaces","title":"Abstract Vector Spaces","text":"<p>An abstract vector space is a set \\(V\\) equipped with two operations\u2014vector addition and scalar multiplication\u2014that satisfy certain axioms. The elements of \\(V\\) are called vectors, though they need not be column vectors in the traditional sense.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#vector-space-axioms","title":"Vector Space Axioms","text":"<p>A vector space over the real numbers must satisfy the following vector space axioms:</p> <p>Addition axioms:</p> <ol> <li>Closure under addition: For all \\(\\mathbf{u}, \\mathbf{v} \\in V\\), we have \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</li> <li>Commutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associativity: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Zero vector: There exists \\(\\mathbf{0} \\in V\\) such that \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\) for all \\(\\mathbf{v}\\)</li> <li>Additive inverse: For each \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ol> <p>Scalar multiplication axioms:</p> <ol> <li>Closure under scalar multiplication: For all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in V\\), we have \\(c\\mathbf{v} \\in V\\)</li> <li>Distributivity over vector addition: \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> <li>Distributivity over scalar addition: \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</li> <li>Associativity of scalar multiplication: \\(c(d\\mathbf{v}) = (cd)\\mathbf{v}\\)</li> <li>Identity element: \\(1 \\cdot \\mathbf{v} = \\mathbf{v}\\)</li> </ol> <p>These axioms capture the essential algebraic properties needed for linear algebra to work.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#examples-of-vector-spaces","title":"Examples of Vector Spaces","text":"Vector Space Elements Addition Scalar Multiplication \\(\\mathbb{R}^n\\) Column vectors Component-wise Component-wise \\(\\mathcal{P}_n\\) Polynomials of degree \\(\\leq n\\) Add coefficients Multiply coefficients \\(\\mathcal{C}[a,b]\\) Continuous functions on \\([a,b]\\) \\((f+g)(x) = f(x)+g(x)\\) \\((cf)(x) = c \\cdot f(x)\\) \\(\\mathbb{R}^{m \\times n}\\) \\(m \\times n\\) matrices Entry-wise Entry-wise Solutions to \\(A\\mathbf{x} = \\mathbf{0}\\) Null space vectors Inherited from \\(\\mathbb{R}^n\\) Inherited from \\(\\mathbb{R}^n\\) <p>The Zero Vector</p> <p>Every vector space must contain a zero vector \\(\\mathbf{0}\\). In \\(\\mathbb{R}^n\\), it's the origin. In function spaces, it's the function \\(f(x) = 0\\). In matrix spaces, it's the zero matrix.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-vector-space-examples-gallery","title":"Diagram: Vector Space Examples Gallery","text":"Vector Space Examples Gallery <p>Type: infographic</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Recognize diverse examples of vector spaces and identify the zero vector and operations in each</p> <p>Layout: Grid of 6 cards, each representing a different vector space</p> <p>Cards: 1. \"\\(\\mathbb{R}^2\\): The Plane\"    - Visual: 2D coordinate system with vectors    - Zero: Origin (0, 0)    - Example: \\(\\mathbf{v} = (3, 4)\\)</p> <ol> <li>\"\\(\\mathbb{R}^3\\): 3D Space\"</li> <li>Visual: 3D coordinate system</li> <li>Zero: Origin (0, 0, 0)</li> <li> <p>Example: \\(\\mathbf{v} = (1, 2, 3)\\)</p> </li> <li> <p>\"\\(\\mathcal{P}_2\\): Quadratic Polynomials\"</p> </li> <li>Visual: Parabola graphs</li> <li>Zero: \\(p(x) = 0\\)</li> <li> <p>Example: \\(p(x) = 2x^2 - 3x + 1\\)</p> </li> <li> <p>\"Continuous Functions\"</p> </li> <li>Visual: Function curve plot</li> <li>Zero: \\(f(x) = 0\\) (horizontal axis)</li> <li> <p>Example: \\(f(x) = \\sin(x)\\)</p> </li> <li> <p>\"\\(\\mathbb{R}^{2 \\times 2}\\): 2\u00d72 Matrices\"</p> </li> <li>Visual: Matrix grid representation</li> <li>Zero: Zero matrix</li> <li> <p>Example: \\(A = [[1,2],[3,4]]\\)</p> </li> <li> <p>\"Null Space of A\"</p> </li> <li>Visual: Plane through origin in 3D</li> <li>Zero: Origin</li> <li>Example: All \\(\\mathbf{x}\\) where \\(A\\mathbf{x} = \\mathbf{0}\\)</li> </ol> <p>Interactive elements: - Hover to see addition and scalar multiplication examples - Click to verify axioms interactively</p> <p>Visual style: - Consistent card format with icons - Color-coded by dimension (finite vs. infinite)</p> <p>Implementation: HTML/CSS grid with SVG visualizations</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#subspaces","title":"Subspaces","text":"<p>A subspace of a vector space \\(V\\) is a non-empty subset \\(W \\subseteq V\\) that is itself a vector space under the same operations. Rather than checking all ten axioms, we can use a simpler test.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#subspace-test","title":"Subspace Test","text":"<p>A non-empty subset \\(W\\) of \\(V\\) is a subspace if and only if:</p> <ol> <li>Closed under addition: For all \\(\\mathbf{u}, \\mathbf{w} \\in W\\), we have \\(\\mathbf{u} + \\mathbf{w} \\in W\\)</li> <li>Closed under scalar multiplication: For all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{w} \\in W\\), we have \\(c\\mathbf{w} \\in W\\)</li> </ol> <p>Equivalently (single condition): For all \\(\\mathbf{u}, \\mathbf{w} \\in W\\) and scalars \\(c, d\\): \\(c\\mathbf{u} + d\\mathbf{w} \\in W\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#important-subspaces","title":"Important Subspaces","text":"<p>Every matrix \\(A\\) has associated subspaces:</p> <ul> <li>Column space \\(\\text{col}(A)\\): All linear combinations of columns of \\(A\\)</li> <li>Null space \\(\\text{null}(A)\\): All solutions to \\(A\\mathbf{x} = \\mathbf{0}\\)</li> <li>Row space \\(\\text{row}(A)\\): All linear combinations of rows of \\(A\\)</li> <li>Left null space \\(\\text{null}(A^T)\\): All solutions to \\(A^T\\mathbf{y} = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#non-examples","title":"Non-Examples","text":"<p>Not every subset is a subspace:</p> <ul> <li>The unit circle in \\(\\mathbb{R}^2\\) is not a subspace (not closed under addition)</li> <li>The first quadrant in \\(\\mathbb{R}^2\\) is not a subspace (not closed under scalar multiplication by negatives)</li> <li>A line not through the origin is not a subspace (no zero vector)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-subspace-tester","title":"Diagram: Subspace Tester","text":"Subspace Tester MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Test whether sets are subspaces by checking closure under linear combinations</p> <p>Visual elements: - 2D coordinate plane - Set definition displayed (equation or description) - Test vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) as draggable arrows - Linear combination \\(c\\mathbf{u} + d\\mathbf{v}\\) shown - Set boundary highlighted</p> <p>Interactive controls: - Preset sets dropdown: \"Line through origin\", \"Line not through origin\", \"First quadrant\", \"Circle\", \"Plane in 3D\" - Sliders for scalars c and d - Draggable points for u and v (constrained to set) - \"Check if Subspace\" button with explanation</p> <p>Default parameters: - Set: Line through origin (y = 2x) - Scalars c = 1, d = 1 - Canvas: responsive</p> <p>Behavior: - Highlight when linear combination leaves the set (subspace test fails) - Green indicator when combination stays in set - Explain which property fails for non-subspaces - Show counter-example automatically</p> <p>Implementation: p5.js with interactive geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#inner-products-and-inner-product-spaces","title":"Inner Products and Inner Product Spaces","text":"<p>An inner product generalizes the dot product to abstract vector spaces, enabling us to measure lengths and angles.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#inner-product-definition","title":"Inner Product Definition","text":"<p>An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) satisfying:</p> <ol> <li>Symmetry: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{v}, \\mathbf{u} \\rangle\\)</li> <li>Linearity in first argument: \\(\\langle c\\mathbf{u} + d\\mathbf{w}, \\mathbf{v} \\rangle = c\\langle \\mathbf{u}, \\mathbf{v} \\rangle + d\\langle \\mathbf{w}, \\mathbf{v} \\rangle\\)</li> <li>Positive definiteness: \\(\\langle \\mathbf{v}, \\mathbf{v} \\rangle \\geq 0\\), with equality iff \\(\\mathbf{v} = \\mathbf{0}\\)</li> </ol> <p>A vector space equipped with an inner product is called an inner product space.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#standard-inner-products","title":"Standard Inner Products","text":"Space Inner Product Formula \\(\\mathbb{R}^n\\) Dot product \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^T\\mathbf{v} = \\sum_{i=1}^n u_i v_i\\) \\(\\mathcal{C}[a,b]\\) Integral \\(\\langle f, g \\rangle = \\int_a^b f(x)g(x)\\,dx\\) \\(\\mathbb{R}^{m \\times n}\\) Frobenius \\(\\langle A, B \\rangle = \\text{tr}(A^TB) = \\sum_{i,j} a_{ij}b_{ij}\\) Weighted \\(\\mathbb{R}^n\\) Weighted dot \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle_W = \\mathbf{u}^TW\\mathbf{v}\\) (W positive definite)"},{"location":"chapters/08-vector-spaces-and-inner-products/#norm-from-inner-product","title":"Norm from Inner Product","text":"<p>Every inner product induces a norm (length function):</p> <p>\\(\\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\\)</p> <p>For the standard dot product on \\(\\mathbb{R}^n\\), this gives the Euclidean norm:</p> <p>\\(\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\\)</p> <p>The norm satisfies:</p> <ul> <li>Positivity: \\(\\|\\mathbf{v}\\| \\geq 0\\), with equality iff \\(\\mathbf{v} = \\mathbf{0}\\)</li> <li>Homogeneity: \\(\\|c\\mathbf{v}\\| = |c| \\cdot \\|\\mathbf{v}\\|\\)</li> <li>Triangle inequality: \\(\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-inner-product-visualizer","title":"Diagram: Inner Product Visualizer","text":"Inner Product Space Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how different inner products define different notions of length and angle</p> <p>Visual elements: - 2D plane with two adjustable vectors u and v - Unit circle for standard inner product - Transformed unit \"circle\" (ellipse) for weighted inner product - Angle arc between vectors - Length labels for each vector</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Inner product selector: \"Standard dot product\", \"Weighted (diagonal W)\", \"Weighted (general W)\" - Weight matrix input (for weighted inner products) - Display: inner product value, norms, angle</p> <p>Default parameters: - Standard dot product - u = (3, 1), v = (1, 2) - Canvas: responsive</p> <p>Behavior: - Real-time update of inner product, norms, angle - Show how unit ball changes with different inner products - Demonstrate that angle definition depends on inner product - Verify Cauchy-Schwarz inequality visually</p> <p>Implementation: p5.js with dynamic geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-cauchy-schwarz-inequality","title":"The Cauchy-Schwarz Inequality","text":"<p>The Cauchy-Schwarz inequality is one of the most important results in linear algebra:</p> <p>\\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|\\)</p> <p>where:</p> <ul> <li>Equality holds if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly dependent</li> <li>This inequality holds in every inner product space</li> </ul> <p>For the standard dot product in \\(\\mathbb{R}^n\\):</p> <p>\\(|\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\|_2 \\cdot \\|\\mathbf{v}\\|_2\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#angle-between-vectors","title":"Angle Between Vectors","text":"<p>The Cauchy-Schwarz inequality guarantees that:</p> <p>\\(-1 \\leq \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|} \\leq 1\\)</p> <p>This allows us to define the angle \\(\\theta\\) between vectors:</p> <p>\\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|}\\)</p> <p>Cauchy-Schwarz in Applications</p> <p>Cauchy-Schwarz appears throughout machine learning:</p> <ul> <li>Cosine similarity: \\(\\cos\\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\) measures document/word similarity</li> <li>Correlation coefficient: Normalized covariance uses Cauchy-Schwarz to bound \\(|\\rho| \\leq 1\\)</li> <li>Attention mechanisms: Softmax of dot products for similarity scoring</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthogonality","title":"Orthogonality","text":"<p>Orthogonality is the generalization of perpendicularity to abstract vector spaces.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthogonal-vectors","title":"Orthogonal Vectors","text":"<p>Two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal (written \\(\\mathbf{u} \\perp \\mathbf{v}\\)) if:</p> <p>\\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\)</p> <p>In \\(\\mathbb{R}^n\\) with the standard dot product, this means \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\).</p> <p>Key properties:</p> <ul> <li>The zero vector is orthogonal to every vector</li> <li>Orthogonal non-zero vectors are linearly independent</li> <li>The Pythagorean theorem generalizes: if \\(\\mathbf{u} \\perp \\mathbf{v}\\), then \\(\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthonormal-sets-and-bases","title":"Orthonormal Sets and Bases","text":"<p>An orthonormal set is a set of vectors that are:</p> <ol> <li>Pairwise orthogonal: \\(\\langle \\mathbf{u}_i, \\mathbf{u}_j \\rangle = 0\\) for \\(i \\neq j\\)</li> <li>Unit length: \\(\\|\\mathbf{u}_i\\| = 1\\) for all \\(i\\)</li> </ol> <p>An orthonormal basis is an orthonormal set that spans the entire space.</p> Property Orthogonal Set Orthonormal Set Orthonormal Basis Pairwise orthogonal \u2713 \u2713 \u2713 Unit vectors \u2717 \u2713 \u2713 Spans space \u2717 \u2717 \u2713 Linearly independent \u2713 (if non-zero) \u2713 \u2713"},{"location":"chapters/08-vector-spaces-and-inner-products/#why-orthonormal-bases-matter","title":"Why Orthonormal Bases Matter","text":"<p>Orthonormal bases dramatically simplify computations:</p> <p>Coordinate computation: If \\(\\{\\mathbf{q}_1, \\ldots, \\mathbf{q}_n\\}\\) is orthonormal:</p> <p>\\(\\mathbf{v} = \\sum_{i=1}^n \\langle \\mathbf{v}, \\mathbf{q}_i \\rangle \\mathbf{q}_i\\)</p> <p>The coefficients are just inner products\u2014no matrix inversion needed!</p> <p>Parseval's identity:</p> <p>\\(\\|\\mathbf{v}\\|^2 = \\sum_{i=1}^n |\\langle \\mathbf{v}, \\mathbf{q}_i \\rangle|^2\\)</p> <p>Orthogonal matrices: A matrix \\(Q\\) is orthogonal if its columns form an orthonormal basis:</p> <p>\\(Q^TQ = I \\quad \\Rightarrow \\quad Q^{-1} = Q^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-orthonormal-basis-coordinate-finder","title":"Diagram: Orthonormal Basis Coordinate Finder","text":"Orthonormal Basis Coordinate Finder <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Demonstrate how orthonormal bases simplify finding coordinates via inner products</p> <p>Visual elements: - 2D or 3D coordinate system - Standard basis vectors (gray, dashed) - Orthonormal basis vectors q\u2081, q\u2082 (colored arrows) - Target vector v (black arrow) - Projection lines from v to each q\u1d62 - Coordinate display in both bases</p> <p>Interactive controls: - Draggable orthonormal basis vectors (constrained to stay orthonormal) - Draggable target vector v - Toggle between 2D and 3D - \"Show Projections\" toggle - \"Compare to Standard Basis\" toggle</p> <p>Default parameters: - 2D mode - Orthonormal basis at 45\u00b0 rotation - v = (3, 2) - Canvas: responsive</p> <p>Behavior: - Show coefficients as inner products: c\u1d62 = \u27e8v, q\u1d62\u27e9 - Demonstrate reconstruction: v = c\u2081q\u2081 + c\u2082q\u2082 - Verify Parseval's identity visually - Compare computation effort with non-orthonormal basis</p> <p>Implementation: p5.js with vector geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-gram-schmidt-process","title":"The Gram-Schmidt Process","text":"<p>The Gram-Schmidt process converts any linearly independent set of vectors into an orthonormal set spanning the same subspace.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#algorithm","title":"Algorithm","text":"<p>Given linearly independent vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\):</p> <p>Step 1: First vector</p> <p>\\(\\mathbf{u}_1 = \\mathbf{v}_1, \\quad \\mathbf{q}_1 = \\frac{\\mathbf{u}_1}{\\|\\mathbf{u}_1\\|}\\)</p> <p>Step 2: Subtract projection, normalize</p> <p>For \\(k = 2, \\ldots, n\\):</p> <p>\\(\\mathbf{u}_k = \\mathbf{v}_k - \\sum_{j=1}^{k-1} \\langle \\mathbf{v}_k, \\mathbf{q}_j \\rangle \\mathbf{q}_j\\)</p> <p>\\(\\mathbf{q}_k = \\frac{\\mathbf{u}_k}{\\|\\mathbf{u}_k\\|}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>Each step:</p> <ol> <li>Takes the next input vector \\(\\mathbf{v}_k\\)</li> <li>Subtracts its projections onto all previously computed \\(\\mathbf{q}_j\\)</li> <li>Normalizes the result to unit length</li> </ol> <p>The projection \\(\\langle \\mathbf{v}_k, \\mathbf{q}_j \\rangle \\mathbf{q}_j\\) removes the component of \\(\\mathbf{v}_k\\) in the direction of \\(\\mathbf{q}_j\\), leaving only the component orthogonal to all previous vectors.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-gram-schmidt-process-visualizer","title":"Diagram: Gram-Schmidt Process Visualizer","text":"Gram-Schmidt Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand the Gram-Schmidt process by watching projection and orthogonalization steps</p> <p>Visual elements: - 3D coordinate system - Input vectors v\u2081, v\u2082, v\u2083 (original, semi-transparent after processing) - Current vector being processed (highlighted) - Projection vectors being subtracted (dashed arrows) - Output orthonormal vectors q\u2081, q\u2082, q\u2083 (solid, colored) - Right-angle indicators</p> <p>Interactive controls: - Input matrix (3 column vectors) - \"Next Step\" button - \"Auto Run\" with speed slider - \"Reset\" button - \"Show All Projections\" toggle - \"Show Residual\" toggle</p> <p>Default parameters: - Three linearly independent vectors in 3D - Step-by-step mode - Canvas: responsive 3D view</p> <p>Behavior: - Animate each projection subtraction - Show normalization as length scaling - Highlight orthogonality between output vectors - Display intermediate u vectors before normalization - Warning if vectors become nearly dependent</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#connection-to-qr-decomposition","title":"Connection to QR Decomposition","text":"<p>Gram-Schmidt applied to the columns of matrix \\(A\\) produces:</p> <p>\\(A = QR\\)</p> <p>where:</p> <ul> <li>\\(Q\\) contains the orthonormal vectors \\(\\mathbf{q}_1, \\ldots, \\mathbf{q}_n\\)</li> <li>\\(R\\) is upper triangular with \\(r_{ij} = \\langle \\mathbf{v}_j, \\mathbf{q}_i \\rangle\\) for \\(i &lt; j\\) and \\(r_{ii} = \\|\\mathbf{u}_i\\|\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-subspaces","title":"Projection onto Subspaces","text":"<p>Projection finds the closest point in a subspace to a given vector\u2014the foundation of least squares.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-a-line","title":"Projection onto a Line","text":"<p>The projection of \\(\\mathbf{v}\\) onto the line spanned by \\(\\mathbf{u}\\) is:</p> <p>\\(\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u} = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\mathbf{u}^T\\mathbf{u}} \\mathbf{u}\\)</p> <p>The projection matrix onto the line is:</p> <p>\\(P = \\frac{\\mathbf{u}\\mathbf{u}^T}{\\mathbf{u}^T\\mathbf{u}}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-a-subspace","title":"Projection onto a Subspace","text":"<p>For a subspace \\(W\\) with orthonormal basis \\(\\{\\mathbf{q}_1, \\ldots, \\mathbf{q}_k\\}\\):</p> <p>\\(\\text{proj}_W(\\mathbf{v}) = \\sum_{i=1}^k \\langle \\mathbf{v}, \\mathbf{q}_i \\rangle \\mathbf{q}_i\\)</p> <p>The projection matrix is:</p> <p>\\(P = QQ^T\\)</p> <p>where \\(Q = [\\mathbf{q}_1 | \\cdots | \\mathbf{q}_k]\\).</p> <p>For a general subspace with basis columns of \\(A\\):</p> <p>\\(P = A(A^TA)^{-1}A^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#properties-of-projection-matrices","title":"Properties of Projection Matrices","text":"<p>Projection matrices satisfy:</p> <ul> <li>Symmetric: \\(P = P^T\\)</li> <li>Idempotent: \\(P^2 = P\\) (projecting twice gives the same result)</li> <li>Eigenvalues: Only 0 and 1 (0 for orthogonal complement, 1 for subspace)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-projection-visualizer","title":"Diagram: Projection Visualizer","text":"Projection onto Subspace Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize projection as finding the closest point in a subspace and understand the orthogonal error</p> <p>Visual elements: - 3D coordinate system - Subspace W shown as a plane through origin (or line) - Vector v (starting point) - Projection p = proj_W(v) (on subspace) - Error vector e = v - p (perpendicular to subspace) - Right angle indicator between e and subspace</p> <p>Interactive controls: - Draggable vector v - Subspace definition: basis vectors or normal vector - Toggle between 1D subspace (line) and 2D subspace (plane) - \"Show Projection Matrix\" toggle - \"Show Error Vector\" toggle</p> <p>Default parameters: - 2D subspace (plane) in 3D - v outside the subspace - Canvas: responsive 3D view</p> <p>Behavior: - Real-time projection update as v moves - Show that e is orthogonal to subspace - Display projection formula and computation - Highlight that p is closest point in subspace to v - Show distance ||e|| as length of error</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-least-squares-problem","title":"The Least Squares Problem","text":"<p>When a system \\(A\\mathbf{x} = \\mathbf{b}\\) has no exact solution (overdetermined), we seek the least squares solution\u2014the \\(\\mathbf{x}\\) that minimizes the error \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#geometric-view","title":"Geometric View","text":"<p>The least squares problem asks: find the point \\(A\\mathbf{x}\\) in the column space of \\(A\\) closest to \\(\\mathbf{b}\\).</p> <p>The answer is the projection of \\(\\mathbf{b}\\) onto the column space:</p> <p>\\(A\\hat{\\mathbf{x}} = \\text{proj}_{\\text{col}(A)}(\\mathbf{b})\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-normal-equations","title":"The Normal Equations","text":"<p>The least squares solution satisfies the normal equations:</p> <p>\\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(A^TA\\) is an \\(n \\times n\\) symmetric positive semi-definite matrix</li> <li>\\(A^T\\mathbf{b}\\) is an \\(n \\times 1\\) vector</li> <li>If \\(A\\) has full column rank, \\(A^TA\\) is positive definite and invertible</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#derivation","title":"Derivation","text":"<p>The error vector \\(\\mathbf{e} = \\mathbf{b} - A\\hat{\\mathbf{x}}\\) must be orthogonal to the column space of \\(A\\):</p> <p>\\(A^T\\mathbf{e} = \\mathbf{0}\\)</p> <p>\\(A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}\\)</p> <p>\\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#solving-least-squares","title":"Solving Least Squares","text":"Method Formula When to Use Normal equations \\(\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\) Small, well-conditioned problems QR decomposition \\(R\\hat{\\mathbf{x}} = Q^T\\mathbf{b}\\) General, numerically stable SVD \\(\\hat{\\mathbf{x}} = V\\Sigma^{-1}U^T\\mathbf{b}\\) Rank-deficient or ill-conditioned <p>Numerical Stability</p> <p>Avoid explicitly forming \\(A^TA\\) when possible. It squares the condition number, amplifying numerical errors. Use QR or SVD instead.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-least-squares-visualizer","title":"Diagram: Least Squares Visualizer","text":"Least Squares Problem Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand least squares as projection and visualize the geometric relationship between b, Ax\u0302, and the error</p> <p>Visual elements: - 3D space showing column space of A as a plane - Vector b (outside the plane) - Projection Ax\u0302 (on the plane) - Error vector e = b - Ax\u0302 (perpendicular to plane) - Data points and fitted line (for 2D regression example)</p> <p>Interactive controls: - Switch between \"Geometric View\" and \"Regression View\" - In geometric view: adjust b position - In regression view: drag data points - \"Show Normal Equations\" toggle - \"Compare Methods\" (normal eq vs QR vs SVD)</p> <p>Default parameters: - Simple 2D linear regression example - 5 data points - Canvas: responsive</p> <p>Behavior: - Real-time update of least squares solution - Show residuals as vertical lines to fitted line - Display sum of squared residuals - Highlight that solution minimizes total squared error - Show condition number warning if ill-conditioned</p> <p>Implementation: p5.js with dual view modes</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#linear-regression-as-least-squares","title":"Linear Regression as Least Squares","text":"<p>Fitting a line \\(y = mx + c\\) to data points \\((x_1, y_1), \\ldots, (x_n, y_n)\\):</p> <p>\\(\\begin{bmatrix} x_1 &amp; 1 \\\\ x_2 &amp; 1 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; 1 \\end{bmatrix} \\begin{bmatrix} m \\\\ c \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)</p> <p>This is \\(A\\mathbf{x} = \\mathbf{b}\\) with \\(n &gt; 2\\) equations and 2 unknowns\u2014overdetermined!</p> <p>The least squares solution minimizes \\(\\sum_{i=1}^n (y_i - mx_i - c)^2\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-four-fundamental-subspaces","title":"The Four Fundamental Subspaces","text":"<p>Every \\(m \\times n\\) matrix \\(A\\) defines four fundamental subspaces that partition \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#column-space-and-row-space","title":"Column Space and Row Space","text":"<p>The column space \\(\\text{col}(A)\\) is the span of the columns of \\(A\\):</p> <p>\\(\\text{col}(A) = \\{A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{R}^n\\} \\subseteq \\mathbb{R}^m\\)</p> <p>The row space \\(\\text{row}(A)\\) is the span of the rows of \\(A\\) (equivalently, column space of \\(A^T\\)):</p> <p>\\(\\text{row}(A) = \\text{col}(A^T) \\subseteq \\mathbb{R}^n\\)</p> <p>Both have dimension equal to the rank of \\(A\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#null-space-and-left-null-space","title":"Null Space and Left Null Space","text":"<p>The null space (kernel) \\(\\text{null}(A)\\) contains all solutions to \\(A\\mathbf{x} = \\mathbf{0}\\):</p> <p>\\(\\text{null}(A) = \\{\\mathbf{x} \\in \\mathbb{R}^n : A\\mathbf{x} = \\mathbf{0}\\}\\)</p> <p>The left null space \\(\\text{null}(A^T)\\) contains all solutions to \\(A^T\\mathbf{y} = \\mathbf{0}\\):</p> <p>\\(\\text{null}(A^T) = \\{\\mathbf{y} \\in \\mathbb{R}^m : A^T\\mathbf{y} = \\mathbf{0}\\}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-fundamental-theorem","title":"The Fundamental Theorem","text":"<p>The Four Subspaces Theorem reveals beautiful orthogonal relationships:</p> Subspace Dimension Orthogonal Complement Column space \\(\\text{col}(A)\\) \\(r\\) Left null space \\(\\text{null}(A^T)\\) Row space \\(\\text{row}(A)\\) \\(r\\) Null space \\(\\text{null}(A)\\) Null space \\(\\text{null}(A)\\) \\(n - r\\) Row space \\(\\text{row}(A)\\) Left null space \\(\\text{null}(A^T)\\) \\(m - r\\) Column space \\(\\text{col}(A)\\) <p>where \\(r = \\text{rank}(A)\\).</p> <p>Key insights:</p> <ul> <li>\\(\\mathbb{R}^n = \\text{row}(A) \\oplus \\text{null}(A)\\) (direct sum)</li> <li>\\(\\mathbb{R}^m = \\text{col}(A) \\oplus \\text{null}(A^T)\\) (direct sum)</li> <li>The matrix \\(A\\) maps row space to column space (bijectively if full rank)</li> <li>The matrix \\(A\\) maps null space to zero</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-four-fundamental-subspaces","title":"Diagram: Four Fundamental Subspaces","text":"Four Fundamental Subspaces Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize the four fundamental subspaces and their orthogonal relationships</p> <p>Visual elements: - Two side-by-side panels: Domain (R^n) and Codomain (R^m) - In R^n: Row space and null space as orthogonal subspaces - In R^m: Column space and left null space as orthogonal subspaces - Arrow showing A maps row space \u2192 column space - Arrow showing A maps null space \u2192 {0} - Dimension labels on each subspace</p> <p>Interactive controls: - Matrix A input (up to 4\u00d74) - \"Compute Subspaces\" button - Toggle to show basis vectors for each subspace - Toggle to show orthogonality verification - Slider to highlight one subspace at a time</p> <p>Default parameters: - 3\u00d74 matrix with rank 2 - Show all four subspaces - Canvas: responsive dual-panel layout</p> <p>Behavior: - Compute and display bases for each subspace - Verify orthogonality numerically - Show rank and dimension formulas - Animate vector mapping from domain to codomain - Highlight which vectors map to zero</p> <p>Implementation: p5.js with SVG diagrams</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#visualization","title":"Visualization","text":"<pre><code>             A\n    R^n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  R^m\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502      \u2502             \u2502\n\u2502  row(A)     \u2502 \u2500\u2500\u2500\u2192 \u2502   col(A)    \u2502\n\u2502  dim = r    \u2502      \u2502   dim = r   \u2502\n\u2502             \u2502      \u2502             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502             \u2502      \u2502             \u2502\n\u2502  null(A)    \u2502 \u2500\u2500\u2500\u2192 \u2502   {0}       \u2502\n\u2502  dim = n-r  \u2502      \u2502             \u2502\n\u2502             \u2502      \u2502  null(A^T)  \u2502\n\u2502             \u2502      \u2502  dim = m-r  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u22a5                  \u22a5\n</code></pre>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-pseudoinverse","title":"The Pseudoinverse","text":"<p>The pseudoinverse (Moore-Penrose inverse) \\(A^+\\) generalizes the matrix inverse to rectangular and singular matrices.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#definition-via-svd","title":"Definition via SVD","text":"<p>If \\(A = U\\Sigma V^T\\) is the SVD with non-zero singular values \\(\\sigma_1, \\ldots, \\sigma_r\\):</p> <p>\\(A^+ = V\\Sigma^+ U^T\\)</p> <p>where:</p> <p>\\(\\Sigma^+ = \\begin{bmatrix} \\sigma_1^{-1} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r^{-1} \\\\ &amp; \\mathbf{0} &amp; \\end{bmatrix}^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#properties","title":"Properties","text":"<p>The pseudoinverse satisfies the Moore-Penrose conditions:</p> <ol> <li>\\(AA^+A = A\\)</li> <li>\\(A^+AA^+ = A^+\\)</li> <li>\\((AA^+)^T = AA^+\\) (symmetric)</li> <li>\\((A^+A)^T = A^+A\\) (symmetric)</li> </ol>"},{"location":"chapters/08-vector-spaces-and-inner-products/#special-cases","title":"Special Cases","text":"Matrix Type Pseudoinverse Invertible \\(A^+ = A^{-1}\\) Full column rank (\\(m &gt; n\\)) \\(A^+ = (A^TA)^{-1}A^T\\) (left inverse) Full row rank (\\(m &lt; n\\)) \\(A^+ = A^T(AA^T)^{-1}\\) (right inverse) Rank-deficient Use SVD formula"},{"location":"chapters/08-vector-spaces-and-inner-products/#least-squares-via-pseudoinverse","title":"Least Squares via Pseudoinverse","text":"<p>The least squares solution is:</p> <p>\\(\\hat{\\mathbf{x}} = A^+\\mathbf{b}\\)</p> <p>When \\(A\\) has full column rank, this equals \\((A^TA)^{-1}A^T\\mathbf{b}\\).</p> <p>When \\(A\\) is rank-deficient, the pseudoinverse gives the minimum-norm least squares solution\u2014the smallest \\(\\hat{\\mathbf{x}}\\) that minimizes \\(\\|A\\mathbf{x} - \\mathbf{b}\\|\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-pseudoinverse-application","title":"Diagram: Pseudoinverse Application","text":"Pseudoinverse Solver MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how the pseudoinverse solves least squares problems, especially for rank-deficient systems</p> <p>Visual elements: - Matrix A display with rank indicator - Vector b input - Solution x = A\u207ab display - Residual ||Ax - b|| display - SVD components visualization - Comparison: exact solution (if exists) vs least squares</p> <p>Interactive controls: - Matrix A entry fields (up to 4\u00d74) - Vector b entry fields - \"Compute Pseudoinverse\" button - \"Show SVD\" toggle - Preset examples: full rank, rank deficient, underdetermined</p> <p>Default parameters: - 3\u00d72 matrix (overdetermined) - Show solution and residual - Canvas: responsive</p> <p>Behavior: - Compute and display A\u207a - Solve x = A\u207ab - Show residual and verify it's minimal - For underdetermined: show minimum-norm property - Compare with direct (A^TA)^{-1}A^T when applicable</p> <p>Implementation: p5.js with numerical computation</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#practical-implementation","title":"Practical Implementation","text":"<pre><code>import numpy as np\nfrom scipy import linalg\n\n# Vector space operations\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\n\n# Inner product (dot product)\ninner = np.dot(v1, v2)\n\n# Norm from inner product\nnorm_v1 = np.sqrt(np.dot(v1, v1))  # or np.linalg.norm(v1)\n\n# Angle between vectors (Cauchy-Schwarz)\ncos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\ntheta = np.arccos(cos_theta)\n\n# Gram-Schmidt (via QR)\nA = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]], dtype=float)\nQ, R = np.linalg.qr(A)\nprint(\"Orthonormal basis Q:\\n\", Q)\n\n# Projection onto column space\ndef project_onto_colspace(A, b):\n    \"\"\"Project b onto column space of A.\"\"\"\n    Q, _ = np.linalg.qr(A)\n    return Q @ Q.T @ b\n\n# Least squares\nA = np.array([[1, 1], [1, 2], [1, 3]], dtype=float)\nb = np.array([1, 2, 2], dtype=float)\n\n# Method 1: Normal equations (less stable)\nx_normal = np.linalg.solve(A.T @ A, A.T @ b)\n\n# Method 2: QR decomposition (stable)\nx_qr, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n\n# Method 3: Pseudoinverse\nA_pinv = np.linalg.pinv(A)\nx_pinv = A_pinv @ b\n\n# Four fundamental subspaces via SVD\nU, s, Vh = np.linalg.svd(A)\nrank = np.sum(s &gt; 1e-10)\n\ncol_space_basis = U[:, :rank]      # Column space\nleft_null_basis = U[:, rank:]       # Left null space\nrow_space_basis = Vh[:rank, :].T    # Row space\nnull_space_basis = Vh[rank:, :].T   # Null space\n\nprint(f\"Rank: {rank}\")\nprint(f\"Column space dim: {rank}, Left null space dim: {A.shape[0] - rank}\")\nprint(f\"Row space dim: {rank}, Null space dim: {A.shape[1] - rank}\")\n</code></pre>"},{"location":"chapters/08-vector-spaces-and-inner-products/#summary_1","title":"Summary","text":"<p>This chapter developed the abstract framework for linear algebra:</p> <p>Vector Spaces:</p> <ul> <li>Abstract vector spaces satisfy ten axioms enabling addition and scalar multiplication</li> <li>Subspaces are closed under linear combinations</li> <li>The framework applies to functions, matrices, and beyond \\(\\mathbb{R}^n\\)</li> </ul> <p>Inner Products:</p> <ul> <li>Inner products generalize dot products, enabling length and angle measurement</li> <li>Norms derive from inner products: \\(\\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\\)</li> <li>Cauchy-Schwarz inequality: \\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> </ul> <p>Orthogonality:</p> <ul> <li>Orthogonal vectors satisfy \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\)</li> <li>Orthonormal bases simplify coordinate computation to inner products</li> <li>Gram-Schmidt converts any basis to orthonormal</li> </ul> <p>Projections and Least Squares:</p> <ul> <li>Projection finds the closest point in a subspace</li> <li>Least squares minimizes \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\)</li> <li>Normal equations: \\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</li> </ul> <p>Fundamental Subspaces:</p> <ul> <li>Every matrix has four fundamental subspaces: column, row, null, left null</li> <li>Row space \\(\\perp\\) null space; column space \\(\\perp\\) left null space</li> <li>Dimensions sum correctly: \\(r + (n-r) = n\\) and \\(r + (m-r) = m\\)</li> </ul> <p>Pseudoinverse:</p> <ul> <li>\\(A^+\\) generalizes inversion to any matrix</li> <li>Provides minimum-norm least squares solutions</li> <li>Computed via SVD: \\(A^+ = V\\Sigma^+ U^T\\)</li> </ul> Self-Check: Why must the error vector in least squares be orthogonal to the column space of A? <p>The error \\(\\mathbf{e} = \\mathbf{b} - A\\hat{\\mathbf{x}}\\) must be orthogonal to \\(\\text{col}(A)\\) because projection gives the closest point. If \\(\\mathbf{e}\\) had any component in \\(\\text{col}(A)\\), we could subtract that component from \\(\\mathbf{e}\\) to get closer to \\(\\mathbf{b}\\), contradicting minimality. Mathematically, the first-order optimality condition \\(\\nabla_\\mathbf{x}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 = 0\\) yields \\(A^T(A\\mathbf{x} - \\mathbf{b}) = 0\\), meaning \\(A^T\\mathbf{e} = 0\\)\u2014the error is orthogonal to every column of \\(A\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/","title":"Quiz: Vector Spaces and Inner Products","text":"<p>Test your understanding of abstract vector spaces and inner product concepts.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#1-a-set-v-is-a-vector-space-if-it-satisfies","title":"1. A set \\(V\\) is a vector space if it satisfies:","text":"<ol> <li>Contains only numerical vectors</li> <li>Is closed under vector addition and scalar multiplication with 8 axioms satisfied</li> <li>Contains exactly three vectors</li> <li>Has a unique basis</li> </ol> Show Answer <p>The correct answer is B. A vector space must be closed under vector addition and scalar multiplication, and satisfy eight axioms including associativity, commutativity of addition, existence of zero vector, and distributive laws.</p> <p>Concept Tested: Vector Space Axioms</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#2-a-subspace-of-a-vector-space-must","title":"2. A subspace of a vector space must:","text":"<ol> <li>Have the same dimension as the parent space</li> <li>Contain the zero vector and be closed under addition and scalar multiplication</li> <li>Be finite-dimensional</li> <li>Contain at least two vectors</li> </ol> Show Answer <p>The correct answer is B. A subspace must contain the zero vector and be closed under vector addition and scalar multiplication. These conditions ensure the subspace is itself a vector space under the inherited operations.</p> <p>Concept Tested: Subspace</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#3-an-inner-product-langle-mathbfu-mathbfv-rangle-must-satisfy-all-except","title":"3. An inner product \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) must satisfy all EXCEPT:","text":"<ol> <li>Linearity in the first argument</li> <li>Conjugate symmetry</li> <li>Positive definiteness</li> <li>Multiplicativity: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{u} \\rangle \\cdot \\langle \\mathbf{v} \\rangle\\)</li> </ol> Show Answer <p>The correct answer is D. Inner products require linearity (in first argument for real, conjugate-linear in second for complex), conjugate symmetry \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}\\), and positive definiteness. There is no multiplicativity requirement.</p> <p>Concept Tested: Inner Product Properties</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#4-the-standard-inner-product-in-mathbbrn-is","title":"4. The standard inner product in \\(\\mathbb{R}^n\\) is:","text":"<ol> <li>The sum of vector components</li> <li>The dot product \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum u_i v_i\\)</li> <li>The cross product</li> <li>The Euclidean distance</li> </ol> Show Answer <p>The correct answer is B. The standard inner product in \\(\\mathbb{R}^n\\) is the dot product: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^n u_i v_i\\). It induces the Euclidean norm.</p> <p>Concept Tested: Standard Inner Product</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#5-an-orthonormal-basis-has-vectors-that-are","title":"5. An orthonormal basis has vectors that are:","text":"<ol> <li>Parallel and of any length</li> <li>Mutually orthogonal and each with unit length</li> <li>Linearly dependent</li> <li>All equal to each other</li> </ol> Show Answer <p>The correct answer is B. An orthonormal basis consists of vectors that are mutually orthogonal (pairwise dot product is zero) and each normalized to unit length. This makes coordinate representation and computation especially convenient.</p> <p>Concept Tested: Orthonormal Basis</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#6-the-projection-of-vector-mathbfu-onto-vector-mathbfv-is","title":"6. The projection of vector \\(\\mathbf{u}\\) onto vector \\(\\mathbf{v}\\) is:","text":"<ol> <li>\\(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}\\)</li> <li>\\(\\mathbf{u} + \\mathbf{v}\\)</li> <li>\\(\\mathbf{u} \\times \\mathbf{v}\\)</li> <li>\\(\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> </ol> Show Answer <p>The correct answer is A. The orthogonal projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}\\). This gives the component of \\(\\mathbf{u}\\) in the direction of \\(\\mathbf{v}\\).</p> <p>Concept Tested: Orthogonal Projection</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#7-the-orthogonal-complement-of-a-subspace-w-contains","title":"7. The orthogonal complement of a subspace \\(W\\) contains:","text":"<ol> <li>All vectors parallel to \\(W\\)</li> <li>All vectors orthogonal to every vector in \\(W\\)</li> <li>Only the zero vector</li> <li>The basis vectors of \\(W\\)</li> </ol> Show Answer <p>The correct answer is B. The orthogonal complement \\(W^\\perp\\) consists of all vectors that are orthogonal to every vector in \\(W\\). For any \\(\\mathbf{u} \\in W^\\perp\\) and \\(\\mathbf{w} \\in W\\), we have \\(\\langle \\mathbf{u}, \\mathbf{w} \\rangle = 0\\).</p> <p>Concept Tested: Orthogonal Complement</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#8-the-cauchy-schwarz-inequality-states","title":"8. The Cauchy-Schwarz inequality states:","text":"<ol> <li>\\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> <li>\\(\\|\\mathbf{u} + \\mathbf{v}\\| = \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> <li>\\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\|\\mathbf{u}\\|^2\\)</li> <li>\\(\\|\\mathbf{u} - \\mathbf{v}\\| &gt; \\|\\mathbf{u}\\|\\)</li> </ol> Show Answer <p>The correct answer is A. The Cauchy-Schwarz inequality bounds the inner product: \\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\). Equality holds if and only if the vectors are parallel.</p> <p>Concept Tested: Cauchy-Schwarz Inequality</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#9-an-inner-product-space-is","title":"9. An inner product space is:","text":"<ol> <li>A vector space with a defined inner product</li> <li>A space containing only unit vectors</li> <li>A subspace of \\(\\mathbb{R}^3\\)</li> <li>A space with no zero vector</li> </ol> Show Answer <p>The correct answer is A. An inner product space is a vector space equipped with an inner product function that satisfies the required axioms. This structure enables defining lengths, angles, and orthogonality.</p> <p>Concept Tested: Inner Product Space</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#10-the-angle-theta-between-two-non-zero-vectors-is-determined-by","title":"10. The angle \\(\\theta\\) between two non-zero vectors is determined by:","text":"<ol> <li>\\(\\cos\\theta = \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> <li>\\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\)</li> <li>\\(\\theta = \\mathbf{u} \\cdot \\mathbf{v}\\)</li> <li>\\(\\sin\\theta = \\frac{\\|\\mathbf{u}\\|}{\\|\\mathbf{v}\\|}\\)</li> </ol> Show Answer <p>The correct answer is B. The angle between vectors is given by \\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\). This generalizes the geometric definition of angle to any inner product space.</p> <p>Concept Tested: Angle Between Vectors</p>"},{"location":"chapters/09-machine-learning-foundations/","title":"Machine Learning Foundations","text":""},{"location":"chapters/09-machine-learning-foundations/#summary","title":"Summary","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques. You will learn how data is represented as matrices, understand covariance and correlation, master Principal Component Analysis (PCA) for dimensionality reduction, and implement linear regression with regularization. Gradient descent, the workhorse of machine learning optimization, is covered in detail.</p>"},{"location":"chapters/09-machine-learning-foundations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"chapters/09-machine-learning-foundations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 7: Matrix Decompositions</li> <li>Chapter 8: Vector Spaces and Inner Products</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#introduction","title":"Introduction","text":"<p>Machine learning is, at its core, applied linear algebra. When you train a model, you perform matrix operations. When you reduce dimensions, you compute eigendecompositions. When you optimize, you follow gradients through high-dimensional spaces. Understanding the linear algebra behind these operations transforms you from a user of black-box algorithms into a practitioner who can debug, optimize, and innovate.</p> <p>This chapter bridges abstract linear algebra and practical machine learning. We start with how data becomes matrices, develop statistical tools like covariance, build up to PCA for dimensionality reduction, implement regression with regularization, and master gradient descent for optimization. Each section reinforces that machine learning \"magic\" is really linear algebra in action.</p>"},{"location":"chapters/09-machine-learning-foundations/#data-as-matrices","title":"Data as Matrices","text":"<p>In machine learning, data is organized into matrices where each row represents an observation and each column represents a feature.</p>"},{"location":"chapters/09-machine-learning-foundations/#feature-vectors","title":"Feature Vectors","text":"<p>A feature vector represents a single data point as a vector of measurements or attributes:</p> <p>\\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\in \\mathbb{R}^d\\)</p> <p>where:</p> <ul> <li>\\(d\\) is the number of features (dimensionality)</li> <li>Each \\(x_i\\) is a measurement (e.g., height, weight, pixel intensity)</li> </ul> <p>Examples of feature vectors:</p> Domain Features Dimensionality Housing bedrooms, sqft, age, location 4+ Images pixel intensities 784 (28\u00d728) to millions Text word counts or embeddings 100 to 768+ Tabular mixed numerical/categorical varies"},{"location":"chapters/09-machine-learning-foundations/#feature-matrix-and-data-matrix","title":"Feature Matrix and Data Matrix","text":"<p>A feature matrix (also called data matrix) stacks \\(n\\) feature vectors as rows:</p> <p>\\(X = \\begin{bmatrix} \u2014 \\mathbf{x}_1^T \u2014 \\\\ \u2014 \\mathbf{x}_2^T \u2014 \\\\ \\vdots \\\\ \u2014 \\mathbf{x}_n^T \u2014 \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(X\\) is \\(n \\times d\\) (n samples, d features)</li> <li>Row \\(i\\) is sample \\(\\mathbf{x}_i^T\\)</li> <li>Column \\(j\\) contains all values of feature \\(j\\)</li> </ul> <p>Convention Alert</p> <p>Some texts use columns for samples (X is d\u00d7n). We follow the more common machine learning convention where rows are samples, matching NumPy/Pandas defaults. Always check the convention when reading papers or documentation.</p>"},{"location":"chapters/09-machine-learning-foundations/#diagram-data-matrix-structure","title":"Diagram: Data Matrix Structure","text":"Data Matrix Structure Visualizer <p>Type: infographic</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize the structure of data matrices and understand the relationship between rows (samples) and columns (features)</p> <p>Layout: Interactive matrix representation with labeled dimensions</p> <p>Visual elements: - Main panel: Color-coded matrix grid - Row labels: \"Sample 1\", \"Sample 2\", ..., \"Sample n\" - Column labels: \"Feature 1\", \"Feature 2\", ..., \"Feature d\" - Highlighted single row showing feature vector - Highlighted single column showing all values of one feature - Dimension annotations: n (rows) and d (columns)</p> <p>Interactive elements: - Click a row to highlight as feature vector - Click a column to highlight as feature across all samples - Hover to see individual cell value - Toggle to show actual example data (iris, housing, etc.)</p> <p>Example datasets: 1. Iris: 150 samples, 4 features (petal/sepal dimensions) 2. MNIST digit: 1 sample, 784 features (pixel values) 3. Housing: 506 samples, 13 features</p> <p>Visual style: - Heat map coloring for numerical values - Clean grid lines - Responsive sizing</p> <p>Implementation: HTML/CSS/JavaScript with interactive highlighting</p>"},{"location":"chapters/09-machine-learning-foundations/#statistical-foundations","title":"Statistical Foundations","text":"<p>Before applying machine learning algorithms, we must understand the statistical structure of our data.</p>"},{"location":"chapters/09-machine-learning-foundations/#standardization","title":"Standardization","text":"<p>Standardization transforms features to have zero mean and unit variance:</p> <p>\\(z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\\)</p> <p>where:</p> <ul> <li>\\(\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}\\) is the mean of feature \\(j\\)</li> <li>\\(\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\\) is the standard deviation</li> </ul> <p>In matrix form, if \\(\\boldsymbol{\\mu}\\) is the row vector of means:</p> <p>\\(Z = (X - \\mathbf{1}\\boldsymbol{\\mu}) \\text{diag}(\\boldsymbol{\\sigma})^{-1}\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#why-standardize","title":"Why Standardize?","text":"<ul> <li>Scale invariance: Features measured in different units become comparable</li> <li>Numerical stability: Prevents features with large values from dominating</li> <li>Algorithm requirements: Many algorithms (PCA, gradient descent, regularization) assume or benefit from standardized data</li> </ul> Algorithm Standardization PCA Required for meaningful results k-Means Recommended SVM Required (especially with RBF kernel) Neural Networks Strongly recommended Decision Trees Not necessary Linear Regression Recommended for regularization"},{"location":"chapters/09-machine-learning-foundations/#covariance-matrix","title":"Covariance Matrix","text":"<p>The covariance matrix captures how features vary together:</p> <p>\\(\\Sigma = \\frac{1}{n-1}(X - \\mathbf{1}\\boldsymbol{\\mu})^T(X - \\mathbf{1}\\boldsymbol{\\mu}) = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</p> <p>where:</p> <ul> <li>\\(\\tilde{X}\\) is the centered data matrix (mean subtracted)</li> <li>\\(\\Sigma\\) is a \\(d \\times d\\) symmetric positive semi-definite matrix</li> <li>\\(\\Sigma_{jk} = \\text{Cov}(X_j, X_k)\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#covariance-formula","title":"Covariance Formula","text":"<p>The covariance between features \\(j\\) and \\(k\\):</p> <p>\\(\\text{Cov}(X_j, X_k) = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\mu_j)(x_{ik} - \\mu_k)\\)</p> <p>Properties:</p> <ul> <li>Diagonal entries \\(\\Sigma_{jj} = \\text{Var}(X_j)\\) are variances</li> <li>Off-diagonal entries measure linear relationships</li> <li>\\(\\Sigma_{jk} &gt; 0\\): features increase together</li> <li>\\(\\Sigma_{jk} &lt; 0\\): one increases as other decreases</li> <li>\\(\\Sigma_{jk} = 0\\): no linear relationship (not necessarily independent)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#correlation-matrix","title":"Correlation Matrix","text":"<p>The correlation matrix is the standardized covariance:</p> <p>\\(R = D^{-1}\\Sigma D^{-1}\\)</p> <p>where:</p> <ul> <li>\\(D = \\text{diag}(\\sigma_1, \\ldots, \\sigma_d)\\) contains standard deviations</li> <li>\\(R_{jk} = \\frac{\\Sigma_{jk}}{\\sigma_j \\sigma_k} = \\frac{\\text{Cov}(X_j, X_k)}{\\sqrt{\\text{Var}(X_j)\\text{Var}(X_k)}}\\)</li> </ul> <p>Properties:</p> <ul> <li>Diagonal entries are 1 (features perfectly correlate with themselves)</li> <li>Off-diagonal entries satisfy \\(-1 \\leq R_{jk} \\leq 1\\)</li> <li>\\(R_{jk} = \\pm 1\\): perfect linear relationship</li> <li>The correlation matrix is the covariance matrix of standardized data</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-covariance-and-correlation-visualizer","title":"Diagram: Covariance and Correlation Visualizer","text":"Covariance and Correlation Matrix Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how covariance and correlation capture relationships between features through interactive exploration</p> <p>Visual elements: - Left panel: Scatter plot matrix (pairs of features) - Center panel: Covariance matrix as heatmap - Right panel: Correlation matrix as heatmap - Color scale: Blue (negative) to White (zero) to Red (positive) - Eigenvalue display for covariance matrix</p> <p>Interactive controls: - Dataset selector (generated bivariate, iris, custom) - Draggable data points to modify dataset - \"Standardize\" toggle to see effect on covariance - Highlight cell to see corresponding scatter plot - Slider to add/remove correlation between features</p> <p>Default parameters: - 2D generated data with moderate positive correlation - 100 sample points - Canvas: responsive three-panel layout</p> <p>Behavior: - Real-time update of matrices as data changes - Show how correlation normalizes for scale - Highlight relationship between scatter plot shape and correlation value - Display eigenvalues/eigenvectors of covariance matrix - Demonstrate that standardized data has correlation = covariance</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Principal Component Analysis is a technique for dimensionality reduction that finds the directions of maximum variance in data.</p>"},{"location":"chapters/09-machine-learning-foundations/#the-goal-of-dimensionality-reduction","title":"The Goal of Dimensionality Reduction","text":"<p>High-dimensional data presents challenges:</p> <ul> <li>Visualization: Cannot plot more than 3 dimensions</li> <li>Computation: Many algorithms scale poorly with dimensions</li> <li>Curse of dimensionality: Data becomes sparse in high dimensions</li> <li>Noise: Some dimensions may be noise rather than signal</li> </ul> <p>Dimensionality reduction projects data from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}^k\\) where \\(k &lt; d\\), preserving as much information as possible.</p>"},{"location":"chapters/09-machine-learning-foundations/#principal-components","title":"Principal Components","text":"<p>Principal components are the eigenvectors of the covariance matrix, ordered by their eigenvalues:</p> <p>\\(\\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_1\\) is the first principal component (direction of maximum variance)</li> <li>\\(\\mathbf{v}_2\\) is orthogonal to \\(\\mathbf{v}_1\\) and captures maximum remaining variance</li> <li>\\(\\lambda_i\\) is the variance explained by the \\(i\\)-th component</li> </ul> <p>The principal components form an orthonormal basis aligned with the data's natural axes of variation.</p>"},{"location":"chapters/09-machine-learning-foundations/#pca-algorithm","title":"PCA Algorithm","text":"<p>Step 1: Center the data</p> <p>\\(\\tilde{X} = X - \\mathbf{1}\\boldsymbol{\\mu}\\)</p> <p>Step 2: Compute covariance matrix</p> <p>\\(\\Sigma = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</p> <p>Step 3: Eigendecomposition</p> <p>\\(\\Sigma = V\\Lambda V^T\\)</p> <p>where \\(V = [\\mathbf{v}_1 | \\cdots | \\mathbf{v}_d]\\) and \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_d)\\) with \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_d\\).</p> <p>Step 4: Project onto top \\(k\\) components</p> <p>\\(Z = \\tilde{X}V_k\\)</p> <p>where \\(V_k = [\\mathbf{v}_1 | \\cdots | \\mathbf{v}_k]\\) contains the first \\(k\\) principal components.</p>"},{"location":"chapters/09-machine-learning-foundations/#variance-explained","title":"Variance Explained","text":"<p>The variance explained by each principal component is its eigenvalue:</p> <ul> <li>Total variance: \\(\\sum_{i=1}^d \\lambda_i = \\text{trace}(\\Sigma)\\)</li> <li>Proportion of variance explained by component \\(i\\): \\(\\frac{\\lambda_i}{\\sum_{j=1}^d \\lambda_j}\\)</li> <li>Cumulative variance explained by first \\(k\\) components: \\(\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^d \\lambda_j}\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#scree-plot","title":"Scree Plot","text":"<p>A scree plot visualizes eigenvalues to help choose the number of components:</p> <ul> <li>X-axis: Component number (1, 2, 3, ...)</li> <li>Y-axis: Eigenvalue (variance explained) or proportion of variance</li> <li>Look for an \"elbow\" where eigenvalues drop sharply</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-pca-interactive-explorer","title":"Diagram: PCA Interactive Explorer","text":"PCA Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand PCA by visualizing each step from raw data to projected low-dimensional representation</p> <p>Visual elements: - Panel 1: Original 2D/3D data with mean point - Panel 2: Centered data (translated to origin) - Panel 3: Principal component vectors overlaid on data - Panel 4: Projected 1D data along first PC - Scree plot showing eigenvalues - Variance explained percentage display</p> <p>Interactive controls: - Data generator: cluster shape, spread, rotation - Number of points slider (20-200) - Dimension selector (2D or 3D) - Step-through buttons: \"Center\", \"Find PCs\", \"Project\" - Number of components to keep (k) - \"Show Reconstruction\" toggle</p> <p>Default parameters: - Elongated 2D Gaussian cluster - 100 points - Canvas: responsive multi-panel</p> <p>Behavior: - Animate centering transformation - Show eigenvectors with length proportional to eigenvalue - Demonstrate projection onto first PC - Show reconstruction error when reducing dimensions - Display scree plot updating with data changes</p> <p>Implementation: p5.js with eigenvalue computation</p>"},{"location":"chapters/09-machine-learning-foundations/#pca-via-svd","title":"PCA via SVD","text":"<p>In practice, PCA is computed using SVD for numerical stability:</p> <p>\\(\\tilde{X} = U\\Sigma V^T\\)</p> <p>The relationship to eigendecomposition:</p> <ul> <li>Right singular vectors \\(V\\) are the principal components</li> <li>Singular values relate to eigenvalues: \\(\\lambda_i = \\frac{\\sigma_i^2}{n-1}\\)</li> <li>This avoids forming \\(\\tilde{X}^T\\tilde{X}\\) explicitly</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-scree-plot-interactive","title":"Diagram: Scree Plot Interactive","text":"Scree Plot and Component Selection <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Use scree plots and cumulative variance to select the optimal number of principal components</p> <p>Visual elements: - Left panel: Scree plot (bar chart of eigenvalues) - Right panel: Cumulative variance explained (line plot) - Threshold line for desired variance (e.g., 95%) - Elbow point detection and highlight - Reconstruction comparison at different k values</p> <p>Interactive controls: - Dataset selector (synthetic, iris, digits subset) - Draggable threshold line for variance target - Number of components slider - \"Show Reconstructed Data\" toggle - \"Compare Original vs Reconstructed\" toggle</p> <p>Default parameters: - Synthetic dataset with clear elbow at k=3 - 95% variance threshold line - Canvas: responsive dual-panel</p> <p>Behavior: - Highlight suggested k based on elbow detection - Show which k achieves target variance - Display reconstruction error as k changes - For image data: show visual reconstruction quality - Kaiser criterion line (eigenvalue = 1 for standardized data)</p> <p>Implementation: p5.js with statistical visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#linear-regression","title":"Linear Regression","text":"<p>Linear regression fits a linear model to predict a target variable from features.</p>"},{"location":"chapters/09-machine-learning-foundations/#the-model","title":"The Model","text":"<p>Given features \\(\\mathbf{x} \\in \\mathbb{R}^d\\) and target \\(y \\in \\mathbb{R}\\):</p> <p>\\(y = \\mathbf{w}^T\\mathbf{x} + b = w_1x_1 + w_2x_2 + \\cdots + w_dx_d + b\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector</li> <li>\\(b \\in \\mathbb{R}\\) is the bias (intercept)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#design-matrix","title":"Design Matrix","text":"<p>The design matrix augments features with a column of ones to absorb the bias:</p> <p>\\(X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\end{bmatrix}\\)</p> <p>Now the model becomes:</p> <p>\\(\\mathbf{y} = X\\boldsymbol{\\theta}\\)</p> <p>where \\(\\boldsymbol{\\theta} = [b, w_1, \\ldots, w_d]^T\\) combines bias and weights.</p>"},{"location":"chapters/09-machine-learning-foundations/#ordinary-least-squares","title":"Ordinary Least Squares","text":"<p>The least squares solution minimizes:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 = \\sum_{i=1}^n (x_i^T\\boldsymbol{\\theta} - y_i)^2\\)</p> <p>The closed-form solution (normal equations):</p> <p>\\(\\hat{\\boldsymbol{\\theta}} = (X^TX)^{-1}X^T\\mathbf{y}\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#diagram-linear-regression-visualizer","title":"Diagram: Linear Regression Visualizer","text":"Linear Regression Interactive Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand linear regression as finding the best-fit line/plane by minimizing squared errors</p> <p>Visual elements: - Main panel: Scatter plot of data points - Fitted line/plane (2D or 3D) - Residual lines from points to fitted line - Loss function surface (for 2D: 3D surface of loss vs w, b) - Current parameter values display</p> <p>Interactive controls: - Drag data points to modify dataset - Manual sliders for w and b to see effect on fit and loss - \"Fit OLS\" button to compute optimal parameters - Toggle residual visualization - Switch between 1D (line fit) and 2D (plane fit) examples</p> <p>Default parameters: - 2D scatter with linear relationship plus noise - 20 data points - Canvas: responsive</p> <p>Behavior: - Real-time residual and loss computation - Show that OLS solution is at minimum of loss surface - Display R\u00b2 score for goodness of fit - Highlight vertical (y) residuals vs perpendicular distance - Show normal equations computation</p> <p>Implementation: p5.js with regression computation</p>"},{"location":"chapters/09-machine-learning-foundations/#regularization","title":"Regularization","text":"<p>Regularization adds a penalty term to prevent overfitting by constraining model complexity.</p>"},{"location":"chapters/09-machine-learning-foundations/#why-regularize","title":"Why Regularize?","text":"<p>Without regularization, models can:</p> <ul> <li>Overfit to noise in training data</li> <li>Have large, unstable weights</li> <li>Perform poorly on new data</li> <li>Fail when features are correlated (multicollinearity)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#ridge-regression-l2","title":"Ridge Regression (L2)","text":"<p>Ridge regression adds an L2 penalty on weights:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 + \\alpha\\|\\boldsymbol{\\theta}\\|^2\\)</p> <p>where:</p> <ul> <li>\\(\\alpha \\geq 0\\) is the regularization strength</li> <li>\\(\\|\\boldsymbol{\\theta}\\|^2 = \\sum_j \\theta_j^2\\) (typically excluding bias)</li> </ul> <p>The closed-form solution:</p> <p>\\(\\hat{\\boldsymbol{\\theta}} = (X^TX + \\alpha I)^{-1}X^T\\mathbf{y}\\)</p> <p>Key properties:</p> <ul> <li>Always invertible (even if \\(X^TX\\) is singular)</li> <li>Shrinks weights toward zero</li> <li>Keeps all features (no feature selection)</li> <li>Equivalent to adding \"fake\" data points</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#lasso-regression-l1","title":"Lasso Regression (L1)","text":"<p>Lasso regression uses an L1 penalty:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 + \\alpha\\|\\boldsymbol{\\theta}\\|_1\\)</p> <p>where:</p> <ul> <li>\\(\\|\\boldsymbol{\\theta}\\|_1 = \\sum_j |\\theta_j|\\) is the L1 norm</li> </ul> <p>Key properties:</p> <ul> <li>Produces sparse solutions (some weights exactly zero)</li> <li>Performs automatic feature selection</li> <li>No closed-form solution (requires iterative optimization)</li> <li>Useful when only few features are truly relevant</li> </ul> Property Ridge (L2) Lasso (L1) Penalty \\(\\sum \\theta_j^2\\) $\\sum Sparsity No Yes (feature selection) Closed-form Yes No Multicollinearity Handles well Picks one of correlated features Geometry Circular constraint Diamond constraint"},{"location":"chapters/09-machine-learning-foundations/#diagram-regularization-geometry","title":"Diagram: Regularization Geometry","text":"Regularization Geometry Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how L1 and L2 regularization constrain weights geometrically and why L1 produces sparsity</p> <p>Visual elements: - 2D parameter space (\u03b8\u2081 vs \u03b8\u2082) - Contour lines of unregularized loss function - L2 constraint region (circle) - L1 constraint region (diamond) - OLS solution point - Regularized solution point - Regularization path as \u03b1 varies</p> <p>Interactive controls: - Slider for regularization strength \u03b1 - Toggle between L1 and L2 - Drag ellipse center (changing OLS solution location) - \"Show Regularization Path\" toggle - Animation of solution as \u03b1 increases</p> <p>Default parameters: - OLS solution at (3, 2) - Moderate \u03b1 - Canvas: responsive</p> <p>Behavior: - Show how constraint region intersects loss contours - Demonstrate L1 hitting corners (sparse solution) - Animate solution moving toward origin as \u03b1 increases - Show weight values and their evolution - Display sparsity count for L1</p> <p>Implementation: p5.js with geometric visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-based-optimization","title":"Gradient-Based Optimization","text":"<p>When closed-form solutions don't exist or are too expensive, we use iterative gradient descent.</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-vector","title":"Gradient Vector","text":"<p>The gradient vector of a scalar function \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) collects all partial derivatives:</p> <p>\\(\\nabla f(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial \\theta_1} \\\\ \\frac{\\partial f}{\\partial \\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial \\theta_d} \\end{bmatrix}\\)</p> <p>Key properties:</p> <ul> <li>Points in the direction of steepest increase</li> <li>Magnitude indicates rate of change</li> <li>At a minimum, \\(\\nabla f = \\mathbf{0}\\)</li> </ul> <p>For linear regression loss \\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2\\):</p> <p>\\(\\nabla J(\\boldsymbol{\\theta}) = 2X^T(X\\boldsymbol{\\theta} - \\mathbf{y})\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-descent-algorithm","title":"Gradient Descent Algorithm","text":"<p>Gradient descent iteratively moves in the negative gradient direction:</p> <p>\\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta \\nabla J(\\boldsymbol{\\theta}^{(t)})\\)</p> <p>where:</p> <ul> <li>\\(\\eta &gt; 0\\) is the learning rate (step size)</li> <li>\\(t\\) is the iteration number</li> <li>We move against the gradient to decrease the function</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#batch-gradient-descent","title":"Batch Gradient Descent","text":"<p>Batch gradient descent uses all training samples to compute the gradient at each step:</p> <pre><code>Initialize \u03b8 randomly\nFor t = 1, 2, ..., max_iterations:\n    gradient = (2/n) * X^T @ (X @ \u03b8 - y)  # Full batch gradient\n    \u03b8 = \u03b8 - \u03b7 * gradient\n    If ||gradient|| &lt; tolerance:\n        break\nReturn \u03b8\n</code></pre> <p>Characteristics:</p> <ul> <li>Deterministic updates (same path from same initialization)</li> <li>Smooth convergence</li> <li>Expensive per iteration for large datasets</li> <li>May be slow for large \\(n\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#learning-rate","title":"Learning Rate","text":"<p>The learning rate \\(\\eta\\) controls step size and critically affects convergence:</p> Learning Rate Behavior Too small Very slow convergence, may take forever Just right Smooth, efficient convergence Too large Oscillation, overshooting Way too large Divergence (loss increases) <p>Choosing the learning rate:</p> <ul> <li>Start with \\(\\eta = 0.01\\) or \\(0.001\\)</li> <li>Use learning rate schedules (decay over time)</li> <li>Adaptive methods (Adam, RMSprop) adjust per-parameter</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-gradient-descent-visualizer","title":"Diagram: Gradient Descent Visualizer","text":"Gradient Descent Interactive Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how gradient descent navigates the loss surface and how learning rate affects convergence</p> <p>Visual elements: - Main panel: 3D surface plot of loss function J(\u03b8\u2081, \u03b8\u2082) - Contour plot view (top-down) - Current position marker - Gradient arrow at current position - Path traced by optimization - Loss vs iteration plot</p> <p>Interactive controls: - Learning rate slider (0.001 to 1.0, log scale) - \"Step\" button for single iteration - \"Run\" button for continuous optimization - \"Reset\" button to reinitialize - Starting point selector (click on surface) - Loss function selector (quadratic, Rosenbrock, etc.)</p> <p>Default parameters: - Simple quadratic loss with single minimum - Learning rate = 0.1 - Starting point away from minimum - Canvas: responsive multi-view</p> <p>Behavior: - Show gradient vector at each step - Trace optimization path on contour plot - Display convergence (or divergence) in loss plot - Demonstrate oscillation with high learning rate - Show slow progress with low learning rate - Count iterations to convergence</p> <p>Implementation: p5.js with 3D surface rendering (WEBGL)</p>"},{"location":"chapters/09-machine-learning-foundations/#variants-of-gradient-descent","title":"Variants of Gradient Descent","text":"<p>Beyond batch gradient descent, several variants improve efficiency:</p> <p>Stochastic Gradient Descent (SGD):</p> <ul> <li>Uses single sample per update: \\(\\nabla J_i(\\boldsymbol{\\theta})\\)</li> <li>Fast iterations but noisy updates</li> <li>Can escape local minima due to noise</li> </ul> <p>Mini-batch Gradient Descent:</p> <ul> <li>Uses subset of samples (batch size \\(b\\)): \\(\\frac{1}{b}\\sum_{i \\in B}\\nabla J_i(\\boldsymbol{\\theta})\\)</li> <li>Balances noise and efficiency</li> <li>Standard in deep learning (batch size 32-256)</li> </ul> <p>Momentum:</p> <ul> <li>Accumulates velocity: \\(\\mathbf{v}^{(t+1)} = \\beta\\mathbf{v}^{(t)} + \\nabla J(\\boldsymbol{\\theta}^{(t)})\\)</li> <li>Update: \\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta\\mathbf{v}^{(t+1)}\\)</li> <li>Accelerates through flat regions, dampens oscillations</li> </ul> Method Per-Iteration Cost Convergence Noise Batch GD \\(O(nd)\\) Smooth None SGD \\(O(d)\\) Noisy High Mini-batch \\(O(bd)\\) Moderate Moderate"},{"location":"chapters/09-machine-learning-foundations/#diagram-learning-rate-effect-visualizer","title":"Diagram: Learning Rate Effect Visualizer","text":"Learning Rate Effect on Convergence <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how learning rate choice affects optimization behavior through side-by-side comparison</p> <p>Visual elements: - Three parallel contour plots with different learning rates - Path traces showing optimization trajectories - Loss curves for each learning rate - Status indicators: \"Converging\", \"Oscillating\", \"Diverging\" - Step count to convergence</p> <p>Interactive controls: - Individual learning rate sliders for each panel - Preset buttons: \"Too Small\", \"Just Right\", \"Too Large\" - Shared \"Run All\" button - \"Reset All\" button - Speed slider for animation</p> <p>Default parameters: - Left: \u03b7 = 0.01 (too small) - Center: \u03b7 = 0.1 (good) - Right: \u03b7 = 0.5 (too large) - Same starting point for all</p> <p>Behavior: - Simultaneous animation of all three optimizations - Real-time loss comparison plot - Show oscillation in too-large case - Show slow progress in too-small case - Identify optimal learning rate region - Display final loss values</p> <p>Implementation: p5.js with synchronized animations</p>"},{"location":"chapters/09-machine-learning-foundations/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete machine learning pipeline using these concepts:</p> <pre><code>import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Load data\nX_train, y_train = load_data()  # n\u00d7d feature matrix, n\u00d71 target\n\n# Step 1: Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# Step 2: PCA for dimensionality reduction\npca = PCA(n_components=0.95)  # Keep 95% variance\nX_reduced = pca.fit_transform(X_scaled)\nprint(f\"Reduced from {X_train.shape[1]} to {X_reduced.shape[1]} dimensions\")\n\n# Step 3: Examine variance explained\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(pca.explained_variance_ratio_)),\n        pca.explained_variance_ratio_)\nplt.title('Scree Plot')\nplt.subplot(1, 2, 2)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.axhline(0.95, color='r', linestyle='--')\nplt.title('Cumulative Variance Explained')\nplt.show()\n\n# Step 4: Ridge regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_reduced, y_train)\n\n# Step 5: Lasso for feature selection (on original features)\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_scaled, y_train)\nselected_features = np.where(lasso.coef_ != 0)[0]\nprint(f\"Lasso selected {len(selected_features)} features\")\n\n# Step 6: Manual gradient descent implementation\ndef gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n    n, d = X.shape\n    theta = np.zeros(d)\n    losses = []\n\n    for i in range(n_iterations):\n        # Compute predictions\n        y_pred = X @ theta\n\n        # Compute gradient\n        gradient = (2/n) * X.T @ (y_pred - y)\n\n        # Update parameters\n        theta = theta - learning_rate * gradient\n\n        # Track loss\n        loss = np.mean((y_pred - y)**2)\n        losses.append(loss)\n\n    return theta, losses\n\n# Add bias column and run gradient descent\nX_with_bias = np.c_[np.ones(len(X_reduced)), X_reduced]\ntheta_gd, losses = gradient_descent(X_with_bias, y_train,\n                                     learning_rate=0.1,\n                                     n_iterations=500)\n</code></pre>"},{"location":"chapters/09-machine-learning-foundations/#diagram-ml-pipeline-workflow","title":"Diagram: ML Pipeline Workflow","text":"Machine Learning Pipeline Workflow <p>Type: workflow</p> <p>Bloom Taxonomy Level: Create</p> <p>Learning Objective: Understand the complete ML pipeline from raw data to trained model</p> <p>Visual style: Flowchart with processing stages</p> <p>Steps: 1. Start: \"Raw Data\"    Hover text: \"Original features, possibly different scales and units\"</p> <ol> <li>Process: \"Standardization\"    Hover text: \"Transform to zero mean, unit variance\"</li> <li>Input: Raw features X</li> <li> <p>Output: Standardized Z</p> </li> <li> <p>Process: \"PCA (optional)\"    Hover text: \"Reduce dimensionality while preserving variance\"</p> </li> <li>Input: Standardized data Z</li> <li>Output: Reduced data (k dimensions)</li> <li> <p>Decision: Scree plot analysis</p> </li> <li> <p>Process: \"Train/Test Split\"    Hover text: \"Hold out data for evaluation\"</p> </li> <li> <p>Process: \"Model Selection\"    Hover text: \"Choose algorithm and hyperparameters\"</p> </li> <li>Branch A: OLS (no regularization)</li> <li>Branch B: Ridge (L2)</li> <li> <p>Branch C: Lasso (L1)</p> </li> <li> <p>Process: \"Optimization\"    Hover text: \"Find optimal parameters\"</p> </li> <li>Closed-form (Ridge) or</li> <li> <p>Gradient descent (Lasso, Neural Networks)</p> </li> <li> <p>Process: \"Evaluation\"    Hover text: \"Assess on test set\"</p> </li> <li> <p>Metrics: MSE, R\u00b2, etc.</p> </li> <li> <p>End: \"Trained Model\"</p> </li> </ol> <p>Color coding: - Blue: Data processing - Green: Modeling - Orange: Optimization - Purple: Evaluation</p> <p>Interactive: - Click nodes to see code examples - Hover for detailed explanations</p> <p>Implementation: D3.js or Mermaid.js</p>"},{"location":"chapters/09-machine-learning-foundations/#summary_1","title":"Summary","text":"<p>This chapter connected linear algebra to machine learning:</p> <p>Data Representation:</p> <ul> <li>Feature vectors represent samples as \\(d\\)-dimensional vectors</li> <li>Data matrices organize \\(n\\) samples as rows, \\(d\\) features as columns</li> <li>Consistent conventions are crucial for correct matrix operations</li> </ul> <p>Statistical Foundations:</p> <ul> <li>Standardization ensures comparable scales and improves algorithm performance</li> <li>Covariance matrices capture feature relationships: \\(\\Sigma = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</li> <li>Correlation matrices are standardized covariances with values in \\([-1, 1]\\)</li> </ul> <p>Dimensionality Reduction:</p> <ul> <li>PCA finds directions of maximum variance via eigendecomposition</li> <li>Principal components are eigenvectors of the covariance matrix</li> <li>Scree plots help choose the number of components to retain</li> <li>Use SVD for numerical stability in practice</li> </ul> <p>Regression:</p> <ul> <li>Linear regression minimizes squared error: \\(J = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2\\)</li> <li>Design matrices incorporate the bias term</li> <li>Ridge (L2) shrinks weights, handles multicollinearity</li> <li>Lasso (L1) produces sparse solutions for feature selection</li> </ul> <p>Optimization:</p> <ul> <li>Gradient vectors point in the direction of steepest increase</li> <li>Gradient descent iteratively minimizes: \\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta\\nabla J\\)</li> <li>Learning rate is critical: too small = slow, too large = diverge</li> <li>Batch, mini-batch, and stochastic variants trade off noise vs. efficiency</li> </ul> Self-Check: Why does PCA use the covariance matrix of centered data rather than the original data? <p>Centering (subtracting the mean) is essential because PCA seeks directions of maximum variance from the data's center of mass. Without centering, the first principal component would largely capture the offset from the origin rather than the true variation structure. The covariance matrix of centered data measures how features vary around their means, which is exactly what PCA needs to find the principal directions of spread.</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/","title":"Quiz: Machine Learning Foundations","text":"<p>Test your understanding of linear algebra concepts in machine learning.</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#1-in-machine-learning-a-feature-vector-represents","title":"1. In machine learning, a feature vector represents:","text":"<ol> <li>The output label of a data point</li> <li>The numerical attributes of a single data sample</li> <li>The weights of a neural network</li> <li>The training algorithm</li> </ol> Show Answer <p>The correct answer is B. A feature vector contains the numerical attributes (features) that describe a single data sample. For example, an image might be represented as a vector of pixel values.</p> <p>Concept Tested: Feature Vector</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#2-the-weight-matrix-in-a-linear-model-maps","title":"2. The weight matrix in a linear model maps:","text":"<ol> <li>Labels to features</li> <li>Inputs to outputs through linear transformation</li> <li>Features to labels through division</li> <li>Errors to gradients</li> </ol> Show Answer <p>The correct answer is B. The weight matrix performs a linear transformation from input features to outputs. In \\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\), the matrix \\(W\\) determines how inputs combine to produce outputs.</p> <p>Concept Tested: Weight Matrix</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#3-the-gradient-of-a-loss-function-with-respect-to-parameters-indicates","title":"3. The gradient of a loss function with respect to parameters indicates:","text":"<ol> <li>The minimum value of the loss</li> <li>The direction of steepest increase</li> <li>The optimal parameters</li> <li>The training data size</li> </ol> Show Answer <p>The correct answer is B. The gradient points in the direction of steepest increase of the loss function. To minimize loss, gradient descent moves in the opposite direction (negative gradient).</p> <p>Concept Tested: Gradient</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#4-a-cost-function-in-machine-learning-measures","title":"4. A cost function in machine learning measures:","text":"<ol> <li>The monetary expense of training</li> <li>The discrepancy between predictions and true values</li> <li>The number of features</li> <li>The model complexity</li> </ol> Show Answer <p>The correct answer is B. The cost (or loss) function measures how far the model's predictions deviate from the true values. Training aims to minimize this function by adjusting parameters.</p> <p>Concept Tested: Cost Function</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#5-in-gradient-descent-the-update-rule-is","title":"5. In gradient descent, the update rule is:","text":"<ol> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} / \\nabla L\\)</li> </ol> Show Answer <p>The correct answer is B. Gradient descent subtracts the gradient scaled by learning rate \\(\\alpha\\): \\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla L\\). This moves parameters in the direction that decreases the loss.</p> <p>Concept Tested: Gradient Descent</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#6-the-learning-rate-alpha-controls","title":"6. The learning rate \\(\\alpha\\) controls:","text":"<ol> <li>The size of the dataset</li> <li>The step size in parameter updates</li> <li>The number of iterations</li> <li>The model architecture</li> </ol> Show Answer <p>The correct answer is B. The learning rate determines how large a step to take in the direction of the negative gradient. Too large can cause divergence; too small leads to slow convergence.</p> <p>Concept Tested: Learning Rate</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#7-linear-regression-finds-parameters-that-minimize","title":"7. Linear regression finds parameters that minimize:","text":"<ol> <li>The number of features</li> <li>The sum of squared residuals</li> <li>The number of training samples</li> <li>The weight magnitudes</li> </ol> Show Answer <p>The correct answer is B. Linear regression minimizes the sum of squared residuals: \\(\\sum_i (y_i - \\hat{y}_i)^2\\), where \\(\\hat{y}_i\\) are predictions. This has a closed-form solution via the normal equations.</p> <p>Concept Tested: Linear Regression</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#8-regularization-adds-to-the-loss-function","title":"8. Regularization adds to the loss function:","text":"<ol> <li>More training data</li> <li>A penalty term based on weight magnitudes</li> <li>Additional features</li> <li>More layers</li> </ol> Show Answer <p>The correct answer is B. Regularization adds a penalty term (like \\(\\lambda\\|\\mathbf{w}\\|^2\\) for L2) to prevent overfitting. This encourages smaller weights and simpler models.</p> <p>Concept Tested: Regularization</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#9-a-hyperplane-in-classification","title":"9. A hyperplane in classification:","text":"<ol> <li>Maximizes the loss function</li> <li>Separates data points of different classes</li> <li>Is always vertical</li> <li>Contains all training points</li> </ol> Show Answer <p>The correct answer is B. A hyperplane is a linear decision boundary that separates data points belonging to different classes. In \\(n\\) dimensions, a hyperplane has \\(n-1\\) dimensions.</p> <p>Concept Tested: Hyperplane</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#10-the-bias-term-in-a-linear-model","title":"10. The bias term in a linear model:","text":"<ol> <li>Must always be zero</li> <li>Allows the model to fit data not passing through the origin</li> <li>Increases overfitting</li> <li>Is the same as the weight</li> </ol> Show Answer <p>The correct answer is B. The bias term \\(\\mathbf{b}\\) in \\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\) allows the model to represent functions that don't pass through the origin, providing a translation or offset.</p> <p>Concept Tested: Bias Term</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/","title":"Neural Networks and Deep Learning","text":""},{"location":"chapters/10-neural-networks-and-deep-learning/#summary","title":"Summary","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning, covering neurons, activation functions, weight matrices, forward propagation, and backpropagation. You will also learn about specialized architectures including convolutional layers, batch normalization, and tensor operations.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 26 concepts from the learning graph:</p> <ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"chapters/10-neural-networks-and-deep-learning/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 13: Image Processing (for convolution concepts)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#introduction","title":"Introduction","text":"<p>Deep learning has revolutionized artificial intelligence, powering breakthroughs in image recognition, natural language processing, and game playing. Behind the impressive applications lies elegant mathematics: neural networks are compositions of linear transformations (matrix multiplications) and elementwise nonlinearities.</p> <p>Understanding neural networks through the lens of linear algebra provides crucial insights:</p> <ul> <li>Why networks need nonlinear activation functions</li> <li>How gradients flow backward through matrix operations</li> <li>Why certain architectures are more effective than others</li> <li>How to debug and optimize network training</li> </ul> <p>This chapter develops neural networks from first principles, revealing the matrix operations at every step. You'll see that deep learning is not magic\u2014it's linear algebra with a few clever twists.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-perceptron-where-it-began","title":"The Perceptron: Where It Began","text":"<p>The perceptron, invented by Frank Rosenblatt in 1958, is the simplest neural network\u2014a single artificial neuron that performs binary classification.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#perceptron-model","title":"Perceptron Model","text":"<p>Given input vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\), the perceptron computes:</p> <p>\\(y = \\text{sign}(\\mathbf{w}^T\\mathbf{x} + b)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector</li> <li>\\(b \\in \\mathbb{R}\\) is the bias</li> <li>\\(\\text{sign}(z) = +1\\) if \\(z \\geq 0\\), else \\(-1\\)</li> </ul> <p>The perceptron defines a hyperplane \\(\\mathbf{w}^T\\mathbf{x} + b = 0\\) that separates the input space into two half-spaces.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>The weight vector \\(\\mathbf{w}\\) is perpendicular to the decision boundary, and the bias \\(b\\) controls the boundary's offset from the origin. Points on one side of the hyperplane are classified as \\(+1\\), points on the other side as \\(-1\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-perceptron-decision-boundary","title":"Diagram: Perceptron Decision Boundary","text":"Perceptron Decision Boundary Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how perceptron weights and bias define a linear decision boundary</p> <p>Visual elements: - 2D input space with data points (two classes, different colors) - Decision boundary line - Weight vector shown perpendicular to boundary - Shaded regions for each class - Misclassified points highlighted</p> <p>Interactive controls: - Draggable weight vector (changes boundary orientation) - Bias slider (shifts boundary) - \"Add Point\" mode to create custom datasets - \"Run Perceptron Learning\" button - Preset datasets: linearly separable, XOR (not separable)</p> <p>Default parameters: - Linearly separable 2D dataset - Initial random weights - Canvas: responsive</p> <p>Behavior: - Real-time boundary update as weights change - Show classification accuracy - Highlight that XOR cannot be solved - Animate perceptron learning algorithm steps - Display weight update rule</p> <p>Implementation: p5.js with interactive geometry</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#limitations","title":"Limitations","text":"<p>The perceptron can only learn linearly separable functions. The famous XOR problem demonstrated that a single perceptron cannot compute XOR\u2014motivating multilayer networks.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-neuron-model","title":"The Neuron Model","text":"<p>A neuron (or unit) generalizes the perceptron with a continuous activation function:</p> <p>\\(a = \\sigma(\\mathbf{w}^T\\mathbf{x} + b) = \\sigma(z)\\)</p> <p>where:</p> <ul> <li>\\(z = \\mathbf{w}^T\\mathbf{x} + b\\) is the pre-activation (weighted sum)</li> <li>\\(\\sigma\\) is the activation function</li> <li>\\(a\\) is the activation (output)</li> </ul> <p>This two-step process\u2014linear combination followed by nonlinearity\u2014is the fundamental building block of all neural networks.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#activation-functions","title":"Activation Functions","text":"<p>Activation functions introduce nonlinearity, enabling neural networks to learn complex patterns. Without them, any depth of linear layers would collapse to a single linear transformation.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<p>The ReLU activation is the most widely used in modern deep learning:</p> <p>\\(\\text{ReLU}(z) = \\max(0, z) = \\begin{cases} z &amp; \\text{if } z &gt; 0 \\\\ 0 &amp; \\text{if } z \\leq 0 \\end{cases}\\)</p> <p>Properties:</p> <ul> <li>Computationally efficient (just a threshold)</li> <li>Sparse activations (many zeros)</li> <li>Avoids vanishing gradients for positive inputs</li> <li>\"Dead ReLU\" problem: neurons can get stuck at zero</li> </ul> <p>Derivative: \\(\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases} 1 &amp; \\text{if } z &gt; 0 \\\\ 0 &amp; \\text{if } z &lt; 0 \\end{cases}\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid function squashes inputs to the range \\((0, 1)\\):</p> <p>\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</p> <p>Properties:</p> <ul> <li>Output interpretable as probability</li> <li>Smooth and differentiable everywhere</li> <li>Saturates for large \\(|z|\\) (vanishing gradients)</li> <li>Outputs not zero-centered</li> </ul> <p>Derivative: \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tanh","title":"Tanh","text":"<p>The tanh function maps inputs to \\((-1, 1)\\):</p> <p>\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1\\)</p> <p>Properties:</p> <ul> <li>Zero-centered outputs (often better than sigmoid)</li> <li>Still suffers from vanishing gradients at extremes</li> <li>Commonly used in RNNs and LSTMs</li> </ul> <p>Derivative: \\(\\tanh'(z) = 1 - \\tanh^2(z)\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#softmax","title":"Softmax","text":"<p>The softmax function converts a vector of scores to a probability distribution:</p> <p>\\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\)</p> <p>where:</p> <ul> <li>Input: \\(\\mathbf{z} \\in \\mathbb{R}^K\\) (logits for \\(K\\) classes)</li> <li>Output: probability vector with \\(\\sum_i \\text{softmax}(\\mathbf{z})_i = 1\\)</li> </ul> <p>Properties:</p> <ul> <li>Used for multi-class classification output layer</li> <li>Exponential amplifies differences between scores</li> <li>Numerically stabilized by subtracting \\(\\max(\\mathbf{z})\\) before exponentiating</li> </ul> Activation Range Use Case Gradient Issue ReLU \\([0, \\infty)\\) Hidden layers Dead neurons Sigmoid \\((0, 1)\\) Binary output Vanishing Tanh \\((-1, 1)\\) Hidden layers, RNNs Vanishing Softmax \\((0, 1)^K\\), sums to 1 Multi-class output \u2014"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-activation-function-comparison","title":"Diagram: Activation Function Comparison","text":"Activation Functions Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare activation functions by their shape, range, and gradient behavior</p> <p>Visual elements: - Main panel: Graph showing selected activation function - Derivative overlay (toggleable) - Gradient magnitude heatmap for different input values - Side panel: Function properties summary</p> <p>Interactive controls: - Activation selector: ReLU, Sigmoid, Tanh, Leaky ReLU, Softplus - Input range slider - \"Show Derivative\" toggle - \"Compare All\" mode showing functions overlaid - Input value slider to trace along curve</p> <p>Default parameters: - ReLU selected - Input range: [-5, 5] - Canvas: responsive</p> <p>Behavior: - Real-time function and derivative plotting - Highlight saturation regions (near-zero gradient) - Show numerical values at traced point - For softmax: show 3-class probability bar chart - Display gradient flow implications</p> <p>Implementation: p5.js with function plotting</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#network-architecture-layers-and-matrices","title":"Network Architecture: Layers and Matrices","text":"<p>A neural network organizes neurons into layers, with each layer performing a matrix operation.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#weight-matrix-and-bias-vector","title":"Weight Matrix and Bias Vector","text":"<p>For a layer with \\(n_{in}\\) inputs and \\(n_{out}\\) outputs:</p> <ul> <li>Weight matrix \\(W \\in \\mathbb{R}^{n_{out} \\times n_{in}}\\) contains connection strengths</li> <li>Bias vector \\(\\mathbf{b} \\in \\mathbb{R}^{n_{out}}\\) provides learnable offsets</li> </ul> <p>The layer computes:</p> <p>\\(\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x} \\in \\mathbb{R}^{n_{in}}\\) is the input</li> <li>\\(\\mathbf{z} \\in \\mathbb{R}^{n_{out}}\\) is the pre-activation</li> </ul> <p>Each row of \\(W\\) contains the weights for one output neuron.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#neural-network-layer","title":"Neural Network Layer","text":"<p>A complete neural network layer combines the linear transformation with an activation:</p> <p>\\(\\mathbf{a} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\)</p> <p>where \\(\\sigma\\) is applied elementwise.</p> <p>For a batch of \\(m\\) inputs (stored as columns of \\(X \\in \\mathbb{R}^{n_{in} \\times m}\\)):</p> <p>\\(A = \\sigma(WX + \\mathbf{b}\\mathbf{1}^T)\\)</p> <p>where \\(\\mathbf{1}^T = [1, 1, \\ldots, 1]\\) broadcasts the bias to all samples.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#hidden-layers-and-deep-networks","title":"Hidden Layers and Deep Networks","text":"<ul> <li>Hidden layers are layers between input and output\u2014their activations are not directly observed</li> <li>A deep network has multiple hidden layers, enabling hierarchical feature learning</li> </ul> <p>A network with \\(L\\) layers computes:</p> <p>\\(\\mathbf{a}^{[0]} = \\mathbf{x}\\) (input)</p> <p>\\(\\mathbf{z}^{[l]} = W^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\\) for \\(l = 1, \\ldots, L\\)</p> <p>\\(\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})\\) for \\(l = 1, \\ldots, L\\)</p> <p>The output is \\(\\mathbf{a}^{[L]}\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-network-architecture-visualizer","title":"Diagram: Network Architecture Visualizer","text":"Neural Network Architecture Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize neural network architecture and understand the dimensions of weight matrices at each layer</p> <p>Visual elements: - Node-and-edge diagram of network - Layer labels with dimensions - Weight matrix dimensions displayed on connections - Activation function icons at each layer - Parameter count summary</p> <p>Interactive controls: - Layer count slider (1-5 hidden layers) - Neurons per layer sliders - Activation function selector per layer - \"Show Dimensions\" toggle - \"Show Weight Matrices\" toggle (expands to show matrix shapes) - Input/output dimension selectors</p> <p>Default parameters: - 2 hidden layers - Architecture: 4 \u2192 8 \u2192 8 \u2192 2 - ReLU hidden, Softmax output - Canvas: responsive</p> <p>Behavior: - Real-time architecture update - Calculate and display total parameters - Show data flow animation - Highlight one layer at a time with matrix equation - Demonstrate dimension matching requirements</p> <p>Implementation: p5.js with network diagram rendering</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#forward-propagation","title":"Forward Propagation","text":"<p>Forward propagation computes the network output by passing input through all layers sequentially.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#algorithm","title":"Algorithm","text":"<pre><code>Input: x, weights {W[l], b[l]} for l = 1,...,L\n\na[0] = x\nFor l = 1 to L:\n    z[l] = W[l] @ a[l-1] + b[l]\n    a[l] = activation[l](z[l])\n\nOutput: a[L]\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#matrix-form-for-batches","title":"Matrix Form for Batches","text":"<p>For a batch of \\(m\\) samples (columns of \\(X\\)):</p> <p>\\(A^{[0]} = X\\)</p> <p>\\(Z^{[l]} = W^{[l]}A^{[l-1]} + \\mathbf{b}^{[l]}\\mathbf{1}^T\\)</p> <p>\\(A^{[l]} = \\sigma^{[l]}(Z^{[l]})\\)</p> <p>The output \\(A^{[L]}\\) has shape \\(n_L \\times m\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#why-nonlinearity-is-essential","title":"Why Nonlinearity Is Essential","text":"<p>Consider a 2-layer network without activations:</p> <p>\\(\\mathbf{y} = W^{[2]}(W^{[1]}\\mathbf{x}) = (W^{[2]}W^{[1]})\\mathbf{x} = W'\\mathbf{x}\\)</p> <p>The composition of linear functions is linear! Without nonlinear activations, deep networks would have no more expressive power than a single layer.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-forward-propagation-visualizer","title":"Diagram: Forward Propagation Visualizer","text":"Forward Propagation Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Trace data flow through a neural network and see the matrix operations at each layer</p> <p>Visual elements: - Network diagram with numerical values - Current layer highlighted - Matrix multiplication visualization - Activation function application shown - Values flowing through connections</p> <p>Interactive controls: - \"Next Step\" button - \"Auto Run\" with speed slider - Input vector editor - Weight matrix display (expandable) - \"Show Matrix Multiplication\" detail toggle</p> <p>Default parameters: - 3-layer network: 2 \u2192 3 \u2192 2 - Random initialized weights - Input: [1.0, 0.5] - Canvas: responsive</p> <p>Behavior: - Step through z = Wa + b computation - Show activation function application - Display intermediate values at each neuron - Animate data flow - Verify dimensions at each step</p> <p>Implementation: p5.js with matrix computation display</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#loss-functions","title":"Loss Functions","text":"<p>Loss functions measure how well the network's predictions match the targets, providing the signal for learning.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#common-loss-functions","title":"Common Loss Functions","text":"<p>Mean Squared Error (MSE) for regression:</p> <p>\\(\\mathcal{L}_{MSE} = \\frac{1}{m}\\sum_{i=1}^m \\|\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\|^2\\)</p> <p>Cross-Entropy Loss for classification:</p> <p>For binary classification with sigmoid output \\(\\hat{y} \\in (0, 1)\\):</p> <p>\\(\\mathcal{L}_{BCE} = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\\)</p> <p>For multi-class with softmax output \\(\\hat{\\mathbf{y}} \\in \\mathbb{R}^K\\):</p> <p>\\(\\mathcal{L}_{CE} = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K y_{ik} \\log(\\hat{y}_{ik})\\)</p> <p>where \\(y_{ik}\\) is the one-hot encoded true label.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#why-cross-entropy","title":"Why Cross-Entropy?","text":"<p>Cross-entropy loss has favorable gradient properties:</p> <ul> <li>Combined with softmax, the gradient simplifies to \\(\\hat{\\mathbf{y}} - \\mathbf{y}\\)</li> <li>Penalizes confident wrong predictions heavily</li> <li>Derived from maximum likelihood estimation</li> </ul> Loss Function Output Layer Use Case MSE Linear Regression Binary Cross-Entropy Sigmoid Binary classification Categorical Cross-Entropy Softmax Multi-class classification"},{"location":"chapters/10-neural-networks-and-deep-learning/#backpropagation","title":"Backpropagation","text":"<p>Backpropagation efficiently computes gradients of the loss with respect to all parameters using the chain rule.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-chain-rule-for-matrices","title":"The Chain Rule for Matrices","text":"<p>For composed functions \\(\\mathcal{L} = f(g(h(\\theta)))\\), the chain rule gives:</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{L}}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial \\theta}\\)</p> <p>In neural networks, this chain extends through all layers.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#chain-rule-matrices-jacobians","title":"Chain Rule Matrices (Jacobians)","text":"<p>For vector-valued functions, derivatives become Jacobian matrices. If \\(\\mathbf{z} = f(\\mathbf{a})\\) where \\(\\mathbf{z} \\in \\mathbb{R}^n\\) and \\(\\mathbf{a} \\in \\mathbb{R}^m\\):</p> <p>\\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{a}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial a_1} &amp; \\cdots &amp; \\frac{\\partial z_1}{\\partial a_m} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial z_n}{\\partial a_1} &amp; \\cdots &amp; \\frac{\\partial z_n}{\\partial a_m} \\end{bmatrix}\\)</p> <p>For scalar loss \\(\\mathcal{L}\\), we work with gradient vectors and propagate them backward.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#backpropagation-algorithm","title":"Backpropagation Algorithm","text":"<p>Starting from the output layer and moving backward:</p> <p>Output layer gradient:</p> <p>\\(\\delta^{[L]} = \\nabla_{\\mathbf{a}^{[L]}} \\mathcal{L} \\odot \\sigma'^{[L]}(\\mathbf{z}^{[L]})\\)</p> <p>Propagate backward: For \\(l = L-1, \\ldots, 1\\):</p> <p>\\(\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})\\)</p> <p>Parameter gradients:</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\delta^{[l]} (\\mathbf{a}^{[l-1]})^T\\)</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[l]}} = \\delta^{[l]}\\)</p> <p>where:</p> <ul> <li>\\(\\delta^{[l]}\\) is the error signal at layer \\(l\\)</li> <li>\\(\\odot\\) is elementwise multiplication</li> <li>\\(\\sigma'\\) is the derivative of the activation function</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-backpropagation-visualizer","title":"Diagram: Backpropagation Visualizer","text":"Backpropagation Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how gradients flow backward through a network via the chain rule</p> <p>Visual elements: - Network diagram with forward values - Backward arrows showing gradient flow - Current layer highlighted - Gradient values displayed at each node - Matrix transpose visualization for weight gradients</p> <p>Interactive controls: - \"Forward Pass\" button (must run first) - \"Backward Step\" button - \"Auto Backprop\" with speed slider - Target value input - Loss function selector (MSE, Cross-Entropy) - \"Show Chain Rule\" detail toggle</p> <p>Default parameters: - 3-layer network: 2 \u2192 3 \u2192 1 - MSE loss - Single training example - Canvas: responsive</p> <p>Behavior: - Compute and display output error - Show gradient flowing backward through each layer - Display \u03b4 values at each neuron - Show weight gradient computation - Verify gradient dimensions match weight dimensions</p> <p>Implementation: p5.js with gradient computation display</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#gradient-flow-and-vanishingexploding-gradients","title":"Gradient Flow and Vanishing/Exploding Gradients","text":"<p>Gradients are multiplied by weight matrices at each layer during backpropagation. If weights are:</p> <ul> <li>Too small: gradients shrink exponentially \u2192 vanishing gradients</li> <li>Too large: gradients grow exponentially \u2192 exploding gradients</li> </ul> <p>This motivates careful weight initialization and architectural choices like residual connections.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>Convolutional layers exploit spatial structure in images and sequences through weight sharing.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolution-kernel","title":"Convolution Kernel","text":"<p>A convolution kernel (or filter) is a small matrix of learnable weights that slides across the input:</p> <p>\\((\\mathbf{I} * \\mathbf{K})_{i,j} = \\sum_{m}\\sum_{n} I_{i+m, j+n} \\cdot K_{m,n}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{I}\\) is the input (e.g., image)</li> <li>\\(\\mathbf{K}\\) is the kernel (e.g., \\(3 \\times 3\\))</li> <li>The sum is over all kernel positions</li> </ul> <p>A \\(3 \\times 3\\) kernel has only 9 parameters but processes arbitrarily large inputs\u2014this is weight sharing.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolutional-layer","title":"Convolutional Layer","text":"<p>A convolutional layer applies multiple kernels to produce multiple output channels:</p> <p>\\(\\text{Output}_{c_{out}, i, j} = \\sum_{c_{in}} (\\mathbf{I}_{c_{in}} * \\mathbf{K}_{c_{out}, c_{in}})_{i,j} + b_{c_{out}}\\)</p> <p>where:</p> <ul> <li>\\(c_{in}\\) input channels, \\(c_{out}\\) output channels</li> <li>Each output channel has its own set of kernels (one per input channel)</li> <li>Total parameters: \\(c_{out} \\times c_{in} \\times k_h \\times k_w + c_{out}\\)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#stride-and-padding","title":"Stride and Padding","text":"<p>Stride controls how many pixels the kernel moves between applications:</p> <ul> <li>Stride 1: kernel moves one pixel at a time (full resolution)</li> <li>Stride 2: kernel moves two pixels (halves spatial dimensions)</li> </ul> <p>Padding adds zeros around the input border:</p> <ul> <li>\"Valid\" padding: no padding, output shrinks</li> <li>\"Same\" padding: pad to keep output size equal to input</li> </ul> <p>Output dimension formula:</p> <p>\\(\\text{out\\_size} = \\frac{\\text{in\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#pooling-layer","title":"Pooling Layer","text":"<p>Pooling layers downsample spatial dimensions:</p> <ul> <li>Max pooling: takes maximum value in each window</li> <li>Average pooling: takes average value in each window</li> </ul> <p>Common configuration: \\(2 \\times 2\\) pool with stride 2 halves each spatial dimension.</p> Operation Effect Parameters Convolution Feature extraction Kernel weights Stride &gt; 1 Downsampling None (hyperparameter) Padding Preserve dimensions None (hyperparameter) Pooling Downsample, invariance None"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-convolution-visualizer","title":"Diagram: Convolution Visualizer","text":"Convolution Operation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how convolution kernels slide across images and the effect of stride and padding</p> <p>Visual elements: - Input image (grayscale, small e.g., 7\u00d77) - Kernel overlay showing current position - Output feature map being built - Kernel weights displayed - Highlighted multiplication-addition operation</p> <p>Interactive controls: - Step through kernel positions manually - Kernel size selector (3\u00d73, 5\u00d75) - Stride selector (1, 2) - Padding selector (valid, same) - \"Animate Convolution\" button - Preset kernels: edge detection, blur, sharpen</p> <p>Default parameters: - 7\u00d77 grayscale input - 3\u00d73 kernel - Stride 1, valid padding - Edge detection kernel</p> <p>Behavior: - Show kernel sliding across input - Display element-wise multiplication - Show sum being placed in output - Output dimensions update with settings - Visualize different kernel effects on sample image</p> <p>Implementation: p5.js with image processing</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#normalization-techniques","title":"Normalization Techniques","text":"<p>Normalization stabilizes training by controlling activation distributions.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#batch-normalization","title":"Batch Normalization","text":"<p>Batch normalization normalizes activations across the batch dimension:</p> <p>For a mini-batch \\(\\{x_i\\}_{i=1}^m\\):</p> <p>\\(\\mu_B = \\frac{1}{m}\\sum_{i=1}^m x_i\\)</p> <p>\\(\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_B)^2\\)</p> <p>\\(\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\)</p> <p>\\(y_i = \\gamma \\hat{x}_i + \\beta\\)</p> <p>where:</p> <ul> <li>\\(\\gamma\\) and \\(\\beta\\) are learnable scale and shift parameters</li> <li>\\(\\epsilon\\) is a small constant for numerical stability</li> <li>During inference, running averages of \\(\\mu\\) and \\(\\sigma\\) are used</li> </ul> <p>Benefits:</p> <ul> <li>Enables higher learning rates</li> <li>Reduces sensitivity to initialization</li> <li>Acts as regularization (due to batch statistics noise)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#layer-normalization","title":"Layer Normalization","text":"<p>Layer normalization normalizes across the feature dimension instead of batch:</p> <p>\\(\\mu_l = \\frac{1}{d}\\sum_{j=1}^d x_j\\)</p> <p>\\(\\sigma_l^2 = \\frac{1}{d}\\sum_{j=1}^d (x_j - \\mu_l)^2\\)</p> <p>Benefits:</p> <ul> <li>Works with batch size 1</li> <li>Preferred in transformers and RNNs</li> <li>No difference between training and inference</li> </ul> Normalization Normalize Over Best For Batch Norm Batch dimension CNNs, large batches Layer Norm Feature dimension Transformers, RNNs Instance Norm Spatial dimensions Style transfer Group Norm Channel groups Small batch CNNs"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-normalization-comparison","title":"Diagram: Normalization Comparison","text":"Batch vs Layer Normalization <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand the difference between normalization techniques by visualizing which dimensions they operate over</p> <p>Layout: Side-by-side comparison with 3D tensor diagrams</p> <p>Visual elements: - 3D tensor representation (batch \u00d7 channels \u00d7 spatial) - Highlighted region showing normalization scope - Before/after distribution plots - Learnable parameter display</p> <p>Sections: 1. Batch Normalization    - Normalize across batch (vertical slice)    - Show statistics computed per channel    - Training vs inference difference noted</p> <ol> <li>Layer Normalization</li> <li>Normalize across features (horizontal slice)</li> <li>Show statistics computed per sample</li> <li> <p>Same computation in training and inference</p> </li> <li> <p>Comparison table</p> </li> <li>Batch dependency</li> <li>Use cases</li> <li>Computational considerations</li> </ol> <p>Interactive elements: - Toggle to highlight normalization region - Slider to show effect on activation distribution - Animation of statistics computation</p> <p>Implementation: HTML/CSS/JavaScript with 3D tensor SVG</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensors-and-tensor-operations","title":"Tensors and Tensor Operations","text":"<p>Modern deep learning operates on tensors\u2014multi-dimensional arrays that generalize vectors and matrices.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensor-definition","title":"Tensor Definition","text":"<p>A tensor is a multi-dimensional array:</p> <ul> <li>0D tensor: scalar (e.g., \\(3.14\\))</li> <li>1D tensor: vector (e.g., \\(\\mathbf{x} \\in \\mathbb{R}^n\\))</li> <li>2D tensor: matrix (e.g., \\(A \\in \\mathbb{R}^{m \\times n}\\))</li> <li>3D tensor: e.g., RGB image \\(\\in \\mathbb{R}^{H \\times W \\times 3}\\)</li> <li>4D tensor: batch of images \\(\\in \\mathbb{R}^{B \\times H \\times W \\times C}\\)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#common-tensor-operations","title":"Common Tensor Operations","text":"Operation Description Example Reshape Change shape, preserve elements \\((6,) \\to (2, 3)\\) Transpose Permute axes \\((B, H, W, C) \\to (B, C, H, W)\\) Broadcasting Expand dimensions for elementwise ops \\((3,) + (2, 3) \\to (2, 3)\\) Concatenate Join along axis Two \\((B, 10) \\to (B, 20)\\) Stack Create new axis Two \\((H, W) \\to (2, H, W)\\) Squeeze/Unsqueeze Remove/add size-1 dimensions \\((1, 3, 1) \\to (3,)\\)"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensor-operations-in-neural-networks","title":"Tensor Operations in Neural Networks","text":"<p>Batched matrix multiplication:</p> <p>For batched inputs \\(A \\in \\mathbb{R}^{B \\times M \\times K}\\) and \\(B \\in \\mathbb{R}^{B \\times K \\times N}\\):</p> <p>\\((A @ B)_{b,i,j} = \\sum_k A_{b,i,k} \\cdot B_{b,k,j}\\)</p> <p>Result has shape \\(B \\times M \\times N\\).</p> <p>Einsum notation provides a powerful way to express tensor operations:</p> <pre><code># Batched matrix multiply\ntorch.einsum('bik,bkj-&gt;bij', A, B)\n\n# Attention: Q @ K^T\ntorch.einsum('bqd,bkd-&gt;bqk', Q, K)\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-tensor-shape-visualizer","title":"Diagram: Tensor Shape Visualizer","text":"Tensor Operations Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand tensor shapes and how common operations transform them</p> <p>Visual elements: - 3D/4D tensor visualization as nested boxes - Shape annotation on each dimension - Operation selector showing input \u2192 output shapes - Animated transformation between shapes</p> <p>Interactive controls: - Input tensor shape editor - Operation selector: reshape, transpose, squeeze, concatenate, broadcast - Target shape input (for reshape) - Axis selector (for operations that need it) - \"Apply Operation\" button</p> <p>Default parameters: - Input tensor shape: (2, 3, 4) - Operation: reshape to (6, 4) - Canvas: responsive</p> <p>Behavior: - Visualize tensor as nested rectangles - Show valid reshape targets - Animate element rearrangement - Error message for invalid operations - Display resulting shape</p> <p>Implementation: p5.js with 3D tensor rendering</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete neural network implementation using these concepts:</p> <pre><code>import numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, layer_sizes):\n        \"\"\"Initialize network with given layer sizes.\"\"\"\n        self.L = len(layer_sizes) - 1  # number of layers\n        self.weights = []\n        self.biases = []\n\n        # Xavier initialization\n        for i in range(self.L):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n            b = np.zeros((n_out, 1))\n            self.weights.append(W)\n            self.biases.append(b)\n\n    def relu(self, z):\n        return np.maximum(0, z)\n\n    def relu_derivative(self, z):\n        return (z &gt; 0).astype(float)\n\n    def softmax(self, z):\n        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\n    def forward(self, X):\n        \"\"\"Forward propagation.\"\"\"\n        self.activations = [X]\n        self.pre_activations = []\n\n        A = X\n        for l in range(self.L):\n            Z = self.weights[l] @ A + self.biases[l]\n            self.pre_activations.append(Z)\n\n            if l == self.L - 1:  # output layer\n                A = self.softmax(Z)\n            else:  # hidden layers\n                A = self.relu(Z)\n            self.activations.append(A)\n\n        return A\n\n    def backward(self, Y):\n        \"\"\"Backpropagation with cross-entropy loss.\"\"\"\n        m = Y.shape[1]\n        self.weight_grads = []\n        self.bias_grads = []\n\n        # Output layer gradient (softmax + cross-entropy)\n        dA = self.activations[-1] - Y  # simplified gradient\n\n        for l in range(self.L - 1, -1, -1):\n            if l == self.L - 1:\n                dZ = dA  # softmax-CE gradient\n            else:\n                dZ = dA * self.relu_derivative(self.pre_activations[l])\n\n            dW = (1/m) * dZ @ self.activations[l].T\n            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n            self.weight_grads.insert(0, dW)\n            self.bias_grads.insert(0, db)\n\n            if l &gt; 0:\n                dA = self.weights[l].T @ dZ\n\n    def update(self, learning_rate):\n        \"\"\"Gradient descent update.\"\"\"\n        for l in range(self.L):\n            self.weights[l] -= learning_rate * self.weight_grads[l]\n            self.biases[l] -= learning_rate * self.bias_grads[l]\n\n    def train(self, X, Y, epochs, learning_rate):\n        \"\"\"Training loop.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            predictions = self.forward(X)\n\n            # Compute loss\n            loss = -np.mean(Y * np.log(predictions + 1e-8))\n\n            # Backward pass\n            self.backward(Y)\n\n            # Update weights\n            self.update(learning_rate)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Example usage\nnet = NeuralNetwork([784, 128, 64, 10])  # MNIST-like\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#summary_1","title":"Summary","text":"<p>This chapter revealed the linear algebra powering neural networks:</p> <p>Foundations:</p> <ul> <li>Perceptrons compute linear decision boundaries: \\(y = \\text{sign}(\\mathbf{w}^T\\mathbf{x} + b)\\)</li> <li>Neurons add nonlinear activations: \\(a = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)\\)</li> </ul> <p>Activation Functions:</p> <ul> <li>ReLU: \\(\\max(0, z)\\) \u2014 efficient, sparse, avoids vanishing gradients</li> <li>Sigmoid: \\((0, 1)\\) output for probabilities</li> <li>Softmax: probability distribution over \\(K\\) classes</li> </ul> <p>Network Architecture:</p> <ul> <li>Weight matrices \\(W^{[l]}\\) transform between layers</li> <li>Hidden layers enable hierarchical feature learning</li> <li>Deep networks compose multiple transformations</li> </ul> <p>Training:</p> <ul> <li>Forward propagation: sequential matrix operations through layers</li> <li>Loss functions: MSE for regression, cross-entropy for classification</li> <li>Backpropagation: chain rule computes gradients via matrix transposes</li> </ul> <p>Convolutional Networks:</p> <ul> <li>Kernels slide across spatial dimensions with weight sharing</li> <li>Stride and padding control output dimensions</li> <li>Pooling provides downsampling and translation invariance</li> </ul> <p>Normalization and Tensors:</p> <ul> <li>Batch normalization: normalize across batch dimension</li> <li>Layer normalization: normalize across feature dimension</li> <li>Tensors: multi-dimensional arrays with reshape, transpose, broadcast operations</li> </ul> Self-Check: Why does backpropagation use the transpose of the weight matrix when propagating gradients? <p>When propagating gradients backward from layer \\(l+1\\) to layer \\(l\\), we need to compute how changes in \\(\\mathbf{a}^{[l]}\\) affect the loss. During forward propagation, we computed \\(\\mathbf{z}^{[l+1]} = W^{[l+1]}\\mathbf{a}^{[l]}\\). The Jacobian \\(\\frac{\\partial \\mathbf{z}^{[l+1]}}{\\partial \\mathbf{a}^{[l]}} = W^{[l+1]}\\). By the chain rule, the gradient with respect to \\(\\mathbf{a}^{[l]}\\) is \\((W^{[l+1]})^T \\delta^{[l+1]}\\), where the transpose distributes the error from each output neuron back to the input neurons that contributed to it.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/","title":"Quiz: Neural Networks and Deep Learning","text":"<p>Test your understanding of neural network architecture and deep learning concepts.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#1-a-fully-connected-dense-layer-computes","title":"1. A fully connected (dense) layer computes:","text":"<ol> <li>Element-wise multiplication only</li> <li>\\(\\mathbf{y} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\) with linear transform and activation</li> <li>Convolution of input with a kernel</li> <li>Max pooling of input values</li> </ol> Show Answer <p>The correct answer is B. A dense layer performs a linear transformation followed by a nonlinear activation: \\(\\mathbf{y} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\), where \\(W\\) is the weight matrix, \\(\\mathbf{b}\\) is the bias, and \\(\\sigma\\) is the activation function.</p> <p>Concept Tested: Dense Layer</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#2-the-purpose-of-an-activation-function-is-to","title":"2. The purpose of an activation function is to:","text":"<ol> <li>Initialize weights randomly</li> <li>Introduce nonlinearity into the network</li> <li>Reduce the number of parameters</li> <li>Normalize input data</li> </ol> Show Answer <p>The correct answer is B. Activation functions introduce nonlinearity, enabling neural networks to learn complex patterns. Without them, stacking layers would just produce another linear transformation.</p> <p>Concept Tested: Activation Function</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#3-the-relu-activation-function-is-defined-as","title":"3. The ReLU activation function is defined as:","text":"<ol> <li>\\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\)</li> <li>\\(\\sigma(x) = \\max(0, x)\\)</li> <li>\\(\\sigma(x) = \\tanh(x)\\)</li> <li>\\(\\sigma(x) = x^2\\)</li> </ol> Show Answer <p>The correct answer is B. ReLU (Rectified Linear Unit) is defined as \\(\\max(0, x)\\)\u2014it outputs \\(x\\) for positive inputs and 0 for negative inputs. It's computationally efficient and helps mitigate vanishing gradients.</p> <p>Concept Tested: ReLU Activation</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#4-backpropagation-computes","title":"4. Backpropagation computes:","text":"<ol> <li>The forward pass predictions</li> <li>Gradients of the loss with respect to all parameters</li> <li>The optimal learning rate</li> <li>The activation function values</li> </ol> Show Answer <p>The correct answer is B. Backpropagation efficiently computes gradients of the loss with respect to all network parameters using the chain rule, enabling gradient-based optimization.</p> <p>Concept Tested: Backpropagation</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#5-the-softmax-function","title":"5. The softmax function:","text":"<ol> <li>Sets all outputs to zero</li> <li>Converts scores to probabilities that sum to 1</li> <li>Applies ReLU to each element</li> <li>Computes the maximum value</li> </ol> Show Answer <p>The correct answer is B. Softmax converts a vector of real-valued scores to a probability distribution: \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\). All outputs are positive and sum to 1.</p> <p>Concept Tested: Softmax Function</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#6-a-tensor-in-deep-learning-is","title":"6. A tensor in deep learning is:","text":"<ol> <li>Always a 2D matrix</li> <li>A multidimensional array of numbers</li> <li>A type of activation function</li> <li>A loss function</li> </ol> Show Answer <p>The correct answer is B. A tensor is a multidimensional array\u2014a generalization of scalars (0D), vectors (1D), and matrices (2D) to higher dimensions. Tensors represent data and parameters in neural networks.</p> <p>Concept Tested: Tensor</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#7-cross-entropy-loss-is-commonly-used-for","title":"7. Cross-entropy loss is commonly used for:","text":"<ol> <li>Regression problems</li> <li>Classification problems with probability outputs</li> <li>Dimensionality reduction</li> <li>Feature extraction</li> </ol> Show Answer <p>The correct answer is B. Cross-entropy loss measures the difference between predicted probability distributions and true labels. It's the standard loss for classification, especially with softmax outputs.</p> <p>Concept Tested: Cross-Entropy Loss</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#8-batch-normalization","title":"8. Batch normalization:","text":"<ol> <li>Increases batch size automatically</li> <li>Normalizes layer inputs to have zero mean and unit variance</li> <li>Removes all biases from the network</li> <li>Converts images to grayscale</li> </ol> Show Answer <p>The correct answer is B. Batch normalization normalizes activations across a mini-batch to zero mean and unit variance, then applies learned scale and shift. This stabilizes training and can act as regularization.</p> <p>Concept Tested: Batch Normalization</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#9-the-chain-rule-in-backpropagation-enables","title":"9. The chain rule in backpropagation enables:","text":"<ol> <li>Computing gradients through composed functions</li> <li>Increasing network depth automatically</li> <li>Selecting the best activation function</li> <li>Determining batch size</li> </ol> Show Answer <p>The correct answer is A. The chain rule allows computing gradients through composed functions: \\(\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\\). This is essential for propagating gradients through all network layers.</p> <p>Concept Tested: Chain Rule</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#10-dropout-during-training","title":"10. Dropout during training:","text":"<ol> <li>Removes entire layers permanently</li> <li>Randomly sets some neuron outputs to zero</li> <li>Doubles the learning rate</li> <li>Freezes all weights</li> </ol> Show Answer <p>The correct answer is B. Dropout randomly sets a fraction of neuron outputs to zero during training, preventing co-adaptation of neurons. This serves as regularization and improves generalization.</p> <p>Concept Tested: Dropout</p>"},{"location":"chapters/11-generative-ai-and-llms/","title":"Generative AI and Large Language Models","text":""},{"location":"chapters/11-generative-ai-and-llms/#summary","title":"Summary","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of transformers and large language models, including embedding spaces, attention mechanisms, query-key-value matrices, and multi-head attention. You will also learn about LoRA for efficient fine-tuning and latent space interpolation in generative models.</p>"},{"location":"chapters/11-generative-ai-and-llms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 7: Matrix Decompositions (for low-rank approximation)</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#introduction","title":"Introduction","text":"<p>Large language models like GPT and Claude have transformed artificial intelligence, demonstrating remarkable capabilities in understanding and generating human language. At their core, these models are sophisticated linear algebra engines\u2014they represent words as vectors, compute relationships through matrix operations, and generate output by manipulating high-dimensional spaces.</p> <p>This chapter reveals the mathematical machinery behind modern generative AI:</p> <ul> <li>Embeddings map discrete tokens to continuous vector spaces</li> <li>Attention mechanisms compute dynamic, context-dependent relationships</li> <li>Transformers stack these operations to build powerful models</li> <li>Low-rank adaptation enables efficient fine-tuning</li> </ul> <p>Understanding these foundations demystifies AI systems and enables you to reason about their capabilities and limitations.</p>"},{"location":"chapters/11-generative-ai-and-llms/#embeddings-from-symbols-to-vectors","title":"Embeddings: From Symbols to Vectors","text":"<p>Natural language consists of discrete symbols\u2014words, characters, or subword tokens. Neural networks require continuous numerical input. Embeddings bridge this gap.</p>"},{"location":"chapters/11-generative-ai-and-llms/#what-is-an-embedding","title":"What Is an Embedding?","text":"<p>An embedding is a learned mapping from discrete items to continuous vectors:</p> <p>\\(e: \\{1, 2, \\ldots, V\\} \\to \\mathbb{R}^d\\)</p> <p>where:</p> <ul> <li>\\(V\\) is the vocabulary size (e.g., 50,000 tokens)</li> <li>\\(d\\) is the embedding dimension (e.g., 768, 1024, or 4096)</li> </ul> <p>In practice, embeddings are stored as an embedding matrix \\(E \\in \\mathbb{R}^{V \\times d}\\):</p> <p>\\(\\mathbf{e}_i = E[i, :] \\quad \\text{(row } i \\text{ of } E \\text{)}\\)</p>"},{"location":"chapters/11-generative-ai-and-llms/#embedding-space","title":"Embedding Space","text":"<p>The embedding space is the \\(d\\)-dimensional vector space where embeddings live. This space has remarkable properties:</p> <ul> <li>Similar items are mapped to nearby points</li> <li>Relationships are encoded as directions</li> <li>Arithmetic operations have semantic meaning</li> </ul> <p>The famous Word2Vec example: \\(\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\\)</p> <p>This works because the embedding space encodes the \"royalty\" and \"gender\" concepts as roughly orthogonal directions.</p>"},{"location":"chapters/11-generative-ai-and-llms/#word-embeddings","title":"Word Embeddings","text":"<p>Word embeddings specifically represent words (or subword tokens) as vectors. They are learned from large text corpora by predicting context:</p> <ul> <li>Word2Vec: Predict surrounding words from center word (or vice versa)</li> <li>GloVe: Factorize word co-occurrence matrix</li> <li>Transformer embeddings: Learned end-to-end with the model</li> </ul> Method Approach Context Word2Vec (Skip-gram) Predict context from word Fixed window Word2Vec (CBOW) Predict word from context Fixed window GloVe Matrix factorization Global co-occurrence BERT/GPT End-to-end transformer Full sequence"},{"location":"chapters/11-generative-ai-and-llms/#diagram-embedding-space-visualizer","title":"Diagram: Embedding Space Visualizer","text":"Embedding Space Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how words are positioned in embedding space and explore semantic relationships</p> <p>Visual elements: - 2D projection (t-SNE or PCA) of word embeddings - Clusters of related words (colors, animals, countries) - Vector arithmetic visualization - Distance/similarity measurements</p> <p>Interactive controls: - Word search to highlight specific embeddings - \"Find Similar\" to show nearest neighbors - Vector arithmetic input: word1 - word2 + word3 = ? - Projection method selector (PCA, t-SNE) - Zoom and pan controls</p> <p>Default parameters: - Pre-computed embeddings for 1000 common words - 2D PCA projection - Sample clusters highlighted - Canvas: responsive</p> <p>Behavior: - Hover to see word labels - Click word to show nearest neighbors - Enter vector arithmetic to see result - Animate projection computation - Show similarity scores</p> <p>Implementation: p5.js with pre-computed embedding data</p>"},{"location":"chapters/11-generative-ai-and-llms/#measuring-similarity","title":"Measuring Similarity","text":"<p>How do we determine if two embeddings represent similar concepts?</p>"},{"location":"chapters/11-generative-ai-and-llms/#semantic-similarity","title":"Semantic Similarity","text":"<p>Semantic similarity measures how related two concepts are in meaning. In embedding space, semantic similarity corresponds to geometric proximity.</p> <p>Two approaches:</p> <ol> <li>Euclidean distance: \\(d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2\\)</li> <li>Cosine similarity: \\(\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\)</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#cosine-similarity","title":"Cosine Similarity","text":"<p>Cosine similarity measures the angle between vectors, ignoring magnitude:</p> <p>\\(\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\|\\mathbf{u}\\|_2 \\|\\mathbf{v}\\|_2} = \\frac{\\sum_{i=1}^d u_i v_i}{\\sqrt{\\sum_{i=1}^d u_i^2} \\sqrt{\\sum_{i=1}^d v_i^2}}\\)</p> <p>Properties:</p> <ul> <li>Range: \\([-1, 1]\\)</li> <li>\\(+1\\): identical direction (most similar)</li> <li>\\(0\\): orthogonal (unrelated)</li> <li>\\(-1\\): opposite direction (most dissimilar)</li> </ul> <p>Why cosine over Euclidean?</p> <ul> <li>Invariant to vector magnitude (important for variable-length documents)</li> <li>Efficient computation in high dimensions</li> <li>More robust to the \"curse of dimensionality\"</li> </ul> <p>For normalized embeddings (\\(\\|\\mathbf{u}\\| = \\|\\mathbf{v}\\| = 1\\)):</p> <p>\\(\\text{cosine}(\\mathbf{u}, \\mathbf{v}) = \\mathbf{u}^T\\mathbf{v}\\)</p> <p>This is just the dot product\u2014extremely fast to compute.</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-similarity-comparison","title":"Diagram: Similarity Comparison","text":"Cosine vs Euclidean Similarity <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare cosine and Euclidean similarity measures and understand when each is appropriate</p> <p>Visual elements: - 2D plane with origin - Two adjustable vectors (draggable endpoints) - Angle arc between vectors - Distance line between endpoints - Similarity scores displayed</p> <p>Interactive controls: - Drag vector endpoints - \"Normalize Vectors\" toggle - Show cosine similarity value - Show Euclidean distance value - Preset examples: same direction different magnitude, orthogonal, similar angle</p> <p>Default parameters: - Two vectors in 2D - Both similarity measures shown - Canvas: responsive</p> <p>Behavior: - Real-time similarity updates - Show how cosine ignores magnitude - Demonstrate normalization effect - Highlight angle vs distance - Show formulas with current values</p> <p>Implementation: p5.js with vector geometry</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-attention-mechanism","title":"The Attention Mechanism","text":"<p>Attention is the key innovation enabling transformers to model long-range dependencies. Rather than processing sequences left-to-right, attention allows every position to directly interact with every other position.</p>"},{"location":"chapters/11-generative-ai-and-llms/#core-idea","title":"Core Idea","text":"<p>Given a sequence of vectors, attention computes a weighted combination where the weights depend on the content of the vectors themselves. This is \"content-based\" addressing\u2014the model decides what to focus on based on what it's looking at.</p>"},{"location":"chapters/11-generative-ai-and-llms/#self-attention","title":"Self-Attention","text":"<p>Self-attention computes attention within a single sequence. Each position attends to all positions in the same sequence (including itself):</p> <p>For input sequence \\(X = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_n]^T \\in \\mathbb{R}^{n \\times d}\\):</p> <ol> <li>Each position queries: \"What should I pay attention to?\"</li> <li>Each position offers keys: \"Here's what I contain\"</li> <li>Each position provides values: \"Here's my information\"</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#cross-attention","title":"Cross-Attention","text":"<p>Cross-attention computes attention between two different sequences:</p> <ul> <li>Queries come from one sequence (e.g., decoder)</li> <li>Keys and values come from another sequence (e.g., encoder)</li> </ul> <p>This enables:</p> <ul> <li>Machine translation (attend to source while generating target)</li> <li>Image captioning (attend to image regions while generating text)</li> <li>Question answering (attend to context while generating answer)</li> </ul> Attention Type Queries From Keys/Values From Use Case Self-attention Sequence X Sequence X Language modeling Cross-attention Sequence Y Sequence X Translation, QA Masked self-attention Sequence X Past positions of X Autoregressive generation"},{"location":"chapters/11-generative-ai-and-llms/#query-key-value-matrices","title":"Query, Key, Value Matrices","text":"<p>The attention mechanism is implemented using three learned linear projections.</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-qkv-framework","title":"The QKV Framework","text":"<p>For input \\(X \\in \\mathbb{R}^{n \\times d_{model}}\\):</p> <p>Query Matrix:</p> <p>\\(Q = XW^Q\\)</p> <p>where \\(W^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\)</p> <p>Key Matrix:</p> <p>\\(K = XW^K\\)</p> <p>where \\(W^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\)</p> <p>Value Matrix:</p> <p>\\(V = XW^V\\)</p> <p>where \\(W^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\)</p> <p>Resulting shapes:</p> <ul> <li>\\(Q \\in \\mathbb{R}^{n \\times d_k}\\) \u2014 one query per position</li> <li>\\(K \\in \\mathbb{R}^{n \\times d_k}\\) \u2014 one key per position</li> <li>\\(V \\in \\mathbb{R}^{n \\times d_v}\\) \u2014 one value per position</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#intuition","title":"Intuition","text":"<p>Think of attention as a soft dictionary lookup:</p> <ul> <li>Query: \"I'm looking for information about X\"</li> <li>Keys: \"I contain information about Y\"</li> <li>Values: \"Here's my actual content\"</li> </ul> <p>The query-key dot product measures compatibility. High compatibility means \"this key matches my query,\" so that value should contribute more to my output.</p>"},{"location":"chapters/11-generative-ai-and-llms/#attention-scores","title":"Attention Scores","text":"<p>Attention scores measure query-key compatibility:</p> <p>\\(S = QK^T \\in \\mathbb{R}^{n \\times n}\\)</p> <p>where:</p> <ul> <li>\\(S_{ij} = \\mathbf{q}_i^T \\mathbf{k}_j\\) is the score between position \\(i\\)'s query and position \\(j\\)'s key</li> <li>Higher score means stronger relevance</li> </ul> <p>The scores are scaled to prevent softmax saturation:</p> <p>\\(S_{scaled} = \\frac{QK^T}{\\sqrt{d_k}}\\)</p> <p>The \\(\\sqrt{d_k}\\) factor keeps the variance of dot products manageable as dimension increases.</p>"},{"location":"chapters/11-generative-ai-and-llms/#attention-weights","title":"Attention Weights","text":"<p>Attention weights are normalized scores (probabilities):</p> <p>\\(A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\\)</p> <p>where softmax is applied row-wise:</p> <p>\\(A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^n \\exp(S_{ik})}\\)</p> <p>Properties:</p> <ul> <li>Each row sums to 1: \\(\\sum_j A_{ij} = 1\\)</li> <li>All entries non-negative: \\(A_{ij} \\geq 0\\)</li> <li>Represents a probability distribution over positions</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#complete-attention-formula","title":"Complete Attention Formula","text":"<p>The attention output combines values weighted by attention:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\)</p> <p>Output shape: \\(n \\times d_v\\) \u2014 one output vector per position.</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-attention-mechanism-visualizer","title":"Diagram: Attention Mechanism Visualizer","text":"Attention Mechanism Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how attention computes weighted combinations through QKV projections</p> <p>Visual elements: - Input sequence (tokens with embeddings) - Q, K, V projection matrices - Attention score matrix (heatmap) - Softmax attention weights - Output vectors as weighted sums</p> <p>Interactive controls: - Input sequence editor (3-5 tokens) - Step-through: \"Project Q\", \"Project K\", \"Project V\", \"Compute Scores\", \"Softmax\", \"Weighted Sum\" - Query position selector (which position to visualize) - \"Show All Attention Heads\" toggle - Matrix dimension display</p> <p>Default parameters: - 4-token sequence - d_model = 8, d_k = d_v = 4 - Single attention head - Canvas: responsive</p> <p>Behavior: - Animate each projection step - Show attention score computation - Visualize softmax normalization - Demonstrate weighted sum of values - Highlight which positions attend to which</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Multi-head attention runs multiple attention operations in parallel, each learning different relationship patterns.</p>"},{"location":"chapters/11-generative-ai-and-llms/#why-multiple-heads","title":"Why Multiple Heads?","text":"<p>A single attention head might focus on one type of relationship (e.g., syntactic). Multiple heads can capture diverse patterns:</p> <ul> <li>Head 1: Subject-verb agreement</li> <li>Head 2: Coreference (pronouns to antecedents)</li> <li>Head 3: Positional patterns</li> <li>Head 4: Semantic similarity</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#multi-head-computation","title":"Multi-Head Computation","text":"<p>For \\(h\\) attention heads:</p> <p>\\(\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\\)</p> <p>where each head has its own projection matrices.</p> <p>Heads are concatenated and projected:</p> <p>\\(\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\\)</p> <p>where \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}\\) is the output projection.</p>"},{"location":"chapters/11-generative-ai-and-llms/#dimension-management","title":"Dimension Management","text":"<p>Typical configuration (e.g., \\(d_{model} = 512\\), \\(h = 8\\)):</p> <ul> <li>\\(d_k = d_v = d_{model}/h = 64\\)</li> <li>Each head operates on lower dimension</li> <li>Total computation similar to single full-dimension head</li> <li>But captures richer patterns</li> </ul> Component Shape Input \\(X\\) \\(n \\times d_{model}\\) Per-head \\(W^Q, W^K\\) \\(d_{model} \\times d_k\\) Per-head \\(W^V\\) \\(d_{model} \\times d_v\\) Per-head output \\(n \\times d_v\\) Concatenated \\(n \\times hd_v\\) After \\(W^O\\) \\(n \\times d_{model}\\)"},{"location":"chapters/11-generative-ai-and-llms/#diagram-multi-head-attention-visualizer","title":"Diagram: Multi-Head Attention Visualizer","text":"Multi-Head Attention Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how multiple attention heads capture different patterns and combine their outputs</p> <p>Visual elements: - Parallel attention head diagrams - Per-head attention weight heatmaps - Concatenation visualization - Output projection - Comparison of what each head attends to</p> <p>Interactive controls: - Number of heads slider (1-8) - Input sequence editor - Head selector to highlight individual heads - \"Compare Heads\" mode - \"Show Learned Patterns\" toggle</p> <p>Default parameters: - 4 attention heads - 5-token sequence - Pre-trained attention patterns - Canvas: responsive</p> <p>Behavior: - Show each head's attention independently - Visualize concatenation step - Demonstrate different heads learning different patterns - Compare head outputs before/after combination - Display dimension flow through computation</p> <p>Implementation: p5.js with parallel visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-transformer-architecture","title":"The Transformer Architecture","text":"<p>The transformer stacks attention layers with feedforward networks to build powerful sequence models.</p>"},{"location":"chapters/11-generative-ai-and-llms/#transformer-block","title":"Transformer Block","text":"<p>A single transformer block contains:</p> <ol> <li>Multi-head self-attention (with residual connection and layer norm)</li> <li>Feedforward network (with residual connection and layer norm)</li> </ol> <pre><code>Input x\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nMultiHeadAttn          \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nFeedForward            \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\nOutput\n</code></pre>"},{"location":"chapters/11-generative-ai-and-llms/#residual-connections","title":"Residual Connections","text":"<p>Residual connections add the input to the output:</p> <p>\\(\\text{output} = x + \\text{SubLayer}(x)\\)</p> <p>Benefits:</p> <ul> <li>Enable gradient flow through deep networks</li> <li>Allow layers to learn \"refinements\" rather than full transformations</li> <li>Stabilize training</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#layer-normalization","title":"Layer Normalization","text":"<p>Layer norm normalizes across features (not batch):</p> <p>\\(\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta\\)</p> <p>Applied before or after each sublayer (pre-norm vs post-norm).</p>"},{"location":"chapters/11-generative-ai-and-llms/#position-encoding","title":"Position Encoding","text":"<p>Self-attention is permutation-invariant\u2014it treats input as a set, not a sequence. Position encodings inject positional information.</p> <p>Sinusoidal encoding (original transformer):</p> <p>\\(PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})\\)</p> <p>\\(PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\\)</p> <p>where:</p> <ul> <li>\\(pos\\) is the position in the sequence</li> <li>\\(i\\) is the dimension index</li> </ul> <p>Properties:</p> <ul> <li>Unique encoding for each position</li> <li>Relative positions can be computed from absolute encodings</li> <li>Generalizes to longer sequences than seen during training</li> </ul> <p>Learned position embeddings (GPT, BERT):</p> <ul> <li>Add learnable position embedding matrix \\(P \\in \\mathbb{R}^{L_{max} \\times d_{model}}\\)</li> <li>\\(\\text{input} = \\text{token\\_embedding} + \\text{position\\_embedding}\\)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-transformer-architecture","title":"Diagram: Transformer Architecture","text":"Transformer Block Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize data flow through a transformer block including attention, FFN, residuals, and layer norms</p> <p>Visual elements: - Block diagram of transformer layer - Data tensors at each stage (shape annotations) - Residual connection paths - Layer norm visualization - Stacking multiple blocks</p> <p>Interactive controls: - Number of blocks slider (1-6) - \"Step Through\" one operation at a time - \"Show Dimensions\" toggle - \"Highlight Residuals\" toggle - Input sequence length slider</p> <p>Default parameters: - 2 transformer blocks - Sequence length 4 - d_model = 512, d_ff = 2048 - Canvas: responsive</p> <p>Behavior: - Animate data flow through block - Show dimension changes at each stage - Visualize residual addition - Display parameter count per component - Compare pre-norm vs post-norm</p> <p>Implementation: p5.js with architecture diagram</p>"},{"location":"chapters/11-generative-ai-and-llms/#efficient-fine-tuning-with-lora","title":"Efficient Fine-Tuning with LoRA","text":"<p>Training all parameters of large models is expensive. LoRA (Low-Rank Adaptation) enables efficient fine-tuning.</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-lora-idea","title":"The LoRA Idea","text":"<p>Instead of updating full weight matrices, LoRA adds trainable low-rank decomposition:</p> <p>\\(W' = W + \\Delta W = W + BA\\)</p> <p>where:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\) is the frozen pre-trained weight</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) are trainable</li> <li>\\(r \\ll \\min(d, k)\\) is the rank (e.g., 4, 8, 16)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>For a weight matrix of size \\(d \\times k\\):</p> <ul> <li>Full fine-tuning: \\(dk\\) parameters</li> <li>LoRA: \\(r(d + k)\\) parameters</li> </ul> <p>Example: \\(d = k = 4096\\), \\(r = 8\\):</p> <ul> <li>Full: \\(16.7M\\) parameters per matrix</li> <li>LoRA: \\(65K\\) parameters per matrix (0.4%)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#why-low-rank-works","title":"Why Low-Rank Works","text":"<p>The update \\(\\Delta W\\) has rank at most \\(r\\). Research suggests that:</p> <ul> <li>Model adaptation often lies in a low-dimensional subspace</li> <li>The \"intrinsic dimension\" of fine-tuning is much smaller than parameter count</li> <li>Low-rank updates capture task-specific modifications</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#implementation","title":"Implementation","text":"<p>During fine-tuning:</p> <ol> <li>Freeze all original parameters \\(W\\)</li> <li>Initialize \\(A\\) with small random values, \\(B\\) with zeros</li> <li>Train only \\(A\\) and \\(B\\)</li> <li>Forward pass: \\(h = (W + BA)x = Wx + BAx\\)</li> </ol> <p>For inference, can merge: \\(W' = W + BA\\) (no added latency).</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-lora-visualization","title":"Diagram: LoRA Visualization","text":"LoRA Low-Rank Adaptation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how LoRA approximates weight updates with low-rank matrices</p> <p>Visual elements: - Original weight matrix W (large, frozen) - Low-rank factors A and B - Product BA visualization - Updated matrix W' = W + BA - Parameter count comparison</p> <p>Interactive controls: - Matrix dimensions d and k sliders - Rank r slider (1-32) - \"Show Decomposition\" toggle - \"Compare to Full Fine-tune\" toggle - Animation of forward pass</p> <p>Default parameters: - d = k = 64 (for visualization) - r = 4 - Canvas: responsive</p> <p>Behavior: - Show A and B shapes change with r - Display parameter savings percentage - Visualize low-rank approximation quality - Compare BA to random full-rank update - Show merged weight option</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#latent-spaces-in-generative-models","title":"Latent Spaces in Generative Models","text":"<p>Generative models learn to map between data and abstract latent spaces.</p>"},{"location":"chapters/11-generative-ai-and-llms/#what-is-a-latent-space","title":"What Is a Latent Space?","text":"<p>A latent space is a compressed, continuous representation where:</p> <ul> <li>Each point corresponds to a potential data sample</li> <li>Similar points produce similar outputs</li> <li>The space is typically lower-dimensional than data space</li> </ul> <p>For images:</p> <ul> <li>Data space: \\(\\mathbb{R}^{H \\times W \\times 3}\\) (millions of pixels)</li> <li>Latent space: \\(\\mathbb{R}^{512}\\) (compressed representation)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#structure-in-latent-space","title":"Structure in Latent Space","text":"<p>Well-trained generative models learn latent spaces with meaningful structure:</p> <ul> <li>Clusters: Similar items group together</li> <li>Directions: Moving along certain directions changes specific attributes</li> <li>Arithmetic: Vector operations have semantic meaning</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#interpolation","title":"Interpolation","text":"<p>Interpolation generates intermediate points between two latent vectors:</p> <p>Linear interpolation:</p> <p>\\(\\mathbf{z}(t) = (1-t)\\mathbf{z}_1 + t\\mathbf{z}_2, \\quad t \\in [0, 1]\\)</p> <p>Spherical interpolation (slerp): Better for normalized latent spaces:</p> <p>\\(\\mathbf{z}(t) = \\frac{\\sin((1-t)\\theta)}{\\sin\\theta}\\mathbf{z}_1 + \\frac{\\sin(t\\theta)}{\\sin\\theta}\\mathbf{z}_2\\)</p> <p>where \\(\\theta = \\arccos(\\mathbf{z}_1 \\cdot \\mathbf{z}_2)\\) for normalized vectors.</p>"},{"location":"chapters/11-generative-ai-and-llms/#applications","title":"Applications","text":"<ul> <li>Image morphing: Smooth transition between faces</li> <li>Style mixing: Combine attributes of different samples</li> <li>Attribute editing: Move along discovered directions</li> <li>Data augmentation: Generate novel training samples</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-latent-space-interpolation","title":"Diagram: Latent Space Interpolation","text":"Latent Space Interpolation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand latent space structure through interpolation and vector arithmetic</p> <p>Visual elements: - 2D latent space projection - Two endpoint vectors (selectable) - Interpolation path (linear vs spherical) - Generated samples along path - Vector arithmetic demonstration</p> <p>Interactive controls: - Select two points in latent space - Interpolation method: linear vs slerp - Number of intermediate steps slider - \"Show Path\" toggle - \"Try Vector Arithmetic\" mode</p> <p>Default parameters: - Pre-computed latent points for simple shapes - Linear interpolation - 5 intermediate steps - Canvas: responsive</p> <p>Behavior: - Visualize interpolation path - Show how generated output changes along path - Compare linear vs spherical interpolation - Demonstrate attribute manipulation via vector arithmetic - Show distance metrics along path</p> <p>Implementation: p5.js with latent space visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#practical-implementation","title":"Practical Implementation","text":"<p>Here's a simplified attention implementation:</p> <pre><code>import numpy as np\n\ndef softmax(x, axis=-1):\n    \"\"\"Numerically stable softmax.\"\"\"\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention.\n\n    Args:\n        Q: Queries, shape (n, d_k)\n        K: Keys, shape (m, d_k)\n        V: Values, shape (m, d_v)\n        mask: Optional attention mask\n\n    Returns:\n        Attention output, shape (n, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n\n    # Compute attention scores\n    scores = Q @ K.T / np.sqrt(d_k)  # (n, m)\n\n    # Apply mask if provided (for causal attention)\n    if mask is not None:\n        scores = np.where(mask, scores, -1e9)\n\n    # Softmax to get attention weights\n    attention_weights = softmax(scores, axis=-1)  # (n, m)\n\n    # Weighted sum of values\n    output = attention_weights @ V  # (n, d_v)\n\n    return output, attention_weights\n\ndef multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):\n    \"\"\"\n    Multi-head attention.\n\n    Args:\n        X: Input, shape (n, d_model)\n        W_Q, W_K, W_V: Per-head projection weights\n        W_O: Output projection weight\n        num_heads: Number of attention heads\n    \"\"\"\n    n, d_model = X.shape\n    d_k = d_model // num_heads\n\n    heads = []\n    for h in range(num_heads):\n        Q = X @ W_Q[h]\n        K = X @ W_K[h]\n        V = X @ W_V[h]\n        head_output, _ = scaled_dot_product_attention(Q, K, V)\n        heads.append(head_output)\n\n    # Concatenate heads\n    concat = np.concatenate(heads, axis=-1)\n\n    # Output projection\n    output = concat @ W_O\n\n    return output\n\ndef cosine_similarity(u, v):\n    \"\"\"Compute cosine similarity between vectors.\"\"\"\n    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n\ndef lora_forward(x, W, A, B):\n    \"\"\"Forward pass with LoRA adaptation.\"\"\"\n    # Original path + low-rank update\n    return x @ W.T + x @ A.T @ B.T\n\n# Example: Embedding lookup\ndef embed(token_ids, embedding_matrix):\n    \"\"\"Look up embeddings for token IDs.\"\"\"\n    return embedding_matrix[token_ids]\n\n# Example: Position encoding\ndef positional_encoding(seq_len, d_model):\n    \"\"\"Generate sinusoidal position encodings.\"\"\"\n    positions = np.arange(seq_len)[:, np.newaxis]\n    dims = np.arange(d_model)[np.newaxis, :]\n\n    angles = positions / np.power(10000, (2 * (dims // 2)) / d_model)\n\n    # Apply sin to even, cos to odd\n    pe = np.zeros((seq_len, d_model))\n    pe[:, 0::2] = np.sin(angles[:, 0::2])\n    pe[:, 1::2] = np.cos(angles[:, 1::2])\n\n    return pe\n\n# Example usage\nseq_len, d_model = 10, 64\nX = np.random.randn(seq_len, d_model)\n\n# Add position encoding\npos_enc = positional_encoding(seq_len, d_model)\nX_with_pos = X + pos_enc\n\nprint(f\"Input shape: {X.shape}\")\nprint(f\"Position encoding shape: {pos_enc.shape}\")\n</code></pre>"},{"location":"chapters/11-generative-ai-and-llms/#summary_1","title":"Summary","text":"<p>This chapter explored the linear algebra foundations of modern generative AI:</p> <p>Embeddings:</p> <ul> <li>Embeddings map discrete tokens to continuous vectors</li> <li>Embedding spaces encode semantic relationships geometrically</li> <li>Cosine similarity measures semantic relatedness: \\(\\frac{\\mathbf{u}^T\\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\\)</li> </ul> <p>Attention Mechanism:</p> <ul> <li>Self-attention allows each position to attend to all others</li> <li>Query, Key, Value matrices create content-based addressing</li> <li>Attention scores \\(QK^T/\\sqrt{d_k}\\) measure query-key compatibility</li> <li>Attention weights are softmax-normalized scores</li> </ul> <p>Multi-Head and Transformers:</p> <ul> <li>Multi-head attention captures diverse relationship patterns</li> <li>Transformer blocks stack attention with feedforward layers</li> <li>Position encodings inject sequential order information</li> <li>Residual connections enable deep network training</li> </ul> <p>Efficient Adaptation:</p> <ul> <li>LoRA adds low-rank trainable matrices: \\(W' = W + BA\\)</li> <li>Reduces trainable parameters by 100-1000x</li> </ul> <p>Latent Spaces:</p> <ul> <li>Latent spaces provide compressed, continuous representations</li> <li>Interpolation generates smooth transitions between points</li> <li>Vector arithmetic enables semantic manipulation</li> </ul> Self-Check: Why does attention use \\(\\sqrt{d_k}\\) scaling in the score computation? <p>The dot product \\(QK^T\\) produces values with variance proportional to \\(d_k\\) (the key/query dimension). Without scaling, as \\(d_k\\) grows large, the dot products grow large, pushing softmax into regions of extreme gradients (near-zero for most values, near-one for the maximum). Dividing by \\(\\sqrt{d_k}\\) normalizes the variance to approximately 1, keeping softmax in a region where gradients flow well and the attention distribution isn't too peaked.</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/","title":"Quiz: Generative AI and Large Language Models","text":"<p>Test your understanding of embeddings, attention mechanisms, and transformer architecture.</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#1-an-embedding-maps","title":"1. An embedding maps:","text":"<ol> <li>Continuous values to discrete tokens</li> <li>Discrete tokens to continuous vectors</li> <li>Images to text</li> <li>Gradients to weights</li> </ol> Show Answer <p>The correct answer is B. An embedding maps discrete items (like words or tokens) to continuous vector representations. This enables neural networks to process symbolic data like text.</p> <p>Concept Tested: Embedding</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#2-cosine-similarity-between-two-vectors-measures","title":"2. Cosine similarity between two vectors measures:","text":"<ol> <li>Their Euclidean distance</li> <li>The angle between them (ignoring magnitude)</li> <li>Their element-wise product sum</li> <li>The difference in their norms</li> </ol> Show Answer <p>The correct answer is B. Cosine similarity measures the cosine of the angle between vectors: \\(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\\). It ranges from -1 to 1 and ignores vector magnitudes.</p> <p>Concept Tested: Cosine Similarity</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#3-in-attention-mechanisms-the-query-key-dot-product-determines","title":"3. In attention mechanisms, the query-key dot product determines:","text":"<ol> <li>The size of the output</li> <li>How much each position attends to other positions</li> <li>The number of attention heads</li> <li>The embedding dimension</li> </ol> Show Answer <p>The correct answer is B. The query-key dot product computes attention scores that determine how much each position should focus on other positions. Higher scores mean stronger attention.</p> <p>Concept Tested: Attention Score</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#4-the-scaling-factor-sqrtd_k-in-attention-is-used-to","title":"4. The scaling factor \\(\\sqrt{d_k}\\) in attention is used to:","text":"<ol> <li>Increase the attention weights</li> <li>Prevent softmax saturation from large dot products</li> <li>Reduce the number of parameters</li> <li>Speed up training</li> </ol> Show Answer <p>The correct answer is B. Dividing by \\(\\sqrt{d_k}\\) prevents the dot products from becoming too large as dimension increases, which would push softmax into regions with very small gradients.</p> <p>Concept Tested: Scaled Dot-Product Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#5-multi-head-attention","title":"5. Multi-head attention:","text":"<ol> <li>Uses only one attention computation</li> <li>Runs multiple attention operations in parallel to capture different patterns</li> <li>Eliminates the need for queries and keys</li> <li>Replaces the transformer architecture</li> </ol> Show Answer <p>The correct answer is B. Multi-head attention runs several attention computations in parallel, each potentially learning different relationship patterns (syntactic, semantic, positional, etc.), then combines their outputs.</p> <p>Concept Tested: Multi-Head Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#6-self-attention-allows-each-position-to","title":"6. Self-attention allows each position to:","text":"<ol> <li>Only attend to itself</li> <li>Attend to all positions in the same sequence</li> <li>Only attend to previous positions</li> <li>Ignore all other positions</li> </ol> Show Answer <p>The correct answer is B. Self-attention allows each position in a sequence to attend to all other positions (including itself), enabling direct modeling of long-range dependencies.</p> <p>Concept Tested: Self-Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#7-position-encoding-in-transformers-provides","title":"7. Position encoding in transformers provides:","text":"<ol> <li>Word meaning information</li> <li>Sequence order information since attention is permutation-invariant</li> <li>Grammar rules</li> <li>Vocabulary size</li> </ol> Show Answer <p>The correct answer is B. Since self-attention treats input as a set (permutation-invariant), position encodings inject information about the order of tokens in the sequence.</p> <p>Concept Tested: Position Encoding</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#8-lora-low-rank-adaptation-reduces-fine-tuning-cost-by","title":"8. LoRA (Low-Rank Adaptation) reduces fine-tuning cost by:","text":"<ol> <li>Removing all attention layers</li> <li>Adding trainable low-rank matrices instead of updating full weights</li> <li>Using smaller vocabulary</li> <li>Eliminating position encodings</li> </ol> Show Answer <p>The correct answer is B. LoRA keeps original weights frozen and adds small, trainable low-rank matrices (\\(A\\) and \\(B\\)) such that \\(W' = W + BA\\). This dramatically reduces the number of trainable parameters.</p> <p>Concept Tested: LoRA</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#9-in-a-latent-space-similar-items-typically","title":"9. In a latent space, similar items typically:","text":"<ol> <li>Have very different vector representations</li> <li>Are mapped to nearby points</li> <li>Have zero cosine similarity</li> <li>Require different embedding dimensions</li> </ol> Show Answer <p>The correct answer is B. A well-trained latent space maps semantically similar items to nearby points. This structure enables meaningful interpolation and arithmetic operations.</p> <p>Concept Tested: Latent Space</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#10-the-value-matrix-in-attention-provides","title":"10. The Value matrix in attention provides:","text":"<ol> <li>The content to be aggregated based on attention weights</li> <li>The query for matching</li> <li>The key for compatibility</li> <li>The position information</li> </ol> Show Answer <p>The correct answer is A. The Value matrix contains the actual information to be retrieved. Attention weights (from Query-Key matching) determine how to combine Values to produce the output.</p> <p>Concept Tested: Value Matrix</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/","title":"Optimization and Learning Algorithms","text":""},{"location":"chapters/12-optimization-and-learning-algorithms/#summary","title":"Summary","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms. This chapter covers the Hessian matrix, convexity, Newton's method, and modern adaptive optimizers like Adam and RMSprop. You will also learn constrained optimization with Lagrange multipliers and KKT conditions.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 14 concepts from the learning graph:</p> <ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"chapters/12-optimization-and-learning-algorithms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 9: Machine Learning Foundations</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#introduction","title":"Introduction","text":"<p>Training machine learning models fundamentally requires solving optimization problems\u2014finding the parameters that minimize a loss function. From simple linear regression to complex deep neural networks, optimization algorithms determine both the quality and efficiency of learning. Linear algebra provides essential tools for understanding these algorithms:</p> <ul> <li>Gradients indicate the direction of steepest ascent</li> <li>Hessians capture curvature information</li> <li>Matrix operations enable efficient computation</li> <li>Eigenvalue analysis reveals optimization landscapes</li> </ul> <p>This chapter builds from foundational concepts like convexity through classical algorithms like Newton's method to modern adaptive optimizers used in deep learning.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#convexity-and-convex-functions","title":"Convexity and Convex Functions","text":"<p>Before diving into optimization algorithms, we need to understand the landscape we're optimizing over. Convexity is the most important property that makes optimization tractable.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#what-makes-a-set-convex","title":"What Makes a Set Convex?","text":"<p>A set \\(S\\) is convex if for any two points \\(\\mathbf{x}, \\mathbf{y} \\in S\\), the line segment connecting them lies entirely within \\(S\\):</p> <p>\\(\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y} \\in S \\quad \\text{for all } \\lambda \\in [0, 1]\\)</p> <p>Intuitively, a convex set has no \"dents\" or \"holes.\"</p> Set Type Examples Convex? Ball/Sphere interior \\(\\{\\mathbf{x} : \\|\\mathbf{x}\\| \\leq r\\}\\) Yes Hyperplane \\(\\{\\mathbf{x} : \\mathbf{a}^\\top \\mathbf{x} = b\\}\\) Yes Half-space \\(\\{\\mathbf{x} : \\mathbf{a}^\\top \\mathbf{x} \\leq b\\}\\) Yes Donut/Annulus \\(\\{\\mathbf{x} : r_1 \\leq \\|\\mathbf{x}\\| \\leq r_2\\}\\) No"},{"location":"chapters/12-optimization-and-learning-algorithms/#convex-functions","title":"Convex Functions","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if its domain is a convex set and for all points in its domain:</p> <p>\\(f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y})\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x}, \\mathbf{y}\\) are any two points in the domain</li> <li>\\(\\lambda \\in [0, 1]\\) is a mixing coefficient</li> </ul> <p>Geometrically, the function lies below the chord connecting any two points on its graph\u2014it \"curves upward.\"</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-convex-function-visualizer","title":"Diagram: Convex Function Visualizer","text":"Convex Function Visualizer <p>Type: microsim</p> <p>Learning objective: Understand the geometric definition of convexity by visualizing the chord condition (Bloom: Understand)</p> <p>Visual elements: - 2D plot showing function curve f(x) - Two draggable points on the curve (x1, f(x1)) and (x2, f(x2)) - Line segment (chord) connecting the two points - Shaded region between chord and curve - Color indicator: green if convex condition satisfied, red otherwise</p> <p>Interactive controls: - Dropdown to select function: x\u00b2, |x|, x\u2074, x\u00b2 + sin(x), -x\u00b2 (non-convex) - Draggable points to adjust x1 and x2 positions - Slider for lambda (0 to 1) to show interpolation point - Display showing f(\u03bbx+(1-\u03bb)y) vs \u03bbf(x)+(1-\u03bb)f(y)</p> <p>Canvas layout: 700x500px with function plot area and value comparison</p> <p>Behavior: - As user drags points, chord updates in real-time - Lambda slider moves a point along chord and on curve to compare heights - Text displays the convexity inequality with current values - Red highlight appears when a non-convex function violates the condition</p> <p>Implementation: p5.js with interactive draggable elements</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#first-order-condition-for-convexity","title":"First-Order Condition for Convexity","text":"<p>For differentiable functions, convexity has an equivalent characterization using the gradient:</p> <p>\\(f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top (\\mathbf{y} - \\mathbf{x})\\)</p> <p>This says the function always lies above its tangent hyperplane\u2014the linearization underestimates the function everywhere.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#second-order-condition-for-convexity","title":"Second-Order Condition for Convexity","text":"<p>For twice-differentiable functions, convexity is equivalent to the Hessian being positive semidefinite everywhere:</p> <p>\\(\\nabla^2 f(\\mathbf{x}) \\succeq 0 \\quad \\text{for all } \\mathbf{x}\\)</p> <p>This connects convexity to the eigenvalues of the Hessian matrix.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-hessian-matrix","title":"The Hessian Matrix","text":"<p>The Hessian matrix captures all second-order partial derivative information of a function. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\):</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#hessian-matrix-definition","title":"Hessian Matrix Definition","text":"<p>\\(\\mathbf{H} = \\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{H}\\) is an \\(n \\times n\\) symmetric matrix (if \\(f\\) is twice continuously differentiable)</li> <li>\\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\) is the \\((i,j)\\) entry</li> <li>Diagonal entries are second derivatives with respect to single variables</li> <li>Off-diagonal entries capture how the gradient changes in one direction as we move in another</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#eigenvalue-interpretation","title":"Eigenvalue Interpretation","text":"<p>The eigenvalues of the Hessian reveal the curvature of the function:</p> Eigenvalue Pattern Curvature Type Optimization Implication All positive Bowl (minimum) Local minimum found All negative Dome (maximum) Local maximum Mixed signs Saddle point Neither min nor max Zero eigenvalue Flat direction Infinitely many solutions"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-hessian-and-curvature-visualizer","title":"Diagram: Hessian and Curvature Visualizer","text":"Hessian and Curvature Visualizer <p>Type: microsim</p> <p>Learning objective: Connect Hessian eigenvalues to geometric curvature of 2D functions (Bloom: Analyze)</p> <p>Visual elements: - 3D surface plot of f(x,y) - Contour plot below the surface - Current point indicator (draggable) - Principal curvature directions shown as arrows at current point - Eigenvalue display with color coding</p> <p>Interactive controls: - Function selector: x\u00b2+y\u00b2, x\u00b2-y\u00b2 (saddle), x\u00b2+0.1y\u00b2, 2x\u00b2+y\u00b2 - Draggable point on contour plot to change evaluation location - Toggle to show/hide principal directions - Toggle to show local quadratic approximation</p> <p>Canvas layout: 800x600px with 3D plot and eigenvector overlay</p> <p>Behavior: - As point moves, Hessian is recomputed and displayed as matrix - Eigenvalues update with color (green=positive, red=negative) - Principal direction arrows scale with eigenvalue magnitude - Quadratic approximation surface overlays at current point</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-hessian-in-taylor-expansion","title":"The Hessian in Taylor Expansion","text":"<p>The Hessian appears in the second-order Taylor expansion:</p> <p>\\(f(\\mathbf{x} + \\mathbf{d}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\mathbf{d} + \\frac{1}{2} \\mathbf{d}^\\top \\mathbf{H} \\mathbf{d}\\)</p> <p>This quadratic approximation is the foundation of Newton's method.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#newtons-method-for-optimization","title":"Newton's Method for Optimization","text":"<p>Newton's method uses second-order information to find where the gradient equals zero. By setting the gradient of the quadratic approximation to zero:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#newton-update-rule","title":"Newton Update Rule","text":"<p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{H}^{-1} \\nabla f(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x}_k\\) is the current iterate</li> <li>\\(\\mathbf{H} = \\nabla^2 f(\\mathbf{x}_k)\\) is the Hessian at the current point</li> <li>\\(\\nabla f(\\mathbf{x}_k)\\) is the gradient at the current point</li> <li>The term \\(\\mathbf{H}^{-1} \\nabla f\\) is called the Newton direction</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#comparison-with-gradient-descent","title":"Comparison with Gradient Descent","text":"Property Gradient Descent Newton's Method Update direction \\(-\\nabla f\\) (steepest descent) \\(-\\mathbf{H}^{-1} \\nabla f\\) (Newton direction) Step size Requires tuning \\(\\alpha\\) Natural step size of 1 Convergence rate Linear Quadratic (near solution) Cost per iteration \\(O(n)\\) gradient \\(O(n^3)\\) Hessian solve Condition number sensitivity High Low <p>Newton's method effectively rescales the problem to have uniform curvature in all directions, making it condition-number independent.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-newton-vs-gradient-descent","title":"Diagram: Newton vs Gradient Descent","text":"Newton vs Gradient Descent Comparison <p>Type: microsim</p> <p>Learning objective: Compare convergence behavior of gradient descent and Newton's method on ill-conditioned problems (Bloom: Analyze)</p> <p>Visual elements: - Contour plot of 2D quadratic function with adjustable condition number - Two optimization paths: gradient descent (blue) and Newton (orange) - Current iterate markers for both methods - Iteration counter and function value display</p> <p>Interactive controls: - Slider: Condition number (1 to 100) - Slider: Learning rate for gradient descent (0.001 to 0.1) - Button: \"Step\" (advance one iteration) - Button: \"Run to Convergence\" - Button: \"Reset\" - Starting point selector (click on contour plot)</p> <p>Canvas layout: 700x600px with contour plot and convergence comparison</p> <p>Behavior: - Newton's method converges in 1 step for quadratics - Gradient descent shows zig-zag pattern for high condition numbers - Display shows iteration count to reach tolerance - Side panel shows current gradient norm for both methods</p> <p>Implementation: p5.js with eigenvalue-based ellipse generation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#challenges-with-newtons-method","title":"Challenges with Newton's Method","text":"<p>Despite quadratic convergence, Newton's method has practical limitations:</p> <ul> <li>Hessian computation: \\(O(n^2)\\) storage and \\(O(n^3)\\) inversion</li> <li>Non-convexity: May converge to saddle points or maxima</li> <li>Numerical stability: Requires Hessian to be positive definite</li> <li>Scalability: Impractical for deep learning with millions of parameters</li> </ul> <p>These limitations motivate quasi-Newton methods.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Quasi-Newton methods approximate the Hessian (or its inverse) using only gradient information, avoiding explicit Hessian computation.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-secant-equation","title":"The Secant Equation","text":"<p>Quasi-Newton methods build approximations \\(\\mathbf{B}_k \\approx \\mathbf{H}_k\\) satisfying the secant equation:</p> <p>\\(\\mathbf{B}_{k+1} \\mathbf{s}_k = \\mathbf{y}_k\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k\\) is the step taken</li> <li>\\(\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)\\) is the gradient difference</li> <li>This equation states that \\(\\mathbf{B}_{k+1}\\) correctly maps the step to the gradient change</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-bfgs-algorithm","title":"The BFGS Algorithm","text":"<p>The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is the most successful quasi-Newton method. It directly maintains an approximation \\(\\mathbf{M}_k \\approx \\mathbf{H}_k^{-1}\\):</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#bfgs-update-formula","title":"BFGS Update Formula","text":"<p>\\(\\mathbf{M}_{k+1} = \\left(\\mathbf{I} - \\rho_k \\mathbf{s}_k \\mathbf{y}_k^\\top\\right) \\mathbf{M}_k \\left(\\mathbf{I} - \\rho_k \\mathbf{y}_k \\mathbf{s}_k^\\top\\right) + \\rho_k \\mathbf{s}_k \\mathbf{s}_k^\\top\\)</p> <p>where:</p> <ul> <li>\\(\\rho_k = \\frac{1}{\\mathbf{y}_k^\\top \\mathbf{s}_k}\\)</li> <li>\\(\\mathbf{M}_k\\) is the current inverse Hessian approximation</li> <li>\\(\\mathbf{s}_k\\) and \\(\\mathbf{y}_k\\) are the step and gradient difference</li> </ul> <p>Key properties of BFGS:</p> <ul> <li>Maintains positive definiteness if initialized positive definite</li> <li>Converges superlinearly (faster than linear, slower than quadratic)</li> <li>Only requires gradient evaluations, not Hessian</li> <li>\\(O(n^2)\\) storage and update cost</li> </ul> Method Hessian Cost Storage Convergence Newton \\(O(n^3)\\) exact \\(O(n^2)\\) Quadratic BFGS \\(O(n^2)\\) approximate \\(O(n^2)\\) Superlinear L-BFGS \\(O(mn)\\) limited memory \\(O(mn)\\) Superlinear Gradient Descent None \\(O(n)\\) Linear <p>L-BFGS (Limited-memory BFGS) stores only the \\(m\\) most recent \\((\\mathbf{s}_k, \\mathbf{y}_k)\\) pairs, making it suitable for large-scale problems.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>While Newton and quasi-Newton methods work well for moderate-sized problems, deep learning requires optimizing millions of parameters using billions of data points. Stochastic Gradient Descent (SGD) trades accuracy for efficiency.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-sgd-algorithm","title":"The SGD Algorithm","text":"<p>Instead of computing the full gradient over all training examples:</p> <p>\\(\\nabla f(\\mathbf{x}) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla f_i(\\mathbf{x})\\)</p> <p>SGD uses a single randomly selected example:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#sgd-update-rule","title":"SGD Update Rule","text":"<p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f_{i_k}(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is the learning rate</li> <li>\\(i_k\\) is a randomly selected index from \\(\\{1, \\ldots, N\\}\\)</li> <li>\\(\\nabla f_{i_k}\\) is the gradient for example \\(i_k\\)</li> </ul> <p>The stochastic gradient is an unbiased estimator of the true gradient:</p> <p>\\(\\mathbb{E}[\\nabla f_{i_k}(\\mathbf{x})] = \\nabla f(\\mathbf{x})\\)</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#mini-batch-sgd","title":"Mini-Batch SGD","text":"<p>Mini-batch SGD balances the efficiency of single-sample SGD with the stability of full-batch gradient descent:</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\cdot \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla f_i(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(B_k\\) is a randomly sampled mini-batch of size \\(|B_k|\\) (typically 32-512)</li> <li>The average reduces gradient variance by factor \\(|B_k|\\)</li> </ul> Batch Size Gradient Variance GPU Utilization Generalization 1 (pure SGD) High Poor Often good 32-64 Moderate Good Good 256-512 Low Excellent May need tuning Full batch Zero Memory limited May overfit"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-sgd-trajectory-visualizer","title":"Diagram: SGD Trajectory Visualizer","text":"SGD Trajectory Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how batch size affects SGD convergence behavior (Bloom: Apply)</p> <p>Visual elements: - 2D contour plot of loss landscape - Optimization trajectory showing recent steps - \"True gradient\" arrow and \"stochastic gradient\" arrow at current point - Noise cloud visualization around true gradient</p> <p>Interactive controls: - Slider: Batch size (1, 8, 32, 128, full) - Slider: Learning rate (0.001 to 0.5) - Button: \"Step\" (one update) - Button: \"Run 100 steps\" - Button: \"Reset\" - Toggle: Show gradient noise distribution</p> <p>Canvas layout: 700x600px with contour plot and gradient visualization</p> <p>Behavior: - Small batch sizes show noisy, erratic paths - Large batch sizes show smoother convergence - Display running average of gradient norm - Show variance estimate of stochastic gradient</p> <p>Implementation: p5.js with random gradient noise simulation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#momentum-based-optimization","title":"Momentum-Based Optimization","text":"<p>Pure SGD can oscillate in ravines\u2014directions where the curvature differs significantly. Momentum addresses this by accumulating gradient information across iterations.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#classical-momentum","title":"Classical Momentum","text":"<p>The momentum update maintains a velocity vector:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#momentum-update","title":"Momentum Update","text":"<p>\\(\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k)\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_k\\) is the velocity (accumulated gradient)</li> <li>\\(\\beta \\in [0, 1)\\) is the momentum coefficient (typically 0.9)</li> <li>\\(\\alpha\\) is the learning rate</li> </ul> <p>Momentum has a physical interpretation: the parameters move like a ball rolling downhill with friction. The ball builds up speed in consistent gradient directions while damping oscillations.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#nesterov-accelerated-gradient","title":"Nesterov Accelerated Gradient","text":"<p>Nesterov momentum looks ahead before computing the gradient:</p> <p>\\(\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k - \\alpha \\beta \\mathbf{v}_k)\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}\\)</p> <p>This \"lookahead\" provides a correction that improves convergence, especially near the optimum.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-momentum-dynamics","title":"Diagram: Momentum Dynamics","text":"Momentum Dynamics Visualizer <p>Type: microsim</p> <p>Learning objective: Visualize how momentum accumulates and dampens oscillations (Bloom: Understand)</p> <p>Visual elements: - 2D contour plot with elongated elliptical contours (ill-conditioned) - Three simultaneous optimization paths: SGD (blue), Momentum (green), Nesterov (orange) - Velocity vectors shown as arrows at current positions - Trace of recent positions for each method</p> <p>Interactive controls: - Slider: Momentum coefficient beta (0 to 0.99) - Slider: Learning rate (0.001 to 0.1) - Button: \"Step\" - Button: \"Run to convergence\" - Button: \"Reset\" - Checkbox: Show velocity vectors</p> <p>Canvas layout: 750x600px with contour visualization</p> <p>Behavior: - SGD shows characteristic zig-zag oscillation - Momentum shows smooth acceleration toward minimum - Nesterov shows slightly faster convergence - Velocity vectors grow in consistent directions, shrink during oscillation</p> <p>Implementation: p5.js with physics-based animation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#adaptive-learning-rate-methods","title":"Adaptive Learning Rate Methods","text":"<p>Different parameters often require different learning rates. Adaptive methods automatically adjust per-parameter learning rates based on historical gradient information.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#rmsprop","title":"RMSprop","text":"<p>RMSprop (Root Mean Square Propagation) maintains a running average of squared gradients:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#rmsprop-update","title":"RMSprop Update","text":"<p>\\(\\mathbf{s}_{k+1} = \\gamma \\mathbf{s}_k + (1 - \\gamma) \\nabla f(\\mathbf{x}_k)^2\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\frac{\\alpha}{\\sqrt{\\mathbf{s}_{k+1} + \\epsilon}} \\odot \\nabla f(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{s}_k\\) is the running average of squared gradients (element-wise)</li> <li>\\(\\gamma \\approx 0.9\\) is the decay rate</li> <li>\\(\\epsilon \\approx 10^{-8}\\) prevents division by zero</li> <li>\\(\\odot\\) denotes element-wise multiplication</li> <li>The division is element-wise</li> </ul> <p>RMSprop divides the learning rate by the root mean square of recent gradients, effectively:</p> <ul> <li>Reducing step size for parameters with large gradients</li> <li>Increasing step size for parameters with small gradients</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-adam-optimizer","title":"The Adam Optimizer","text":"<p>Adam (Adaptive Moment Estimation) combines momentum with RMSprop:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#adam-update","title":"Adam Update","text":"<p>\\(\\mathbf{m}_{k+1} = \\beta_1 \\mathbf{m}_k + (1 - \\beta_1) \\nabla f(\\mathbf{x}_k)\\)</p> <p>\\(\\mathbf{v}_{k+1} = \\beta_2 \\mathbf{v}_k + (1 - \\beta_2) \\nabla f(\\mathbf{x}_k)^2\\)</p> <p>\\(\\hat{\\mathbf{m}} = \\frac{\\mathbf{m}_{k+1}}{1 - \\beta_1^{k+1}}, \\quad \\hat{\\mathbf{v}} = \\frac{\\mathbf{v}_{k+1}}{1 - \\beta_2^{k+1}}\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\frac{\\alpha}{\\sqrt{\\hat{\\mathbf{v}}} + \\epsilon} \\odot \\hat{\\mathbf{m}}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{m}_k\\) is the first moment (mean) estimate</li> <li>\\(\\mathbf{v}_k\\) is the second moment (variance) estimate</li> <li>\\(\\beta_1 \\approx 0.9\\) is the first moment decay</li> <li>\\(\\beta_2 \\approx 0.999\\) is the second moment decay</li> <li>\\(\\hat{\\mathbf{m}}, \\hat{\\mathbf{v}}\\) are bias-corrected estimates</li> </ul> <p>The bias correction compensates for initialization at zero, which otherwise underestimates moments early in training.</p> Optimizer First Moment Second Moment Bias Correction SGD No No N/A Momentum Yes No No RMSprop No Yes No Adam Yes Yes Yes AdaGrad No Yes (cumulative) No"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-optimizer-comparison-arena","title":"Diagram: Optimizer Comparison Arena","text":"Optimizer Comparison Arena <p>Type: microsim</p> <p>Learning objective: Compare convergence behavior of different optimizers on various loss landscapes (Bloom: Evaluate)</p> <p>Visual elements: - 2D loss landscape (selectable) - Multiple optimization paths: SGD, Momentum, RMSprop, Adam - Color-coded trajectories with position markers - Loss vs iteration plot below main visualization</p> <p>Interactive controls: - Dropdown: Loss landscape (Quadratic, Rosenbrock, Beale, Saddle) - Sliders for each optimizer's hyperparameters - Button: \"Race!\" (run all optimizers simultaneously) - Button: \"Reset\" - Checkboxes to enable/disable each optimizer</p> <p>Canvas layout: 800x700px with landscape and convergence plot</p> <p>Behavior: - All optimizers start from same initial point - Animate simultaneous optimization - Display current loss value for each - Declare \"winner\" when first reaches tolerance - Show iteration count for each optimizer</p> <p>Implementation: p5.js with multiple optimizer state tracking</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\nclass Adam:\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.m = None  # First moment\n        self.v = None  # Second moment\n        self.t = 0     # Time step\n\n    def step(self, params, grads):\n        if self.m is None:\n            self.m = np.zeros_like(params)\n            self.v = np.zeros_like(params)\n\n        self.t += 1\n\n        # Update biased first and second moment estimates\n        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n\n        # Bias correction\n        m_hat = self.m / (1 - self.beta1**self.t)\n        v_hat = self.v / (1 - self.beta2**self.t)\n\n        # Update parameters\n        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n        return params\n</code></pre>"},{"location":"chapters/12-optimization-and-learning-algorithms/#constrained-optimization","title":"Constrained Optimization","text":"<p>Many optimization problems have constraints\u2014parameter bounds, equality requirements, or inequality restrictions. Constrained optimization finds optima while satisfying these constraints.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#problem-formulation","title":"Problem Formulation","text":"<p>A general constrained optimization problem:</p> <p>\\(\\min_{\\mathbf{x}} f(\\mathbf{x})\\)</p> <p>subject to:</p> <ul> <li>\\(g_i(\\mathbf{x}) \\leq 0\\) for \\(i = 1, \\ldots, m\\) (inequality constraints)</li> <li>\\(h_j(\\mathbf{x}) = 0\\) for \\(j = 1, \\ldots, p\\) (equality constraints)</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#lagrange-multipliers","title":"Lagrange Multipliers","text":"<p>For equality-constrained problems, Lagrange multipliers convert the constrained problem to an unconstrained one.</p> <p>Consider minimizing \\(f(\\mathbf{x})\\) subject to \\(h(\\mathbf{x}) = 0\\). We form the Lagrangian:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#lagrangian-function","title":"Lagrangian Function","text":"<p>\\(\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) + \\lambda h(\\mathbf{x})\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the Lagrange multiplier</li> <li>The optimal \\((\\mathbf{x}^*, \\lambda^*)\\) satisfies \\(\\nabla_{\\mathbf{x}} \\mathcal{L} = 0\\) and \\(\\nabla_\\lambda \\mathcal{L} = 0\\)</li> </ul> <p>The condition \\(\\nabla_{\\mathbf{x}} \\mathcal{L} = 0\\) gives:</p> <p>\\(\\nabla f(\\mathbf{x}^*) = -\\lambda \\nabla h(\\mathbf{x}^*)\\)</p> <p>At the optimum, the gradient of \\(f\\) is parallel to the constraint gradient\u2014we can't improve \\(f\\) while staying on the constraint surface.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-lagrange-multiplier-geometry","title":"Diagram: Lagrange Multiplier Geometry","text":"Lagrange Multiplier Geometry <p>Type: microsim</p> <p>Learning objective: Visualize the geometric interpretation of Lagrange multipliers (Bloom: Understand)</p> <p>Visual elements: - 2D contour plot of objective function f(x,y) - Constraint curve h(x,y) = 0 (bold line) - Gradient vectors: \u2207f at selected point (blue arrow) - Constraint normal: \u2207h at selected point (red arrow) - Highlighted optimal point where gradients are parallel</p> <p>Interactive controls: - Draggable point along constraint curve - Toggle: Show gradient field of f - Toggle: Show constraint normal field - Button: \"Find Optimum\" (animate to solution) - Display: Current f value, \u03bb value</p> <p>Canvas layout: 700x600px with contour and vector field</p> <p>Behavior: - As user drags point along constraint, show gradient angles - Highlight when \u2207f and \u2207h become parallel - Display computed \u03bb = -||\u2207f||/||\u2207h|| at parallel point - Animate optimization path along constraint</p> <p>Implementation: p5.js with gradient computation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#kkt-conditions","title":"KKT Conditions","text":"<p>The Karush-Kuhn-Tucker (KKT) conditions generalize Lagrange multipliers to include inequality constraints. For the problem with both equality and inequality constraints, the KKT conditions are:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#kkt-conditions_1","title":"KKT Conditions","text":"<ol> <li> <p>Stationarity: \\(\\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^{m} \\mu_i \\nabla g_i(\\mathbf{x}^*) + \\sum_{j=1}^{p} \\lambda_j \\nabla h_j(\\mathbf{x}^*) = 0\\)</p> </li> <li> <p>Primal feasibility: \\(g_i(\\mathbf{x}^*) \\leq 0\\) and \\(h_j(\\mathbf{x}^*) = 0\\)</p> </li> <li> <p>Dual feasibility: \\(\\mu_i \\geq 0\\) for all \\(i\\)</p> </li> <li> <p>Complementary slackness: \\(\\mu_i g_i(\\mathbf{x}^*) = 0\\) for all \\(i\\)</p> </li> </ol> <p>where:</p> <ul> <li>\\(\\mu_i\\) are multipliers for inequality constraints</li> <li>\\(\\lambda_j\\) are multipliers for equality constraints</li> <li>Complementary slackness means either \\(\\mu_i = 0\\) or \\(g_i(\\mathbf{x}^*) = 0\\)</li> </ul> Condition Meaning Stationarity Gradient balance at optimum Primal feasibility Solution satisfies constraints Dual feasibility Inequality multipliers are non-negative Complementary slackness Inactive constraints have zero multipliers"},{"location":"chapters/12-optimization-and-learning-algorithms/#active-constraints","title":"Active Constraints","text":"<p>The complementary slackness condition is particularly important:</p> <ul> <li>If \\(g_i(\\mathbf{x}^*) &lt; 0\\), the constraint is inactive and \\(\\mu_i = 0\\)</li> <li>If \\(g_i(\\mathbf{x}^*) = 0\\), the constraint is active and may have \\(\\mu_i &gt; 0\\)</li> </ul> <p>Inactive constraints don't affect the solution\u2014they're not \"binding.\"</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-kkt-conditions-visualizer","title":"Diagram: KKT Conditions Visualizer","text":"KKT Conditions Visualizer <p>Type: microsim</p> <p>Learning objective: Understand KKT conditions for inequality-constrained optimization (Bloom: Analyze)</p> <p>Visual elements: - 2D contour plot of objective function - Feasible region shaded (where all g_i \u2264 0) - Constraint boundaries with active/inactive coloring - Optimal point with gradient vectors - KKT condition checklist with live status</p> <p>Interactive controls: - Draggable objective function center - Toggle constraints on/off - Button: \"Solve\" (find KKT point) - Display: Multiplier values \u03bc_i for each constraint</p> <p>Canvas layout: 800x650px with visualization and KKT status panel</p> <p>Behavior: - Show feasible region as intersection of half-planes - Animate solution finding - Display which constraints are active at solution - Show complementary slackness: \u03bc_i = 0 for inactive constraints - Color-code constraints: green (inactive), orange (active)</p> <p>Implementation: p5.js with linear programming solver</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#applications-in-machine-learning","title":"Applications in Machine Learning","text":""},{"location":"chapters/12-optimization-and-learning-algorithms/#regularized-optimization","title":"Regularized Optimization","text":"<p>Many machine learning problems add regularization terms that can be viewed through the lens of constrained optimization:</p> Formulation Constraint View Effect \\(\\min f(\\mathbf{x}) + \\lambda\\|\\mathbf{x}\\|_2^2\\) \\(\\|\\mathbf{x}\\|_2 \\leq r\\) L2 regularization (weight decay) \\(\\min f(\\mathbf{x}) + \\lambda\\|\\mathbf{x}\\|_1\\) \\(\\|\\mathbf{x}\\|_1 \\leq r\\) L1 regularization (sparsity) \\(\\min f(\\mathbf{x})\\) s.t. \\(\\|\\mathbf{x}\\|_2 = 1\\) Unit sphere Normalized embeddings"},{"location":"chapters/12-optimization-and-learning-algorithms/#support-vector-machines","title":"Support Vector Machines","text":"<p>SVMs solve a constrained quadratic program:</p> <p>\\(\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2\\)</p> <p>subject to \\(y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1\\) for all training examples.</p> <p>The KKT conditions reveal which training examples are support vectors (those with active constraints).</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#neural-network-constraints","title":"Neural Network Constraints","text":"<p>Modern deep learning increasingly uses constrained optimization:</p> <ul> <li>Spectral normalization: Constrain weight matrix spectral norm</li> <li>Gradient clipping: Constrain gradient magnitude</li> <li>Projected gradient descent: Project onto feasible set after each step</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#summary_1","title":"Summary","text":"<p>Optimization algorithms form the computational backbone of machine learning. This chapter covered:</p> <p>Foundational Concepts:</p> <ul> <li>Convexity guarantees global optima are local optima</li> <li>The Hessian matrix captures curvature and determines convergence behavior</li> <li>Positive definite Hessians indicate convexity</li> </ul> <p>Classical Methods:</p> <ul> <li>Newton's method uses the Hessian for quadratic convergence</li> <li>Quasi-Newton methods (BFGS) approximate the Hessian efficiently</li> <li>L-BFGS scales to large problems with limited memory</li> </ul> <p>Stochastic Methods:</p> <ul> <li>SGD enables optimization with large datasets</li> <li>Mini-batches balance variance reduction with efficiency</li> <li>Momentum accelerates convergence and dampens oscillations</li> </ul> <p>Adaptive Methods:</p> <ul> <li>RMSprop adapts learning rates using gradient magnitudes</li> <li>Adam combines momentum with adaptive rates</li> <li>Bias correction handles initialization effects</li> </ul> <p>Constrained Optimization:</p> <ul> <li>Lagrange multipliers handle equality constraints</li> <li>KKT conditions extend to inequality constraints</li> <li>Complementary slackness identifies active constraints</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#self-check-questions","title":"Self-Check Questions","text":"Why does Newton's method converge faster than gradient descent? <p>Newton's method uses second-order (curvature) information from the Hessian to account for the shape of the objective function. While gradient descent takes steps proportional to the gradient magnitude, Newton's method takes steps that account for how quickly the gradient changes. In the quadratic approximation, Newton's method jumps directly to the minimum. For well-behaved functions near a minimum, this gives quadratic convergence: the number of correct digits roughly doubles each iteration.</p> What is the purpose of bias correction in Adam? <p>The first and second moment estimates in Adam are initialized to zero. In early iterations, these running averages are biased toward zero because they haven't accumulated enough gradient history. Bias correction divides by \\((1 - \\beta^t)\\) to compensate, essentially scaling up the estimates early in training. As \\(t \\to \\infty\\), the correction factor approaches 1 and has no effect.</p> Explain the geometric meaning of the KKT complementary slackness condition. <p>Complementary slackness states that \\(\\mu_i g_i(\\mathbf{x}^*) = 0\\) for each inequality constraint. This means either the constraint is inactive (\\(g_i &lt; 0\\)) and its multiplier is zero (\\(\\mu_i = 0\\)), or the constraint is active (\\(g_i = 0\\)) and the multiplier may be positive. Geometrically, only constraints that the solution \"touches\" can exert force on the solution. Constraints that are strictly satisfied (with slack) don't affect where the optimum lies.</p> Why is mini-batch SGD preferred over pure SGD in practice? <p>Pure SGD (batch size 1) has high variance in gradient estimates, leading to noisy optimization paths. While this noise can help escape local minima, it also slows convergence and makes training unstable. Mini-batches reduce variance by averaging over multiple examples while still being much faster than full-batch gradient descent. Additionally, mini-batches enable efficient GPU parallelization\u2014processing 32 examples takes nearly the same time as 1 on modern hardware.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/","title":"Quiz: Optimization and Learning Algorithms","text":"<p>Test your understanding of optimization methods for machine learning.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#1-the-hessian-matrix-contains","title":"1. The Hessian matrix contains:","text":"<ol> <li>First-order partial derivatives</li> <li>Second-order partial derivatives</li> <li>The gradient vector</li> <li>The loss function values</li> </ol> Show Answer <p>The correct answer is B. The Hessian matrix contains all second-order partial derivatives: \\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\). It captures the curvature of the function.</p> <p>Concept Tested: Hessian Matrix</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#2-a-function-is-convex-if","title":"2. A function is convex if:","text":"<ol> <li>It has multiple local minima</li> <li>Any chord lies above or on the function graph</li> <li>Its gradient is always zero</li> <li>It is defined only for positive inputs</li> </ol> Show Answer <p>The correct answer is B. A convex function lies below any chord connecting two points on its graph: \\(f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\). Convex functions have a single global minimum.</p> <p>Concept Tested: Convex Function</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#3-newtons-method-uses","title":"3. Newton's method uses:","text":"<ol> <li>Only first-order gradient information</li> <li>Second-order Hessian information for faster convergence</li> <li>Random search</li> <li>No derivatives at all</li> </ol> Show Answer <p>The correct answer is B. Newton's method uses both gradient and Hessian: \\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - H^{-1}\\nabla f\\). The Hessian provides curvature information enabling quadratic convergence near the solution.</p> <p>Concept Tested: Newton's Method</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#4-stochastic-gradient-descent-sgd-differs-from-batch-gradient-descent-by","title":"4. Stochastic Gradient Descent (SGD) differs from batch gradient descent by:","text":"<ol> <li>Never converging</li> <li>Using gradients from random subsets of data</li> <li>Requiring second-order derivatives</li> <li>Only working on convex functions</li> </ol> Show Answer <p>The correct answer is B. SGD computes gradients using random mini-batches rather than the full dataset. This introduces noise but enables much faster iteration, especially with large datasets.</p> <p>Concept Tested: SGD</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#5-momentum-in-optimization","title":"5. Momentum in optimization:","text":"<ol> <li>Slows down convergence</li> <li>Accumulates gradient information to accelerate and dampen oscillations</li> <li>Eliminates the learning rate</li> <li>Only works for linear functions</li> </ol> Show Answer <p>The correct answer is B. Momentum maintains a velocity vector that accumulates past gradients, helping to accelerate convergence in consistent directions while dampening oscillations in others.</p> <p>Concept Tested: Momentum</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#6-the-adam-optimizer-combines","title":"6. The Adam optimizer combines:","text":"<ol> <li>Newton's method and SGD</li> <li>Momentum and adaptive learning rates (RMSprop-like)</li> <li>L1 and L2 regularization</li> <li>Batch and online learning</li> </ol> Show Answer <p>The correct answer is B. Adam combines momentum (first moment) with RMSprop-style adaptive learning rates (second moment), plus bias correction. This makes it robust across many problem types.</p> <p>Concept Tested: Adam Optimizer</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#7-rmsprop-adapts-learning-rates-by","title":"7. RMSprop adapts learning rates by:","text":"<ol> <li>Keeping them constant</li> <li>Dividing by the running average of squared gradients</li> <li>Multiplying by the gradient magnitude</li> <li>Using second-order derivatives</li> </ol> Show Answer <p>The correct answer is B. RMSprop divides the learning rate by the root mean square of recent gradients, reducing step size for parameters with large gradients and increasing it for those with small gradients.</p> <p>Concept Tested: RMSprop</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#8-lagrange-multipliers-are-used-to","title":"8. Lagrange multipliers are used to:","text":"<ol> <li>Increase the number of variables</li> <li>Convert constrained optimization to unconstrained</li> <li>Compute the Hessian</li> <li>Initialize weights randomly</li> </ol> Show Answer <p>The correct answer is B. Lagrange multipliers transform constrained optimization problems into unconstrained ones by incorporating constraints into the objective function through the Lagrangian.</p> <p>Concept Tested: Lagrange Multiplier</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#9-the-kkt-conditions-generalize-lagrange-multipliers-to-handle","title":"9. The KKT conditions generalize Lagrange multipliers to handle:","text":"<ol> <li>Only equality constraints</li> <li>Inequality constraints</li> <li>Unconstrained problems</li> <li>Non-differentiable functions</li> </ol> Show Answer <p>The correct answer is B. The Karush-Kuhn-Tucker (KKT) conditions extend Lagrange multipliers to problems with inequality constraints, introducing complementary slackness conditions.</p> <p>Concept Tested: KKT Conditions</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#10-mini-batch-sgd-is-preferred-over-pure-sgd-batch-size-1-because","title":"10. Mini-batch SGD is preferred over pure SGD (batch size 1) because:","text":"<ol> <li>It never converges</li> <li>It reduces gradient variance while maintaining computational efficiency</li> <li>It requires more memory than full-batch</li> <li>It eliminates the need for a learning rate</li> </ol> Show Answer <p>The correct answer is B. Mini-batches reduce gradient variance (averaging over multiple samples) while still being much faster than full-batch. They also enable efficient GPU parallelization.</p> <p>Concept Tested: Mini-Batch SGD</p>"},{"location":"chapters/13-image-processing-and-computer-vision/","title":"Image Processing and Computer Vision","text":""},{"location":"chapters/13-image-processing-and-computer-vision/#summary","title":"Summary","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision. This chapter covers image representation as matrices and tensors, convolution operations, image filtering (blur, sharpen, edge detection), Fourier transforms, and feature detection. You will also learn about homography for perspective transformations.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"chapters/13-image-processing-and-computer-vision/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 7: Matrix Decompositions (for SVD-based compression)</li> <li>Chapter 10: Neural Networks and Deep Learning (for tensors)</li> </ul>"},{"location":"chapters/13-image-processing-and-computer-vision/#introduction","title":"Introduction","text":"<p>Every digital image you see\u2014from smartphone photos to medical scans\u2014is fundamentally a matrix of numbers. This mathematical representation enables powerful image processing operations using linear algebra:</p> <ul> <li>Matrix operations enable pixel-wise transformations</li> <li>Convolution applies local filters for blurring, sharpening, and edge detection</li> <li>Fourier transforms reveal frequency content for compression and filtering</li> <li>Linear transformations correct perspective and align images</li> </ul> <p>Understanding these foundations illuminates how image editing software, computer vision systems, and neural networks process visual information.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-representation","title":"Image Representation","text":""},{"location":"chapters/13-image-processing-and-computer-vision/#the-image-matrix","title":"The Image Matrix","text":"<p>A digital image is stored as a matrix where each entry represents a pixel's intensity or color value. For an image with height \\(H\\) and width \\(W\\):</p> <p>\\(\\mathbf{I} \\in \\mathbb{R}^{H \\times W}\\)</p> <p>The matrix entry \\(I_{i,j}\\) corresponds to the pixel at row \\(i\\) (from top) and column \\(j\\) (from left). Pixel values are typically integers in \\([0, 255]\\) or floats in \\([0, 1]\\).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#grayscale-images","title":"Grayscale Images","text":"<p>A grayscale image uses a single value per pixel representing light intensity:</p> <ul> <li>0 = black (no light)</li> <li>255 = white (maximum light)</li> <li>Intermediate values = shades of gray</li> </ul> Value Range Interpretation 0-50 Very dark 50-100 Dark gray 100-150 Medium gray 150-200 Light gray 200-255 Very bright <p>A 1920\u00d71080 HD grayscale image is a matrix with over 2 million entries.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-image-matrix-visualizer","title":"Diagram: Image Matrix Visualizer","text":"Image Matrix Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how pixel values in a matrix correspond to image appearance (Bloom: Understand)</p> <p>Visual elements: - Left panel: Small grayscale image (e.g., 8\u00d78 or 16\u00d716 pixels) - Right panel: Matrix showing numeric values - Hover highlight connecting matrix cell to corresponding pixel - Color gradient legend (0=black to 255=white)</p> <p>Interactive controls: - Dropdown: Select sample image (checkerboard, gradient, simple shape, random) - Slider: Zoom level for image display - Click on matrix cell to highlight corresponding pixel - Click on pixel to highlight corresponding matrix cell - Button: \"Edit mode\" to modify individual pixel values</p> <p>Canvas layout: 800x500px with side-by-side image and matrix</p> <p>Behavior: - Hovering over matrix cell highlights pixel with colored border - Editing a value immediately updates the image - Show real-time pixel coordinates and value on hover - Display image dimensions and total pixel count</p> <p>Implementation: p5.js with matrix rendering and image display</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#rgb-images","title":"RGB Images","text":"<p>Color images use three channels\u2014Red, Green, Blue\u2014each stored as a separate matrix:</p> <p>\\(\\mathbf{I}_R, \\mathbf{I}_G, \\mathbf{I}_B \\in \\mathbb{R}^{H \\times W}\\)</p> <p>Each pixel is a triplet \\((r, g, b)\\) where each component ranges from 0 to 255:</p> Color RGB Value Red (255, 0, 0) Green (0, 255, 0) Blue (0, 0, 255) Yellow (255, 255, 0) Cyan (0, 255, 255) Magenta (255, 0, 255) White (255, 255, 255) Black (0, 0, 0) <p>The RGB color model is additive\u2014colors are created by adding light. Mixing all three at full intensity produces white.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-tensors","title":"Image Tensors","text":"<p>For computational purposes, color images are often represented as 3D tensors:</p> <p>\\(\\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times C}\\)</p> <p>where:</p> <ul> <li>\\(H\\) is the height (rows)</li> <li>\\(W\\) is the width (columns)</li> <li>\\(C\\) is the number of channels (3 for RGB, 4 for RGBA with transparency)</li> </ul> <p>In deep learning, batches of images form 4D tensors:</p> <p>\\(\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times H \\times W}\\)</p> <p>where \\(N\\) is the batch size. The ordering of dimensions varies by framework (PyTorch uses \\(N \\times C \\times H \\times W\\); TensorFlow often uses \\(N \\times H \\times W \\times C\\)).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-rgb-channel-decomposition","title":"Diagram: RGB Channel Decomposition","text":"RGB Channel Decomposition <p>Type: microsim</p> <p>Learning objective: Visualize how RGB channels combine to form color images (Bloom: Analyze)</p> <p>Visual elements: - Original color image (center or top) - Three grayscale images showing R, G, B channels separately - Color-tinted versions of each channel (red, green, blue tints) - Reconstructed image from selected channels</p> <p>Interactive controls: - Image selector: Choose from sample images - Channel toggles: Enable/disable R, G, B for reconstruction - Slider: Adjust individual channel intensity (0-100%) - Toggle: Show grayscale vs color-tinted channel views</p> <p>Canvas layout: 800x600px with original and channel displays</p> <p>Behavior: - Disabling a channel shows image without that color component - Adjusting channel intensity shows over/under-saturation effects - Hover over pixel to show (r, g, b) values - Real-time reconstruction as channels are modified</p> <p>Implementation: p5.js with pixel array manipulation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#color-space-transforms","title":"Color Space Transforms","text":"<p>While RGB is standard for display, other color spaces are useful for specific tasks:</p> Color Space Components Use Cases RGB Red, Green, Blue Display, storage HSV/HSB Hue, Saturation, Value Color selection, segmentation HSL Hue, Saturation, Lightness Web design, color adjustment YCbCr Luminance, Chrominance JPEG compression LAB Lightness, a, b Perceptual uniformity <p>Converting between color spaces involves matrix transformations. For example, RGB to YCbCr:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#rgb-to-ycbcr-transform","title":"RGB to YCbCr Transform","text":"<p>\\(\\begin{bmatrix} Y \\\\ C_b \\\\ C_r \\end{bmatrix} = \\begin{bmatrix} 0.299 &amp; 0.587 &amp; 0.114 \\\\ -0.169 &amp; -0.331 &amp; 0.500 \\\\ 0.500 &amp; -0.419 &amp; -0.081 \\end{bmatrix} \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 128 \\\\ 128 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(Y\\) is luminance (brightness information)</li> <li>\\(C_b\\) and \\(C_r\\) are chrominance (color difference signals)</li> <li>The matrix weights reflect human perception (green contributes most to perceived brightness)</li> </ul> <p>This separation enables chroma subsampling\u2014reducing color resolution while preserving brightness detail\u2014which is fundamental to JPEG compression.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-convolution","title":"Image Convolution","text":"<p>Convolution is the foundational operation for image filtering. It applies a small matrix called a kernel or filter to every location in the image.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-convolution-operation","title":"The Convolution Operation","text":"<p>For a 2D image \\(\\mathbf{I}\\) and kernel \\(\\mathbf{K}\\) of size \\((2k+1) \\times (2k+1)\\):</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#2d-convolution-formula","title":"2D Convolution Formula","text":"<p>\\((\\mathbf{I} * \\mathbf{K})[i, j] = \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I[i+m, j+n] \\cdot K[m, n]\\)</p> <p>where:</p> <ul> <li>\\(*\\) denotes convolution (not multiplication)</li> <li>\\((i, j)\\) is the output pixel location</li> <li>The kernel is centered at each pixel</li> <li>Values outside the image boundary require padding (zero, replicate, or reflect)</li> </ul> <p>Convolution is a linear operation: convolving with a sum of kernels equals the sum of individual convolutions.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-filters","title":"Image Filters","text":"<p>An image filter is a kernel designed to achieve a specific effect. The kernel values determine what features are enhanced or suppressed.</p> Filter Type Effect Kernel Property Low-pass (blur) Smooths, reduces noise Positive values, sums to 1 High-pass (sharpen) Enhances edges Positive center, negative surround Derivative (edge) Detects changes Sums to 0, antisymmetric Identity No change 1 at center, 0 elsewhere"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-convolution-visualizer","title":"Diagram: Convolution Visualizer","text":"Convolution Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how convolution applies a kernel to an image (Bloom: Apply)</p> <p>Visual elements: - Input image (grayscale, small for clarity) - Sliding kernel window highlighted on input - Kernel matrix with current values displayed - Output image being constructed - Calculation panel showing element-wise products and sum</p> <p>Interactive controls: - Kernel selector: Identity, Blur 3\u00d73, Sharpen, Edge detect - Custom kernel editor: 3\u00d73 grid of editable values - Animation speed slider - Button: \"Step\" (advance one pixel) - Button: \"Run\" (animate full convolution) - Toggle: Show/hide calculation details</p> <p>Canvas layout: 900x600px with input, kernel, and output panels</p> <p>Behavior: - Animate kernel sliding across input image - Show element-wise multiplication at current position - Display running sum being computed - Fill in output pixel when calculation completes - Handle boundary conditions visually</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#blur-filters","title":"Blur Filters","text":"<p>Blur filters smooth images by averaging neighboring pixels, reducing noise and fine detail.</p> <p>Box Blur (Mean Filter):</p> <p>\\(\\mathbf{K}_{box} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\)</p> <p>Every pixel contributes equally. Simple but creates blocky artifacts.</p> <p>Gaussian Blur:</p> <p>\\(\\mathbf{K}_{gauss} = \\frac{1}{16} \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\)</p> <p>Weights follow a Gaussian distribution\u2014center pixels contribute more. Produces smoother, more natural blurring.</p> <p>The general Gaussian kernel is:</p> <p>\\(G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}\\)</p> <p>where \\(\\sigma\\) controls the blur radius. Larger \\(\\sigma\\) means more blurring.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#sharpen-filters","title":"Sharpen Filters","text":"<p>Sharpen filters enhance edges and fine detail by emphasizing differences between neighboring pixels:</p> <p>\\(\\mathbf{K}_{sharpen} = \\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ -1 &amp; 5 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}\\)</p> <p>This kernel can be understood as: - Identity (center = 1) plus - Negative Laplacian (edge enhancement)</p> <p>The result amplifies differences while preserving overall brightness.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-filter-effects-gallery","title":"Diagram: Filter Effects Gallery","text":"Filter Effects Gallery <p>Type: microsim</p> <p>Learning objective: Compare effects of different filters on the same image (Bloom: Evaluate)</p> <p>Visual elements: - Original image (top center) - Grid of filtered results (4-6 variants) - Kernel display below each filtered image - Difference image option (filtered - original)</p> <p>Interactive controls: - Image selector: Choose sample image - Filter checkboxes: Select which filters to display - Slider: Filter strength/radius where applicable - Toggle: Show difference images - Toggle: Show kernel matrices</p> <p>Canvas layout: 900x700px with grid layout</p> <p>Behavior: - All filters applied to same source image - Hover over filtered image to see kernel - Click to show full-size comparison - Side-by-side slider for before/after on selected filter</p> <p>Implementation: p5.js with multiple filter implementations</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#edge-detection","title":"Edge Detection","text":"<p>Edge detection identifies boundaries in images where intensity changes rapidly. Edges correspond to object boundaries, texture changes, and depth discontinuities.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#derivative-based-edge-detection","title":"Derivative-Based Edge Detection","text":"<p>Edges are locations of high gradient magnitude. The image gradient at pixel \\((i, j)\\):</p> <p>\\(\\nabla I = \\begin{bmatrix} \\frac{\\partial I}{\\partial x} \\\\ \\frac{\\partial I}{\\partial y} \\end{bmatrix}\\)</p> <p>The gradient magnitude and direction:</p> <p>\\(|\\nabla I| = \\sqrt{\\left(\\frac{\\partial I}{\\partial x}\\right)^2 + \\left(\\frac{\\partial I}{\\partial y}\\right)^2}\\)</p> <p>\\(\\theta = \\arctan\\left(\\frac{\\partial I / \\partial y}{\\partial I / \\partial x}\\right)\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-sobel-operator","title":"The Sobel Operator","text":"<p>The Sobel operator approximates image derivatives using convolution:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#sobel-kernels","title":"Sobel Kernels","text":"<p>\\(\\mathbf{G}_x = \\begin{bmatrix} -1 &amp; 0 &amp; +1 \\\\ -2 &amp; 0 &amp; +2 \\\\ -1 &amp; 0 &amp; +1 \\end{bmatrix}, \\quad \\mathbf{G}_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ +1 &amp; +2 &amp; +1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{G}_x\\) detects vertical edges (horizontal gradient)</li> <li>\\(\\mathbf{G}_y\\) detects horizontal edges (vertical gradient)</li> <li>The weights provide smoothing (via the 1-2-1 pattern) combined with differentiation</li> </ul> <p>The Sobel operator combines Gaussian smoothing with differentiation, making it more robust to noise than simple difference operators.</p> Edge Detector Kernel Size Noise Sensitivity Edge Localization Simple difference 1\u00d72 High Good Prewitt 3\u00d73 Medium Good Sobel 3\u00d73 Low Good Scharr 3\u00d73 Low Better rotational symmetry"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-edge-detection-visualizer","title":"Diagram: Edge Detection Visualizer","text":"Edge Detection Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Sobel operators detect edges in different orientations (Bloom: Analyze)</p> <p>Visual elements: - Original grayscale image - Gx result (horizontal gradient) with positive/negative coloring - Gy result (vertical gradient) with positive/negative coloring - Gradient magnitude image - Gradient direction overlay (as arrows or color wheel)</p> <p>Interactive controls: - Image selector: Simple shapes, photos, text - Toggle: Show Gx, Gy, magnitude, direction - Slider: Threshold for edge display - Toggle: Show gradient arrows at sample points - Dropdown: Edge detector (Sobel, Prewitt, Scharr)</p> <p>Canvas layout: 850x650px with multi-panel display</p> <p>Behavior: - Compute and display gradient components - Color-code positive (white) and negative (black) gradients - Threshold slider converts magnitude to binary edges - Arrow overlay shows gradient direction at grid points</p> <p>Implementation: p5.js with Sobel convolution</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#edge-detection-pipeline","title":"Edge Detection Pipeline","text":"<p>Complete edge detection typically involves:</p> <ol> <li>Smoothing: Gaussian blur to reduce noise</li> <li>Gradient computation: Sobel or similar operator</li> <li>Non-maximum suppression: Thin edges to single-pixel width</li> <li>Thresholding: Keep only strong edges (hysteresis thresholding in Canny)</li> </ol> <p>The Canny edge detector implements this full pipeline and remains widely used.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#fourier-transform-for-images","title":"Fourier Transform for Images","text":"<p>The Fourier transform represents images in the frequency domain, revealing periodic patterns and enabling frequency-based filtering.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-2d-discrete-fourier-transform","title":"The 2D Discrete Fourier Transform","text":"<p>For an \\(M \\times N\\) image, the 2D DFT is:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#2d-discrete-fourier-transform","title":"2D Discrete Fourier Transform","text":"<p>\\(F(u, v) = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x, y) \\cdot e^{-2\\pi i \\left(\\frac{ux}{M} + \\frac{vy}{N}\\right)}\\)</p> <p>where:</p> <ul> <li>\\(f(x, y)\\) is the spatial domain image</li> <li>\\(F(u, v)\\) is the frequency domain representation</li> <li>\\((u, v)\\) are frequency coordinates</li> <li>Low frequencies (near center) represent gradual changes</li> <li>High frequencies (near edges) represent rapid changes</li> </ul> <p>The inverse transform recovers the original image:</p> <p>\\(f(x, y) = \\frac{1}{MN} \\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} F(u, v) \\cdot e^{2\\pi i \\left(\\frac{ux}{M} + \\frac{vy}{N}\\right)}\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#frequency-domain-interpretation","title":"Frequency Domain Interpretation","text":"Frequency Region Spatial Meaning DC component (center) Average brightness Low frequencies Smooth regions, gradual changes High frequencies Edges, textures, fine detail Specific frequency Periodic patterns (e.g., stripes) <p>The magnitude spectrum \\(|F(u,v)|\\) shows which frequencies are present.</p> <p>The phase spectrum \\(\\angle F(u,v)\\) encodes spatial structure\u2014surprisingly, phase carries more perceptual information than magnitude.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-fourier-transform-visualizer","title":"Diagram: Fourier Transform Visualizer","text":"Fourier Transform Visualizer <p>Type: microsim</p> <p>Learning objective: Connect spatial image features to frequency domain representation (Bloom: Analyze)</p> <p>Visual elements: - Original image (left) - Magnitude spectrum (center, log-scaled for visibility) - Phase spectrum (right, as color wheel or grayscale) - Reconstructed image after filtering</p> <p>Interactive controls: - Image selector: Stripes, checkerboard, photo, text - Frequency filter: Low-pass, high-pass, band-pass (adjustable radius) - Toggle: Show magnitude/phase/both - Slider: Filter cutoff frequency - Button: Apply filter and show reconstruction</p> <p>Canvas layout: 900x600px with three-panel display</p> <p>Behavior: - Compute and display FFT magnitude (log scale) - Interactive frequency selection by clicking on spectrum - Show effect of zeroing selected frequencies - Real-time filter preview on reconstruction</p> <p>Implementation: p5.js with FFT library (or pre-computed examples)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#frequency-domain-filtering","title":"Frequency Domain Filtering","text":"<p>Filtering in the frequency domain involves:</p> <ol> <li>Compute FFT of image: \\(F = \\mathcal{F}(f)\\)</li> <li>Multiply by filter: \\(G = F \\cdot H\\)</li> <li>Compute inverse FFT: \\(g = \\mathcal{F}^{-1}(G)\\)</li> </ol> <p>Common frequency domain filters:</p> Filter Frequency Mask Effect Ideal low-pass 1 if \\(\\sqrt{u^2+v^2} &lt; r\\), else 0 Blur (with ringing) Gaussian low-pass \\(e^{-(u^2+v^2)/2\\sigma^2}\\) Smooth blur Ideal high-pass 0 if \\(\\sqrt{u^2+v^2} &lt; r\\), else 1 Edge enhancement Notch filter 0 at specific frequencies Remove periodic noise <p>The convolution theorem connects spatial and frequency domains:</p> <p>\\(f * g = \\mathcal{F}^{-1}(\\mathcal{F}(f) \\cdot \\mathcal{F}(g))\\)</p> <p>Convolution in spatial domain equals multiplication in frequency domain.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-compression","title":"Image Compression","text":"<p>Image compression reduces storage size by exploiting redundancies in image data. Linear algebra provides key tools for lossy compression.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#svd-based-compression","title":"SVD-Based Compression","text":"<p>The Singular Value Decomposition represents an image as a sum of rank-1 matrices:</p> <p>\\(\\mathbf{I} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_i\\) are singular values (in decreasing order)</li> <li>\\(\\mathbf{u}_i, \\mathbf{v}_i\\) are left and right singular vectors</li> <li>\\(r\\) is the rank of the image matrix</li> </ul> <p>Keeping only the \\(k\\) largest singular values gives the best rank-\\(k\\) approximation:</p> <p>\\(\\mathbf{I}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\)</p> Original Storage Compressed Storage Compression Ratio \\(H \\times W\\) values \\(k(H + W + 1)\\) values \\(\\frac{HW}{k(H+W+1)}\\) <p>For a 1000\u00d71000 image with \\(k=50\\): compression ratio \u2248 10:1.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-svd-compression-visualizer","title":"Diagram: SVD Compression Visualizer","text":"SVD Compression Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how SVD achieves image compression by truncating singular values (Bloom: Apply)</p> <p>Visual elements: - Original image - Reconstructed image from k singular values - Singular value plot (bar chart or line, log scale) - Error image (original - reconstructed) - Compression statistics display</p> <p>Interactive controls: - Slider: Number of singular values k (1 to full rank) - Image selector: Choose sample image - Toggle: Show error image - Toggle: Log scale for singular values - Display: Compression ratio, PSNR, storage size</p> <p>Canvas layout: 850x650px with image comparison and statistics</p> <p>Behavior: - Real-time reconstruction as k changes - Show how image quality improves with more singular values - Display storage calculation - Highlight \"knee\" in singular value curve</p> <p>Implementation: p5.js with pre-computed SVD (for performance)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#transform-coding-jpeg","title":"Transform Coding (JPEG)","text":"<p>JPEG compression uses the Discrete Cosine Transform (DCT):</p> <ol> <li>Divide image into 8\u00d78 blocks</li> <li>Apply 2D DCT to each block</li> <li>Quantize coefficients (lossy step)</li> <li>Entropy encode (lossless)</li> </ol> <p>The DCT concentrates energy in low-frequency coefficients, which are preserved, while high-frequency coefficients (fine detail) are quantized more aggressively.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#comparison-of-compression-approaches","title":"Comparison of Compression Approaches","text":"Method Type Compression Quality Control SVD truncation Lossy Variable Number of singular values JPEG (DCT) Lossy High Quality factor (1-100) PNG (lossless) Lossless Moderate N/A WebP Both High Quality factor"},{"location":"chapters/13-image-processing-and-computer-vision/#feature-detection","title":"Feature Detection","text":"<p>Feature detection identifies distinctive points in images that can be matched across views, enabling applications like image stitching, object recognition, and 3D reconstruction.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#what-makes-a-good-feature","title":"What Makes a Good Feature?","text":"<p>A good feature should be:</p> <ul> <li>Distinctive: Different from its neighbors</li> <li>Repeatable: Detectable despite changes in viewpoint, lighting, scale</li> <li>Localizable: Precisely positioned</li> </ul> Feature Type Detection Method Invariance Corners Harris, Shi-Tomasi Rotation Blobs DoG (SIFT), Hessian (SURF) Rotation, Scale Edges Canny, Sobel Limited"},{"location":"chapters/13-image-processing-and-computer-vision/#harris-corner-detection","title":"Harris Corner Detection","text":"<p>The Harris corner detector uses the structure tensor (second moment matrix):</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#structure-tensor","title":"Structure Tensor","text":"<p>\\(\\mathbf{M} = \\sum_{(x,y) \\in W} \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(I_x, I_y\\) are image derivatives</li> <li>\\(W\\) is a local window (often Gaussian-weighted)</li> <li>The eigenvalues of \\(\\mathbf{M}\\) characterize the local structure</li> </ul> Eigenvalue Pattern Structure Feature Type Both small Flat region Not a feature One large, one small Edge Edge point Both large Corner Good feature <p>The Harris corner response:</p> <p>\\(R = \\det(\\mathbf{M}) - k \\cdot \\text{trace}(\\mathbf{M})^2 = \\lambda_1 \\lambda_2 - k(\\lambda_1 + \\lambda_2)^2\\)</p> <p>where \\(k \\approx 0.04\\). Corners have high \\(R\\) values.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-corner-detection-visualizer","title":"Diagram: Corner Detection Visualizer","text":"Corner Detection Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how eigenvalues of the structure tensor identify corners (Bloom: Analyze)</p> <p>Visual elements: - Input image with detected corners marked - Structure tensor ellipse visualization at selected points - Eigenvalue scatter plot (\u03bb1 vs \u03bb2) - Corner response heat map</p> <p>Interactive controls: - Image selector: Checkerboard, building, shapes - Slider: Harris k parameter (0.01 to 0.1) - Slider: Response threshold - Click on image to show local structure tensor - Toggle: Show response heat map vs corner points</p> <p>Canvas layout: 850x650px with image and analysis panels</p> <p>Behavior: - Compute Harris response for entire image - Mark corners above threshold - Click on pixel to show structure tensor as ellipse - Ellipse axes aligned with eigenvectors, lengths proportional to eigenvalues</p> <p>Implementation: p5.js with gradient and matrix computation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#scale-invariant-features","title":"Scale-Invariant Features","text":"<p>SIFT (Scale-Invariant Feature Transform) detects features at multiple scales:</p> <ol> <li>Build scale space using Gaussian blur at multiple \\(\\sigma\\) values</li> <li>Detect blob-like structures as extrema in Difference-of-Gaussian (DoG)</li> <li>Compute orientation from local gradients</li> <li>Build descriptor from gradient histograms</li> </ol> <p>The 128-dimensional SIFT descriptor is robust to scale, rotation, and moderate viewpoint changes.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#homography","title":"Homography","text":"<p>A homography is a projective transformation between two planes, essential for perspective correction, panorama stitching, and augmented reality.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-homography-matrix","title":"The Homography Matrix","text":"<p>A homography maps points from one image to another via a 3\u00d73 matrix:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#homography-transformation","title":"Homography Transformation","text":"<p>\\(\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\sim \\mathbf{H} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} h_{11} &amp; h_{12} &amp; h_{13} \\\\ h_{21} &amp; h_{22} &amp; h_{23} \\\\ h_{31} &amp; h_{32} &amp; h_{33} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\((x, y)\\) and \\((x', y')\\) are corresponding points in homogeneous coordinates</li> <li>\\(\\sim\\) denotes equality up to scale</li> <li>\\(\\mathbf{H}\\) has 8 degrees of freedom (9 elements minus 1 for scale)</li> </ul> <p>To recover Cartesian coordinates:</p> <p>\\(x' = \\frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}, \\quad y' = \\frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#estimating-homographies","title":"Estimating Homographies","text":"<p>Given \\(n \\geq 4\\) point correspondences, we can estimate \\(\\mathbf{H}\\) by solving a linear system.</p> <p>Each correspondence \\((x_i, y_i) \\leftrightarrow (x_i', y_i')\\) provides two equations:</p> <p>\\(\\begin{bmatrix} x_i &amp; y_i &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -x_i'x_i &amp; -x_i'y_i &amp; -x_i' \\\\ 0 &amp; 0 &amp; 0 &amp; x_i &amp; y_i &amp; 1 &amp; -y_i'x_i &amp; -y_i'y_i &amp; -y_i' \\end{bmatrix} \\mathbf{h} = \\mathbf{0}\\)</p> <p>With 4+ correspondences, solve using SVD (find the null space of the coefficient matrix).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#transformation-hierarchy","title":"Transformation Hierarchy","text":"<p>Homographies are part of a hierarchy of 2D transformations:</p> Transformation DOF Preserves Matrix Form Translation 2 Everything $[I Euclidean (rigid) 3 Distances, angles $[R Similarity 4 Angles, ratios $[sR Affine 6 Parallelism $[A Projective (homography) 8 Straight lines General 3\u00d73"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-homography-transformation-demo","title":"Diagram: Homography Transformation Demo","text":"Homography Transformation Demo <p>Type: microsim</p> <p>Learning objective: Understand how homographies transform images for perspective correction (Bloom: Apply)</p> <p>Visual elements: - Source image with draggable corner points - Transformed image showing perspective effect - Homography matrix display - Grid overlay showing transformation</p> <p>Interactive controls: - Drag corners of source quadrilateral - Preset buttons: Perspective, rotation, shear, stretch - Button: \"Reset to identity\" - Toggle: Show transformation grid - Display: Homography matrix values</p> <p>Canvas layout: 800x600px with side-by-side source and result</p> <p>Behavior: - Compute homography from corner positions in real-time - Apply inverse warp to display transformed image - Show how parallel lines may not remain parallel - Demonstrate perspective correction (make rectangle from trapezoid)</p> <p>Implementation: p5.js with bilinear interpolation for warping</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#applications-of-homography","title":"Applications of Homography","text":"Application How Homography Is Used Panorama stitching Align overlapping images Perspective correction Straighten tilted documents/signs Augmented reality Overlay virtual objects on planar surfaces Sports graphics Insert ads on playing field Image rectification Correct lens distortion"},{"location":"chapters/13-image-processing-and-computer-vision/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\ndef compute_homography(src_pts, dst_pts):\n    \"\"\"\n    Compute homography from 4+ point correspondences.\n    src_pts, dst_pts: Nx2 arrays of corresponding points\n    \"\"\"\n    n = src_pts.shape[0]\n    A = []\n\n    for i in range(n):\n        x, y = src_pts[i]\n        xp, yp = dst_pts[i]\n        A.append([-x, -y, -1, 0, 0, 0, x*xp, y*xp, xp])\n        A.append([0, 0, 0, -x, -y, -1, x*yp, y*yp, yp])\n\n    A = np.array(A)\n\n    # Solve using SVD\n    _, _, Vt = np.linalg.svd(A)\n    H = Vt[-1].reshape(3, 3)\n\n    return H / H[2, 2]  # Normalize\n\ndef apply_homography(H, point):\n    \"\"\"Apply homography to a single point.\"\"\"\n    x, y = point\n    p = np.array([x, y, 1])\n    p_prime = H @ p\n    return p_prime[:2] / p_prime[2]\n</code></pre>"},{"location":"chapters/13-image-processing-and-computer-vision/#summary_1","title":"Summary","text":"<p>Image processing and computer vision are built on linear algebra foundations:</p> <p>Image Representation:</p> <ul> <li>Images are matrices (grayscale) or tensors (color)</li> <li>RGB images have three channels; other color spaces serve specific purposes</li> <li>Color space transforms are matrix operations</li> </ul> <p>Filtering and Convolution:</p> <ul> <li>Convolution applies kernels to extract features</li> <li>Blur filters average neighbors; sharpen filters enhance differences</li> <li>Edge detection uses derivative-approximating kernels like Sobel</li> </ul> <p>Frequency Domain:</p> <ul> <li>Fourier transform reveals periodic structure</li> <li>Low frequencies encode smooth regions; high frequencies encode edges</li> <li>Frequency filtering multiplies spectra</li> </ul> <p>Compression:</p> <ul> <li>SVD truncation provides rank-based compression</li> <li>Transform coding (DCT/JPEG) exploits frequency concentration</li> </ul> <p>Features and Geometry:</p> <ul> <li>Corner detection uses structure tensor eigenvalues</li> <li>Homographies are projective transformations between planes</li> <li>4 point correspondences determine a homography</li> </ul>"},{"location":"chapters/13-image-processing-and-computer-vision/#self-check-questions","title":"Self-Check Questions","text":"Why does Gaussian blur produce smoother results than box blur? <p>Box blur weights all pixels in the kernel equally, creating abrupt transitions at the kernel boundary. This causes visible artifacts, especially in smooth gradients. Gaussian blur weights central pixels more heavily, with weights falling off smoothly according to the Gaussian function. This smooth weighting produces gradual blending without sharp discontinuities. Additionally, the Gaussian kernel is separable, meaning a 2D Gaussian blur can be computed as two 1D blurs, making it computationally efficient.</p> How does the Sobel operator combine smoothing with differentiation? <p>The Sobel kernels have a specific structure: they can be decomposed as the outer product of a smoothing filter [1, 2, 1] and a difference filter [-1, 0, 1]. For example, Gx = [1, 2, 1]^T \u00d7 [-1, 0, 1]. The [1, 2, 1] component provides Gaussian-like smoothing perpendicular to the derivative direction, reducing noise sensitivity. The [-1, 0, 1] component computes the derivative. This combination makes Sobel more robust than simple difference operators while maintaining good edge localization.</p> Why does keeping the largest singular values give the best low-rank approximation? <p>The Eckart-Young-Mirsky theorem proves that the truncated SVD gives the optimal low-rank approximation in both Frobenius and spectral norms. Intuitively, each singular value represents the \"energy\" captured by that component\u2014larger singular values correspond to more significant patterns in the image. By keeping the largest singular values, we retain the most important structure while discarding fine details associated with smaller singular values. The approximation error equals the sum of squared discarded singular values.</p> What is the geometric meaning of the Harris corner response being large? <p>The Harris response R = \u03bb\u2081\u03bb\u2082 - k(\u03bb\u2081 + \u03bb\u2082)\u00b2 is large when both eigenvalues of the structure tensor are large. The eigenvalues measure how quickly image intensity changes in the principal directions. At a corner, intensity changes significantly in both directions (both eigenvalues large). At an edge, intensity changes in only one direction (one eigenvalue large, one small). In a flat region, intensity is constant (both eigenvalues small). The product \u03bb\u2081\u03bb\u2082 in the response ensures both must be large.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/","title":"Quiz: Image Processing and Computer Vision","text":"<p>Test your understanding of image representation, filtering, and computer vision concepts.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#1-a-grayscale-image-is-represented-as","title":"1. A grayscale image is represented as:","text":"<ol> <li>A 3D tensor with RGB channels</li> <li>A 2D matrix of intensity values</li> <li>A 1D vector of pixel positions</li> <li>A list of color names</li> </ol> Show Answer <p>The correct answer is B. A grayscale image is a 2D matrix where each entry represents the intensity (brightness) of a pixel, typically ranging from 0 (black) to 255 (white).</p> <p>Concept Tested: Grayscale Image</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#2-an-rgb-image-is-stored-as","title":"2. An RGB image is stored as:","text":"<ol> <li>A single 2D matrix</li> <li>Three separate matrices (one per color channel)</li> <li>A 1D array of values</li> <li>A text file</li> </ol> Show Answer <p>The correct answer is B. An RGB image consists of three 2D matrices (or a 3D tensor), one for each color channel: Red, Green, and Blue. Each pixel has three values.</p> <p>Concept Tested: RGB Image</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#3-image-convolution","title":"3. Image convolution:","text":"<ol> <li>Increases image resolution</li> <li>Applies a kernel to extract features or transform the image</li> <li>Converts color to grayscale</li> <li>Compresses the image</li> </ol> Show Answer <p>The correct answer is B. Convolution slides a kernel (small matrix) across the image, computing weighted sums at each position. Different kernels produce different effects like blurring, sharpening, or edge detection.</p> <p>Concept Tested: Image Convolution</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#4-a-gaussian-blur-filter","title":"4. A Gaussian blur filter:","text":"<ol> <li>Sharpens edges in the image</li> <li>Smooths the image by averaging with Gaussian-weighted neighbors</li> <li>Detects vertical edges only</li> <li>Increases image contrast</li> </ol> Show Answer <p>The correct answer is B. Gaussian blur uses weights that follow a Gaussian distribution, giving more weight to nearby pixels. This produces smooth, natural-looking blur without blocky artifacts.</p> <p>Concept Tested: Blur Filter</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#5-the-sobel-operator-is-used-for","title":"5. The Sobel operator is used for:","text":"<ol> <li>Image compression</li> <li>Edge detection by computing image gradients</li> <li>Color space conversion</li> <li>Image resizing</li> </ol> Show Answer <p>The correct answer is B. The Sobel operator approximates image derivatives (gradients) to detect edges. It uses two kernels: one for horizontal gradients (\\(G_x\\)) and one for vertical gradients (\\(G_y\\)).</p> <p>Concept Tested: Sobel Operator</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#6-the-fourier-transform-of-an-image-reveals","title":"6. The Fourier transform of an image reveals:","text":"<ol> <li>The color distribution</li> <li>Frequency components (how quickly intensity changes)</li> <li>The number of objects</li> <li>The image dimensions</li> </ol> Show Answer <p>The correct answer is B. The Fourier transform decomposes an image into frequency components. Low frequencies represent gradual changes (smooth areas); high frequencies represent rapid changes (edges, textures).</p> <p>Concept Tested: Fourier Transform</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#7-svd-based-image-compression-works-by","title":"7. SVD-based image compression works by:","text":"<ol> <li>Removing all color information</li> <li>Keeping only the largest singular values and discarding the rest</li> <li>Reducing the number of pixels</li> <li>Converting to a different file format</li> </ol> Show Answer <p>The correct answer is B. SVD compression keeps only the \\(k\\) largest singular values, which capture the most important structure. The approximation \\(A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T\\) requires less storage.</p> <p>Concept Tested: Image Compression</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#8-in-harris-corner-detection-a-corner-is-characterized-by","title":"8. In Harris corner detection, a corner is characterized by:","text":"<ol> <li>Only one large eigenvalue of the structure tensor</li> <li>Two large eigenvalues of the structure tensor</li> <li>Zero eigenvalues</li> <li>Negative eigenvalues</li> </ol> Show Answer <p>The correct answer is B. At a corner, the structure tensor has two large eigenvalues, indicating significant intensity changes in both directions. An edge has one large eigenvalue; flat regions have none.</p> <p>Concept Tested: Feature Detection</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#9-a-homography-transformation","title":"9. A homography transformation:","text":"<ol> <li>Only handles rotation</li> <li>Maps points between two planes, handling perspective</li> <li>Is always the identity</li> <li>Only works on 1D signals</li> </ol> Show Answer <p>The correct answer is B. A homography is a 3\u00d73 projective transformation that maps points between two planes. It can represent perspective distortion, rotation, scaling, and translation.</p> <p>Concept Tested: Homography</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#10-the-convolution-theorem-states-that-convolution-in-spatial-domain-equals","title":"10. The convolution theorem states that convolution in spatial domain equals:","text":"<ol> <li>Addition in frequency domain</li> <li>Multiplication in frequency domain</li> <li>Division in frequency domain</li> <li>Convolution in frequency domain</li> </ol> Show Answer <p>The correct answer is B. The convolution theorem states that \\(f * g = \\mathcal{F}^{-1}(\\mathcal{F}(f) \\cdot \\mathcal{F}(g))\\). Convolution in spatial domain corresponds to element-wise multiplication in frequency domain.</p> <p>Concept Tested: Frequency Domain</p>"},{"location":"chapters/14-3d-geometry-and-transformations/","title":"3D Geometry and Transformations","text":""},{"location":"chapters/14-3d-geometry-and-transformations/#summary","title":"Summary","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers 3D coordinate systems, rotation representations including Euler angles and quaternions, homogeneous coordinates, camera models with intrinsic and extrinsic parameters, stereo vision, and point cloud processing. These concepts form the foundation for any system that operates in 3D space.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 13: Image Processing and Computer Vision</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#introduction","title":"Introduction","text":"<p>From robot arms assembling cars to AR headsets overlaying digital content on the physical world, systems that interact with 3D space require precise mathematical representations of position, orientation, and motion. Linear algebra provides the essential tools:</p> <ul> <li>Coordinate systems establish frames of reference</li> <li>Rotation matrices and quaternions represent orientation</li> <li>Homogeneous coordinates unify rotation and translation</li> <li>Camera models project 3D scenes onto 2D images</li> <li>Stereo vision recovers 3D structure from multiple views</li> </ul> <p>This chapter develops the geometric foundations needed for robotics, computer vision, augmented reality, and autonomous systems.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#3d-coordinate-systems","title":"3D Coordinate Systems","text":"<p>A 3D coordinate system defines how we specify positions in three-dimensional space using three numbers \\((x, y, z)\\).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#right-hand-vs-left-hand-systems","title":"Right-Hand vs Left-Hand Systems","text":"<p>Two conventions exist for orienting the axes:</p> Convention X Direction Y Direction Z Direction Used By Right-hand Right/East Up/North Out of screen OpenGL, ROS, most math Left-hand Right Up Into screen DirectX, Unity <p>In a right-hand system, if you curl your right hand's fingers from X toward Y, your thumb points in the Z direction.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#common-coordinate-frames","title":"Common Coordinate Frames","text":"<p>Different applications use different conventions:</p> Domain X Y Z Origin Computer graphics Right Up Out (toward viewer) Screen center Robotics (ROS) Forward Left Up Robot base Aviation Forward Right Down Aircraft center Camera Right Down Forward (optical axis) Camera center <p>Understanding and converting between coordinate frames is essential for integrating sensors, actuators, and displays.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-3d-coordinate-system-visualizer","title":"Diagram: 3D Coordinate System Visualizer","text":"3D Coordinate System Visualizer <p>Type: microsim</p> <p>Learning objective: Understand different 3D coordinate system conventions and handedness (Bloom: Understand)</p> <p>Visual elements: - 3D view with X, Y, Z axes (color coded: X=red, Y=green, Z=blue) - Origin sphere - Sample point with coordinates displayed - Grid planes (XY, XZ, YZ) toggleable - Animated right-hand rule demonstration</p> <p>Interactive controls: - Toggle: Right-hand vs Left-hand system - Dropdown: Convention preset (OpenGL, DirectX, ROS, Camera) - Draggable point in 3D space - Rotate view with mouse drag - Toggle grid planes on/off</p> <p>Canvas layout: 700x600px with 3D view using WEBGL</p> <p>Behavior: - Switching handedness mirrors the Z axis - Presets reorient axes according to convention - Coordinate display updates as point moves - Show transformation matrix between conventions</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>Homogeneous coordinates extend Cartesian coordinates with an additional dimension, enabling rotation and translation to be combined in a single matrix multiplication.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#from-cartesian-to-homogeneous","title":"From Cartesian to Homogeneous","text":"<p>A 3D point \\((x, y, z)\\) becomes a 4D homogeneous point:</p> <p>\\((x, y, z) \\rightarrow \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}\\)</p> <p>To convert back, divide by the fourth component:</p> <p>\\(\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ W \\end{bmatrix} \\rightarrow \\left(\\frac{X}{W}, \\frac{Y}{W}, \\frac{Z}{W}\\right)\\)</p> <p>Points with \\(W = 0\\) represent directions (points at infinity).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#why-homogeneous-coordinates","title":"Why Homogeneous Coordinates?","text":"<p>In standard 3D coordinates:</p> <ul> <li>Rotation: \\(\\mathbf{p}' = \\mathbf{R}\\mathbf{p}\\) (matrix multiplication)</li> <li>Translation: \\(\\mathbf{p}' = \\mathbf{p} + \\mathbf{t}\\) (addition)</li> </ul> <p>We cannot combine these into a single matrix. With homogeneous coordinates:</p> <p>\\(\\begin{bmatrix} \\mathbf{p}' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{R} &amp; \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{p} \\\\ 1 \\end{bmatrix}\\)</p> <p>Now both operations are matrix multiplications, enabling efficient composition of multiple transformations.</p> Operation 3\u00d73 Matrix 4\u00d74 Homogeneous Rotation only Yes Yes Translation only No Yes Scale Yes Yes Rotation + Translation No Yes Projection No Yes"},{"location":"chapters/14-3d-geometry-and-transformations/#rotation-representations","title":"Rotation Representations","text":"<p>Representing 3D rotations is surprisingly subtle. Several representations exist, each with trade-offs.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#euler-angles","title":"Euler Angles","text":"<p>Euler angles describe a rotation as three successive rotations about coordinate axes. The most common convention is:</p> <ol> <li>Rotate by \\(\\psi\\) (yaw) about Z axis</li> <li>Rotate by \\(\\theta\\) (pitch) about Y axis</li> <li>Rotate by \\(\\phi\\) (roll) about X axis</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#euler-angle-rotation-matrices","title":"Euler Angle Rotation Matrices","text":"<p>\\(\\mathbf{R}_z(\\psi) = \\begin{bmatrix} \\cos\\psi &amp; -\\sin\\psi &amp; 0 \\\\ \\sin\\psi &amp; \\cos\\psi &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>\\(\\mathbf{R}_y(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; 0 &amp; \\sin\\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\theta &amp; 0 &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>\\(\\mathbf{R}_x(\\phi) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\phi &amp; -\\sin\\phi \\\\ 0 &amp; \\sin\\phi &amp; \\cos\\phi \\end{bmatrix}\\)</p> <p>The combined rotation: \\(\\mathbf{R} = \\mathbf{R}_z(\\psi) \\mathbf{R}_y(\\theta) \\mathbf{R}_x(\\phi)\\)</p> <p>Different conventions exist (XYZ, ZYX, ZXZ, etc.)\u2014always verify which convention is used!</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-euler-angles-visualizer","title":"Diagram: Euler Angles Visualizer","text":"Euler Angles Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Euler angles compose to form rotations (Bloom: Apply)</p> <p>Visual elements: - 3D coordinate frame (object frame) - Reference coordinate frame (world frame) - 3D object (e.g., airplane or box) showing orientation - Arc indicators for each angle - Rotation axis highlighted during animation</p> <p>Interactive controls: - Slider: Yaw (\u03c8) from -180\u00b0 to 180\u00b0 - Slider: Pitch (\u03b8) from -90\u00b0 to 90\u00b0 - Slider: Roll (\u03c6) from -180\u00b0 to 180\u00b0 - Dropdown: Convention (ZYX, XYZ, ZXZ) - Button: \"Animate sequence\" (show rotations step by step) - Button: \"Reset\"</p> <p>Canvas layout: 750x600px with 3D WEBGL view</p> <p>Behavior: - Update object orientation in real-time as sliders change - Animation shows each rotation applied sequentially - Display rotation matrix numerically - Highlight gimbal lock region when pitch approaches \u00b190\u00b0</p> <p>Implementation: p5.js with WEBGL and matrix computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#gimbal-lock","title":"Gimbal Lock","text":"<p>Gimbal lock occurs when two rotation axes align, causing loss of one degree of freedom. In the ZYX convention, when pitch \\(\\theta = \\pm 90\u00b0\\):</p> <p>\\(\\mathbf{R} = \\begin{bmatrix} 0 &amp; \\mp\\sin(\\psi \\mp \\phi) &amp; \\pm\\cos(\\psi \\mp \\phi) \\\\ 0 &amp; \\cos(\\psi \\mp \\phi) &amp; \\sin(\\psi \\mp \\phi) \\\\ \\mp 1 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> <p>Notice that yaw and roll combine into a single term \\((\\psi \\mp \\phi)\\)\u2014we can only control their sum or difference, not each independently. The system has lost a degree of freedom.</p> Pitch Value Effect \\(\\theta = 0\u00b0\\) Full 3 DOF, no gimbal lock \\(\\theta = \\pm 45\u00b0\\) Slight coupling between yaw/roll \\(\\theta = \\pm 90\u00b0\\) Complete gimbal lock, only 2 DOF <p>Gimbal lock is a fundamental problem with Euler angles, not a bug in implementation. It motivated the development of quaternions.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-gimbal-lock-demonstration","title":"Diagram: Gimbal Lock Demonstration","text":"Gimbal Lock Demonstration <p>Type: microsim</p> <p>Learning objective: Experience gimbal lock and understand why it occurs (Bloom: Analyze)</p> <p>Visual elements: - Physical gimbal mechanism (three nested rings) - Object attached to innermost ring - Color-coded rotation axes - Warning indicator when approaching gimbal lock</p> <p>Interactive controls: - Slider: Outer ring rotation (yaw) - Slider: Middle ring rotation (pitch) - Slider: Inner ring rotation (roll) - Button: \"Go to gimbal lock\" (set pitch to 90\u00b0) - Button: \"Reset\"</p> <p>Canvas layout: 700x600px with 3D gimbal visualization</p> <p>Behavior: - As pitch approaches 90\u00b0, outer and inner rings align visually - At gimbal lock, changing yaw and roll produces same motion - Display effective degrees of freedom (3 normally, 2 at lock) - Show that some orientations become unreachable</p> <p>Implementation: p5.js with WEBGL, mechanical gimbal model</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternions","title":"Quaternions","text":"<p>Quaternions provide a singularity-free representation of 3D rotations, avoiding gimbal lock.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-definition","title":"Quaternion Definition","text":"<p>A quaternion has four components:</p> <p>\\(\\mathbf{q} = w + xi + yj + zk = (w, x, y, z)\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the scalar (real) part</li> <li>\\((x, y, z)\\) is the vector (imaginary) part</li> <li>\\(i, j, k\\) satisfy: \\(i^2 = j^2 = k^2 = ijk = -1\\)</li> </ul> <p>For representing rotations, we use unit quaternions with \\(\\|\\mathbf{q}\\| = 1\\):</p> <p>\\(w^2 + x^2 + y^2 + z^2 = 1\\)</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-from-axis-angle","title":"Quaternion from Axis-Angle","text":"<p>A rotation of angle \\(\\theta\\) about unit axis \\(\\hat{\\mathbf{n}} = (n_x, n_y, n_z)\\):</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#axis-angle-to-quaternion","title":"Axis-Angle to Quaternion","text":"<p>\\(\\mathbf{q} = \\left(\\cos\\frac{\\theta}{2}, \\sin\\frac{\\theta}{2} \\cdot n_x, \\sin\\frac{\\theta}{2} \\cdot n_y, \\sin\\frac{\\theta}{2} \\cdot n_z\\right)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the rotation angle</li> <li>\\(\\hat{\\mathbf{n}}\\) is the rotation axis (unit vector)</li> <li>Note the half-angle: rotating by \\(\\theta\\) uses \\(\\frac{\\theta}{2}\\) in the quaternion</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-rotation","title":"Quaternion Rotation","text":"<p>To rotate a point \\(\\mathbf{p} = (p_x, p_y, p_z)\\) by quaternion \\(\\mathbf{q}\\):</p> <ol> <li>Express the point as a pure quaternion: \\(\\mathbf{p}_q = (0, p_x, p_y, p_z)\\)</li> <li>Compute: \\(\\mathbf{p}'_q = \\mathbf{q} \\cdot \\mathbf{p}_q \\cdot \\mathbf{q}^*\\)</li> <li>Extract the vector part as the rotated point</li> </ol> <p>where \\(\\mathbf{q}^* = (w, -x, -y, -z)\\) is the quaternion conjugate.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-multiplication","title":"Quaternion Multiplication","text":"<p>Quaternion multiplication (non-commutative):</p> <p>\\(\\mathbf{q}_1 \\cdot \\mathbf{q}_2 = (w_1 w_2 - \\mathbf{v}_1 \\cdot \\mathbf{v}_2, \\; w_1 \\mathbf{v}_2 + w_2 \\mathbf{v}_1 + \\mathbf{v}_1 \\times \\mathbf{v}_2)\\)</p> <p>Composing rotations: \\(\\mathbf{q}_{total} = \\mathbf{q}_2 \\cdot \\mathbf{q}_1\\) (apply \\(\\mathbf{q}_1\\) first, then \\(\\mathbf{q}_2\\))</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#comparison-of-rotation-representations","title":"Comparison of Rotation Representations","text":"Representation Parameters Singularities Interpolation Composition Euler angles 3 Gimbal lock Poor Matrix mult Rotation matrix 9 (with constraints) None Complex Matrix mult Axis-angle 4 (3 + 1) At \\(\\theta = 0\\) Linear Complex Quaternion 4 (unit constraint) None SLERP Quaternion mult"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-quaternion-rotation-visualizer","title":"Diagram: Quaternion Rotation Visualizer","text":"Quaternion Rotation Visualizer <p>Type: microsim</p> <p>Learning objective: Understand quaternion representation and rotation operation (Bloom: Apply)</p> <p>Visual elements: - 3D object with current orientation - Rotation axis displayed as arrow through origin - Arc showing rotation angle - Quaternion components displayed numerically - Unit sphere showing quaternion (projected)</p> <p>Interactive controls: - Axis input: Unit vector (nx, ny, nz) or draggable direction - Slider: Rotation angle \u03b8 (0\u00b0 to 360\u00b0) - Button: \"Apply rotation\" - Button: \"Compose with previous\" - Toggle: Show equivalent Euler angles</p> <p>Canvas layout: 800x600px with 3D visualization</p> <p>Behavior: - Display quaternion (w, x, y, z) updating with axis/angle - Show rotation applied to 3D object - Demonstrate composition of multiple rotations - Compare with Euler angle representation - Show smooth interpolation (SLERP) between orientations</p> <p>Implementation: p5.js with WEBGL and quaternion math</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#rigid-body-transforms","title":"Rigid Body Transforms","text":"<p>A rigid body transform combines rotation and translation, preserving distances and angles. It represents the pose (position and orientation) of an object in 3D space.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#se3-the-special-euclidean-group","title":"SE(3): The Special Euclidean Group","text":"<p>SE(3) is the mathematical group of all rigid body transformations in 3D. An SE(3) transform consists of:</p> <ul> <li>A rotation \\(\\mathbf{R} \\in SO(3)\\) (3\u00d73 orthogonal matrix with det = 1)</li> <li>A translation \\(\\mathbf{t} \\in \\mathbb{R}^3\\)</li> </ul> <p>In homogeneous coordinates:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#se3-transformation-matrix","title":"SE(3) Transformation Matrix","text":"<p>\\(\\mathbf{T} = \\begin{bmatrix} \\mathbf{R} &amp; \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\\\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{R}\\) is a 3\u00d73 rotation matrix</li> <li>\\(\\mathbf{t} = (t_x, t_y, t_z)^\\top\\) is the translation vector</li> <li>The bottom row \\([0, 0, 0, 1]\\) maintains homogeneous structure</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#composing-transforms","title":"Composing Transforms","text":"<p>Multiple transformations compose by matrix multiplication:</p> <p>\\(\\mathbf{T}_{total} = \\mathbf{T}_n \\cdot \\mathbf{T}_{n-1} \\cdots \\mathbf{T}_2 \\cdot \\mathbf{T}_1\\)</p> <p>Transforms apply right-to-left: \\(\\mathbf{T}_1\\) first, then \\(\\mathbf{T}_2\\), etc.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#inverse-transform","title":"Inverse Transform","text":"<p>The inverse of an SE(3) transform:</p> <p>\\(\\mathbf{T}^{-1} = \\begin{bmatrix} \\mathbf{R}^\\top &amp; -\\mathbf{R}^\\top \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix}\\)</p> <p>Note: We use \\(\\mathbf{R}^\\top\\) instead of computing a matrix inverse since rotation matrices are orthogonal.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-rigid-body-transform-chain","title":"Diagram: Rigid Body Transform Chain","text":"Rigid Body Transform Chain <p>Type: microsim</p> <p>Learning objective: Visualize composition of rigid body transforms (Bloom: Apply)</p> <p>Visual elements: - World coordinate frame at origin - Multiple linked coordinate frames (like robot arm joints) - 3D objects attached to each frame - Transform arrows showing relationships</p> <p>Interactive controls: - Sliders for each joint angle (rotations) - Sliders for link lengths (translations) - Toggle: Show individual transforms vs composed - Button: \"Add link\" - Button: \"Reset\" - Display: Full transform matrix from world to end effector</p> <p>Canvas layout: 850x650px with 3D kinematic chain</p> <p>Behavior: - Moving one joint updates all downstream frames - Show transform matrices at each stage - Highlight how composition order matters - Demonstrate forward kinematics visually</p> <p>Implementation: p5.js with WEBGL, matrix chain computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#camera-models","title":"Camera Models","text":"<p>A camera model describes how 3D points in the world project onto a 2D image. The most common model is the pinhole camera.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-pinhole-camera-model","title":"The Pinhole Camera Model","text":"<p>Light rays pass through a single point (the pinhole or optical center) and project onto an image plane:</p> <ul> <li>3D world point: \\(\\mathbf{P}_w = (X, Y, Z)\\)</li> <li>2D image point: \\(\\mathbf{p} = (u, v)\\)</li> </ul> <p>The projection involves two stages:</p> <ol> <li>Extrinsic parameters: Transform from world to camera coordinates</li> <li>Intrinsic parameters: Project from camera coordinates to image pixels</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#intrinsic-parameters","title":"Intrinsic Parameters","text":"<p>Intrinsic parameters describe the camera's internal geometry:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#camera-intrinsic-matrix","title":"Camera Intrinsic Matrix","text":"<p>\\(\\mathbf{K} = \\begin{bmatrix} f_x &amp; s &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(f_x, f_y\\) are focal lengths in pixels (may differ if pixels are non-square)</li> <li>\\((c_x, c_y)\\) is the principal point (usually near image center)</li> <li>\\(s\\) is the skew coefficient (usually 0 for modern cameras)</li> </ul> Parameter Meaning Typical Value \\(f_x, f_y\\) Focal length \u00d7 pixels/mm 500-2000 pixels \\(c_x\\) Principal point x image_width / 2 \\(c_y\\) Principal point y image_height / 2 \\(s\\) Skew 0"},{"location":"chapters/14-3d-geometry-and-transformations/#extrinsic-parameters","title":"Extrinsic Parameters","text":"<p>Extrinsic parameters describe the camera's pose in the world:</p> <p>\\([\\mathbf{R} | \\mathbf{t}]\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{R}\\) is the 3\u00d73 rotation from world to camera frame</li> <li>\\(\\mathbf{t}\\) is the translation (camera position in world, negated and rotated)</li> </ul> <p>The extrinsic matrix transforms points from world coordinates to camera coordinates.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-full-camera-matrix","title":"The Full Camera Matrix","text":"<p>The complete projection matrix combines intrinsic and extrinsic parameters:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#projection-matrix","title":"Projection Matrix","text":"<p>\\(\\mathbf{P} = \\mathbf{K} [\\mathbf{R} | \\mathbf{t}]\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{P}\\) is a 3\u00d74 matrix</li> <li>\\(\\mathbf{K}\\) is the 3\u00d73 intrinsic matrix</li> <li>\\([\\mathbf{R} | \\mathbf{t}]\\) is the 3\u00d74 extrinsic matrix</li> </ul> <p>Projection from world to image:</p> <p>\\(\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} \\sim \\mathbf{P} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\)</p> <p>The \\(\\sim\\) indicates equality up to scale (divide by the third coordinate to get pixel coordinates).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-camera-model-visualizer","title":"Diagram: Camera Model Visualizer","text":"Camera Model Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how camera parameters affect projection (Bloom: Analyze)</p> <p>Visual elements: - 3D scene with objects at various depths - Camera frustum showing field of view - Image plane with projected points - Ray lines from 3D points through camera center to image</p> <p>Interactive controls: - Slider: Focal length (affects FOV) - Slider: Principal point (cx, cy) - Camera pose controls: position and orientation - Toggle: Show projection rays - Toggle: Show camera frustum</p> <p>Canvas layout: 900x650px with 3D scene and 2D image view</p> <p>Behavior: - Changing focal length zooms in/out - Moving principal point shifts image - Moving camera updates all projections - Display intrinsic and extrinsic matrices - Show correspondence between 3D points and 2D projections</p> <p>Implementation: p5.js with WEBGL and matrix projection</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#perspective-projection","title":"Perspective Projection","text":"<p>Perspective projection is the mathematical model of how 3D points map to 2D:</p> <p>For a point \\((X, Y, Z)\\) in camera coordinates:</p> <p>\\(u = f_x \\frac{X}{Z} + c_x, \\quad v = f_y \\frac{Y}{Z} + c_y\\)</p> <p>Key properties:</p> <ul> <li>Objects farther away appear smaller (division by \\(Z\\))</li> <li>Parallel lines may converge to vanishing points</li> <li>Depth information is lost (many 3D points map to the same 2D point)</li> </ul> Projection Type Properties Use Case Perspective Realistic, depth-dependent scaling Photos, human vision Orthographic No depth scaling, parallel projection Engineering drawings, CAD Weak perspective Uniform depth for scene Approximate when depth range is small"},{"location":"chapters/14-3d-geometry-and-transformations/#stereo-vision","title":"Stereo Vision","text":"<p>Stereo vision uses two cameras to recover 3D structure, mimicking human binocular vision.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-stereo-setup","title":"The Stereo Setup","text":"<p>Two cameras with known relative pose observe the same scene:</p> <ul> <li>Baseline \\(b\\): Distance between camera centers</li> <li>Disparity \\(d\\): Difference in horizontal image position of corresponding points</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#depth-from-disparity","title":"Depth from Disparity","text":"<p>\\(Z = \\frac{f \\cdot b}{d}\\)</p> <p>where:</p> <ul> <li>\\(Z\\) is the depth (distance to the point)</li> <li>\\(f\\) is the focal length</li> <li>\\(b\\) is the baseline (distance between cameras)</li> <li>\\(d = u_L - u_R\\) is the disparity (difference in x-coordinates)</li> </ul> <p>Key insight: Larger baseline gives better depth resolution but makes matching harder.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#epipolar-geometry","title":"Epipolar Geometry","text":"<p>Epipolar geometry describes the geometric relationship between two views of the same scene.</p> <p>Key concepts:</p> <ul> <li>Epipole: Where the line connecting camera centers intersects each image plane</li> <li>Epipolar line: For a point in one image, the corresponding point in the other image lies on this line</li> <li>Epipolar plane: Plane containing both camera centers and the 3D point</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-fundamental-matrix","title":"The Fundamental Matrix","text":"<p>The fundamental matrix \\(\\mathbf{F}\\) encodes epipolar geometry:</p> <p>\\(\\mathbf{p}_R^\\top \\mathbf{F} \\mathbf{p}_L = 0\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{p}_L, \\mathbf{p}_R\\) are corresponding points in left and right images (homogeneous)</li> <li>\\(\\mathbf{F}\\) is a 3\u00d73 matrix with rank 2</li> <li>This constraint means \\(\\mathbf{p}_R\\) lies on the epipolar line \\(\\mathbf{l}_R = \\mathbf{F} \\mathbf{p}_L\\)</li> </ul> <p>The fundamental matrix has 7 degrees of freedom (9 elements minus scale minus rank constraint).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-epipolar-geometry-visualizer","title":"Diagram: Epipolar Geometry Visualizer","text":"Epipolar Geometry Visualizer <p>Type: microsim</p> <p>Learning objective: Understand epipolar constraints in stereo vision (Bloom: Analyze)</p> <p>Visual elements: - Top-down view showing two cameras and 3D point - Left and right image planes - Epipolar lines drawn on both images - Epipoles marked - Epipolar plane visualization</p> <p>Interactive controls: - Draggable 3D point position - Draggable camera positions - Click on left image point to show epipolar line on right - Toggle: Show epipolar plane in 3D - Toggle: Show all epipolar lines (for multiple points)</p> <p>Canvas layout: 900x700px with 3D view and stereo pair</p> <p>Behavior: - Moving 3D point shows how image points move along epipolar lines - Display fundamental matrix - Show that corresponding points satisfy epipolar constraint - Demonstrate baseline effect on epipole positions</p> <p>Implementation: p5.js with WEBGL and epipolar line computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#triangulation","title":"Triangulation","text":"<p>Triangulation computes the 3D position of a point from its projections in two or more images.</p> <p>Given:</p> <ul> <li>Camera matrices \\(\\mathbf{P}_L, \\mathbf{P}_R\\)</li> <li>Corresponding image points \\(\\mathbf{p}_L, \\mathbf{p}_R\\)</li> </ul> <p>Find the 3D point \\(\\mathbf{P}\\) such that:</p> <p>\\(\\mathbf{p}_L \\sim \\mathbf{P}_L \\mathbf{P}, \\quad \\mathbf{p}_R \\sim \\mathbf{P}_R \\mathbf{P}\\)</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#linear-triangulation","title":"Linear Triangulation","text":"<p>Each correspondence provides two equations. Stack them into a linear system:</p> <p>\\(\\mathbf{A} \\mathbf{P} = \\mathbf{0}\\)</p> <p>where \\(\\mathbf{A}\\) is a 4\u00d74 matrix built from camera matrices and image points. Solve using SVD (find the null space of \\(\\mathbf{A}\\)).</p> Method Accuracy Robustness Computation Linear (DLT) Moderate Sensitive to noise Fast Optimal (geometric) Best Good Iterative Mid-point Low Fast Very fast"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-triangulation-visualizer","title":"Diagram: Triangulation Visualizer","text":"Triangulation Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how 3D points are recovered from stereo correspondences (Bloom: Apply)</p> <p>Visual elements: - Two cameras with projection rays - Intersection point (triangulated 3D position) - Error visualization when rays don't perfectly intersect - Left and right image planes with point markers</p> <p>Interactive controls: - Draggable image points on left and right images - Camera baseline slider - Add noise toggle (shows triangulation error) - Display: 3D coordinates, reprojection error</p> <p>Canvas layout: 800x650px with 3D scene and stereo pair</p> <p>Behavior: - Rays from each camera displayed in 3D - Show exact intersection or closest point when noisy - Moving image points updates 3D reconstruction - Display triangulation uncertainty with depth</p> <p>Implementation: p5.js with WEBGL and linear triangulation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-clouds","title":"Point Clouds","text":"<p>A point cloud is a set of 3D points representing the surface or structure of objects and environments.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-representation","title":"Point Cloud Representation","text":"<p>Each point typically includes:</p> Attribute Type Description Position \\((x, y, z)\\) 3D coordinates Color \\((r, g, b)\\) Optional RGB values Normal \\((n_x, n_y, n_z)\\) Surface orientation Intensity scalar Reflection strength (lidar) <p>Point clouds can contain millions to billions of points for large-scale scenes.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#sources-of-point-clouds","title":"Sources of Point Clouds","text":"Sensor Principle Typical Density Range Stereo cameras Triangulation Dense (per-pixel) 1-50m Structured light Projected pattern Dense 0.5-5m Time-of-flight Light travel time Medium 0.5-10m Lidar Laser scanning Sparse to medium 1-200m Photogrammetry Multi-view Variable Any"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-processing","title":"Point Cloud Processing","text":"<p>Common operations on point clouds:</p> <ul> <li>Downsampling: Reduce density using voxel grids</li> <li>Normal estimation: Compute surface orientation from local neighborhoods</li> <li>Registration: Align multiple point clouds (ICP algorithm)</li> <li>Segmentation: Separate ground, objects, etc.</li> <li>Surface reconstruction: Create meshes from points</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-point-cloud-visualizer","title":"Diagram: Point Cloud Visualizer","text":"Point Cloud Visualizer <p>Type: microsim</p> <p>Learning objective: Explore point cloud data and common processing operations (Bloom: Understand)</p> <p>Visual elements: - 3D point cloud rendering - Color by height/intensity/normal - Bounding box - Selected point information display</p> <p>Interactive controls: - Rotate/zoom/pan 3D view - Point size slider - Color mode: height, intensity, RGB, normals - Downsampling slider (voxel size) - Toggle: Show surface normals as arrows - Sample dataset selector</p> <p>Canvas layout: 800x650px with 3D point cloud view</p> <p>Behavior: - Load and display sample point clouds - Real-time downsampling preview - Click point to show coordinates - Display point count and bounding box dimensions</p> <p>Implementation: p5.js with WEBGL, point rendering</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-registration","title":"Point Cloud Registration","text":"<p>Registration aligns two point clouds into a common coordinate frame. The classic algorithm is Iterative Closest Point (ICP):</p> <ol> <li>For each point in source cloud, find nearest neighbor in target</li> <li>Estimate rigid transform minimizing point-to-point distances</li> <li>Apply transform to source cloud</li> <li>Repeat until convergence</li> </ol> <p>ICP finds the transform \\((\\mathbf{R}, \\mathbf{t})\\) minimizing:</p> <p>\\(E = \\sum_{i} \\| \\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i \\|^2\\)</p> <p>where \\(\\mathbf{p}_i\\) are source points and \\(\\mathbf{q}_i\\) are corresponding target points.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\ndef quaternion_from_axis_angle(axis, angle):\n    \"\"\"Create quaternion from axis-angle representation.\"\"\"\n    axis = axis / np.linalg.norm(axis)  # Ensure unit axis\n    half_angle = angle / 2\n    w = np.cos(half_angle)\n    xyz = np.sin(half_angle) * axis\n    return np.array([w, xyz[0], xyz[1], xyz[2]])\n\ndef quaternion_rotate(q, point):\n    \"\"\"Rotate a 3D point by a quaternion.\"\"\"\n    # Point as pure quaternion\n    p = np.array([0, point[0], point[1], point[2]])\n\n    # q * p * q_conjugate\n    q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n    result = quaternion_multiply(quaternion_multiply(q, p), q_conj)\n\n    return result[1:4]\n\ndef quaternion_multiply(q1, q2):\n    \"\"\"Multiply two quaternions.\"\"\"\n    w1, x1, y1, z1 = q1\n    w2, x2, y2, z2 = q2\n    return np.array([\n        w1*w2 - x1*x2 - y1*y2 - z1*z2,\n        w1*x2 + x1*w2 + y1*z2 - z1*y2,\n        w1*y2 - x1*z2 + y1*w2 + z1*x2,\n        w1*z2 + x1*y2 - y1*x2 + z1*w2\n    ])\n\ndef project_point(P, K, R, t):\n    \"\"\"Project 3D point to 2D using camera model.\"\"\"\n    # World to camera coordinates\n    P_cam = R @ P + t\n\n    # Perspective projection\n    x = P_cam[0] / P_cam[2]\n    y = P_cam[1] / P_cam[2]\n\n    # Apply intrinsic matrix\n    u = K[0, 0] * x + K[0, 2]\n    v = K[1, 1] * y + K[1, 2]\n\n    return np.array([u, v])\n\ndef triangulate(P1, P2, p1, p2):\n    \"\"\"Triangulate 3D point from two views.\"\"\"\n    # Build the A matrix for DLT\n    A = np.array([\n        p1[0] * P1[2, :] - P1[0, :],\n        p1[1] * P1[2, :] - P1[1, :],\n        p2[0] * P2[2, :] - P2[0, :],\n        p2[1] * P2[2, :] - P2[1, :]\n    ])\n\n    # SVD solution\n    _, _, Vt = np.linalg.svd(A)\n    X = Vt[-1]\n\n    # Convert from homogeneous\n    return X[:3] / X[3]\n</code></pre>"},{"location":"chapters/14-3d-geometry-and-transformations/#summary_1","title":"Summary","text":"<p>3D geometry and transformations provide the mathematical foundation for spatial computing:</p> <p>Coordinate Systems and Representations:</p> <ul> <li>3D coordinate systems use handedness conventions (right-hand vs left-hand)</li> <li>Homogeneous coordinates unify rotation and translation as matrix multiplication</li> <li>Different domains (robotics, graphics, vision) use different conventions</li> </ul> <p>Rotation Representations:</p> <ul> <li>Euler angles are intuitive but suffer from gimbal lock</li> <li>Quaternions provide singularity-free rotation representation</li> <li>Unit quaternions form a double cover of SO(3)</li> </ul> <p>Rigid Body Transforms:</p> <ul> <li>SE(3) combines rotation and translation</li> <li>4\u00d74 homogeneous matrices enable efficient composition</li> <li>Transform chains model articulated systems like robot arms</li> </ul> <p>Camera Models:</p> <ul> <li>Intrinsic parameters describe internal camera geometry</li> <li>Extrinsic parameters describe camera pose in the world</li> <li>The projection matrix combines both for 3D-to-2D mapping</li> </ul> <p>Stereo Vision:</p> <ul> <li>Two views enable 3D reconstruction via triangulation</li> <li>Epipolar geometry constrains correspondence search</li> <li>Depth is inversely proportional to disparity</li> </ul> <p>Point Clouds:</p> <ul> <li>Represent 3D structure as sets of points</li> <li>Generated by various sensors (lidar, stereo, structured light)</li> <li>Registration aligns multiple scans into a common frame</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#self-check-questions","title":"Self-Check Questions","text":"Why do we use half the rotation angle in the quaternion representation? <p>Quaternions provide a double cover of the rotation group SO(3), meaning both \\(\\mathbf{q}\\) and \\(-\\mathbf{q}\\) represent the same rotation. Using \\(\\frac{\\theta}{2}\\) in the quaternion formula ensures that rotating by \\(\\theta\\) actually produces a rotation of \\(\\theta\\) (not \\(2\\theta\\)) when we compute \\(\\mathbf{q} \\cdot \\mathbf{p} \\cdot \\mathbf{q}^*\\). The quaternion multiplication effectively applies the rotation twice (once with \\(\\mathbf{q}\\), once with \\(\\mathbf{q}^*\\)), so we need half angles to get the correct total rotation.</p> What information is lost during perspective projection, and why does this matter for 3D reconstruction? <p>Perspective projection loses depth information: all points along a ray from the camera center project to the same image point. Mathematically, \\((X, Y, Z)\\) and \\((kX, kY, kZ)\\) for any \\(k &gt; 0\\) produce identical image coordinates. This is why a single image cannot determine 3D structure\u2014we need additional constraints like multiple views (stereo), known object size, or depth sensors. Stereo vision solves this by observing the same point from two locations, where the disparity (difference in image position) encodes depth.</p> How does the baseline affect stereo depth estimation? <p>The depth-disparity relationship is \\(Z = \\frac{fb}{d}\\). With larger baseline \\(b\\), the same depth produces larger disparity, improving depth resolution (we can distinguish smaller depth differences). However, larger baseline makes stereo matching harder because: (1) occlusions become more common\u2014parts visible in one camera may be hidden in the other, (2) appearance changes more between views, and (3) the search range for correspondences increases. Practical systems balance these trade-offs based on the target depth range.</p> Why is gimbal lock a fundamental problem rather than an implementation bug? <p>Gimbal lock is inherent to representing 3D rotation with three sequential rotations about fixed axes. Any three-parameter rotation representation that uses Euler-type angles will have singularities because SO(3) (the rotation group) cannot be smoothly parameterized by \\(\\mathbb{R}^3\\). At the singularity, two rotation axes align, reducing the effective degrees of freedom from 3 to 2. No choice of axis order eliminates the problem\u2014it just moves the singularity. Quaternions avoid this by using four parameters with a unit constraint, providing a smooth parameterization of rotations.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/","title":"Quiz: 3D Geometry and Transformations","text":"<p>Test your understanding of 3D coordinate systems, rotations, and camera models.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#1-a-right-hand-coordinate-system-is-characterized-by","title":"1. A right-hand coordinate system is characterized by:","text":"<ol> <li>All axes pointing to the right</li> <li>Curling right-hand fingers from X to Y points thumb in Z direction</li> <li>The Z-axis always pointing down</li> <li>Negative coordinates only</li> </ol> Show Answer <p>The correct answer is B. In a right-hand system, if you curl your right hand's fingers from the X-axis toward the Y-axis, your thumb points in the Z direction. This is the standard convention in mathematics and many graphics systems.</p> <p>Concept Tested: 3D Coordinate System</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#2-euler-angles-can-suffer-from-gimbal-lock-when","title":"2. Euler angles can suffer from gimbal lock when:","text":"<ol> <li>All angles are zero</li> <li>Two rotation axes become aligned</li> <li>The angles are too small</li> <li>Only yaw is non-zero</li> </ol> Show Answer <p>The correct answer is B. Gimbal lock occurs when two rotation axes align (typically when pitch is \u00b190\u00b0), causing loss of one degree of freedom. This is an inherent limitation of Euler angle representation.</p> <p>Concept Tested: Gimbal Lock</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#3-a-quaternion-representing-rotation-uses","title":"3. A quaternion representing rotation uses:","text":"<ol> <li>Three components</li> <li>Four components with a unit norm constraint</li> <li>Nine components like a rotation matrix</li> <li>Two complex numbers</li> </ol> Show Answer <p>The correct answer is B. A unit quaternion has four components \\((w, x, y, z)\\) with \\(w^2 + x^2 + y^2 + z^2 = 1\\). This provides a singularity-free representation of 3D rotations.</p> <p>Concept Tested: Quaternion</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#4-homogeneous-coordinates-allow","title":"4. Homogeneous coordinates allow:","text":"<ol> <li>Only rotation operations</li> <li>Combining rotation and translation in a single matrix multiplication</li> <li>Eliminating the need for matrices</li> <li>Representing only 2D points</li> </ol> Show Answer <p>The correct answer is B. Homogeneous coordinates add an extra dimension (e.g., \\((x, y, z, 1)\\) for 3D points), enabling both rotation and translation to be represented as matrix multiplication.</p> <p>Concept Tested: Homogeneous Coordinates</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#5-an-se3-transformation-represents","title":"5. An SE(3) transformation represents:","text":"<ol> <li>Scaling only</li> <li>A rigid body motion (rotation and translation) in 3D</li> <li>Projection to 2D</li> <li>Color transformation</li> </ol> Show Answer <p>The correct answer is B. SE(3) is the Special Euclidean group in 3D, representing all rigid body transformations\u2014combinations of rotation and translation that preserve distances and angles.</p> <p>Concept Tested: SE3 Transform</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#6-the-camera-intrinsic-matrix-k-contains","title":"6. The camera intrinsic matrix \\(K\\) contains:","text":"<ol> <li>The camera's position in the world</li> <li>Focal length and principal point (internal camera geometry)</li> <li>The rotation of the camera</li> <li>The image pixel values</li> </ol> Show Answer <p>The correct answer is B. The intrinsic matrix contains internal camera parameters: focal lengths (\\(f_x\\), \\(f_y\\)), principal point (\\(c_x\\), \\(c_y\\)), and optionally skew. These describe how 3D camera coordinates project to 2D pixels.</p> <p>Concept Tested: Intrinsic Parameters</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#7-perspective-projection-causes","title":"7. Perspective projection causes:","text":"<ol> <li>All objects to appear the same size</li> <li>Distant objects to appear smaller than near objects</li> <li>Parallel lines to remain parallel in the image</li> <li>Colors to change</li> </ol> Show Answer <p>The correct answer is B. Perspective projection divides by depth (Z-coordinate), causing distant objects to appear smaller. This mimics human vision and camera optics.</p> <p>Concept Tested: Perspective Projection</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#8-in-stereo-vision-depth-is-inversely-proportional-to","title":"8. In stereo vision, depth is inversely proportional to:","text":"<ol> <li>The baseline between cameras</li> <li>The disparity (difference in image positions)</li> <li>The focal length</li> <li>The image resolution</li> </ol> Show Answer <p>The correct answer is B. The relationship \\(Z = \\frac{fb}{d}\\) shows depth is inversely proportional to disparity. Larger disparity means closer objects; smaller disparity means farther objects.</p> <p>Concept Tested: Stereo Vision</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#9-triangulation-in-stereo-vision","title":"9. Triangulation in stereo vision:","text":"<ol> <li>Detects triangular shapes</li> <li>Computes 3D position from corresponding 2D points in multiple views</li> <li>Measures triangle areas</li> <li>Filters noise from images</li> </ol> Show Answer <p>The correct answer is B. Triangulation uses the intersection of projection rays from multiple cameras to determine the 3D position of a point observed in multiple images.</p> <p>Concept Tested: Triangulation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#10-a-point-cloud-is","title":"10. A point cloud is:","text":"<ol> <li>A type of weather pattern</li> <li>A set of 3D points representing surfaces or structure</li> <li>A 2D image format</li> <li>A neural network architecture</li> </ol> Show Answer <p>The correct answer is B. A point cloud is a collection of 3D points, often with associated attributes (color, intensity, normals), representing the surface or structure of objects and environments.</p> <p>Concept Tested: Point Cloud</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/","title":"Autonomous Systems and Sensor Fusion","text":""},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#summary","title":"Summary","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles and robotics. You will learn LIDAR point cloud processing, camera calibration, sensor fusion with Kalman filters, state estimation and prediction, SLAM (Simultaneous Localization and Mapping), object detection and tracking, and path planning algorithms. These are the techniques that enable self-driving cars and autonomous robots.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> <li>Chapter 12: Optimization and Learning Algorithms</li> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Chapter 14: 3D Geometry and Transformations</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#introduction","title":"Introduction","text":"<p>Autonomous systems\u2014self-driving cars, delivery robots, drones\u2014must perceive their environment, understand their location, and plan safe paths through the world. These capabilities require integrating everything we've learned about linear algebra:</p> <ul> <li>Sensor data arrives as vectors and matrices (images, point clouds)</li> <li>State estimation uses matrix equations to fuse noisy measurements</li> <li>Transformations relate sensor frames to world coordinates</li> <li>Optimization finds efficient, collision-free trajectories</li> </ul> <p>This capstone chapter synthesizes these concepts into the complete autonomy stack used by real-world robots and vehicles.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#sensing-the-environment","title":"Sensing the Environment","text":"<p>Autonomous systems perceive the world through multiple sensors, each providing complementary information.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#lidar-point-clouds","title":"LIDAR Point Clouds","text":"<p>LIDAR (Light Detection and Ranging) measures distances by timing laser pulses. A spinning LIDAR captures a point cloud\u2014a set of 3D points representing surfaces in the environment.</p> <p>Each LIDAR point contains:</p> Attribute Description Typical Range \\((x, y, z)\\) 3D position 0.5-200 meters Intensity Reflection strength 0-255 Ring/Channel Which laser beam 0-127 Timestamp Acquisition time Nanosecond precision <p>A single LIDAR scan may contain 100,000+ points, producing a sparse but accurate 3D representation.</p> <p>Processing pipeline:</p> <ol> <li>Ground removal: Segment points into ground vs obstacles</li> <li>Clustering: Group nearby points into objects</li> <li>Classification: Identify object types (car, pedestrian, cyclist)</li> <li>Tracking: Associate objects across frames</li> </ol>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-lidar-point-cloud-visualizer","title":"Diagram: LIDAR Point Cloud Visualizer","text":"LIDAR Point Cloud Visualizer <p>Type: microsim</p> <p>Learning objective: Understand LIDAR data structure and basic processing (Bloom: Understand)</p> <p>Visual elements: - 3D point cloud rendering with intensity coloring - Ground plane visualization - Clustered objects highlighted in different colors - Ego vehicle position marker at origin - Distance rings at 10m, 25m, 50m</p> <p>Interactive controls: - Rotate/zoom/pan 3D view - Toggle: Show ground points - Toggle: Show clustered objects - Slider: Point size - Slider: Intensity threshold - Dropdown: Color by (intensity, height, distance, cluster)</p> <p>Canvas layout: 850x650px with 3D point cloud view</p> <p>Behavior: - Load sample LIDAR scans from driving dataset - Real-time ground segmentation preview - Click on cluster to show point count and bounding box - Animate through sequence of scans</p> <p>Implementation: p5.js with WEBGL, point rendering</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#camera-calibration","title":"Camera Calibration","text":"<p>Camera calibration determines the intrinsic and extrinsic parameters needed to accurately project 3D points to image pixels and vice versa.</p> <p>Intrinsic calibration finds the camera matrix \\(\\mathbf{K}\\):</p> <p>\\(\\mathbf{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Plus distortion coefficients \\((k_1, k_2, p_1, p_2, k_3)\\) for radial and tangential lens distortion.</p> <p>Extrinsic calibration finds the transformation between sensors:</p> <ul> <li>Camera-to-LIDAR: \\(\\mathbf{T}_{CL}\\) for projecting LIDAR points onto images</li> <li>Camera-to-vehicle: \\(\\mathbf{T}_{CV}\\) for relating detections to vehicle frame</li> </ul> Calibration Type What It Determines Method Intrinsic Focal length, principal point, distortion Checkerboard pattern Extrinsic (camera-LIDAR) Relative pose Correspondences or joint optimization Extrinsic (multi-camera) Camera arrangement Overlapping views"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-camera-calibration-visualizer","title":"Diagram: Camera Calibration Visualizer","text":"Camera Calibration Visualizer <p>Type: microsim</p> <p>Learning objective: Understand camera calibration process and distortion correction (Bloom: Apply)</p> <p>Visual elements: - Raw image with detected checkerboard corners - Undistorted image showing correction - 3D view of checkerboard poses - Reprojection error visualization</p> <p>Interactive controls: - Slider: Radial distortion k1, k2 - Slider: Focal length - Toggle: Show/hide detected corners - Toggle: Show reprojection errors as vectors - Button: \"Calibrate\" (run optimization)</p> <p>Canvas layout: 900x600px with raw/corrected image comparison</p> <p>Behavior: - Display checkerboard corner detection - Show barrel/pincushion distortion effects - Visualize how calibration reduces reprojection error - Display calibration matrix values</p> <p>Implementation: p5.js with corner detection simulation</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#state-estimation","title":"State Estimation","text":"<p>Autonomous systems must estimate their state (position, velocity, orientation) from noisy sensor measurements. State estimation is the process of inferring the true state from uncertain observations.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-state-vector","title":"The State Vector","text":"<p>The state vector \\(\\mathbf{x}\\) contains all variables we want to estimate:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#vehicle-state-vector-example","title":"Vehicle State Vector Example","text":"<p>\\(\\mathbf{x} = \\begin{bmatrix} p_x \\\\ p_y \\\\ v_x \\\\ v_y \\\\ \\theta \\\\ \\omega \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\((p_x, p_y)\\) is position</li> <li>\\((v_x, v_y)\\) is velocity</li> <li>\\(\\theta\\) is heading angle</li> <li>\\(\\omega\\) is angular velocity (yaw rate)</li> </ul> <p>The state dimension depends on what we're tracking:</p> Application State Variables Dimension 2D position only \\(x, y\\) 2 2D with velocity \\(x, y, v_x, v_y\\) 4 2D with heading \\(x, y, \\theta, v, \\omega\\) 5 3D pose \\(x, y, z, \\phi, \\theta, \\psi\\) 6 Full 3D dynamics Position, velocity, orientation, angular velocity 12+"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-measurement-vector","title":"The Measurement Vector","text":"<p>The measurement vector \\(\\mathbf{z}\\) contains sensor observations:</p> <p>\\(\\mathbf{z} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_m \\end{bmatrix}\\)</p> <p>Measurements are related to the state by the observation model:</p> <p>\\(\\mathbf{z} = \\mathbf{H}\\mathbf{x} + \\mathbf{v}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{H}\\) is the observation matrix (which state variables are observed)</li> <li>\\(\\mathbf{v}\\) is measurement noise (typically Gaussian with covariance \\(\\mathbf{R}\\))</li> </ul> Sensor Measures Observation Matrix \\(\\mathbf{H}\\) GPS Position only \\([I_{2\u00d72}, 0_{2\u00d74}]\\) Speedometer Speed magnitude Nonlinear IMU Acceleration, angular velocity Depends on state definition Camera Bearing to landmarks Nonlinear"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-filter","title":"The Kalman Filter","text":"<p>The Kalman filter is the optimal estimator for linear systems with Gaussian noise. It recursively estimates the state by combining predictions with measurements.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-filter-model","title":"The Kalman Filter Model","text":"<p>The system evolves according to:</p> <p>State transition: \\(\\mathbf{x}_k = \\mathbf{F}\\mathbf{x}_{k-1} + \\mathbf{B}\\mathbf{u}_k + \\mathbf{w}_k\\)</p> <p>Observation: \\(\\mathbf{z}_k = \\mathbf{H}\\mathbf{x}_k + \\mathbf{v}_k\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{F}\\) is the state transition matrix</li> <li>\\(\\mathbf{B}\\) is the control input matrix</li> <li>\\(\\mathbf{u}_k\\) is the control input (e.g., acceleration command)</li> <li>\\(\\mathbf{w}_k \\sim \\mathcal{N}(0, \\mathbf{Q})\\) is process noise</li> <li>\\(\\mathbf{v}_k \\sim \\mathcal{N}(0, \\mathbf{R})\\) is measurement noise</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-prediction-step","title":"The Prediction Step","text":"<p>The prediction step propagates the state estimate forward in time:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#kalman-filter-prediction","title":"Kalman Filter Prediction","text":"<p>\\(\\hat{\\mathbf{x}}_k^- = \\mathbf{F}\\hat{\\mathbf{x}}_{k-1} + \\mathbf{B}\\mathbf{u}_k\\)</p> <p>\\(\\mathbf{P}_k^- = \\mathbf{F}\\mathbf{P}_{k-1}\\mathbf{F}^\\top + \\mathbf{Q}\\)</p> <p>where:</p> <ul> <li>\\(\\hat{\\mathbf{x}}_k^-\\) is the predicted state (prior)</li> <li>\\(\\mathbf{P}_k^-\\) is the predicted covariance (uncertainty grows)</li> <li>The superscript \\(^-\\) indicates \"before incorporating measurement\"</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-update-step","title":"The Update Step","text":"<p>The update step incorporates the new measurement:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#kalman-filter-update","title":"Kalman Filter Update","text":"<p>\\(\\mathbf{K}_k = \\mathbf{P}_k^- \\mathbf{H}^\\top (\\mathbf{H}\\mathbf{P}_k^-\\mathbf{H}^\\top + \\mathbf{R})^{-1}\\)</p> <p>\\(\\hat{\\mathbf{x}}_k = \\hat{\\mathbf{x}}_k^- + \\mathbf{K}_k(\\mathbf{z}_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^-)\\)</p> <p>\\(\\mathbf{P}_k = (\\mathbf{I} - \\mathbf{K}_k\\mathbf{H})\\mathbf{P}_k^-\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{K}_k\\) is the Kalman gain</li> <li>\\((\\mathbf{z}_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^-)\\) is the innovation (measurement residual)</li> <li>\\(\\mathbf{P}_k\\) is the updated (posterior) covariance</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-gain","title":"The Kalman Gain","text":"<p>The Kalman gain \\(\\mathbf{K}\\) optimally weights the prediction versus the measurement:</p> <ul> <li>When measurement noise \\(\\mathbf{R}\\) is large: \\(\\mathbf{K} \\to 0\\) (trust prediction)</li> <li>When prediction uncertainty \\(\\mathbf{P}^-\\) is large: \\(\\mathbf{K} \\to \\mathbf{H}^{-1}\\) (trust measurement)</li> </ul> <p>The Kalman gain minimizes the expected squared estimation error\u2014it's the optimal linear estimator.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-kalman-filter-visualizer","title":"Diagram: Kalman Filter Visualizer","text":"Kalman Filter Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Kalman filter combines prediction and measurement (Bloom: Apply)</p> <p>Visual elements: - 2D tracking display with true position (hidden), estimate, and measurements - Uncertainty ellipse showing covariance - Prediction arrow (where we expect to go) - Measurement marker (noisy observation) - Innovation vector (measurement - prediction)</p> <p>Interactive controls: - Slider: Process noise Q (how much motion varies) - Slider: Measurement noise R (sensor accuracy) - Button: \"Step\" (one predict-update cycle) - Button: \"Run\" (continuous tracking) - Toggle: Show true position (for debugging) - Dropdown: Motion model (constant velocity, constant acceleration)</p> <p>Canvas layout: 800x600px with tracking visualization</p> <p>Behavior: - Simulate object moving with random accelerations - Generate noisy measurements - Show uncertainty ellipse growing in prediction, shrinking after update - Display Kalman gain magnitude - Plot estimation error over time</p> <p>Implementation: p5.js with matrix operations for Kalman equations</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#extended-kalman-filter","title":"Extended Kalman Filter","text":"<p>The Extended Kalman Filter (EKF) handles nonlinear systems by linearizing around the current estimate.</p> <p>For nonlinear models:</p> <p>\\(\\mathbf{x}_k = f(\\mathbf{x}_{k-1}, \\mathbf{u}_k) + \\mathbf{w}_k\\)</p> <p>\\(\\mathbf{z}_k = h(\\mathbf{x}_k) + \\mathbf{v}_k\\)</p> <p>The EKF uses Jacobians instead of constant matrices:</p> Standard KF EKF Equivalent \\(\\mathbf{F}\\) $\\mathbf{F}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}\\bigg \\(\\mathbf{H}\\) $\\mathbf{H}_k = \\frac{\\partial h}{\\partial \\mathbf{x}}\\bigg <p>Common nonlinearities in autonomous systems:</p> <ul> <li>Coordinate transforms: Polar to Cartesian for radar</li> <li>Motion models: Constant turn rate and velocity (CTRV)</li> <li>Observation models: Bearing and range from position</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#sensor-fusion","title":"Sensor Fusion","text":"<p>Sensor fusion combines data from multiple sensors to achieve better accuracy and robustness than any single sensor alone.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#why-fuse-sensors","title":"Why Fuse Sensors?","text":"Sensor Strengths Weaknesses GPS Absolute position, global Low rate, poor in tunnels/urban canyons IMU High rate, works everywhere Drift over time LIDAR Accurate 3D, works in dark Expensive, sparse Camera Dense, semantic info Affected by lighting, no direct depth Radar Works in weather, velocity Low resolution <p>Fusion compensates for individual sensor weaknesses:</p> <ul> <li>GPS + IMU: High-rate positioning with drift correction</li> <li>Camera + LIDAR: Dense semantic 3D understanding</li> <li>Radar + Camera: All-weather perception with classification</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#fusion-architectures","title":"Fusion Architectures","text":"Architecture Description Pros Cons Early fusion Combine raw data Maximum information Complex, tight coupling Late fusion Combine detections/tracks Modular, sensor-independent May lose information Mid-level fusion Combine features Balance of flexibility and info Design complexity <p>The Kalman filter naturally performs sensor fusion by incorporating multiple measurement sources in the update step:</p> <p>\\(\\mathbf{z} = \\begin{bmatrix} \\mathbf{z}_{GPS} \\\\ \\mathbf{z}_{IMU} \\\\ \\vdots \\end{bmatrix}, \\quad \\mathbf{H} = \\begin{bmatrix} \\mathbf{H}_{GPS} \\\\ \\mathbf{H}_{IMU} \\\\ \\vdots \\end{bmatrix}\\)</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-sensor-fusion-visualizer","title":"Diagram: Sensor Fusion Visualizer","text":"Sensor Fusion Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how multiple sensors improve state estimation (Bloom: Analyze)</p> <p>Visual elements: - Vehicle track with true position - GPS measurements (blue dots, noisy, low rate) - IMU-based dead reckoning (orange line, drifting) - Fused estimate (green line, best of both) - Uncertainty ellipses for each estimate</p> <p>Interactive controls: - Toggle: Enable/disable GPS - Toggle: Enable/disable IMU - Slider: GPS noise level - Slider: IMU drift rate - Button: \"Run simulation\" - Display: RMSE for each estimation method</p> <p>Canvas layout: 850x650px with tracking comparison</p> <p>Behavior: - Show GPS-only tracking (jumpy, gaps) - Show IMU-only tracking (smooth but drifting) - Show fused result (smooth and accurate) - Quantify improvement from fusion</p> <p>Implementation: p5.js with multi-sensor Kalman filter</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#slam-simultaneous-localization-and-mapping","title":"SLAM: Simultaneous Localization and Mapping","text":"<p>SLAM solves the chicken-and-egg problem: to localize, you need a map; to build a map, you need to know your location.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-slam-problem","title":"The SLAM Problem","text":"<p>SLAM jointly estimates:</p> <ol> <li>Localization: The robot's pose over time</li> <li>Mapping: The positions of landmarks/features in the environment</li> </ol> <p>The state vector grows as new landmarks are discovered:</p> <p>\\(\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}_R \\\\ \\mathbf{m}_1 \\\\ \\mathbf{m}_2 \\\\ \\vdots \\\\ \\mathbf{m}_n \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{x}_R\\) is the robot pose and \\(\\mathbf{m}_i\\) are landmark positions.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#slam-approaches","title":"SLAM Approaches","text":"Approach Representation Scalability Accuracy EKF-SLAM Gaussian, dense covariance O(n\u00b2) landmarks High (small maps) Particle Filter SLAM Particles for pose, EKF for map Medium maps Good Graph-based SLAM Pose graph, optimization Large maps Very high Visual SLAM Features from cameras Real-time capable Good"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#graph-based-slam","title":"Graph-Based SLAM","text":"<p>Modern SLAM systems formulate the problem as graph optimization:</p> <ul> <li>Nodes: Robot poses at different times, landmark positions</li> <li>Edges: Constraints from odometry and observations</li> <li>Optimization: Find poses/landmarks minimizing total constraint error</li> </ul> <p>The optimization minimizes:</p> <p>\\(\\mathbf{x}^* = \\arg\\min_{\\mathbf{x}} \\sum_{\\langle i,j \\rangle} \\| \\mathbf{z}_{ij} - h(\\mathbf{x}_i, \\mathbf{x}_j) \\|^2_{\\mathbf{\\Omega}_{ij}}\\)</p> <p>where \\(\\mathbf{z}_{ij}\\) is the measurement between nodes \\(i\\) and \\(j\\), and \\(\\mathbf{\\Omega}_{ij}\\) is the information matrix (inverse covariance).</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-slam-visualizer","title":"Diagram: SLAM Visualizer","text":"SLAM Visualizer <p>Type: microsim</p> <p>Learning objective: Understand the SLAM problem and loop closure (Bloom: Analyze)</p> <p>Visual elements: - Robot trajectory (estimated vs true) - Landmark positions (estimated vs true) - Pose graph edges - Loop closure detection highlight - Uncertainty ellipses on poses and landmarks</p> <p>Interactive controls: - Button: \"Move robot\" (add new pose) - Button: \"Add landmark observation\" - Button: \"Loop closure\" (recognize previously seen place) - Toggle: Show covariance ellipses - Slider: Odometry noise - Button: \"Optimize graph\"</p> <p>Canvas layout: 900x700px with SLAM visualization</p> <p>Behavior: - Show drift accumulating without loop closures - Demonstrate how loop closure corrects the entire trajectory - Visualize uncertainty reduction after optimization - Display before/after comparison for loop closure</p> <p>Implementation: p5.js with simple graph optimization</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-detection-and-tracking","title":"Object Detection and Tracking","text":"<p>Autonomous systems must detect other road users and track them over time to predict their future behavior.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-detection","title":"Object Detection","text":"<p>Object detection identifies and localizes objects in sensor data:</p> <ul> <li>2D detection (camera): Bounding boxes in image coordinates</li> <li>3D detection (LIDAR): 3D bounding boxes in world coordinates</li> </ul> <p>Modern detection uses deep learning (CNNs for images, PointNets for point clouds).</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#bounding-boxes","title":"Bounding Boxes","text":"<p>A bounding box represents an object's spatial extent:</p> <p>2D Bounding Box: \\((x_{min}, y_{min}, x_{max}, y_{max})\\) or \\((x_{center}, y_{center}, width, height)\\)</p> <p>3D Bounding Box:</p> <p>\\(\\mathbf{B} = (x, y, z, l, w, h, \\theta)\\)</p> <p>where:</p> <ul> <li>\\((x, y, z)\\) is the center position</li> <li>\\((l, w, h)\\) are length, width, height</li> <li>\\(\\theta\\) is the heading angle (yaw)</li> </ul> Detection Output Components Use Case 2D box 4 values + class Image-based detection 3D box 7 values + class LIDAR/fusion detection Oriented box Center, axes, rotation General 3D objects Polygon/mask Variable points Precise shape"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-tracking","title":"Object Tracking","text":"<p>Object tracking associates detections across time to maintain consistent object identities.</p> <p>The tracking pipeline:</p> <ol> <li>Prediction: Where do we expect objects to be? (Kalman filter)</li> <li>Association: Match predictions to new detections</li> <li>Update: Refine state estimates with matched detections</li> <li>Track management: Create new tracks, delete lost tracks</li> </ol> <p>Association uses distance metrics:</p> <ul> <li>IoU (Intersection over Union): Geometric overlap of boxes</li> <li>Mahalanobis distance: Statistical distance using uncertainty</li> <li>Appearance features: Visual similarity (deep learning embeddings)</li> </ul> <p>The Hungarian algorithm finds optimal one-to-one assignment minimizing total cost.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-object-tracking-visualizer","title":"Diagram: Object Tracking Visualizer","text":"Object Tracking Visualizer <p>Type: microsim</p> <p>Learning objective: Understand multi-object tracking with prediction and association (Bloom: Apply)</p> <p>Visual elements: - Top-down view of scene with multiple moving objects - Predicted positions (dashed boxes) - Detected positions (solid boxes) - Track IDs displayed on each object - Association lines connecting predictions to detections</p> <p>Interactive controls: - Button: \"Step\" (advance one frame) - Button: \"Run\" (continuous tracking) - Toggle: Show predictions - Toggle: Show track history (trails) - Slider: Detection noise - Slider: Miss probability (some detections missing)</p> <p>Canvas layout: 800x650px with tracking visualization</p> <p>Behavior: - Objects move with realistic motion patterns - Generate noisy detections with occasional misses - Show prediction-to-detection association - Handle track creation (new objects) and deletion (lost objects) - Display track count and IDs</p> <p>Implementation: p5.js with Kalman tracking and Hungarian assignment</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#path-and-motion-planning","title":"Path and Motion Planning","text":"<p>Given the current state and a goal, planning algorithms find safe, efficient paths through the environment.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#path-planning","title":"Path Planning","text":"<p>Path planning finds a collision-free geometric path from start to goal.</p> <p>Classic algorithms:</p> Algorithm Type Optimality Completeness A* Graph search Optimal (with admissible heuristic) Yes RRT Sampling-based Asymptotically optimal (RRT*) Probabilistically complete Dijkstra Graph search Optimal Yes Potential fields Gradient descent No No (local minima) <p>A* algorithm:</p> <p>\\(f(n) = g(n) + h(n)\\)</p> <p>where:</p> <ul> <li>\\(g(n)\\) is the cost from start to node \\(n\\)</li> <li>\\(h(n)\\) is the heuristic estimate from \\(n\\) to goal</li> <li>A* expands nodes in order of lowest \\(f(n)\\)</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#motion-planning","title":"Motion Planning","text":"<p>Motion planning extends path planning to consider vehicle dynamics:</p> <ul> <li>Kinematic constraints: Minimum turning radius, maximum steering angle</li> <li>Dynamic constraints: Acceleration limits, comfort bounds</li> <li>Temporal constraints: Speed limits, timing requirements</li> </ul> <p>The vehicle must follow paths that are physically achievable.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#trajectory-optimization","title":"Trajectory Optimization","text":"<p>Trajectory optimization finds the optimal trajectory minimizing a cost function:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#trajectory-cost-function","title":"Trajectory Cost Function","text":"<p>\\(J = \\int_0^T \\left[ w_1 \\|\\ddot{\\mathbf{x}}\\|^2 + w_2 \\|\\mathbf{u}\\|^2 + w_3 c_{obstacle}(\\mathbf{x}) \\right] dt + w_4 \\|\\mathbf{x}(T) - \\mathbf{x}_{goal}\\|^2\\)</p> <p>where:</p> <ul> <li>\\(\\|\\ddot{\\mathbf{x}}\\|^2\\) penalizes acceleration (smoothness)</li> <li>\\(\\|\\mathbf{u}\\|^2\\) penalizes control effort</li> <li>\\(c_{obstacle}\\) penalizes proximity to obstacles</li> <li>The final term penalizes distance to goal</li> </ul> <p>This is solved using:</p> <ul> <li>Nonlinear optimization: Sequential quadratic programming (SQP)</li> <li>Model predictive control: Receding horizon optimization</li> <li>Sampling methods: Generate and evaluate candidate trajectories</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-path-planning-visualizer","title":"Diagram: Path Planning Visualizer","text":"Path Planning Visualizer <p>Type: microsim</p> <p>Learning objective: Compare path planning algorithms (Bloom: Evaluate)</p> <p>Visual elements: - 2D environment with obstacles - Start and goal positions - Planned path(s) - Explored nodes/samples (for search visualization) - Collision-free corridor</p> <p>Interactive controls: - Click to place start/goal - Draw obstacles (click and drag) - Dropdown: Algorithm (A, RRT, Dijkstra, RRT) - Slider: Grid resolution (for A*) - Button: \"Plan\" (run algorithm) - Button: \"Clear obstacles\" - Toggle: Show exploration</p> <p>Canvas layout: 850x700px with planning environment</p> <p>Behavior: - Animate algorithm exploration (A* wave, RRT tree growth) - Display path length and computation time - Show algorithm-specific parameters - Compare multiple algorithms on same environment</p> <p>Implementation: p5.js with grid-based and sampling-based planners</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-trajectory-optimization-visualizer","title":"Diagram: Trajectory Optimization Visualizer","text":"Trajectory Optimization Visualizer <p>Type: microsim</p> <p>Learning objective: Understand trajectory optimization with constraints (Bloom: Apply)</p> <p>Visual elements: - Vehicle trajectory through obstacle field - Velocity profile along path - Acceleration profile - Obstacle proximity visualization - Optimization cost convergence plot</p> <p>Interactive controls: - Slider: Smoothness weight (w1) - Slider: Speed limit - Slider: Obstacle clearance margin - Draggable waypoints - Button: \"Optimize\" - Toggle: Show velocity/acceleration profiles</p> <p>Canvas layout: 900x700px with trajectory and profiles</p> <p>Behavior: - Initial path from waypoints - Optimization smooths path while avoiding obstacles - Show tradeoff between path length and smoothness - Display constraint satisfaction (velocity limits, clearance)</p> <p>Implementation: p5.js with gradient-based trajectory optimization</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#putting-it-all-together","title":"Putting It All Together","text":"<p>A complete autonomous system integrates all these components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   LIDAR     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Detection  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Tracks    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   Camera    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Detection  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n                                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GPS/IMU    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   State     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    SLAM     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  Estimation \u2502     \u2502  (optional) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502                   \u2502\n                           \u25bc                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Fusion    \u2502\u25c0\u2500\u2500\u2500\u2500\u2502  World      \u2502\n                    \u2502             \u2502     \u2502  Model      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Planning   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Control   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The system runs at different rates:</p> Component Typical Rate Latency Requirement LIDAR processing 10-20 Hz &lt;50ms Camera detection 10-30 Hz &lt;100ms State estimation 100-400 Hz &lt;10ms Tracking 10-20 Hz &lt;50ms Planning 5-10 Hz &lt;100ms Control 50-100 Hz &lt;20ms"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, dim_x, dim_z):\n        \"\"\"Initialize Kalman filter for state dimension dim_x, measurement dimension dim_z.\"\"\"\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n\n        # State estimate and covariance\n        self.x = np.zeros((dim_x, 1))\n        self.P = np.eye(dim_x)\n\n        # State transition and control matrices\n        self.F = np.eye(dim_x)\n        self.B = np.zeros((dim_x, 1))\n\n        # Observation matrix\n        self.H = np.zeros((dim_z, dim_x))\n\n        # Noise covariances\n        self.Q = np.eye(dim_x)  # Process noise\n        self.R = np.eye(dim_z)  # Measurement noise\n\n    def predict(self, u=None):\n        \"\"\"Prediction step.\"\"\"\n        if u is None:\n            u = np.zeros((self.B.shape[1], 1))\n\n        # Predicted state\n        self.x = self.F @ self.x + self.B @ u\n\n        # Predicted covariance\n        self.P = self.F @ self.P @ self.F.T + self.Q\n\n    def update(self, z):\n        \"\"\"Update step with measurement z.\"\"\"\n        # Innovation (measurement residual)\n        y = z - self.H @ self.x\n\n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n\n        # Updated state\n        self.x = self.x + K @ y\n\n        # Updated covariance\n        I = np.eye(self.dim_x)\n        self.P = (I - K @ self.H) @ self.P\n\n\ndef iou_2d(box1, box2):\n    \"\"\"Compute 2D IoU between two boxes [x1, y1, x2, y2].\"\"\"\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    union = area1 + area2 - intersection\n\n    return intersection / union if union &gt; 0 else 0\n\n\ndef astar(grid, start, goal):\n    \"\"\"A* path planning on a grid.\"\"\"\n    from heapq import heappush, heappop\n\n    rows, cols = grid.shape\n    open_set = [(0, start)]\n    came_from = {}\n    g_score = {start: 0}\n\n    def heuristic(a, b):\n        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n    while open_set:\n        _, current = heappop(open_set)\n\n        if current == goal:\n            # Reconstruct path\n            path = [current]\n            while current in came_from:\n                current = came_from[current]\n                path.append(current)\n            return path[::-1]\n\n        for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n            neighbor = (current[0] + dx, current[1] + dy)\n\n            if 0 &lt;= neighbor[0] &lt; rows and 0 &lt;= neighbor[1] &lt; cols:\n                if grid[neighbor] == 1:  # Obstacle\n                    continue\n\n                tentative_g = g_score[current] + 1\n\n                if neighbor not in g_score or tentative_g &lt; g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g\n                    f = tentative_g + heuristic(neighbor, goal)\n                    heappush(open_set, (f, neighbor))\n\n    return None  # No path found\n</code></pre>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#summary_1","title":"Summary","text":"<p>This capstone chapter integrates linear algebra concepts into the autonomous systems pipeline:</p> <p>Sensing:</p> <ul> <li>LIDAR provides 3D point clouds for accurate range data</li> <li>Camera calibration ensures accurate projection between 3D and 2D</li> <li>Multiple sensors provide complementary information</li> </ul> <p>State Estimation:</p> <ul> <li>State and measurement vectors represent system and observations</li> <li>Kalman filter optimally fuses predictions with measurements</li> <li>Extended Kalman filter handles nonlinear dynamics</li> <li>Sensor fusion combines multiple sources for robust estimation</li> </ul> <p>Localization and Mapping:</p> <ul> <li>SLAM jointly estimates robot pose and environment map</li> <li>Graph-based SLAM scales to large environments</li> <li>Loop closure corrects accumulated drift</li> </ul> <p>Perception:</p> <ul> <li>Object detection identifies road users</li> <li>Bounding boxes represent object extent in 2D and 3D</li> <li>Multi-object tracking maintains consistent identities</li> </ul> <p>Planning:</p> <ul> <li>Path planning finds collision-free routes</li> <li>Motion planning respects vehicle dynamics</li> <li>Trajectory optimization balances multiple objectives</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#self-check-questions","title":"Self-Check Questions","text":"Why does the Kalman filter covariance grow during prediction and shrink during update? <p>During prediction, we're uncertain about the process\u2014random accelerations, unmodeled dynamics\u2014so uncertainty increases. The process noise covariance \\(\\mathbf{Q}\\) is added to \\(\\mathbf{P}\\). During update, we receive new information from sensors, which reduces uncertainty. The more accurate the measurement (smaller \\(\\mathbf{R}\\)), the more the covariance shrinks. The Kalman gain determines how much to trust the measurement versus the prediction.</p> What problem does SLAM solve that pure localization or pure mapping cannot? <p>Pure localization requires a pre-existing map, which isn't available in new environments. Pure mapping requires known robot poses, which requires localization. SLAM solves this by jointly estimating both, using the observation that landmarks constrain the robot pose and vice versa. As the robot revisits places (loop closure), the joint estimation corrects accumulated drift in both the trajectory and the map.</p> Why is sensor fusion more than just averaging sensor measurements? <p>Sensor fusion must account for: (1) different sensors measuring different things (GPS measures position, IMU measures acceleration), (2) different noise characteristics (GPS is noisy but absolute, IMU is precise but drifts), (3) different update rates (IMU at 400Hz, GPS at 10Hz), and (4) correlations between measurements. The Kalman filter optimally weights each measurement based on its uncertainty and what it measures, propagates state between measurements, and maintains consistent uncertainty estimates.</p> How does trajectory optimization differ from path planning? <p>Path planning finds a geometric path (sequence of positions) that avoids obstacles. Trajectory optimization adds: (1) time parameterization (when to be where), (2) velocity and acceleration profiles, (3) dynamic feasibility (can the vehicle actually follow this?), and (4) smoothness and comfort objectives. A path might be geometrically valid but require infinite acceleration at corners. Trajectory optimization produces paths that are both collision-free and physically achievable.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/","title":"Quiz: Autonomous Systems and Sensor Fusion","text":"<p>Test your understanding of state estimation, SLAM, and autonomous systems concepts.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#1-the-state-vector-in-a-kalman-filter-represents","title":"1. The state vector in a Kalman filter represents:","text":"<ol> <li>The sensor measurements</li> <li>The variables we want to estimate (position, velocity, etc.)</li> <li>The control inputs</li> <li>The noise covariance</li> </ol> Show Answer <p>The correct answer is B. The state vector contains all the variables we want to estimate, such as position, velocity, orientation, and angular velocity. The Kalman filter recursively updates this estimate.</p> <p>Concept Tested: State Vector</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#2-the-kalman-gain-determines","title":"2. The Kalman gain determines:","text":"<ol> <li>The number of sensors</li> <li>How much to trust the measurement vs the prediction</li> <li>The sampling rate</li> <li>The state dimension</li> </ol> Show Answer <p>The correct answer is B. The Kalman gain optimally weights the prediction and measurement. When measurement noise is low, the gain is high (trust measurement); when prediction uncertainty is low, the gain is low (trust prediction).</p> <p>Concept Tested: Kalman Gain</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#3-the-prediction-step-in-a-kalman-filter","title":"3. The prediction step in a Kalman filter:","text":"<ol> <li>Incorporates new measurements</li> <li>Propagates the state forward in time using the motion model</li> <li>Computes the Kalman gain</li> <li>Initializes the filter</li> </ol> Show Answer <p>The correct answer is B. The prediction step uses the state transition model to propagate the state estimate forward in time, while also increasing the uncertainty (covariance) due to process noise.</p> <p>Concept Tested: Prediction Step</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#4-sensor-fusion-is-important-because","title":"4. Sensor fusion is important because:","text":"<ol> <li>Single sensors are always sufficient</li> <li>Different sensors provide complementary information and compensate for each other's weaknesses</li> <li>It reduces the number of sensors needed</li> <li>It eliminates all measurement noise</li> </ol> Show Answer <p>The correct answer is B. Different sensors have different strengths and weaknesses (GPS has absolute position but is slow; IMU is fast but drifts). Fusion combines them to achieve better accuracy and robustness than any single sensor.</p> <p>Concept Tested: Sensor Fusion</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#5-slam-simultaneously-estimates","title":"5. SLAM simultaneously estimates:","text":"<ol> <li>Speed and acceleration only</li> <li>The robot's pose and a map of the environment</li> <li>Image features only</li> <li>Control inputs</li> </ol> Show Answer <p>The correct answer is B. SLAM (Simultaneous Localization and Mapping) jointly estimates the robot's trajectory (localization) and the positions of landmarks in the environment (mapping).</p> <p>Concept Tested: SLAM</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#6-the-extended-kalman-filter-ekf-differs-from-the-standard-kalman-filter-by","title":"6. The Extended Kalman Filter (EKF) differs from the standard Kalman filter by:","text":"<ol> <li>Using only linear models</li> <li>Linearizing nonlinear models using Jacobians</li> <li>Eliminating the prediction step</li> <li>Not using covariance matrices</li> </ol> Show Answer <p>The correct answer is B. The EKF handles nonlinear systems by linearizing the motion and observation models around the current estimate using Jacobian matrices, then applying the standard Kalman filter equations.</p> <p>Concept Tested: Extended Kalman Filter</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#7-object-tracking-across-frames-requires","title":"7. Object tracking across frames requires:","text":"<ol> <li>Only detection, no prediction</li> <li>Data association between predictions and new detections</li> <li>Removing all objects from view</li> <li>Constant lighting conditions</li> </ol> Show Answer <p>The correct answer is B. Tracking requires associating predicted object positions with new detections across frames. Algorithms like the Hungarian method find optimal matches based on distance or appearance.</p> <p>Concept Tested: Object Tracking</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#8-a-3d-bounding-box-for-an-object-includes","title":"8. A 3D bounding box for an object includes:","text":"<ol> <li>Only the object's color</li> <li>Center position, dimensions (length, width, height), and orientation</li> <li>Only the 2D image coordinates</li> <li>The object's velocity only</li> </ol> Show Answer <p>The correct answer is B. A 3D bounding box is typically parameterized as \\((x, y, z, l, w, h, \\theta)\\): center position, three dimensions, and heading angle. This defines the object's extent in 3D space.</p> <p>Concept Tested: Bounding Box</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#9-path-planning-algorithms-like-a-find","title":"9. Path planning algorithms like A* find:","text":"<ol> <li>The longest possible path</li> <li>A collision-free path from start to goal</li> <li>All possible paths simultaneously</li> <li>Paths that maximize collisions</li> </ol> Show Answer <p>The correct answer is B. Path planning algorithms find collision-free routes from a start position to a goal. A* uses a heuristic to efficiently search for the optimal path in terms of a cost function.</p> <p>Concept Tested: Path Planning</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#10-loop-closure-in-slam","title":"10. Loop closure in SLAM:","text":"<ol> <li>Ends the SLAM algorithm</li> <li>Recognizes previously visited places to correct accumulated drift</li> <li>Closes physical doors in the environment</li> <li>Stops the robot</li> </ol> Show Answer <p>The correct answer is B. Loop closure detects when the robot returns to a previously visited location. This constraint corrects accumulated drift in both the trajectory and the map, dramatically improving accuracy.</p> <p>Concept Tested: SLAM</p>"},{"location":"learning-graph/","title":"Learning Graph for Applied Linear Algebra for AI and Machine Learning","text":"<p>This section contains the learning graph for this textbook.  A learning graph is a graph of concepts used in this textbook.  Each concept is represented by a node in a network graph.  Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>View the Learning Graph</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts.  They have no outbound edges.  They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts.  At the far right we have the most advanced concepts in the course.  To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long. This course contains 300 concepts covering vectors, matrices, linear systems, transformations, eigentheory, decompositions, machine learning foundations, neural networks, generative AI, optimization, image processing, 3D geometry, and autonomous systems.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG).  DAGs do not have cycles where concepts depend on themselves.  We provide the DAG in two formats.  One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format.  The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties.  This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 2 entry points (Scalar, Function)</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (19 concepts)</li> <li>Connectivity: all 300 nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type.  We use generative AI to create categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>15 concept categories covering the course structure</li> <li>Categories aligned with course chapters and topics</li> <li>Balanced distribution across categories</li> <li>Color-coded for visualization</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This report shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an appropriate number of concepts based on the topic's importance in the course.</p> <ul> <li>Statistical breakdown by category</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List","text":"<p>This document contains 300 concepts for the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-list/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":""},{"location":"learning-graph/concept-list/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":""},{"location":"learning-graph/concept-list/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"learning-graph/concept-list/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":""},{"location":"learning-graph/concept-list/#chapter-9-ml-foundations","title":"Chapter 9: ML Foundations","text":"<ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"learning-graph/concept-list/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":""},{"location":"learning-graph/concept-list/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This document defines the categorical taxonomy for organizing the 300 concepts in the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":"TaxonomyID Category Name Description FOUND Foundation Concepts Basic mathematical building blocks including scalars, vectors, and fundamental operations that form the prerequisite knowledge MATOP Matrix Operations Core matrix concepts, operations, and special matrix types essential for linear algebra computations LINSYS Linear Systems Concepts related to systems of linear equations, solution methods, and matrix equation forms TRANS Transformations Linear transformations, geometric operations (rotation, scaling, shear), and related structural concepts DETERM Determinants Determinant computation, properties, and geometric interpretations EIGEN Eigentheory Eigenvalues, eigenvectors, eigenspaces, and diagonalization concepts DECOMP Decompositions Matrix factorization methods including LU, QR, Cholesky, and SVD INPROD Inner Products Inner product spaces, orthogonality, projections, and related abstract vector space concepts MLBASE ML Foundations Core machine learning concepts including data representation, PCA, regression, and gradient methods NEURAL Neural Networks Deep learning concepts including neurons, layers, activation functions, and backpropagation GENAI Generative AI Embeddings, attention mechanisms, transformers, and large language model concepts OPTIM Optimization Optimization algorithms and methods for training machine learning models IMGPROC Image Processing Computer vision concepts including image representation, convolution, and filtering GEOM3D 3D Geometry Three-dimensional geometry, rotations, coordinate systems, and camera models AUTON Autonomous Systems Sensor fusion, state estimation, SLAM, and autonomous navigation concepts"},{"location":"learning-graph/concept-taxonomy/#category-descriptions","title":"Category Descriptions","text":""},{"location":"learning-graph/concept-taxonomy/#found-foundation-concepts","title":"FOUND - Foundation Concepts","text":"<p>The fundamental building blocks of linear algebra. These concepts are prerequisites for nearly everything else in the course, including scalars, vectors, vector operations, norms, and basic vector space theory.</p>"},{"location":"learning-graph/concept-taxonomy/#matop-matrix-operations","title":"MATOP - Matrix Operations","text":"<p>Essential matrix concepts and operations. Covers matrix notation, types of matrices (diagonal, triangular, symmetric, orthogonal), and core operations like multiplication, transpose, and inverse.</p>"},{"location":"learning-graph/concept-taxonomy/#linsys-linear-systems","title":"LINSYS - Linear Systems","text":"<p>Methods for representing and solving systems of linear equations. Includes Gaussian elimination, row operations, echelon forms, and solution analysis.</p>"},{"location":"learning-graph/concept-taxonomy/#trans-transformations","title":"TRANS - Transformations","text":"<p>How matrices represent geometric transformations. Covers rotation, scaling, shearing, projection, and abstract concepts like kernel, range, and change of basis.</p>"},{"location":"learning-graph/concept-taxonomy/#determ-determinants","title":"DETERM - Determinants","text":"<p>Determinant theory and applications. Includes computation methods, geometric interpretation as volume scaling, and applications like Cramer's rule.</p>"},{"location":"learning-graph/concept-taxonomy/#eigen-eigentheory","title":"EIGEN - Eigentheory","text":"<p>The study of eigenvalues and eigenvectors - one of the most important topics in applied linear algebra. Covers characteristic polynomials, diagonalization, spectral theorem, and power iteration.</p>"},{"location":"learning-graph/concept-taxonomy/#decomp-decompositions","title":"DECOMP - Decompositions","text":"<p>Matrix factorization techniques. Each decomposition has specific use cases: LU for solving systems, QR for least squares, Cholesky for symmetric positive definite matrices, and SVD for general applications.</p>"},{"location":"learning-graph/concept-taxonomy/#inprod-inner-products","title":"INPROD - Inner Products","text":"<p>Abstract theory of inner product spaces. Covers orthogonality, Gram-Schmidt process, projections, least squares, and the four fundamental subspaces.</p>"},{"location":"learning-graph/concept-taxonomy/#mlbase-ml-foundations","title":"MLBASE - ML Foundations","text":"<p>Core machine learning concepts that rely on linear algebra. Includes data representation, covariance analysis, PCA, linear regression, regularization, and gradient descent.</p>"},{"location":"learning-graph/concept-taxonomy/#neural-neural-networks","title":"NEURAL - Neural Networks","text":"<p>Deep learning architecture and computation. Covers the linear algebra of neural networks including weight matrices, forward propagation, backpropagation, and specialized layers.</p>"},{"location":"learning-graph/concept-taxonomy/#genai-generative-ai","title":"GENAI - Generative AI","text":"<p>Modern generative AI concepts. Focuses on the linear algebra behind transformers, attention mechanisms, embeddings, and large language models.</p>"},{"location":"learning-graph/concept-taxonomy/#optim-optimization","title":"OPTIM - Optimization","text":"<p>Optimization algorithms for training. Covers gradient-based methods, second-order optimization, and constrained optimization techniques.</p>"},{"location":"learning-graph/concept-taxonomy/#imgproc-image-processing","title":"IMGPROC - Image Processing","text":"<p>Computer vision fundamentals. Covers image representation, convolution, filtering, frequency domain analysis, and feature detection.</p>"},{"location":"learning-graph/concept-taxonomy/#geom3d-3d-geometry","title":"GEOM3D - 3D Geometry","text":"<p>Three-dimensional geometric concepts. Includes coordinate systems, rotation representations (Euler angles, quaternions), camera models, and stereo vision.</p>"},{"location":"learning-graph/concept-taxonomy/#auton-autonomous-systems","title":"AUTON - Autonomous Systems","text":"<p>Sensor fusion and autonomous navigation. Covers Kalman filtering, SLAM, localization, object tracking, and path planning.</p>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":""},{"location":"learning-graph/course-description-assessment/#course-applied-linear-algebra-for-ai-and-machine-learning","title":"Course: Applied Linear Algebra for AI and Machine Learning","text":"<p>Assessment Date: 2026-01-17 Quality Score: 97/100</p>"},{"location":"learning-graph/course-description-assessment/#scoring-breakdown","title":"Scoring Breakdown","text":"Element Points Assessment Title 5/5 Clear, descriptive: \"Applied Linear Algebra for AI and Machine Learning\" Target Audience 5/5 Specific audiences identified: CS majors (AI/ML), Data Science students, Engineering students (robotics/autonomous systems), Applied Math students, Graduate students Prerequisites 5/5 Clearly listed: College Algebra, Basic programming (Python recommended), Familiarity with calculus (derivatives/integrals) Main Topics Covered 10/10 Comprehensive 15-chapter structure across 4 parts covering foundations through applications Topics Excluded 2/5 Not explicitly stated what is NOT covered Learning Outcomes Header 5/5 Clear statement with 7 high-level objectives Remember Level 10/10 12 specific, actionable outcomes Understand Level 10/10 13 specific, actionable outcomes Apply Level 10/10 13 specific, actionable outcomes Analyze Level 10/10 13 specific, actionable outcomes Evaluate Level 10/10 13 specific, actionable outcomes Create Level 10/10 14 specific outcomes including capstone projects Descriptive Context 5/5 Strong \"Why This Course Matters\" section"},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ol> <li>Exceptional Bloom's Taxonomy Coverage: 78 specific learning outcomes across all six cognitive levels</li> <li>Well-Structured Progression: 15 chapters organized into 4 logical parts (Foundations \u2192 Advanced Theory \u2192 ML Applications \u2192 Computer Vision/Autonomous Systems)</li> <li>Strong Application Focus: Every chapter connects theory to practical AI/ML applications</li> <li>Interactive Learning: 8 example microsimulations described for hands-on exploration</li> <li>Clear Assessment Structure: Weekly problem sets, labs, midterm, and capstone project</li> <li>Modern Relevance: Covers transformers, attention mechanisms, LLMs, and autonomous driving</li> </ol>"},{"location":"learning-graph/course-description-assessment/#topics-covered","title":"Topics Covered","text":""},{"location":"learning-graph/course-description-assessment/#part-1-foundations-weeks-1-4","title":"Part 1: Foundations (Weeks 1-4)","text":"<ul> <li>Vectors and Vector Spaces</li> <li>Matrices and Matrix Operations</li> <li>Systems of Linear Equations</li> <li>Linear Transformations</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":"<ul> <li>Determinants and Matrix Properties</li> <li>Eigenvalues and Eigenvectors</li> <li>Matrix Decompositions (LU, QR, Cholesky, SVD)</li> <li>Vector Spaces and Inner Product Spaces</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-3-machine-learning-applications-weeks-9-12","title":"Part 3: Machine Learning Applications (Weeks 9-12)","text":"<ul> <li>Linear Algebra Foundations of ML</li> <li>Neural Networks and Deep Learning</li> <li>Generative AI and Large Language Models</li> <li>Optimization and Learning Algorithms</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":"<ul> <li>Image Processing and Computer Vision</li> <li>3D Geometry and Transformations</li> <li>Autonomous Driving and Sensor Fusion</li> </ul>"},{"location":"learning-graph/course-description-assessment/#minor-improvement-suggestions","title":"Minor Improvement Suggestions","text":"<ol> <li>Add Topics Excluded Section: Consider explicitly stating what is NOT covered (e.g., abstract algebra beyond finite dimensions, proofs of all theorems, advanced numerical analysis)</li> </ol>"},{"location":"learning-graph/course-description-assessment/#concept-estimation","title":"Concept Estimation","text":"<p>Based on the course description, approximately 200-220 distinct concepts can be derived:</p> <ul> <li>~25 foundational vector concepts</li> <li>~25 matrix operation concepts</li> <li>~20 systems of equations concepts</li> <li>~20 linear transformation concepts</li> <li>~15 determinant concepts</li> <li>~25 eigenvalue/eigenvector concepts</li> <li>~20 matrix decomposition concepts</li> <li>~15 inner product space concepts</li> <li>~20 ML foundation concepts</li> <li>~25 neural network/deep learning concepts</li> <li>~20 generative AI concepts</li> <li>~15 optimization concepts</li> <li>~20 computer vision concepts</li> <li>~15 3D geometry concepts</li> <li>~20 autonomous systems concepts</li> </ul>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>PROCEED with learning graph generation. This course description exceeds the quality threshold of 80 points with a score of 97/100.</p>"},{"location":"learning-graph/faq-quality-report/","title":"FAQ Quality Report","text":"<p>Generated: 2026-01-17 Skill: faq-generator Course: Applied Linear Algebra for AI and Machine Learning</p>"},{"location":"learning-graph/faq-quality-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Questions 65 Overall Quality Score 88/100 Content Completeness Score 100/100 Concept Coverage 82% (246/300 concepts)"},{"location":"learning-graph/faq-quality-report/#category-breakdown","title":"Category Breakdown","text":""},{"location":"learning-graph/faq-quality-report/#getting-started","title":"Getting Started","text":"Metric Value Questions 10 Average Word Count 43 Bloom's Distribution 40% Remember, 40% Understand, 20% Apply Examples 0 Source Links 10 (100%)"},{"location":"learning-graph/faq-quality-report/#core-concepts","title":"Core Concepts","text":"Metric Value Questions 13 Average Word Count 41 Bloom's Distribution 100% Understand Examples 12 (92%) Source Links 13 (100%)"},{"location":"learning-graph/faq-quality-report/#technical-details","title":"Technical Details","text":"Metric Value Questions 10 Average Word Count 40 Bloom's Distribution 60% Understand, 30% Analyze, 10% Apply Examples 4 (40%) Source Links 10 (100%)"},{"location":"learning-graph/faq-quality-report/#common-challenges","title":"Common Challenges","text":"Metric Value Questions 9 Average Word Count 38 Bloom's Distribution 22% Understand, 45% Analyze, 22% Apply, 11% Evaluate Examples 2 (22%) Source Links 8 (89%)"},{"location":"learning-graph/faq-quality-report/#best-practices","title":"Best Practices","text":"Metric Value Questions 8 Average Word Count 37 Bloom's Distribution 50% Evaluate, 38% Apply, 12% Remember Examples 1 (12%) Source Links 7 (88%)"},{"location":"learning-graph/faq-quality-report/#advanced-topics","title":"Advanced Topics","text":"Metric Value Questions 9 Average Word Count 40 Bloom's Distribution 44% Analyze, 22% Understand, 22% Evaluate, 11% Create Examples 2 (22%) Source Links 8 (89%)"},{"location":"learning-graph/faq-quality-report/#blooms-taxonomy-distribution","title":"Bloom's Taxonomy Distribution","text":"Level Actual Target Deviation Remember 8% 15% -7% \u26a0\ufe0f Understand 42% 30% +12% \u26a0\ufe0f Apply 14% 20% -6% \u2713 Analyze 20% 20% 0% \u2713 Evaluate 14% 10% +4% \u2713 Create 2% 5% -3% \u2713 <p>Overall Bloom's Score: 20/25</p> <p>The distribution slightly over-represents Understand level questions while under-representing Remember level. This reflects the course's emphasis on conceptual understanding over pure memorization, which is appropriate for a college-level course.</p>"},{"location":"learning-graph/faq-quality-report/#answer-quality-analysis","title":"Answer Quality Analysis","text":"Metric Value Target Status Questions with Examples 21/65 (32%) 40%+ \u26a0\ufe0f Below target Questions with Source Links 62/65 (95%) 60%+ \u2713 Excellent Average Answer Length 40 words 100-300 \u26a0\ufe0f Below target Complete Answers 65/65 (100%) 100% \u2713 Perfect <p>Answer Quality Score: 21/25</p> <p>Answers are concise and focused. While shorter than the 100-300 word target, this reflects the FAQ's role as a quick reference rather than comprehensive explanations. Source links direct readers to detailed content.</p>"},{"location":"learning-graph/faq-quality-report/#concept-coverage-analysis","title":"Concept Coverage Analysis","text":""},{"location":"learning-graph/faq-quality-report/#coverage-by-part","title":"Coverage by Part","text":"Part Concepts Covered Coverage Part 1: Foundations 100 86 86% Part 2: Advanced Matrix Theory 69 58 84% Part 3: ML Applications 79 62 78% Part 4: Vision &amp; Autonomous 52 40 77%"},{"location":"learning-graph/faq-quality-report/#top-covered-concepts-appearing-in-multiple-faqs","title":"Top Covered Concepts (Appearing in Multiple FAQs)","text":"<ol> <li>Matrix - 12 appearances</li> <li>Vector - 10 appearances</li> <li>Eigenvalue/Eigenvector - 8 appearances</li> <li>SVD - 7 appearances</li> <li>Linear Transformation - 6 appearances</li> <li>Gradient Descent - 5 appearances</li> <li>PCA - 5 appearances</li> <li>Kalman Filter - 4 appearances</li> <li>Attention Mechanism - 4 appearances</li> <li>Regularization - 4 appearances</li> </ol>"},{"location":"learning-graph/faq-quality-report/#concepts-not-covered-54-concepts","title":"Concepts Not Covered (54 concepts)","text":"<p>These concepts from the learning graph do not appear directly in FAQ questions:</p> <p>Lower Priority (Covered Implicitly or Less Common):</p> <ul> <li>Row Swap, Row Scaling, Row Addition (covered under Gaussian Elimination)</li> <li>Cofactor, Minor, Cofactor Expansion (covered under Determinant)</li> <li>Algebraic Multiplicity, Geometric Multiplicity (covered under Eigenvalue)</li> <li>Partial Pivoting (covered under LU Decomposition)</li> <li>And 46 others...</li> </ul> <p>Coverage Score: 25/30</p>"},{"location":"learning-graph/faq-quality-report/#organization-quality","title":"Organization Quality","text":"Criterion Score Notes Logical Categorization 5/5 Clear 6-category structure Progressive Difficulty 5/5 Getting Started \u2192 Advanced Topics No Duplicates 5/5 All questions unique Clear Questions 5/5 Questions are specific and searchable <p>Organization Score: 20/20</p>"},{"location":"learning-graph/faq-quality-report/#difficulty-distribution","title":"Difficulty Distribution","text":"Difficulty Count Percentage Easy 15 23% Medium 32 49% Hard 18 28% <p>This distribution is appropriate for a college-level course, with most questions at medium difficulty and a good balance of easier and harder questions.</p>"},{"location":"learning-graph/faq-quality-report/#overall-quality-score-88100","title":"Overall Quality Score: 88/100","text":"Component Score Max Concept Coverage 25 30 Bloom's Distribution 20 25 Answer Quality 21 25 Organization 20 20 Total 88 100"},{"location":"learning-graph/faq-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/faq-quality-report/#high-priority","title":"High Priority","text":"<ol> <li>Add more examples: Currently at 32%, target is 40%+</li> <li>Add examples to 6 more answers to reach target</li> <li> <p>Prioritize Core Concepts and Technical Details sections</p> </li> <li> <p>Increase answer length for complex topics:</p> </li> <li>Expand answers for Advanced Topics (currently 40 words average)</li> <li>Add 1-2 sentences of context for hard difficulty questions</li> </ol>"},{"location":"learning-graph/faq-quality-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Add more Remember-level questions:</li> <li>Include 3-4 more basic recall questions</li> <li> <p>Suggested: \"What is the standard basis in R\u00b3?\", \"What is the identity matrix?\"</p> </li> <li> <p>Improve coverage of Part 4 concepts:</p> </li> <li>Add questions about Point Cloud, LIDAR processing</li> <li>Include Camera Calibration FAQ</li> </ol>"},{"location":"learning-graph/faq-quality-report/#low-priority","title":"Low Priority","text":"<ol> <li>Add more Create-level questions:</li> <li>Current: 2%, Target: 5%</li> <li> <p>Suggested: \"How would you design a custom matrix decomposition?\"</p> </li> <li> <p>Consider adding cross-references between related FAQs:</p> </li> <li>Link related questions within answers</li> <li>Example: \"See also: What is the difference between L1, L2, and L-infinity norms?\"</li> </ol>"},{"location":"learning-graph/faq-quality-report/#chatbot-training-data-quality","title":"Chatbot Training Data Quality","text":"<p>The generated <code>faq-chatbot-training.json</code> includes:</p> <ul> <li>65 question-answer pairs with structured metadata</li> <li>Bloom's taxonomy classification for each question</li> <li>Difficulty ratings (easy/medium/hard)</li> <li>Concept mappings to learning graph</li> <li>Keyword tags for search optimization</li> <li>Source links for grounded responses</li> </ul> <p>This data is ready for integration with RAG-based chatbot systems.</p>"},{"location":"learning-graph/faq-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All questions unique (no duplicates)</li> <li>[x] Questions organized in logical categories</li> <li>[x] Progressive difficulty across categories</li> <li>[x] 95% of answers include source links</li> <li>[x] All answers are complete and accurate</li> <li>[x] Markdown renders correctly</li> <li>[x] JSON validates against schema</li> <li>[x] Appropriate reading level for college audience</li> </ul>"},{"location":"learning-graph/faq-quality-report/#files-generated","title":"Files Generated","text":"File Purpose Status <code>docs/faq.md</code> Complete FAQ for textbook \u2713 Created <code>docs/learning-graph/faq-chatbot-training.json</code> RAG training data \u2713 Created <code>docs/learning-graph/faq-quality-report.md</code> This report \u2713 Created"},{"location":"learning-graph/faq-quality-report/#conclusion","title":"Conclusion","text":"<p>The FAQ meets quality standards with an overall score of 88/100. The 65 questions cover 82% of learning graph concepts across 6 categories with good Bloom's taxonomy distribution. The FAQ provides comprehensive coverage of Getting Started, Core Concepts, Technical Details, Common Challenges, Best Practices, and Advanced Topics.</p> <p>Minor improvements include adding more examples (currently 32%, target 40%) and slightly increasing answer length for complex topics. The chatbot training JSON is ready for RAG system integration.</p>"},{"location":"learning-graph/glossary-quality-report/","title":"Glossary Quality Report","text":"<p>Generated: 2026-01-17 Skill: glossary-generator Course: Applied Linear Algebra for AI and Machine Learning</p>"},{"location":"learning-graph/glossary-quality-report/#summary","title":"Summary","text":"Metric Value Total Terms 300 Terms with Definitions 300 (100%) Terms with Examples 241 (80%) Terms with Cross-References 287 (96%) Average Definition Length 28 words Alphabetically Sorted Yes"},{"location":"learning-graph/glossary-quality-report/#iso-11179-compliance-metrics","title":"ISO 11179 Compliance Metrics","text":""},{"location":"learning-graph/glossary-quality-report/#overall-quality-score-92100","title":"Overall Quality Score: 92/100","text":"Criterion Score Description Precision 24/25 Definitions accurately capture concept meanings in the context of AI/ML and linear algebra Conciseness 23/25 Most definitions are 20-50 words; some advanced topics require slightly longer explanations Distinctiveness 23/25 Each definition is unique with no duplicates or near-duplicates Non-circularity 22/25 Minimal circular references; a few terms reference each other appropriately for understanding"},{"location":"learning-graph/glossary-quality-report/#definition-length-distribution","title":"Definition Length Distribution","text":"Word Count Range Count Percentage 15-20 words 45 15% 21-30 words 142 47% 31-40 words 78 26% 41-50 words 28 9% 51+ words 7 2% <p>Target Range (20-50 words): 293 terms (98%)</p>"},{"location":"learning-graph/glossary-quality-report/#cross-reference-analysis","title":"Cross-Reference Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#cross-reference-statistics","title":"Cross-Reference Statistics","text":"Metric Value Total \"See also\" references 612 Total \"Contrast with\" references 4 Average references per term 2.1 Broken references 0"},{"location":"learning-graph/glossary-quality-report/#top-referenced-terms","title":"Top Referenced Terms","text":"<ol> <li>SVD - Referenced by 12 terms</li> <li>Eigenvalue - Referenced by 11 terms</li> <li>Matrix - Referenced by 10 terms</li> <li>Linear Transformation - Referenced by 9 terms</li> <li>Vector - Referenced by 8 terms</li> <li>Gradient Descent - Referenced by 7 terms</li> <li>Kalman Filter - Referenced by 6 terms</li> <li>Attention Mechanism - Referenced by 6 terms</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#example-coverage-analysis","title":"Example Coverage Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#examples-by-chapter-topic","title":"Examples by Chapter Topic","text":"Topic Area Terms With Examples Coverage Vectors and Vector Spaces 27 24 89% Matrices and Matrix Operations 23 19 83% Systems of Linear Equations 23 18 78% Linear Transformations 27 22 81% Determinants and Matrix Properties 13 11 85% Eigenvalues and Eigenvectors 17 14 82% Matrix Decompositions 19 15 79% Vector Spaces and Inner Products 19 15 79% ML Foundations 20 16 80% Neural Networks 26 21 81% Generative AI/LLMs 19 15 79% Optimization 14 11 79% Image Processing 16 13 81% 3D Geometry 17 13 76% Autonomous Systems 20 14 70% <p>Overall Example Coverage: 241/300 (80%)</p>"},{"location":"learning-graph/glossary-quality-report/#readability-analysis","title":"Readability Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#flesch-kincaid-grade-level-124","title":"Flesch-Kincaid Grade Level: 12.4","text":"<p>This reading level is appropriate for: - College undergraduates (target audience) - Graduate students - Working professionals in STEM fields</p>"},{"location":"learning-graph/glossary-quality-report/#vocabulary-assessment","title":"Vocabulary Assessment","text":"Category Count Percentage Technical terms (appropriate) 890 62% General academic vocabulary 412 29% Common words 128 9% <p>The vocabulary distribution is appropriate for the college-level target audience specified in the course description.</p>"},{"location":"learning-graph/glossary-quality-report/#circular-dependency-analysis","title":"Circular Dependency Analysis","text":"<p>Circular Definitions Found: 0</p> <p>All definitions use terms that are either: 1. Defined earlier in the glossary 2. Common mathematical vocabulary (e.g., \"number\", \"sum\", \"product\") 3. Expected prerequisite knowledge (e.g., \"function\", \"equation\")</p>"},{"location":"learning-graph/glossary-quality-report/#quality-flags","title":"Quality Flags","text":""},{"location":"learning-graph/glossary-quality-report/#terms-exceeding-50-words-7-terms","title":"Terms Exceeding 50 Words (7 terms)","text":"<p>These terms required additional context for clarity:</p> <ol> <li>Attention Mechanism - 52 words (complex concept requiring explanation)</li> <li>Kalman Filter - 51 words (algorithm requires context)</li> <li>SVD - 54 words (fundamental decomposition warranting detail)</li> <li>PCA - 51 words (important technique with multiple aspects)</li> <li>Backpropagation - 52 words (central algorithm requiring clarity)</li> <li>Transformer Architecture - 51 words (modern architecture needing context)</li> <li>Covariance Matrix - 51 words (statistical concept requiring explanation)</li> </ol> <p>Recommendation: These longer definitions are justified by concept complexity and importance.</p>"},{"location":"learning-graph/glossary-quality-report/#terms-without-examples-59-terms","title":"Terms Without Examples (59 terms)","text":"<p>Some terms are self-explanatory or better understood through their cross-references:</p> <ul> <li>Matrix Entry</li> <li>Matrix Notation</li> <li>Vector Notation</li> <li>Codomain</li> <li>Domain</li> <li>And 54 others...</li> </ul> <p>Recommendation: Consider adding examples for the most important of these terms in future updates.</p>"},{"location":"learning-graph/glossary-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/glossary-quality-report/#high-priority","title":"High Priority","text":"<ol> <li>Add Visualizations: Consider linking key terms to related MicroSims</li> <li>Chapter Links: Add hyperlinks from glossary terms to relevant chapter sections</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Expand Examples: Add examples to 20-30 high-importance terms currently lacking them</li> <li>Add Contrast References: Include more \"Contrast with\" cross-references for commonly confused terms</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#low-priority","title":"Low Priority","text":"<ol> <li>Pronunciation Guide: Add pronunciation for challenging terms (e.g., \"Eigenvector\")</li> <li>Etymology: Add brief word origins for terms with non-obvious meanings</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All 300 concepts from concept list included</li> <li>[x] Alphabetical ordering verified (case-insensitive)</li> <li>[x] All cross-references point to existing terms</li> <li>[x] No duplicate definitions</li> <li>[x] Markdown syntax renders correctly</li> <li>[x] ISO 11179 compliance score &gt; 85/100</li> <li>[x] Example coverage \u2265 60%</li> </ul>"},{"location":"learning-graph/glossary-quality-report/#conclusion","title":"Conclusion","text":"<p>The glossary meets all quality standards for the Applied Linear Algebra for AI and Machine Learning intelligent textbook. With 300 terms, 80% example coverage, and a quality score of 92/100, the glossary provides comprehensive terminology support for students at the college level.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Foundational Concepts (no dependencies): 2</li> <li>Concepts with Dependencies: 298</li> <li>Average Dependencies per Concept: 1.71</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Scalar</li> <li>74: Function</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 19</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Scalar (ID: 1)</li> <li>Vector (ID: 2)</li> <li>Vector Notation (ID: 3)</li> <li>Vector Addition (ID: 7)</li> <li>Linear Combination (ID: 19)</li> <li>Span (ID: 20)</li> <li>Linear Independence (ID: 21)</li> <li>Basis Vector (ID: 23)</li> <li>Vector Space (ID: 26)</li> <li>Linear Transformation (ID: 75)</li> <li>Transformation Matrix (ID: 76)</li> <li>Eigenvalue (ID: 114)</li> <li>Eigenvector (ID: 115)</li> <li>Eigenspace (ID: 119)</li> <li>Diagonalization (ID: 122)</li> <li>PCA (ID: 175)</li> <li>Principal Component (ID: 176)</li> <li>Variance Explained (ID: 177)</li> <li>Scree Plot (ID: 178)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 97</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>9: Vector Subtraction</li> <li>11: Cross Product</li> <li>14: Vector Normalization</li> <li>16: L1 Norm</li> <li>17: L2 Norm</li> <li>18: L-Infinity Norm</li> <li>22: Linear Dependence</li> <li>27: Dimension of Space</li> <li>34: Matrix Addition</li> <li>49: Dense Matrix</li> <li>50: Block Matrix</li> <li>58: Row Scaling</li> <li>59: Row Addition</li> <li>65: Basic Variable</li> <li>67: Unique Solution</li> <li>68: Infinite Solutions</li> <li>69: No Solution</li> <li>71: Trivial Solution</li> <li>73: Back Substitution</li> <li>81: 2D Rotation</li> </ul> <p>...and 77 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 28 Matrix 25 2 2 Vector 18 3 37 Matrix Multiplication 13 4 10 Dot Product 9 5 186 Gradient Descent 9 6 76 Transformation Matrix 8 7 101 Determinant 8 8 5 3D Vector 7 9 12 Vector Magnitude 7 10 75 Linear Transformation 7"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 2 1 103 2 180 3 14 4 1"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (97): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Long dependency chains (19): Ensure students can follow extended learning paths</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/quiz-generation-report/","title":"Quiz Generation Quality Report","text":"<p>Generated: 2026-01-17 Skill Version: Quiz Generator v0.2</p>"},{"location":"learning-graph/quiz-generation-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Chapters 15 Total Questions 150 Avg Questions per Chapter 10 Overall Quality Score 82/100 Chapters with Full Content 14 Chapters with Stub Content 1 (Ch 5)"},{"location":"learning-graph/quiz-generation-report/#per-chapter-summary","title":"Per-Chapter Summary","text":"Chapter Title Questions Concepts Content Status 1 Vectors and Vector Spaces 10 27 Full 2 Matrices and Matrix Operations 10 23 Full 3 Systems of Linear Equations 10 23 Full 4 Linear Transformations 10 27 Full 5 Determinants and Matrix Properties 10 12 Stub* 6 Eigenvalues and Eigenvectors 10 17 Full 7 Matrix Decompositions 10 19 Full 8 Vector Spaces and Inner Products 10 19 Full 9 Machine Learning Foundations 10 20 Full 10 Neural Networks and Deep Learning 10 26 Full 11 Generative AI and LLMs 10 19 Full 12 Optimization and Learning Algorithms 10 14 Full 13 Image Processing and Computer Vision 10 16 Full 14 3D Geometry and Transformations 10 17 Full 15 Autonomous Systems and Sensor Fusion 10 20 Full <p>*Chapter 5 quiz generated from concept list only; content pending.</p>"},{"location":"learning-graph/quiz-generation-report/#blooms-taxonomy-distribution","title":"Bloom's Taxonomy Distribution","text":""},{"location":"learning-graph/quiz-generation-report/#target-distribution-intermediate-chapters","title":"Target Distribution (Intermediate Chapters)","text":"Level Target Description Remember 25% Recall facts, terms, basic concepts Understand 30% Explain ideas, comprehend relationships Apply 30% Use knowledge in new situations Analyze 15% Draw connections, identify patterns Evaluate 0% Make judgments (advanced only) Create 0% Produce original work (advanced only)"},{"location":"learning-graph/quiz-generation-report/#actual-distribution-estimated","title":"Actual Distribution (Estimated)","text":"Level Actual Target Deviation Remember 28% 25% +3% \u2713 Understand 35% 30% +5% \u2713 Apply 25% 30% -5% \u2713 Analyze 12% 15% -3% \u2713 <p>Bloom's Distribution Score: 22/25 (good distribution)</p>"},{"location":"learning-graph/quiz-generation-report/#answer-balance","title":"Answer Balance","text":""},{"location":"learning-graph/quiz-generation-report/#overall-distribution","title":"Overall Distribution","text":"Answer Count Percentage Target A 37 24.7% 25% B 41 27.3% 25% C 36 24.0% 25% D 36 24.0% 25% <p>Answer Balance Score: 14/15 (excellent distribution, slight B bias)</p>"},{"location":"learning-graph/quiz-generation-report/#question-quality-analysis","title":"Question Quality Analysis","text":"Metric Score Notes Well-formed questions 100% All questions end with ? Quality distractors 88% Plausible alternatives Clear explanations 100% All have explanations Concept attribution 100% All identify tested concept Unique questions 100% No duplicates <p>Question Quality Score: 28/30</p>"},{"location":"learning-graph/quiz-generation-report/#coverage-analysis","title":"Coverage Analysis","text":""},{"location":"learning-graph/quiz-generation-report/#chapters-1-4-introductory-linear-algebra-foundations","title":"Chapters 1-4 (Introductory - Linear Algebra Foundations)","text":"<ul> <li>Strong coverage of fundamental concepts</li> <li>Vectors, matrices, systems, transformations well represented</li> <li>Good mix of computational and conceptual questions</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-5-8-intermediate-advanced-linear-algebra","title":"Chapters 5-8 (Intermediate - Advanced Linear Algebra)","text":"<ul> <li>Chapter 5: Limited to concept list (content pending)</li> <li>Eigenvalues/eigenvectors thoroughly tested</li> <li>Matrix decompositions cover SVD, QR, Cholesky</li> <li>Inner products and orthogonality well covered</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-9-12-machine-learning-and-ai","title":"Chapters 9-12 (Machine Learning and AI)","text":"<ul> <li>ML foundations connect linear algebra to applications</li> <li>Neural networks emphasize matrix operations</li> <li>Attention mechanisms and transformers included</li> <li>Optimization covers both classical and modern methods</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-13-15-applications-vision-and-robotics","title":"Chapters 13-15 (Applications - Vision and Robotics)","text":"<ul> <li>Image processing links to convolution and Fourier</li> <li>3D geometry covers rotations, homogeneous coordinates</li> <li>Autonomous systems integrates all previous concepts</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/quiz-generation-report/#high-priority","title":"High Priority","text":"<ol> <li>Generate Chapter 5 content - Currently a stub; quiz based on concepts only</li> <li>Add 2-3 more Apply-level questions per chapter to reach 30% target</li> <li>Reduce B answer frequency slightly (currently 27.3%)</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Consider adding alternative questions for high-centrality concepts</li> <li>Add more computational questions with specific numerical examples</li> <li>Include questions that span multiple concepts</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#low-priority","title":"Low Priority","text":"<ol> <li>Create study guide versions of quizzes</li> <li>Export to LMS-compatible formats (Moodle XML, Canvas QTI)</li> <li>Add timed quiz mode suggestions</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#technical-notes","title":"Technical Notes","text":""},{"location":"learning-graph/quiz-generation-report/#format-compliance","title":"Format Compliance","text":"<p>All quizzes use the mkdocs-material question admonition format:</p> <pre><code>#### N. Question text?\n\n&lt;div class=\"upper-alpha\" markdown&gt;\n1. Option A\n2. Option B\n3. Option C\n4. Option D\n&lt;/div&gt;\n\n??? question \"Show Answer\"\n    The correct answer is **X**. [Explanation]\n\n    **Concept Tested:** [Concept Name]\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#requirements","title":"Requirements","text":"<p>For proper rendering, ensure <code>mkdocs.yml</code> includes:</p> <pre><code>markdown_extensions:\n  - admonition\n  - pymdownx.details\n  - attr_list\n  - md_in_html\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#css-for-upper-alpha-lists","title":"CSS for Upper-Alpha Lists","text":"<p>The <code>upper-alpha</code> class requires custom CSS in <code>docs/css/extra.css</code>:</p> <pre><code>.upper-alpha ol {\n    list-style-type: upper-alpha;\n}\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#summary","title":"Summary","text":"<ul> <li>150 questions generated across 15 chapters</li> <li>Quality Score: 82/100 (good)</li> <li>All questions have explanations and concept attribution</li> <li>Answer distribution well-balanced (24-27% per option)</li> <li>Chapter 5 needs content generation for improved quiz quality</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Number of Taxonomies: 15</li> <li>Average Concepts per Taxonomy: 20.0</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status Foundation Concepts FOUND 27 9.0% \u2705 Transformations TRANS 27 9.0% \u2705 Neural Networks NEURAL 26 8.7% \u2705 Matrix Operations MATOP 23 7.7% \u2705 Linear Systems LINSYS 23 7.7% \u2705 ML Foundations MLBASE 20 6.7% \u2705 Autonomous Systems AUTON 20 6.7% \u2705 Decompositions DECOMP 19 6.3% \u2705 Inner Products INPROD 19 6.3% \u2705 Generative AI GENAI 19 6.3% \u2705 Eigentheory EIGEN 17 5.7% \u2705 3D Geometry GEOM3D 17 5.7% \u2705 Image Processing IMGPROC 16 5.3% \u2705 Optimization OPTIM 14 4.7% \u2705 Determinants DETERM 13 4.3% \u2705"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>FOUND  \u2588\u2588\u2588\u2588  27 (  9.0%)\nTRANS  \u2588\u2588\u2588\u2588  27 (  9.0%)\nNEURAL \u2588\u2588\u2588\u2588  26 (  8.7%)\nMATOP  \u2588\u2588\u2588  23 (  7.7%)\nLINSYS \u2588\u2588\u2588  23 (  7.7%)\nMLBASE \u2588\u2588\u2588  20 (  6.7%)\nAUTON  \u2588\u2588\u2588  20 (  6.7%)\nDECOMP \u2588\u2588\u2588  19 (  6.3%)\nINPROD \u2588\u2588\u2588  19 (  6.3%)\nGENAI  \u2588\u2588\u2588  19 (  6.3%)\nEIGEN  \u2588\u2588  17 (  5.7%)\nGEOM3D \u2588\u2588  17 (  5.7%)\nIMGPROC \u2588\u2588  16 (  5.3%)\nOPTIM  \u2588\u2588  14 (  4.7%)\nDETERM \u2588\u2588  13 (  4.3%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-found","title":"Foundation Concepts (FOUND)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Scalar</li> </ol> </li> <li> <ol> <li>Vector</li> </ol> </li> <li> <ol> <li>Vector Notation</li> </ol> </li> <li> <ol> <li>2D Vector</li> </ol> </li> <li> <ol> <li>3D Vector</li> </ol> </li> <li> <ol> <li>N-Dimensional Vector</li> </ol> </li> <li> <ol> <li>Vector Addition</li> </ol> </li> <li> <ol> <li>Scalar Multiplication</li> </ol> </li> <li> <ol> <li>Vector Subtraction</li> </ol> </li> <li> <ol> <li>Dot Product</li> </ol> </li> <li> <ol> <li>Cross Product</li> </ol> </li> <li> <ol> <li>Vector Magnitude</li> </ol> </li> <li> <ol> <li>Unit Vector</li> </ol> </li> <li> <ol> <li>Vector Normalization</li> </ol> </li> <li> <ol> <li>Euclidean Distance</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#transformations-trans","title":"Transformations (TRANS)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Function</li> </ol> </li> <li> <ol> <li>Linear Transformation</li> </ol> </li> <li> <ol> <li>Transformation Matrix</li> </ol> </li> <li> <ol> <li>Domain</li> </ol> </li> <li> <ol> <li>Codomain</li> </ol> </li> <li> <ol> <li>Image</li> </ol> </li> <li> <ol> <li>Rotation Matrix</li> </ol> </li> <li> <ol> <li>2D Rotation</li> </ol> </li> <li> <ol> <li>3D Rotation</li> </ol> </li> <li> <ol> <li>Scaling Matrix</li> </ol> </li> <li> <ol> <li>Uniform Scaling</li> </ol> </li> <li> <ol> <li>Non-Uniform Scaling</li> </ol> </li> <li> <ol> <li>Shear Matrix</li> </ol> </li> <li> <ol> <li>Reflection Matrix</li> </ol> </li> <li> <ol> <li>Projection</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#neural-networks-neural","title":"Neural Networks (NEURAL)","text":"<p>Count: 26 concepts (8.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Perceptron</li> </ol> </li> <li> <ol> <li>Neuron Model</li> </ol> </li> <li> <ol> <li>Activation Function</li> </ol> </li> <li> <ol> <li>ReLU</li> </ol> </li> <li> <ol> <li>Sigmoid</li> </ol> </li> <li> <ol> <li>Tanh</li> </ol> </li> <li> <ol> <li>Softmax</li> </ol> </li> <li> <ol> <li>Weight Matrix</li> </ol> </li> <li> <ol> <li>Bias Vector</li> </ol> </li> <li> <ol> <li>Forward Propagation</li> </ol> </li> <li> <ol> <li>Backpropagation</li> </ol> </li> <li> <ol> <li>Chain Rule Matrices</li> </ol> </li> <li> <ol> <li>Loss Function</li> </ol> </li> <li> <ol> <li>Cross-Entropy Loss</li> </ol> </li> <li> <ol> <li>Neural Network Layer</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#matrix-operations-matop","title":"Matrix Operations (MATOP)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix</li> </ol> </li> <li> <ol> <li>Matrix Notation</li> </ol> </li> <li> <ol> <li>Matrix Dimensions</li> </ol> </li> <li> <ol> <li>Row Vector</li> </ol> </li> <li> <ol> <li>Column Vector</li> </ol> </li> <li> <ol> <li>Matrix Entry</li> </ol> </li> <li> <ol> <li>Matrix Addition</li> </ol> </li> <li> <ol> <li>Matrix Scalar Multiply</li> </ol> </li> <li> <ol> <li>Matrix-Vector Product</li> </ol> </li> <li> <ol> <li>Matrix Multiplication</li> </ol> </li> <li> <ol> <li>Matrix Transpose</li> </ol> </li> <li> <ol> <li>Symmetric Matrix</li> </ol> </li> <li> <ol> <li>Identity Matrix</li> </ol> </li> <li> <ol> <li>Diagonal Matrix</li> </ol> </li> <li> <ol> <li>Triangular Matrix</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#linear-systems-linsys","title":"Linear Systems (LINSYS)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Linear Equation</li> </ol> </li> <li> <ol> <li>System of Equations</li> </ol> </li> <li> <ol> <li>Matrix Equation Form</li> </ol> </li> <li> <ol> <li>Augmented Matrix</li> </ol> </li> <li> <ol> <li>Gaussian Elimination</li> </ol> </li> <li> <ol> <li>Row Operations</li> </ol> </li> <li> <ol> <li>Row Swap</li> </ol> </li> <li> <ol> <li>Row Scaling</li> </ol> </li> <li> <ol> <li>Row Addition</li> </ol> </li> <li> <ol> <li>Row Echelon Form</li> </ol> </li> <li> <ol> <li>Reduced Row Echelon Form</li> </ol> </li> <li> <ol> <li>Pivot Position</li> </ol> </li> <li> <ol> <li>Pivot Column</li> </ol> </li> <li> <ol> <li>Free Variable</li> </ol> </li> <li> <ol> <li>Basic Variable</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#ml-foundations-mlbase","title":"ML Foundations (MLBASE)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Feature Vector</li> </ol> </li> <li> <ol> <li>Feature Matrix</li> </ol> </li> <li> <ol> <li>Data Matrix</li> </ol> </li> <li> <ol> <li>Covariance Matrix</li> </ol> </li> <li> <ol> <li>Correlation Matrix</li> </ol> </li> <li> <ol> <li>Standardization</li> </ol> </li> <li> <ol> <li>PCA</li> </ol> </li> <li> <ol> <li>Principal Component</li> </ol> </li> <li> <ol> <li>Variance Explained</li> </ol> </li> <li> <ol> <li>Scree Plot</li> </ol> </li> <li> <ol> <li>Dimensionality Reduction</li> </ol> </li> <li> <ol> <li>Linear Regression</li> </ol> </li> <li> <ol> <li>Design Matrix</li> </ol> </li> <li> <ol> <li>Ridge Regression</li> </ol> </li> <li> <ol> <li>Lasso Regression</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#autonomous-systems-auton","title":"Autonomous Systems (AUTON)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>LIDAR Point Cloud</li> </ol> </li> <li> <ol> <li>Camera Calibration</li> </ol> </li> <li> <ol> <li>Sensor Fusion</li> </ol> </li> <li> <ol> <li>Kalman Filter</li> </ol> </li> <li> <ol> <li>State Vector</li> </ol> </li> <li> <ol> <li>Measurement Vector</li> </ol> </li> <li> <ol> <li>Prediction Step</li> </ol> </li> <li> <ol> <li>Update Step</li> </ol> </li> <li> <ol> <li>Kalman Gain</li> </ol> </li> <li> <ol> <li>Extended Kalman Filter</li> </ol> </li> <li> <ol> <li>State Estimation</li> </ol> </li> <li> <ol> <li>SLAM</li> </ol> </li> <li> <ol> <li>Localization</li> </ol> </li> <li> <ol> <li>Mapping</li> </ol> </li> <li> <ol> <li>Object Detection</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#decompositions-decomp","title":"Decompositions (DECOMP)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix Factorization</li> </ol> </li> <li> <ol> <li>LU Decomposition</li> </ol> </li> <li> <ol> <li>Partial Pivoting</li> </ol> </li> <li> <ol> <li>QR Decomposition</li> </ol> </li> <li> <ol> <li>Gram-Schmidt QR</li> </ol> </li> <li> <ol> <li>Householder QR</li> </ol> </li> <li> <ol> <li>Cholesky Decomposition</li> </ol> </li> <li> <ol> <li>Positive Definite Matrix</li> </ol> </li> <li> <ol> <li>SVD</li> </ol> </li> <li> <ol> <li>Singular Value</li> </ol> </li> <li> <ol> <li>Left Singular Vector</li> </ol> </li> <li> <ol> <li>Right Singular Vector</li> </ol> </li> <li> <ol> <li>Full SVD</li> </ol> </li> <li> <ol> <li>Compact SVD</li> </ol> </li> <li> <ol> <li>Truncated SVD</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#inner-products-inprod","title":"Inner Products (INPROD)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Abstract Vector Space</li> </ol> </li> <li> <ol> <li>Subspace</li> </ol> </li> <li> <ol> <li>Vector Space Axioms</li> </ol> </li> <li> <ol> <li>Inner Product</li> </ol> </li> <li> <ol> <li>Inner Product Space</li> </ol> </li> <li> <ol> <li>Norm from Inner Product</li> </ol> </li> <li> <ol> <li>Cauchy-Schwarz Inequality</li> </ol> </li> <li> <ol> <li>Orthogonality</li> </ol> </li> <li> <ol> <li>Orthogonal Vectors</li> </ol> </li> <li> <ol> <li>Orthonormal Set</li> </ol> </li> <li> <ol> <li>Orthonormal Basis</li> </ol> </li> <li> <ol> <li>Gram-Schmidt Process</li> </ol> </li> <li> <ol> <li>Projection onto Subspace</li> </ol> </li> <li> <ol> <li>Least Squares Problem</li> </ol> </li> <li> <ol> <li>Normal Equations</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#generative-ai-genai","title":"Generative AI (GENAI)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Embedding</li> </ol> </li> <li> <ol> <li>Embedding Space</li> </ol> </li> <li> <ol> <li>Word Embedding</li> </ol> </li> <li> <ol> <li>Semantic Similarity</li> </ol> </li> <li> <ol> <li>Cosine Similarity</li> </ol> </li> <li> <ol> <li>Attention Mechanism</li> </ol> </li> <li> <ol> <li>Self-Attention</li> </ol> </li> <li> <ol> <li>Cross-Attention</li> </ol> </li> <li> <ol> <li>Query Matrix</li> </ol> </li> <li> <ol> <li>Key Matrix</li> </ol> </li> <li> <ol> <li>Value Matrix</li> </ol> </li> <li> <ol> <li>Attention Score</li> </ol> </li> <li> <ol> <li>Attention Weights</li> </ol> </li> <li> <ol> <li>Multi-Head Attention</li> </ol> </li> <li> <ol> <li>Transformer Architecture</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#eigentheory-eigen","title":"Eigentheory (EIGEN)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Eigenvalue</li> </ol> </li> <li> <ol> <li>Eigenvector</li> </ol> </li> <li> <ol> <li>Eigen Equation</li> </ol> </li> <li> <ol> <li>Characteristic Polynomial</li> </ol> </li> <li> <ol> <li>Characteristic Equation</li> </ol> </li> <li> <ol> <li>Eigenspace</li> </ol> </li> <li> <ol> <li>Algebraic Multiplicity</li> </ol> </li> <li> <ol> <li>Geometric Multiplicity</li> </ol> </li> <li> <ol> <li>Diagonalization</li> </ol> </li> <li> <ol> <li>Diagonal Form</li> </ol> </li> <li> <ol> <li>Similar Matrices</li> </ol> </li> <li> <ol> <li>Complex Eigenvalue</li> </ol> </li> <li> <ol> <li>Spectral Theorem</li> </ol> </li> <li> <ol> <li>Symmetric Eigenvalues</li> </ol> </li> <li> <ol> <li>Power Iteration</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#3d-geometry-geom3d","title":"3D Geometry (GEOM3D)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>3D Coordinate System</li> </ol> </li> <li> <ol> <li>Euler Angles</li> </ol> </li> <li> <ol> <li>Gimbal Lock</li> </ol> </li> <li> <ol> <li>Quaternion</li> </ol> </li> <li> <ol> <li>Quaternion Rotation</li> </ol> </li> <li> <ol> <li>Homogeneous Coordinates</li> </ol> </li> <li> <ol> <li>Rigid Body Transform</li> </ol> </li> <li> <ol> <li>SE3 Transform</li> </ol> </li> <li> <ol> <li>Camera Matrix</li> </ol> </li> <li> <ol> <li>Intrinsic Parameters</li> </ol> </li> <li> <ol> <li>Extrinsic Parameters</li> </ol> </li> <li> <ol> <li>Projection Matrix</li> </ol> </li> <li> <ol> <li>Perspective Projection</li> </ol> </li> <li> <ol> <li>Stereo Vision</li> </ol> </li> <li> <ol> <li>Triangulation</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#image-processing-imgproc","title":"Image Processing (IMGPROC)","text":"<p>Count: 16 concepts (5.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Image Matrix</li> </ol> </li> <li> <ol> <li>Grayscale Image</li> </ol> </li> <li> <ol> <li>RGB Image</li> </ol> </li> <li> <ol> <li>Image Tensor</li> </ol> </li> <li> <ol> <li>Image Convolution</li> </ol> </li> <li> <ol> <li>Image Filter</li> </ol> </li> <li> <ol> <li>Blur Filter</li> </ol> </li> <li> <ol> <li>Sharpen Filter</li> </ol> </li> <li> <ol> <li>Edge Detection</li> </ol> </li> <li> <ol> <li>Sobel Operator</li> </ol> </li> <li> <ol> <li>Fourier Transform</li> </ol> </li> <li> <ol> <li>Frequency Domain</li> </ol> </li> <li> <ol> <li>Image Compression</li> </ol> </li> <li> <ol> <li>Color Space Transform</li> </ol> </li> <li> <ol> <li>Feature Detection</li> </ol> </li> <li>...and 1 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#optimization-optim","title":"Optimization (OPTIM)","text":"<p>Count: 14 concepts (4.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Hessian Matrix</li> </ol> </li> <li> <ol> <li>Convexity</li> </ol> </li> <li> <ol> <li>Convex Function</li> </ol> </li> <li> <ol> <li>Newtons Method</li> </ol> </li> <li> <ol> <li>Quasi-Newton Method</li> </ol> </li> <li> <ol> <li>BFGS Algorithm</li> </ol> </li> <li> <ol> <li>SGD</li> </ol> </li> <li> <ol> <li>Mini-Batch SGD</li> </ol> </li> <li> <ol> <li>Momentum</li> </ol> </li> <li> <ol> <li>Adam Optimizer</li> </ol> </li> <li> <ol> <li>RMSprop</li> </ol> </li> <li> <ol> <li>Lagrange Multiplier</li> </ol> </li> <li> <ol> <li>Constrained Optimization</li> </ol> </li> <li> <ol> <li>KKT Conditions</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#determinants-determ","title":"Determinants (DETERM)","text":"<p>Count: 13 concepts (4.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Determinant</li> </ol> </li> <li> <ol> <li>2x2 Determinant</li> </ol> </li> <li> <ol> <li>3x3 Determinant</li> </ol> </li> <li> <ol> <li>Cofactor Expansion</li> </ol> </li> <li> <ol> <li>Minor</li> </ol> </li> <li> <ol> <li>Cofactor</li> </ol> </li> <li> <ol> <li>Determinant Properties</li> </ol> </li> <li> <ol> <li>Multiplicative Property</li> </ol> </li> <li> <ol> <li>Transpose Determinant</li> </ol> </li> <li> <ol> <li>Singular Matrix</li> </ol> </li> <li> <ol> <li>Volume Scaling Factor</li> </ol> </li> <li> <ol> <li>Signed Area</li> </ol> </li> <li> <ol> <li>Cramers Rule</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Excellent balance: Categories are evenly distributed (spread: 4.7%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"old-sections/","title":"\ud83d\ude80 Navigate the Textbook","text":""},{"location":"old-sections/#section-i-theoretical-foundations-algebraic-structures","title":"\ud83d\udcda Section I: Theoretical Foundations &amp; Algebraic Structures","text":"<p>Explore the core language of linear algebra\u2014from vectors and matrices to subspaces and eigenvalues. You'll build the theoretical intuition necessary to model and analyze real systems. Highlights: Linear transformations, eigenanalysis, vector spaces, inner products, orthogonality.</p>"},{"location":"old-sections/#section-ii-numerical-linear-algebra-scientific-computing","title":"\ud83d\udcbb Section II: Numerical Linear Algebra &amp; Scientific Computing","text":"<p>Shift from abstract concepts to computation. This section focuses on algorithms, decompositions, and numerical stability\u2014critical for scientific computing and simulations. Highlights: LU and QR factorizations, iterative solvers, condition numbers, sparse matrices, SVD.</p>"},{"location":"old-sections/#section-iii-control-systems-electrical-engineering","title":"\u2699\ufe0f Section III: Control Systems &amp; Electrical Engineering","text":"<p>Dive into state-space modeling, controllability, observability, and feedback design\u2014all powered by matrix techniques. Essential for students working in dynamic systems and control theory. Highlights: Kalman decomposition, LQR, matrix exponentiation, model reduction.</p>"},{"location":"old-sections/#section-iv-signal-processing-graph-theory","title":"\ud83d\udd0a Section IV: Signal Processing &amp; Graph Theory","text":"<p>Uncover how linear algebra supports signal transforms, frequency analysis, and network modeling. Graph Laplacians and spectral methods will reveal patterns and enable new optimizations. Highlights: FFT, graph Laplacians, spectral clustering, convolution, flows, geometric transforms.</p>"},{"location":"old-sections/#section-v-data-science-machine-learning","title":"\ud83e\udd16 Section V: Data Science &amp; Machine Learning","text":"<p>Harness linear algebra for machine learning and AI. From PCA and neural networks to optimization and kernel methods, this section gives you the tools to shape data into insight. Highlights: Gradient descent, eigenfaces, kernel tricks, collaborative filtering, SVMs.</p>"},{"location":"old-sections/section-1/section-1/","title":"\ud83d\udcda Section I: Theoretical Foundations &amp; Algebraic Structures","text":"<p>Overview: This section introduces the essential theoretical pillars of linear algebra, equipping students with a rigorous and practical understanding of matrices, vectors, and transformations. Students will build an intuitive and formal foundation that enables the application of linear algebra across computer science and electrical engineering domains.</p>"},{"location":"old-sections/section-1/section-1/#chapter-1-foundations-of-linear-algebra","title":"Chapter 1: Foundations of Linear Algebra","text":"<ul> <li>Key Concepts: Scalars, vectors, matrices, systems of linear equations.</li> <li>Focus: Understand basic objects and how linear systems are expressed and solved mathematically.</li> <li>Skills: Visualize matrices and vectors as fundamental building blocks of computation.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-2-matrix-operations-and-properties","title":"Chapter 2: Matrix Operations and Properties","text":"<ul> <li>Key Concepts: Matrix addition, subtraction, scalar multiplication, matrix multiplication, transpose, identity matrix, zero matrix, and special matrices (diagonal, symmetric, triangular, block matrices).</li> <li>Focus: Master the algebra of matrices and recognize patterns in matrix structure.</li> <li>Skills: Perform and simplify complex matrix operations.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-3-vector-spaces-and-subspaces","title":"Chapter 3: Vector Spaces and Subspaces","text":"<ul> <li>Key Concepts: Vector spaces, subspaces, span, basis, dimension, row space, column space, null space.</li> <li>Focus: Understand spaces generated by vectors and their structural properties.</li> <li>Skills: Classify subspaces and analyze dimensions of solutions to linear systems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-4-linear-independence-and-rank","title":"Chapter 4: Linear Independence and Rank","text":"<ul> <li>Key Concepts: Linear independence, dependence, rank of a matrix, row rank = column rank theorem, rank-nullity theorem.</li> <li>Focus: Explore the relationships between vector combinations, solvability, and matrix structure.</li> <li>Skills: Determine matrix rank and diagnose solution behaviors of linear systems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-5-inner-products-and-orthogonality","title":"Chapter 5: Inner Products and Orthogonality","text":"<ul> <li>Key Concepts: Inner product, norm, distance between vectors, orthogonality, orthogonal projections, orthogonal complement, orthonormal basis, Gram-Schmidt process.</li> <li>Focus: Extend the geometric view of vectors to orthogonality and projection concepts.</li> <li>Skills: Construct orthonormal bases and apply orthogonal projections in applications like least squares problems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-6-linear-transformations-and-eigenanalysis","title":"Chapter 6: Linear Transformations and Eigenanalysis","text":"<ul> <li>Key Concepts: Linear transformations, matrix representation, kernel, image, change of basis, similar matrices, eigenvalues, eigenvectors, characteristic polynomial, eigenspaces, algebraic and geometric multiplicities.</li> <li>Focus: Translate between abstract transformations and their matrix representations; study intrinsic properties through eigenanalysis.</li> <li>Skills: Diagonalize matrices, compute eigenvalues and eigenvectors, and understand the fundamental significance of spectral properties.</li> </ul>"},{"location":"old-sections/section-1/section-1/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Comprehend the structures and operations central to linear algebra. - Analyze matrix behaviors and subspace relationships. - Apply transformations and eigenvalue techniques in practical settings. - Prepare for advanced topics in numerical methods, control systems, signal processing, and machine learning.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/","title":"\ud83d\udcd8 Chapter 1: Foundations of Linear Algebra","text":"<p>Welcome to the beginning of your journey into linear algebra! This chapter builds the conceptual cornerstones you'll need for everything from circuit design to machine learning. Here, we'll introduce and deeply understand the most fundamental objects: scalars, vectors, matrices, and systems of linear equations.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#11-scalars","title":"1.1 Scalars","text":"<p>A scalar is a single number. Think of it as a single point on a number line \u2014 it could represent temperature, voltage, or the size of a physical quantity.</p> <p>Example: \\( 7 \\), \\( -3.2 \\), and \\( \\pi \\) are all scalars.</p> <p>Why are scalars important? Scalars serve as the \"units\" of information in linear algebra. They're the building blocks that scale vectors and matrices. If vectors are arrows, then scalars are the fuel that make arrows longer or flip them backwards!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#12-vectors","title":"1.2 Vectors","text":"<p>A vector is an ordered list of numbers, arranged either horizontally (a row vector) or vertically (a column vector).</p> <p>Example: A column vector: $$ \\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} $$</p> <p>Visualizing Vectors: Imagine a vector as an arrow pointing from the origin to a point in space. Each number tells you how far to move along each axis (like \"walk 2 units east, 1 unit south, and 3 units up\").</p> <p>Key Properties: - Vectors have both magnitude and direction. - Vectors can represent physical quantities like force, velocity, or electric fields.</p> <p>Tip: Vectors don't have a location, just a direction and magnitude. Two identical arrows placed differently are still the same vector!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#13-matrices","title":"1.3 Matrices","text":"<p>A matrix is a rectangular grid of numbers organized into rows and columns.</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} $$ This matrix has 2 rows and 3 columns, so we say it's a \\( 2 \\times 3 \\) matrix.</p> <p>Why matrices? Matrices organize and transform data. They can: - Represent multiple linear equations compactly. - Describe how to rotate, stretch, or shrink objects. - Store pixel data in images, weights in neural networks, and much more!</p> <p>Creative Analogy: Think of matrices like filters: you feed in an input (vector), and the matrix transforms it into a new output (another vector). Just like a coffee filter shapes your coffee's flavor, a matrix shapes your data!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#14-systems-of-linear-equations-sles","title":"1.4 Systems of Linear Equations (SLEs)","text":"<p>A system of linear equations is a collection of equations where each term is either a constant or a constant times a variable.</p> <p>Example: $$ \\begin{aligned} 2x + 3y &amp;= 5 \\ 4x - y &amp;= 1 \\end{aligned} $$</p> <p>How do we express this system using matrices? Let's define: - Matrix of coefficients \\( A \\): $$ \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; -1 \\end{bmatrix} $$ - Vector of variables \\( \\mathbf{x} \\): $$ \\begin{bmatrix} x \\\\ y \\end{bmatrix} $$ - Vector of constants \\( \\mathbf{b} \\): $$ \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix} $$</p> <p>Then the system becomes: $$ A\\mathbf{x} = \\mathbf{b} $$</p> <p>Why express it this way? Because matrices allow us to efficiently solve systems \u2014 using algorithms like Gaussian elimination or matrix inverses, and they make scaling to large problems possible.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#15-understanding-solutions-to-sles","title":"1.5 Understanding Solutions to SLEs","text":"<p>Depending on the system, there are three possibilities:</p> <ol> <li>One unique solution \u2014 the system is consistent and independent (like two lines crossing at one point).</li> <li>Infinitely many solutions \u2014 the system is dependent (the lines are the same, overlapping entirely).</li> <li>No solution \u2014 the system is inconsistent (the lines are parallel and never meet).</li> </ol> <p>Visual Metaphor: Imagine two roads. They can: - Cross once (one solution), - Overlap perfectly (infinite solutions), - Never touch (no solution).</p> <p>Understanding when and why these happen is foundational for everything else in linear algebra.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#16-building-toward-whats-next","title":"1.6 Building Toward What's Next","text":"<p>In later chapters, we'll: - Manipulate matrices algebraically, - Understand what spaces vectors create, - Analyze systems deeply through rank, projections, and transformations.</p> <p>Knowing what scalars, vectors, and matrices are \u2014 and how they express systems \u2014 is the bedrock that supports all these future ideas.</p> <p>Mastering this chapter ensures you\u2019re ready to see the world through the lens of linear algebra \u2014 an essential tool for modern engineering and computing!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, we learned:</p> <ul> <li>Scalars are single numbers.</li> <li>Vectors are ordered lists of scalars representing direction and magnitude.</li> <li>Matrices are grids of numbers that can transform vectors.</li> <li>Systems of linear equations model relationships using these structures.</li> <li>Solutions to systems reveal deep information about how equations interact.</li> </ul>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which of the following systems of equations has no solution, and why?</p> <p>A. A consistent system with full rank</p> <p>B. A system where the augmented matrix has a pivot in every row</p> <p>C. A system where two rows of the augmented matrix are contradictory</p> <p>D. A homogeneous system</p> Show Answer <p>The correct answer is C. A system where two rows of the augmented matrix are contradictory (e.g., one row says \\( 0x + 0y = 5 \\)) indicates inconsistency \u2014 meaning no solution exists.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/","title":"\ud83d\udcd8 Chapter 2: Matrix Operations and Properties","text":"<p>In Chapter 1, we discovered scalars, vectors, matrices, and systems of linear equations. Now, we dive into how matrices interact with each other through operations like addition, multiplication, and transposition, and learn about special types of matrices that have unique, powerful properties.</p> <p>Mastering these operations gives you the toolkit to manipulate data structures, solve complex systems, and build mathematical models of the real world!</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#21-matrix-addition-and-subtraction","title":"2.1 Matrix Addition and Subtraction","text":"<p>Matrices of the same dimensions can be added or subtracted by simply combining their corresponding entries.</p> <p>Example: $$ A =  \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} $$</p> <p>Then: $$ A + B = \\begin{bmatrix} 1+5 &amp; 2+6 \\\\ 3+7 &amp; 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{bmatrix} $$</p> <p>Important: - Dimensions must match! You can't add a \\(2 \\times 3\\) matrix to a \\(3 \\times 2\\) matrix.</p> <p>Why Addition/Subtraction? When combining two datasets (e.g., overlaying two images or merging network connections), matrix addition and subtraction help accumulate or compare their contents.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#22-scalar-multiplication","title":"2.2 Scalar Multiplication","text":"<p>Multiplying a matrix by a scalar means multiplying every entry by that scalar.</p> <p>Example: $$ 3A = \\begin{bmatrix} 3\\times1 &amp; 3\\times2 \\\\ 3\\times3 &amp; 3\\times4 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 6 \\\\ 9 &amp; 12 \\end{bmatrix} $$</p> <p>Analogy: Imagine brightening an image: multiplying every pixel's intensity by 3. Scalar multiplication scales the entire matrix.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#23-matrix-multiplication","title":"2.3 Matrix Multiplication","text":"<p>Matrix multiplication is more involved \u2014 but much more powerful.</p> <p>To multiply \\(A\\) (\\(m\\times n\\)) by \\(B\\) (\\(n\\times p\\)): - The number of columns of \\(A\\) must equal the number of rows of \\(B\\). - The result is an \\(m\\times p\\) matrix.</p> <p>How to Multiply: Each entry in the product matrix is the dot product of a row of \\(A\\) and a column of \\(B\\).</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} $$</p> <p>Compute: $$ AB = \\begin{bmatrix} (1)(5)+(2)(7) &amp; (1)(6)+(2)(8) \\\\ (3)(5)+(4)(7) &amp; (3)(6)+(4)(8) \\end{bmatrix} = \\begin{bmatrix} 19 &amp; 22 \\\\ 43 &amp; 50 \\end{bmatrix} $$</p> <p>Key Observations: - Matrix multiplication is not commutative: \\( AB \\neq BA \\) generally. - Associative: \\( (AB)C = A(BC) \\). - Distributive: \\( A(B+C) = AB + AC \\).</p> <p>Creative Analogy: Think of matrices as machines: passing a vector through matrix \\(A\\) and then matrix \\(B\\) is different from passing it through matrix \\(B\\) first!</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#24-the-transpose","title":"2.4 The Transpose","text":"<p>The transpose of a matrix \\(A\\), denoted \\(A^T\\), is created by flipping rows into columns.</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\quad \\Rightarrow \\quad A^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix} $$</p> <p>Why Transpose? - It rearranges information. - Essential for symmetries, simplifying matrix equations, and working with inner products.</p> <p>Tip: If \\(A\\) is a \\(m\\times n\\) matrix, then \\(A^T\\) is \\(n\\times m\\).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#25-special-matrices","title":"2.5 Special Matrices","text":"<p>Certain matrices are particularly important because of their simplicity and properties:</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#identity-matrix-i","title":"Identity Matrix \\(I\\)","text":"<p>An identity matrix acts like the number 1 in multiplication.</p> <p>Example: $$ I_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$</p> <p>Properties: - \\(AI = IA = A\\) for any compatible matrix \\(A\\). - Solving systems often involves creating or approximating \\(I\\).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#zero-matrix","title":"Zero Matrix","text":"<p>All entries are 0. It's the additive identity: $$ (A + 0 = A) $$ $$ (A - 0 = A) $$</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>Only nonzero entries are on the main diagonal.</p> <p>Example: $$ D = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 5 &amp; 0 \\\\ 0 &amp; 0 &amp; 7 \\end{bmatrix} $$</p> <p>Why Important? Diagonal matrices are incredibly easy to compute with: - Multiplying a diagonal matrix by a vector just scales each coordinate. - Finding powers of a diagonal matrix is just raising diagonal entries to powers.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A matrix \\(A\\) is symmetric if: $$ A^T = A $$</p> <p>Example: $$ \\begin{bmatrix} 1 &amp; 3 \\\\ 3 &amp; 2 \\end{bmatrix} $$</p> <p>Symmetric matrices arise naturally when modeling undirected relationships (like undirected graphs).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#triangular-matrices","title":"Triangular Matrices","text":"<ul> <li>Upper Triangular: All entries below the main diagonal are zero.</li> <li>Lower Triangular: All entries above the main diagonal are zero.</li> </ul> <p>Uses: Critical for simplifying solving systems (e.g., in LU decomposition).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#block-matrices","title":"Block Matrices","text":"<p>Sometimes matrices are better thought of as blocks of smaller matrices.</p> <p>Example: $$ \\begin{bmatrix} A &amp; B \\\\ C &amp; D \\end{bmatrix} $$</p> <p>where \\(A, B, C, D\\) themselves are matrices.</p> <p>Why Blocks? When dealing with large systems, breaking them into blocks makes operations manageable and efficient.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>How matrices add, subtract, and multiply.</li> <li>That matrix multiplication follows specific dimension rules and is not commutative.</li> <li>What the transpose of a matrix is and why it's important.</li> <li>Key special matrices: identity, zero, diagonal, symmetric, triangular, and block matrices.</li> </ul> <p>These operations are the essential \"verbs\" of linear algebra \u2014 they let us speak the language of systems, transformations, and data flows.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which property does matrix multiplication NOT generally satisfy?</p> <p>A. Associativity</p> <p>B. Distributivity over addition</p> <p>C. Commutativity</p> <p>D. Compatibility with scalar multiplication</p> Show Answer <p>The correct answer is C. Matrix multiplication is not commutative \u2014 that is, in general, \\( AB \\neq BA \\). However, it is associative and distributive, and scalar multiplication is compatible.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/","title":"\ud83d\udcd8 Chapter 3: Vector Spaces and Subspaces","text":"<p>In Chapters 1 and 2, we explored scalars, vectors, matrices, and how to operate on them. Now, we shift our focus to something deeper: spaces built from vectors.</p> <p>This chapter unlocks a profound idea: vectors aren't isolated \u2014 they live in vast, structured universes called vector spaces. Understanding these spaces is essential for everything from solving systems of equations to building machine learning models.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#31-what-is-a-vector-space","title":"3.1 What is a Vector Space?","text":"<p>A vector space is a collection of vectors where you can: - Add any two vectors and stay inside the space. - Scale any vector by a scalar and stay inside the space.</p> <p>In short: add and stretch without leaving.</p> <p>Formal Definition: A set \\(V\\) is a vector space over a field (like \\(\\mathbb{R}\\) for real numbers) if it satisfies: - Closure under addition and scalar multiplication, - Existence of a zero vector (an \"origin\"), - Existence of additive inverses (every vector has a \"negative\" vector), - and other technical properties (like associativity, distributivity).</p> <p>Example 1: All 2D vectors \\((x, y)\\) where \\(x, y\\in\\mathbb{R}\\) form a vector space: \\(\\mathbb{R}^2\\).</p> <p>Example 2: All 3D vectors \\((x, y, z)\\) where \\(x, y, z\\in\\mathbb{R}\\) form \\(\\mathbb{R}^3\\).</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#32-subspaces","title":"3.2 Subspaces","text":"<p>A subspace is simply a vector space within another vector space.</p> <p>Criteria for a Subspace: A subset \\(W\\) of a vector space \\(V\\) is a subspace if: 1. The zero vector is in \\(W\\), 2. \\(W\\) is closed under addition, 3. \\(W\\) is closed under scalar multiplication.</p> <p>Example: In \\(\\mathbb{R}^3\\), the set of all vectors of the form \\((x, 0, 0)\\) (i.e., lying on the \\(x\\)-axis) is a subspace.</p> <p>Why Subspaces Matter: Subspaces capture constrained movement: movement along a line, inside a plane, or through a lower-dimensional world embedded inside a bigger space.</p> <p>Creative Analogy: Imagine a vector space like a giant 3D room. Subspaces are like wires, sheets, or corners inside the room where all action is confined!</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#33-span","title":"3.3 Span","text":"<p>The span of a set of vectors is the smallest subspace containing them \u2014 it's everything you can build by adding and scaling those vectors.</p> <p>Example: If you have vectors: $$ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} , \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ in \\(\\mathbb{R}^2\\), then their span is the entire plane \\(\\mathbb{R}^2\\).</p> <p>Why Span? When you ask, \"What directions can I move using just these vectors?\" \u2014 you are really asking about the span.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#34-basis-and-dimension","title":"3.4 Basis and Dimension","text":"<p>A basis is a minimal set of vectors that: - Span the space, - Are linearly independent (none of them is redundant).</p> <p>Example: $$ { \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} } $$ is a basis for \\(\\mathbb{R}^2\\).</p> <p>Dimension: The dimension of a vector space is the number of vectors in any basis.</p> <p>Thus: - \\(\\mathbb{R}^2\\) has dimension 2, - \\(\\mathbb{R}^3\\) has dimension 3, - A line through the origin in \\(\\mathbb{R}^3\\) has dimension 1.</p> <p>Creative Analogy: If the vector space is a world, the basis vectors are its fundamental directions \u2014 the minimum GPS instructions you need to navigate everywhere!</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#35-row-space-column-space-and-null-space","title":"3.5 Row Space, Column Space, and Null Space","text":"<p>When dealing with a matrix \\(A\\), we encounter three critical subspaces:</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#row-space","title":"Row Space","text":"<p>The space spanned by the rows of \\(A\\).</p> <ul> <li>It tells us about relationships among equations.</li> <li>Lives in \\(\\mathbb{R}^n\\) if \\(A\\) has \\(n\\) columns.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#column-space","title":"Column Space","text":"<p>The space spanned by the columns of \\(A\\).</p> <ul> <li>It represents all possible outputs of the system \\(A\\mathbf{x}\\).</li> <li>Lives in \\(\\mathbb{R}^m\\) if \\(A\\) has \\(m\\) rows.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#null-space-kernel","title":"Null Space (Kernel)","text":"<p>The set of all vectors \\(\\mathbf{x}\\) such that: $$ A\\mathbf{x} = \\mathbf{0} $$</p> <ul> <li>It captures all the \"invisible\" directions \u2014 inputs that produce zero output.</li> <li>Tells us about solutions to homogeneous systems.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#36-why-this-chapter-matters","title":"3.6 Why This Chapter Matters","text":"<p>Understanding vector spaces and subspaces helps you:</p> <ul> <li>Solve systems more intelligently (using dimensions and bases),</li> <li>Analyze models (like feature spaces in machine learning),</li> <li>Simplify problems (by recognizing redundancies and patterns).</li> </ul> <p>Link to Previous Chapters: Everything we've learned about vectors and matrices now deepens: Instead of just manipulating individual objects, you begin to analyze entire worlds formed by them.</p> <p>Forward Connection: In the next chapter, we'll ask how to tell if vectors are independent or redundant \u2014 leading us into linear independence and rank.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Vector spaces are collections of vectors closed under addition and scaling.</li> <li>Subspaces are smaller spaces inside vector spaces, satisfying similar rules.</li> <li>The span of a set of vectors is all combinations you can build from them.</li> <li>A basis is a minimal spanning set; the dimension counts its vectors.</li> <li>Matrices naturally give rise to row space, column space, and null space.</li> </ul> <p>These ideas create a bridge from basic matrix operations to the beautiful architecture of linear systems and transformations.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which of the following sets is a subspace of \u211d\u00b3?</p> <p>A. All vectors with \\(x + y + z = 1\\)</p> <p>B. All vectors with \\(x = y = z\\)</p> <p>C. All vectors with \\(x^2 + y^2 + z^2 &lt; 1\\)</p> <p>D. All unit vectors in \u211d\u00b3</p> Show Answer <p>The correct answer is B. The set of all vectors where \\(x = y = z\\) is a subspace: it contains the zero vector, is closed under addition, and is closed under scalar multiplication. Sets defined by non-homogeneous conditions (like \\(x + y + z = 1\\)) or norm constraints (like unit vectors) are not subspaces.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/","title":"\ud83d\udcd8 Chapter 4: Linear Independence and Rank","text":"<p>Building on our understanding of vector spaces and subspaces, we now turn to an important question:  </p> <p>When are vectors truly \"different\" from each other?</p> <p>This chapter focuses on linear independence, dependence, and the crucial matrix property known as rank. These ideas are the keys to understanding when systems have unique solutions, how much information a set of vectors carries, and how efficient our models are.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#41-linear-independence","title":"4.1 Linear Independence","text":"<p>Definition: Vectors are linearly independent if none of them can be written as a combination of the others.</p> <p>Formal Test: Vectors \\( \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\) are independent if the only solution to: $$ c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\dots + c_n\\mathbf{v}_n = \\mathbf{0} $$ is: $$ c_1 = c_2 = \\dots = c_n = 0 $$</p> <p>Otherwise, they are dependent.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#intuitive-picture","title":"Intuitive Picture","text":"<ul> <li>Independent Vectors: Each vector points in a new direction not explained by others.</li> <li>Dependent Vectors: Some vectors \"retrace\" paths created by others.</li> </ul> <p>Creative Analogy: Imagine you're giving directions: - Independent directions are like adding truly new turns. - Dependent directions are like repeating roads you've already traveled!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#simple-examples","title":"Simple Examples","text":"<p>Independent Example: In \\(\\mathbb{R}^2\\), $$ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ are independent \u2014 they point along different axes.</p> <p>Dependent Example: $$ \\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$ are dependent because \\( \\mathbf{v}_1 = 2\\mathbf{v}_2 \\).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#42-the-importance-of-linear-independence","title":"4.2 The Importance of Linear Independence","text":"<ul> <li> <p>Solving Systems:   Independent vectors correspond to systems with unique solutions.</p> </li> <li> <p>Model Building:   In machine learning or statistics, independent features avoid redundancy and boost performance.</p> </li> <li> <p>Efficiency:   Fewer independent vectors mean smaller models without losing information.</p> </li> </ul>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#43-rank-of-a-matrix","title":"4.3 Rank of a Matrix","text":"<p>The rank of a matrix is the dimension of its row space (or equivalently, the column space).</p> <p>In simple terms: - Rank counts the number of truly independent rows or columns.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#finding-rank","title":"Finding Rank","text":"<p>You can find the rank of a matrix by: 1. Row reducing it to Row Echelon Form or Reduced Row Echelon Form (RREF), 2. Counting the number of leading 1s (pivots).</p> <p>Example:</p> <p>Matrix \\(A\\): $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\\\ 3 &amp; 6 &amp; 9 \\end{bmatrix} $$</p> <p>Row reducing: $$ \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} $$</p> <p>Thus, \\(\\text{rank}(A) = 1\\).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#key-properties-of-rank","title":"Key Properties of Rank","text":"<ul> <li>\\(\\text{rank}(A) \\leq \\min(m,n)\\) for an \\(m \\times n\\) matrix.</li> <li>If \\(\\text{rank}(A) = n\\) (number of columns), the columns are independent.</li> <li>If \\(\\text{rank}(A) = m\\) (number of rows), the rows are independent.</li> </ul>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#44-row-rank-column-rank","title":"4.4 Row Rank = Column Rank","text":"<p>One of the most beautiful facts in linear algebra:</p> <p>The dimension of the row space equals the dimension of the column space.</p> <p>Thus, row rank = column rank, and we simply call it the rank.</p> <p>Why is this surprising? Rows and columns \"live\" in different spaces \u2014 yet their structure carries the same amount of independent information!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#45-the-rank-nullity-theorem","title":"4.5 The Rank-Nullity Theorem","text":"<p>This elegant theorem relates: - Rank (number of independent columns), - Nullity (dimension of the null space, i.e., number of free variables).</p> <p>Theorem: $$ \\text{rank}(A) + \\text{nullity}(A) = n $$ where \\(n\\) = number of columns.</p> <p>Meaning: Every variable in a system either: - Contributes to a pivot (making the system more constrained), or - Becomes a free variable (adding flexibility).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#46-why-this-chapter-matters","title":"4.6 Why This Chapter Matters","text":"<p>Understanding independence and rank allows you to:</p> <ul> <li>Diagnose whether a system has a unique solution, infinite solutions, or no solution,</li> <li>Detect redundancy in your models,</li> <li>Simplify matrices for faster computation.</li> </ul> <p>Building on Earlier Ideas: Now we aren't just describing what vectors and matrices are \u2014 we are judging how rich or constrained they are!</p> <p>What's Next: We'll soon build a geometric view of projections and orthogonality \u2014 using these ideas to minimize errors and find best fits.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Linear independence means no vector can be made from others.</li> <li>Linear dependence reveals redundancy.</li> <li>The rank of a matrix counts independent rows/columns.</li> <li>Row rank and column rank are always equal.</li> <li>The Rank-Nullity Theorem ties together the dimensions of important matrix spaces.</li> </ul> <p>Mastering rank and independence is essential for unlocking the deeper structure hidden inside linear systems and transformations!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>If a \\(4 \\times 5\\) matrix has rank 3, what is the dimension of its null space?</p> <p>A. 1</p> <p>B. 2</p> <p>C. 3</p> <p>D. 5</p> Show Answer <p>The correct answer is B. By the Rank-Nullity Theorem: $$ \\text{rank} + \\text{nullity} = \\text{number of columns} $$ So: $$ 3 + \\text{nullity} = 5 \\quad \\Rightarrow \\quad \\text{nullity} = 2 $$</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/","title":"\ud83d\udcd8 Chapter 5: Inner Products and Orthogonality","text":"<p>In the last chapter, we explored independence and rank \u2014 how vectors relate in terms of structure. Now, we expand our view to include geometry:  </p> <p>How can we measure angles, distances, and projections between vectors?</p> <p>This chapter introduces inner products, norms, and the powerful idea of orthogonality \u2014 essential for understanding projections, least squares methods, and so much more.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#51-the-inner-product","title":"5.1 The Inner Product","text":"<p>The inner product (also called the dot product) gives a way to \"multiply\" two vectors and produce a single number.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#definition","title":"Definition","text":"<p>For two vectors \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n\\), $$ \\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n $$</p> <p>Example: $$ \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\quad \\Rightarrow \\quad \\mathbf{u} \\cdot \\mathbf{v} = 1\\cdot4 + 2\\cdot5 + 3\\cdot6 = 32 $$</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>The inner product relates to the angle \\(\\theta\\) between vectors: $$ \\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}| |\\mathbf{v}| \\cos\\theta $$</p> <p>Thus: - If \\(\\cos\\theta &gt; 0\\), the vectors point generally in the same direction. - If \\(\\cos\\theta = 0\\), the vectors are orthogonal (perpendicular). - If \\(\\cos\\theta &lt; 0\\), the vectors point opposite directions.</p> <p>Creative Analogy: Think of the inner product as measuring how much two vectors \"agree\" in direction \u2014 like two people pushing an object: are they cooperating, conflicting, or independent?</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#52-norm-and-distance","title":"5.2 Norm and Distance","text":"<p>Using the inner product, we can define a norm, which measures the length (or magnitude) of a vector.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#norm-length","title":"Norm (Length)","text":"<p>The norm of \\(\\mathbf{v}\\) is: $$ |\\mathbf{v}| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $$</p> <p>Example: $$ |\\mathbf{v}| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{77} $$</p> <p>Why Norms Matter: Norms allow us to measure how big a vector is \u2014 crucial for computing distances, speeds, and energy.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#distance-between-vectors","title":"Distance Between Vectors","text":"<p>The distance between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is: $$ |\\mathbf{u} - \\mathbf{v}| $$</p> <p>It's simply the length of the vector from \\(\\mathbf{u}\\) to \\(\\mathbf{v}\\).</p> <p>Applications: - In machine learning: measuring similarity between data points. - In physics: measuring displacement.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#53-orthogonality","title":"5.3 Orthogonality","text":"<p>Two vectors are orthogonal if: $$ \\mathbf{u} \\cdot \\mathbf{v} = 0 $$</p> <p>Geometric Meaning: They meet at a 90\u00b0 angle \u2014 they are completely independent directionally.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#why-orthogonality-is-powerful","title":"Why Orthogonality Is Powerful","text":"<ul> <li>Simplifies Computations: Orthogonal vectors are easier to work with \u2014 projections, decompositions, and optimizations become cleaner.</li> <li>Decouples Systems: In control systems or signal processing, orthogonal modes can be studied independently.</li> <li>Basis for Least Squares: Approximating solutions in \"best fit\" problems often uses orthogonality.</li> </ul>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#54-orthogonal-projections","title":"5.4 Orthogonal Projections","text":"<p>Sometimes, we want to project a vector onto another vector (or subspace).</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#projecting-onto-a-vector","title":"Projecting onto a Vector","text":"<p>Given a vector \\(\\mathbf{v}\\) and a unit vector \\(\\mathbf{u}\\), the projection of \\(\\mathbf{v}\\) onto \\(\\mathbf{u}\\) is: $$ \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = (\\mathbf{v} \\cdot \\mathbf{u}) \\mathbf{u} $$</p> <p>Creative Picture: Imagine shining a flashlight directly onto \\(\\mathbf{u}\\) \u2014 the \"shadow\" of \\(\\mathbf{v}\\) on \\(\\mathbf{u}\\) is the projection.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#example","title":"Example","text":"<p>Project \\(\\mathbf{v} = [3, 4]\\) onto \\(\\mathbf{u} = [1, 0]\\) (already a unit vector):</p> \\[ \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = (3\\cdot1 + 4\\cdot0) \\times [1,0] = 3 [1,0] = [3,0] \\]"},{"location":"old-sections/section-1/chapter-5/chapter-5/#55-orthogonal-complement","title":"5.5 Orthogonal Complement","text":"<p>The orthogonal complement of a subspace \\(W\\) consists of all vectors orthogonal to every vector in \\(W\\).</p> <p>Notation: \\(W^\\perp\\)</p> <p>Importance: - It provides a way to \"complete\" spaces: everything not captured by \\(W\\) lies in \\(W^\\perp\\). - Essential for decomposition techniques like the Gram-Schmidt process.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#56-orthonormal-bases-and-gram-schmidt-process","title":"5.6 Orthonormal Bases and Gram-Schmidt Process","text":""},{"location":"old-sections/section-1/chapter-5/chapter-5/#orthonormal-basis","title":"Orthonormal Basis","text":"<p>A set of vectors is orthonormal if: - Each vector is a unit vector (\\(\\|\\mathbf{v}\\| = 1\\)), - Vectors are mutually orthogonal.</p> <p>Example: $$ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ is an orthonormal basis for \\(\\mathbb{R}^2\\).</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#gram-schmidt-process","title":"Gram-Schmidt Process","text":"<p>The Gram-Schmidt process converts any basis into an orthonormal basis.</p> <p>Outline: 1. Start with a basis \\( \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots\\} \\). 2. Orthogonalize:    - Subtract projections onto earlier vectors. 3. Normalize:    - Scale to unit length.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#why-orthonormal-bases","title":"Why Orthonormal Bases?","text":"<ul> <li>Simplify calculations (e.g., finding coefficients in expansions).</li> <li>Critical for decompositions (QR decomposition, spectral methods).</li> <li>Basis for least squares approximations and many machine learning algorithms.</li> </ul>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>The inner product measures alignment between vectors.</li> <li>The norm measures the size of a vector; the distance measures separation.</li> <li>Orthogonal vectors are directionally independent.</li> <li>Projections help decompose vectors into components along subspaces.</li> <li>Orthonormal bases are simple, efficient building blocks.</li> <li>Gram-Schmidt provides a systematic method for orthogonalization.</li> </ul> <p>This geometric language makes linear algebra a powerful tool for modeling, optimization, and approximation.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>What is the result of projecting vector \\( \\mathbf{b} \\) onto a unit vector \\( \\mathbf{u} \\)?</p> <p>A. \\( \\mathbf{b} \\cdot \\mathbf{u} \\)</p> <p>B. \\( (\\mathbf{b} \\cdot \\mathbf{u}) \\mathbf{u} \\)</p> <p>C. \\( \\mathbf{u} \\cdot \\mathbf{u} \\)</p> <p>D. \\( \\mathbf{b} \\cdot \\mathbf{b} \\)</p> Show Answer <p>The correct answer is B. The projection of \\( \\mathbf{b} \\) onto a unit vector \\( \\mathbf{u} \\) is \\((\\mathbf{b} \\cdot \\mathbf{u})\\mathbf{u}\\).</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/","title":"\ud83d\udcd8 Chapter 6: Linear Transformations and Eigenanalysis","text":"<p>We've explored matrices, vector spaces, independence, and orthogonality. Now we step into one of the most powerful ideas in linear algebra:  </p> <p>How do matrices act as machines that transform spaces?</p> <p>In this chapter, we'll study linear transformations, their matrix representations, and the profound concepts of eigenvalues and eigenvectors. These ideas form the mathematical core behind control systems, graphics transformations, machine learning models, and beyond.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#61-what-is-a-linear-transformation","title":"6.1 What is a Linear Transformation?","text":"<p>A linear transformation \\( T \\) is a rule that moves vectors around \u2014 but in a special, structure-preserving way.</p> <p>Formally, \\( T: \\mathbb{R}^n \\to \\mathbb{R}^m \\) is linear if:</p> <ol> <li>\\( T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) \\) (Additivity)</li> <li>\\( T(c\\mathbf{u}) = cT(\\mathbf{u}) \\) (Homogeneity)</li> </ol> <p>Creative Analogy: Imagine stretching, rotating, or flipping an entire room \u2014 but no ripping or folding allowed. Every movement is smooth and proportional.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#examples","title":"Examples","text":"<ul> <li>Scaling: Multiply each coordinate by 2.</li> <li>Rotation: Spin vectors around the origin.</li> <li>Projection: Flatten vectors onto a line or plane.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#62-matrix-representation-of-linear-transformations","title":"6.2 Matrix Representation of Linear Transformations","text":"<p>Every linear transformation can be represented by a matrix. Applying a linear transformation is the same as multiplying by its associated matrix.</p> <p>If \\( T \\) is a linear transformation, there exists a matrix \\( A \\) such that: $$ T(\\mathbf{x}) = A\\mathbf{x} $$</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#why-matrices","title":"Why Matrices?","text":"<ul> <li>They encode the action of transformations compactly.</li> <li>They allow easy computation of complex changes.</li> <li>They unify geometric intuition and algebraic manipulation.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#63-kernel-and-image","title":"6.3 Kernel and Image","text":""},{"location":"old-sections/section-1/chapter-6/chapter-6/#kernel-null-space","title":"Kernel (Null Space)","text":"<p>The kernel of a transformation \\(T\\) is the set of vectors sent to zero: $$ \\ker(T) = { \\mathbf{x} \\mid T(\\mathbf{x}) = \\mathbf{0} } $$</p> <p>Interpretation: The kernel captures all directions that are \"flattened\" completely.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#image-range","title":"Image (Range)","text":"<p>The image of a transformation is the set of all possible outputs: $$ \\text{Im}(T) = { T(\\mathbf{x}) \\mid \\mathbf{x} \\in \\mathbb{R}^n } $$</p> <p>Interpretation: The image tells you where vectors can land \u2014 the \"reach\" of the transformation.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#64-change-of-basis-and-similar-matrices","title":"6.4 Change of Basis and Similar Matrices","text":"<p>Sometimes, it's useful to change coordinates to better understand a transformation.</p> <p>Given a basis \\(B\\), we can: - Represent vectors differently (relative to \\(B\\)), - Represent transformations differently (with respect to \\(B\\)).</p> <p>Two matrices are similar if they represent the same transformation but in different bases.</p> <p>Formal Definition: Matrices \\(A\\) and \\(B\\) are similar if: $$ B = P^{-1}AP $$ for some invertible matrix \\(P\\).</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#why-change-basis","title":"Why Change Basis?","text":"<ul> <li>Simplify computations,</li> <li>Reveal hidden structures (like decoupling independent behaviors),</li> <li>Diagonalize matrices when possible.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#65-eigenvalues-and-eigenvectors","title":"6.5 Eigenvalues and Eigenvectors","text":"<p>One of the most astonishing concepts in linear algebra:</p> <p>Some vectors don't change direction when a transformation is applied \u2014 only their magnitude changes.</p> <p>These are eigenvectors.</p> <ul> <li>Given a transformation \\(A\\),</li> <li>An eigenvector \\(\\mathbf{v}\\) satisfies: $$ A\\mathbf{v} = \\lambda \\mathbf{v} $$ where \\(\\lambda\\) is a scalar called the eigenvalue.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#interpretation","title":"Interpretation","text":"<ul> <li>\\(\\mathbf{v}\\) points along a natural direction of the transformation.</li> <li>\\(\\lambda\\) tells you how much \\(\\mathbf{v}\\) is stretched or shrunk.</li> </ul> <p>Creative Analogy: Imagine pushing a swing: the swing moves back and forth along its natural arc \u2014 it doesn\u2019t spin sideways or move unpredictably. Eigenvectors are the natural \"arcs\" of linear transformations.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#66-finding-eigenvalues-and-eigenvectors","title":"6.6 Finding Eigenvalues and Eigenvectors","text":""},{"location":"old-sections/section-1/chapter-6/chapter-6/#step-1-find-eigenvalues","title":"Step 1: Find Eigenvalues","text":"<p>Solve: $$ \\det(A - \\lambda I) = 0 $$ This is the characteristic equation.</p> <p>The solutions \\(\\lambda\\) are the eigenvalues.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#step-2-find-eigenvectors","title":"Step 2: Find Eigenvectors","text":"<p>For each eigenvalue \\(\\lambda\\), solve: $$ (A - \\lambda I)\\mathbf{v} = \\mathbf{0} $$ to find eigenvectors.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#example","title":"Example","text":"<p>Let: $$ A = \\begin{bmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix} $$</p> <p>Find the eigenvalues:</p> <ol> <li> <p>Compute \\( \\det(A - \\lambda I) \\): $$ \\det \\begin{bmatrix} 4-\\lambda &amp; 1 \\\\ 2 &amp; 3-\\lambda \\end{bmatrix} = (4-\\lambda)(3-\\lambda) - 2 $$ Expand: $$ = \\lambda^2 - 7\\lambda + 10 $$</p> </li> <li> <p>Solve \\( \\lambda^2 - 7\\lambda + 10 = 0 \\).</p> </li> </ol> <p>Solutions: \\( \\lambda = 5 \\) and \\( \\lambda = 2 \\).</p> <ol> <li>For each \\(\\lambda\\), solve \\( (A - \\lambda I)\\mathbf{v} = \\mathbf{0} \\) to find eigenvectors.</li> </ol>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#67-eigenspaces-algebraic-and-geometric-multiplicities","title":"6.7 Eigenspaces, Algebraic and Geometric Multiplicities","text":"<ul> <li>The eigenspace for an eigenvalue \\(\\lambda\\) is the set of all corresponding eigenvectors (plus the zero vector).</li> <li>Algebraic multiplicity: how many times \\(\\lambda\\) appears as a root of the characteristic polynomial.</li> <li>Geometric multiplicity: the dimension of the eigenspace.</li> </ul> <p>Important Fact: Geometric multiplicity is always less than or equal to algebraic multiplicity.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#68-why-this-chapter-matters","title":"6.8 Why This Chapter Matters","text":"<p>Understanding linear transformations and eigenanalysis allows you to:</p> <ul> <li>Decompose systems into simple behaviors,</li> <li>Analyze stability in control systems,</li> <li>Perform dimensionality reduction (PCA in machine learning),</li> <li>Diagonalize matrices to simplify powers of matrices.</li> </ul> <p>Building on Earlier Ideas: Matrices don't just store numbers \u2014 they move vectors in structured ways. Eigenvectors and eigenvalues describe the deep patterns of these movements.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Linear transformations preserve structure and can be represented by matrices.</li> <li>Kernels capture vectors that are flattened; images capture vectors that are reachable.</li> <li>Similar matrices describe the same transformation in different coordinate systems.</li> <li>Eigenvalues and eigenvectors reveal natural directions and scaling.</li> <li>Eigenspaces, algebraic multiplicity, and geometric multiplicity structure eigenbehaviors.</li> </ul> <p>This chapter prepares you for real-world applications where transformations need to be analyzed, optimized, and decomposed!</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>What does it mean if a matrix has fewer than \\(n\\) linearly independent eigenvectors?</p> <p>A. It is not square</p> <p>B. It cannot be diagonalized</p> <p>C. It has no eigenvalues</p> <p>D. Its determinant is zero</p> Show Answer <p>The correct answer is B. If a matrix has fewer than \\(n\\) linearly independent eigenvectors, it cannot be diagonalized \u2014 that is, it cannot be written as a diagonal matrix under any basis change.</p>"},{"location":"old-sections/section-2/section-2/","title":"\ud83d\udcda Section II: Numerical Linear Algebra &amp; Scientific Computing","text":"<p>Overview: This section transitions from theoretical constructs to practical, computational techniques essential for modern scientific computing. Students will develop robust skills in solving linear systems, matrix factorization, stability analysis, and iterative algorithms, with an emphasis on numerical precision and efficiency.</p>"},{"location":"old-sections/section-2/section-2/#chapter-7-solving-linear-systems-and-decompositions","title":"Chapter 7: Solving Linear Systems and Decompositions","text":"<ul> <li>Key Concepts: Matrix inverse, determinants and their properties, Cramer's Rule, Gaussian elimination, Reduced Row Echelon Form (RREF), pivot positions, free variables, LU, QR, and Cholesky factorizations.</li> <li>Focus: Understand direct solution methods for systems of equations and learn efficient matrix decomposition strategies.</li> <li>Skills: Solve systems via different matrix decompositions and interpret solution behaviors based on matrix structure.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-8-iterative-methods-and-stability","title":"Chapter 8: Iterative Methods and Stability","text":"<ul> <li>Key Concepts: Iterative methods (Jacobi, Gauss-Seidel, Conjugate Gradient), matrix norms, condition number, stability of linear systems.</li> <li>Focus: Address large-scale or sparse systems where direct methods are impractical; assess numerical stability and error sensitivity.</li> <li>Skills: Implement and analyze iterative algorithms; diagnose and improve system stability based on matrix conditioning.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-9-advanced-matrix-factorizations","title":"Chapter 9: Advanced Matrix Factorizations","text":"<ul> <li>Key Concepts: Singular Value Decomposition (SVD), Moore-Penrose pseudoinverse, Schur decomposition, Householder transformations, Givens rotations.</li> <li>Focus: Explore high-level decompositions that unlock deeper insights into matrix structure and enable powerful numerical applications.</li> <li>Skills: Apply SVD for dimensionality reduction, compute pseudoinverses for under/overdetermined systems, perform orthogonal transformations for stability.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-10-specialized-matrices-and-operations","title":"Chapter 10: Specialized Matrices and Operations","text":"<ul> <li>Key Concepts: Hermitian, unitary, and positive definite matrices; sparse matrices and solvers; block matrix operations; Kronecker products.</li> <li>Focus: Recognize and leverage special matrix properties for computational advantage; manage complex or structured matrix systems efficiently.</li> <li>Skills: Optimize storage and computation for structured matrices, manipulate block structures, and apply Kronecker products in systems modeling.</li> </ul>"},{"location":"old-sections/section-2/section-2/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Solve linear systems efficiently using both direct and iterative methods. - Implement advanced decompositions such as SVD and Schur forms. - Analyze the numerical stability and sensitivity of matrix computations. - Optimize computation for special and sparse matrix types critical in large-scale problems.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/","title":"Chapter 10: Specialized Matrices and Operations","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#overview","title":"Overview","text":"<p>In the previous chapters, we mastered general techniques for solving and analyzing linear systems. In this final chapter of Section II, we focus on special classes of matrices and specialized operations that offer computational shortcuts, deeper insights, and performance optimizations.</p> <p>Recognizing the structure of a matrix allows us to apply tailored methods that are often faster, more stable, and more memory-efficient\u2014essential in fields like signal processing, scientific computing, and machine learning.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#101-hermitian-unitary-and-positive-definite-matrices","title":"10.1 Hermitian, Unitary, and Positive Definite Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#hermitian-matrices","title":"Hermitian Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept","title":"Concept","text":"<p>A Hermitian matrix satisfies:</p> \\[ A = A^* \\] <p>where \\(A^*\\) is the conjugate transpose.</p> <ul> <li>For real matrices, Hermitian reduces to symmetric matrices \\((A = A^T)\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why","title":"How and Why","text":"<ul> <li>Why? Hermitian matrices have real eigenvalues and orthogonal eigenvectors\u2014making them extremely stable in computations.</li> <li>How? Check if each entry satisfies \\(a_{ij} = \\overline{a_{ji}}\\).</li> </ul> Example <p>The matrix \\(\\begin{bmatrix}2 &amp; i \\\\ -i &amp; 3\\end{bmatrix}\\) is Hermitian.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#unitary-matrices","title":"Unitary Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_1","title":"Concept","text":"<p>A unitary matrix satisfies:</p> \\[ U^*U = I \\] <p>meaning its conjugate transpose is also its inverse.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Unitary transformations preserve vector length and angles.</li> <li>How? Columns form an orthonormal set under complex inner product.</li> </ul> Real Analogy <p>In the real case, unitary matrices are just orthogonal matrices \\((Q^T Q = I)\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#positive-definite-matrices","title":"Positive Definite Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_2","title":"Concept","text":"<p>A matrix \\(A\\) is positive definite if:</p> \\[ \\mathbf{x}^T A \\mathbf{x} &gt; 0 \\] <p>for all nonzero vectors \\(\\mathbf{x}\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Positive definite matrices ensure unique solutions and nice behavior in optimization and numerical methods.</li> <li>How? All eigenvalues are positive.</li> </ul> Quick Test <p>A symmetric matrix is positive definite if all its pivots (or all its eigenvalues) are positive!</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#102-sparse-matrices-and-solvers","title":"10.2 Sparse Matrices and Solvers","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_3","title":"Concept","text":"<p>A sparse matrix has most of its elements equal to zero.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? In large-scale systems (e.g., social networks, finite element analysis), storing all zero entries is wasteful.</li> <li>How?</li> <li>Use compressed storage formats (CSR, CSC).</li> <li>Apply sparse solvers that skip computations with zeros.</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#sparse-techniques","title":"Sparse Techniques","text":"<ul> <li>Iterative methods like Conjugate Gradient are highly effective.</li> <li>Specialized factorizations minimize \"fill-in\" (creation of nonzeros).</li> </ul> Real World <p>Imagine simulating a building's structural stress\u2014the connectivity matrix between elements is sparse!</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#103-block-matrices-and-block-operations","title":"10.3 Block Matrices and Block Operations","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_4","title":"Concept","text":"<p>Block matrices organize data into submatrices rather than individual entries.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? Simplifies calculations by working at a larger granularity.</li> <li>How? Matrix multiplication and inversion rules extend naturally to block structure.</li> </ul> <p>Example: If \\(A\\) and \\(B\\) are block matrices:</p> <p>$$ C = \\begin{bmatrix} A &amp; B \\ 0 &amp; D \\end{bmatrix} $$ then the inverse can be computed block-wise!</p> Efficiency Tip <p>Block operations are especially efficient when blocks are diagonal or triangular.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#104-kronecker-product","title":"10.4 Kronecker Product","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_5","title":"Concept","text":"<p>The Kronecker product \\(A \\otimes B\\) creates a larger matrix from two smaller ones, spreading \\(A\\)'s entries across copies of \\(B\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_5","title":"How and Why","text":"<ul> <li>Why? Useful in systems modeling, tensor algebra, and quantum computing.</li> <li>How?</li> </ul> <p>If \\(A\\) is \\(2 \\times 2\\) and \\(B\\) is \\(3 \\times 3\\), then \\(A \\otimes B\\) is \\(6 \\times 6\\).</p> Concrete Example <p>If \\(A = \\begin{bmatrix}1 &amp; 2\\\\ 3 &amp; 4\\end{bmatrix}\\) and \\(B\\) is a 3x3 matrix, then:</p> \\[ A \\otimes B = \\begin{bmatrix} 1B &amp; 2B \\\\ 3B &amp; 4B \\end{bmatrix} \\]"},{"location":"old-sections/section-2/chapter-10/chapter-10/#applications","title":"Applications","text":"<ul> <li>Multidimensional systems modeling.</li> <li>Structured matrix equations.</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#summary","title":"\u2728 Summary","text":"<ul> <li>Hermitian, unitary, and positive definite matrices have special properties that guarantee stability and efficiency.</li> <li>Sparse matrices save memory and computation by exploiting zeros.</li> <li>Block matrices allow solving problems at higher levels of structure.</li> <li>Kronecker products expand modeling capability in structured and multi-dimensional problems.</li> </ul> <p>Recognizing these structures is critical for designing fast and scalable scientific computations.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>Which type of matrix satisfies \\(A^* = A\\) where \\(A^*\\) denotes the conjugate transpose?</p> <p>A. Unitary matrix B. Positive definite matrix C. Hermitian matrix D. Symmetric matrix</p> Show Answer <p>The correct answer is C. Hermitian matrices satisfy \\(A^* = A\\). For real matrices, Hermitian simply means symmetric \\((A^T = A)\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/","title":"Chapter 7: Solving Linear Systems and Decompositions","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#overview","title":"Overview","text":"<p>In this chapter, we dive into practical methods for solving systems of linear equations, a core application of linear algebra in scientific computing. We will explore both classical techniques and efficient matrix factorizations that not only provide solutions but also reveal important structural insights about the system.</p> <p>Solving linear systems lies at the heart of countless engineering and computing problems: simulating circuits, analyzing networks, training machine learning models, and even 3D graphics rendering. </p> <p>We will build on your knowledge of matrix operations and linear transformations from earlier chapters.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#71-matrix-inverse-and-solving-systems","title":"7.1 Matrix Inverse and Solving Systems","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept","title":"Concept","text":"<p>A matrix inverse, much like the reciprocal of a number, undoes the action of a matrix. If a matrix \\(A\\) is invertible, we can solve \\(A\\mathbf{x} = \\mathbf{b}\\) by computing \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why","title":"How and Why","text":"<ul> <li>Why? It gives a direct way to solve equations.</li> <li>How? If \\(A^{-1}\\) exists, it satisfies \\(A^{-1}A = I\\), where \\(I\\) is the identity matrix.</li> </ul> <p>Warning: Computing \\(A^{-1}\\) is computationally expensive and numerically unstable for large matrices. In practice, we rarely compute the inverse explicitly!</p> Visual Metaphor <p>Think of matrix multiplication as \"squeezing\" or \"stretching\" space. An inverse \"unsqueezes\" or \"unstretches\" space exactly.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#72-determinants-and-their-properties","title":"7.2 Determinants and Their Properties","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_1","title":"Concept","text":"<p>The determinant of a matrix gives a scalar value that indicates whether the matrix is invertible and how it transforms space.</p> <ul> <li>If \\(\\text{det}(A) = 0\\), the matrix is not invertible (it \"flattens\" space).</li> <li>If \\(\\text{det}(A) \\neq 0\\), the matrix is invertible.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? The determinant tells us about volume scaling and collapse.</li> <li>How? Computed recursively or via triangular forms after row reductions.</li> </ul> Example <p>The determinant of a 2x2 matrix \\(\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\) is \\(ad - bc\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#73-cramers-rule","title":"7.3 Cramer's Rule","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_2","title":"Concept","text":"<p>Cramer's Rule expresses each variable in \\(A\\mathbf{x} = \\mathbf{b}\\) as a ratio of determinants.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? It provides a formulaic method for small systems.</li> <li>How? Replace the column of \\(A\\) corresponding to the unknown with \\(\\mathbf{b}\\), compute the determinant, and divide by \\(\\det(A)\\).</li> </ul> <p>Warning: Cramer's Rule is not practical for large systems because determinant computation grows factorially with matrix size.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#74-gaussian-elimination-and-rref","title":"7.4 Gaussian Elimination and RREF","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_3","title":"Concept","text":"<p>Gaussian elimination systematically applies row operations to solve a linear system, reducing it to an upper triangular or reduced row echelon form (RREF).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Solving systems becomes easier when matrices are triangular or simple.</li> <li>How?</li> <li>Eliminate variables from equations step by step.</li> <li>Use back substitution to find solutions.</li> </ul> Creative Teaching Tip <p>Think of Gaussian elimination as \"sweeping\" the clutter (variables) under the rug (zeros below the pivots) until only the core (solutions) are visible!</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#75-pivot-positions-and-free-variables","title":"7.5 Pivot Positions and Free Variables","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_4","title":"Concept","text":"<p>Pivot positions are the leading nonzero entries in each row after Gaussian elimination.</p> <ul> <li>A variable corresponding to a pivot is called a basic variable.</li> <li>A variable not corresponding to a pivot is called a free variable.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? They determine the structure of the solution set (unique, infinite, or none).</li> <li>How? Identify pivots during elimination, and classify variables accordingly.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#76-lu-qr-and-cholesky-factorizations","title":"7.6 LU, QR, and Cholesky Factorizations","text":"<p>Matrix factorizations break down a complicated matrix into simpler, structured components that make solving systems faster and more stable.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#lu-decomposition","title":"LU Decomposition","text":"<ul> <li>Concept: Express \\(A = LU\\) where \\(L\\) is lower triangular, \\(U\\) is upper triangular.</li> <li>Application: Solves \\(A\\mathbf{x} = \\mathbf{b}\\) via two easier systems: \\(L\\mathbf{y} = \\mathbf{b}\\), then \\(U\\mathbf{x} = \\mathbf{y}\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#qr-decomposition","title":"QR Decomposition","text":"<ul> <li>Concept: Express \\(A = QR\\) where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.</li> <li>Application: Important in solving least-squares problems and eigenvalue computation.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<ul> <li>Concept: For symmetric positive definite matrices, \\(A = LL^T\\) where \\(L\\) is lower triangular.</li> <li>Application: Twice as efficient as LU decomposition for these special matrices.</li> </ul> Efficiency Tip <p>Cholesky decomposition is faster and more stable for symmetric positive definite matrices because it avoids duplicating computations!</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#summary","title":"\u2728 Summary","text":"<ul> <li>Matrix inverses provide a theoretical solution method but are rarely computed directly.</li> <li>Determinants help determine invertibility and geometric properties.</li> <li>Cramer's Rule, while educational, is inefficient for large systems.</li> <li>Gaussian elimination and RREF are fundamental manual solving methods.</li> <li>Pivot positions and free variables define the nature of the solution set.</li> <li>Matrix decompositions (LU, QR, Cholesky) provide faster, scalable solutions.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>What is the most efficient factorization method for solving systems with symmetric, positive definite matrices?</p> <p>A. LU decomposition B. QR decomposition C. Cholesky decomposition D. RREF</p> Show Answer <p>The correct answer is C. Cholesky decomposition is optimized for symmetric positive definite matrices, offering faster and more stable computation than LU or QR.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/","title":"Chapter 8: Iterative Methods and Stability","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#overview","title":"Overview","text":"<p>In Chapter 7, we explored direct methods for solving linear systems. However, direct methods can become computationally impractical or unstable for very large or sparse systems. In this chapter, we shift to iterative methods\u2014techniques that generate a sequence of approximations that converge to the true solution.</p> <p>We'll also study stability and conditioning\u2014crucial concepts that tell us how errors in the data or rounding errors during computation can impact the final solution.</p> <p>Understanding these topics ensures that we can solve real-world problems reliably, even when systems are huge, messy, and prone to error.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#81-introduction-to-iterative-methods","title":"8.1 Introduction to Iterative Methods","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept","title":"Concept","text":"<p>Rather than solving a system in one shot, iterative methods start with an initial guess and improve it step-by-step.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why","title":"How and Why","text":"<ul> <li>Why? Direct methods are too expensive for large or sparse matrices.</li> <li>How? Each iteration refines the solution based on simple matrix-vector operations.</li> </ul> Real World Example <p>Google's PageRank algorithm uses an iterative method to rank web pages!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#basic-structure","title":"Basic Structure","text":"<ol> <li>Start with an initial guess \\( \\mathbf{x}^{(0)} \\).</li> <li>Apply an update rule to get better approximations \\( \\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots \\).</li> <li>Stop when changes become sufficiently small.</li> </ol>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#82-jacobi-and-gauss-seidel-methods","title":"8.2 Jacobi and Gauss-Seidel Methods","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#jacobi-method","title":"Jacobi Method","text":"<ul> <li>Updates each variable based only on the previous iteration's values.</li> <li>Fully parallelizable\u2014each new value can be computed independently.</li> </ul> <p>Update rule: $$ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right) $$</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#gauss-seidel-method","title":"Gauss-Seidel Method","text":"<ul> <li>Updates each variable immediately, using the newest values available.</li> <li>Faster convergence in many cases because it \"learns\" from updates within the same iteration.</li> </ul> <p>Update rule: $$ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \\right) $$</p> Analogy <p>Think of Jacobi as students taking a test independently, while Gauss-Seidel is like students sharing answers immediately during the test!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#83-conjugate-gradient-method","title":"8.3 Conjugate Gradient Method","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_1","title":"Concept","text":"<p>The Conjugate Gradient (CG) Method is a more advanced iterative method specialized for symmetric, positive-definite matrices.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Much faster and memory-efficient for large sparse systems arising in physics simulations, machine learning, and more.</li> <li>How? CG searches along conjugate directions to minimize the quadratic form associated with \\(A\\).</li> </ul> <p>Key Features: - Only needs matrix-vector multiplications. - Requires fewer iterations for convergence.</p> When to Use CG <p>Solving \\(Ax = b\\) where \\(A\\) comes from discretizing a 2D Poisson equation\u2014a common physics simulation problem.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#84-matrix-norms","title":"8.4 Matrix Norms","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_2","title":"Concept","text":"<p>A matrix norm measures the \"size\" of a matrix\u2014how much it can stretch a vector.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Helps quantify error and convergence behavior in iterative methods.</li> <li>How? Different norms exist:</li> <li>\\( \\|A\\|_1 \\): maximum absolute column sum.</li> <li>\\( \\|A\\|_\\infty \\): maximum absolute row sum.</li> <li>\\( \\|A\\|_2 \\): largest singular value (spectral norm).</li> </ul> Tip <p>Think of a norm as a \"magnifying glass\"\u2014showing how much a matrix exaggerates changes in input.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#85-condition-number","title":"8.5 Condition Number","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_3","title":"Concept","text":"<p>The condition number measures how sensitive the solution \\( \\mathbf{x} \\) is to small changes in \\(A\\) or \\( \\mathbf{b} \\).</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Predicts if small input errors will cause large output errors.</li> <li>How?</li> </ul> <p>Condition number: $$ \\kappa(A) = |A| \\cdot |A^{-1}| $$</p> <ul> <li>\\( \\kappa(A) \\approx 1 \\): well-conditioned (small errors remain small).</li> <li>\\( \\kappa(A) \\gg 1 \\): ill-conditioned (small errors get amplified).</li> </ul> Important <p>Large condition numbers make solving \\(A\\mathbf{x} = \\mathbf{b}\\) numerically risky, even if \\(A\\) is invertible theoretically!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#86-stability-and-error-analysis","title":"8.6 Stability and Error Analysis","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_4","title":"Concept","text":"<p>Stability refers to whether an algorithm produces nearly correct results even when computations have small errors.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? All real-world computations have rounding errors.</li> <li>How? </li> <li>Stable algorithms control error growth.</li> <li>Unstable algorithms amplify errors unpredictably.</li> </ul>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#forward-vs-backward-error","title":"Forward vs. Backward Error","text":"<ul> <li>Forward error: Difference between true solution and computed solution.</li> <li>Backward error: How much we have to perturb \\(A\\) or \\( \\mathbf{b} \\) to make the computed solution exact.</li> </ul> Example <p>A backward stable algorithm might return an answer slightly wrong for your system but exactly right for a very close system.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#summary","title":"\u2728 Summary","text":"<ul> <li>Iterative methods are essential for solving large or sparse systems.</li> <li>Jacobi and Gauss-Seidel are basic iterative schemes.</li> <li>Conjugate Gradient is highly efficient for symmetric positive-definite matrices.</li> <li>Matrix norms measure \"how big\" matrices are.</li> <li>The condition number quantifies problem sensitivity.</li> <li>Stability ensures reliable computation in the presence of unavoidable errors.</li> </ul>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>If a matrix has a large condition number, what does it imply about solving \\(Ax = b\\)?</p> <p>A. The matrix is singular. B. Small input errors can cause large output errors. C. The solution is unique and stable. D. Gaussian elimination will fail completely.</p> Show Answer <p>The correct answer is B. A large condition number means the system is sensitive: small changes in input can cause large changes in output.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/","title":"Chapter 9: Advanced Matrix Factorizations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#overview","title":"Overview","text":"<p>Building on our understanding of solving linear systems and iterative methods, we now venture into the world of advanced matrix factorizations. These techniques allow us to uncover deep structural insights into matrices, simplify complex computations, and enable applications in areas like data science, control systems, and optimization.</p> <p>In this chapter, you will learn about powerful tools like the Singular Value Decomposition (SVD), the Moore-Penrose pseudoinverse, Schur decomposition, and structured orthogonal transformations such as Householder reflections and Givens rotations.</p> <p>These methods are fundamental to mastering modern scientific computing.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#91-singular-value-decomposition-svd","title":"9.1 Singular Value Decomposition (SVD)","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept","title":"Concept","text":"<p>SVD expresses any matrix \\(A\\) as a product:</p> \\[ A = U \\Sigma V^T \\] <p>where: - \\(U\\) and \\(V\\) are orthogonal matrices. - \\(\\Sigma\\) is a diagonal matrix with non-negative real numbers (the singular values).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why","title":"How and Why","text":"<ul> <li>Why? It reveals the \"geometry\" of how \\(A\\) stretches and rotates space.</li> <li>How? Through careful decomposition of \\(A\\) into simpler, interpretable parts.</li> </ul> Geometric Intuition <p>Imagine \\(A\\) as first rotating space (via \\(V\\)), then stretching or shrinking (via \\(\\Sigma\\)), then rotating again (via \\(U\\)).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications","title":"Applications","text":"<ul> <li>Principal Component Analysis (PCA) in data science.</li> <li>Solving least-squares problems.</li> <li>Noise reduction and signal compression.</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#92-moore-penrose-pseudoinverse","title":"9.2 Moore-Penrose Pseudoinverse","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_1","title":"Concept","text":"<p>For matrices that are non-square or singular, the pseudoinverse \\(A^+\\) provides a best-fit solution to \\(A\\mathbf{x} = \\mathbf{b}\\).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Not all systems have exact solutions\u2014sometimes we must seek an approximate (least-squares) one.</li> <li>How?</li> </ul> \\[ A^+ = V \\Sigma^+ U^T \\] <p>where \\(\\Sigma^+\\) replaces each nonzero singular value \\(\\sigma\\) with \\(1/\\sigma\\).</p> Best Fit Solutions <p>When the system is inconsistent, \\(A^+\\mathbf{b}\\) gives the solution \\(\\mathbf{x}\\) minimizing \\(\\|A\\mathbf{x} - \\mathbf{b}\\|\\).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#93-schur-decomposition","title":"9.3 Schur Decomposition","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_2","title":"Concept","text":"<p>Any square matrix \\(A\\) can be decomposed as:</p> \\[ A = Q T Q^* \\] <p>where: - \\(Q\\) is unitary (or orthogonal for real matrices). - \\(T\\) is upper triangular.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Triangular matrices are easier to analyze and work with.</li> <li>How? By applying a sequence of orthogonal transformations to \\(A\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_1","title":"Applications","text":"<ul> <li>Simplifies eigenvalue computation.</li> <li>Forms the basis for many iterative methods (e.g., QR algorithm for eigenvalues).</li> </ul> Triangular Advantage <p>Finding eigenvalues of a triangular matrix \\(T\\) is as easy as reading its diagonal entries!</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#94-householder-transformations","title":"9.4 Householder Transformations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_3","title":"Concept","text":"<p>A Householder transformation reflects a vector across a plane or hyperplane to introduce zeros below the pivot.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Useful in QR decomposition and in reducing matrices to simpler forms.</li> <li>How?</li> </ul> \\[ H = I - 2 \\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T \\mathbf{v}} \\] <p>where \\(\\mathbf{v}\\) is carefully chosen.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_2","title":"Applications","text":"<ul> <li>Efficiently zeroing entries during QR decomposition.</li> <li>Constructing orthogonal matrices numerically stably.</li> </ul> Intuitive Picture <p>Picture a beam of light reflecting off a flat mirror: the vector is flipped symmetrically across a plane!</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#95-givens-rotations","title":"9.5 Givens Rotations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_4","title":"Concept","text":"<p>Givens rotations apply a simple 2D rotation to zero a specific matrix entry.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? Targeted and efficient way to introduce zeros while preserving orthogonality.</li> <li>How? Rotates within the plane of two coordinates.</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_3","title":"Applications","text":"<ul> <li>Building QR decompositions especially for sparse matrices.</li> <li>Fine-grained control over numerical stability.</li> </ul> When to Use Givens <p>When you want to zero just one specific off-diagonal element without touching the rest of the matrix too much.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#summary","title":"\u2728 Summary","text":"<ul> <li>SVD decomposes any matrix into orthogonal transformations and singular values.</li> <li>The Moore-Penrose pseudoinverse finds least-squares solutions for non-square systems.</li> <li>Schur decomposition simplifies matrix analysis by reducing to upper triangular form.</li> <li>Householder reflections efficiently zero entire columns below the pivot.</li> <li>Givens rotations perform selective 2D zeroing while maintaining orthogonality.</li> </ul> <p>These tools are essential for stability, performance, and deeper understanding of linear systems in scientific computing.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>Which decomposition represents any \\(m \\times n\\) matrix as a product of two orthogonal matrices and a diagonal matrix?</p> <p>A. LU Decomposition B. Schur Decomposition C. QR Decomposition D. Singular Value Decomposition (SVD)</p> Show Answer <p>The correct answer is D. Singular Value Decomposition (SVD) expresses any matrix as \\(U\\Sigma V^T\\) with orthogonal \\(U\\) and \\(V\\), and a diagonal \\(\\Sigma\\).</p>"},{"location":"old-sections/section-3/section-3/","title":"\ud83d\udcda Section III: Control Systems &amp; Electrical Engineering","text":"<p>Overview: This section explores how linear algebra provides the mathematical backbone for modeling, analyzing, and designing control systems in electrical engineering. Students will apply matrix techniques to dynamic systems, understand system stability, and master feedback mechanisms critical to modern control theory.</p>"},{"location":"old-sections/section-3/section-3/#chapter-11-linear-systems-in-control-theory","title":"Chapter 11: Linear Systems in Control Theory","text":"<ul> <li>Key Concepts: State-space representation, controllability matrix, observability matrix, Kalman decomposition, transfer function representation.</li> <li>Focus: Model dynamic systems using matrices and analyze their controllability and observability to ensure desired behavior.</li> <li>Skills: Construct and interpret state-space models; evaluate system properties critical for control design.</li> </ul>"},{"location":"old-sections/section-3/section-3/#chapter-12-system-stability-and-feedback","title":"Chapter 12: System Stability and Feedback","text":"<ul> <li>Key Concepts: Stability of state-space systems, feedback control, pole placement, Linear Quadratic Regulator (LQR), Singular Value Decomposition (SVD) applications in control.</li> <li>Focus: Assess system stability and design feedback mechanisms to control system performance effectively.</li> <li>Skills: Design feedback laws to stabilize and optimize dynamic systems; apply SVD in control optimization scenarios.</li> </ul>"},{"location":"old-sections/section-3/section-3/#chapter-13-dynamic-system-modeling","title":"Chapter 13: Dynamic System Modeling","text":"<ul> <li>Key Concepts: Matrix exponentiation, Markov chains (basic overview), matrix powers, model reduction techniques.</li> <li>Focus: Predict system behavior over time and simplify complex models while preserving essential dynamics.</li> <li>Skills: Use matrix exponentials to solve system evolution equations; apply model reduction to improve system efficiency and maintain control integrity.</li> </ul>"},{"location":"old-sections/section-3/section-3/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Model physical and electrical systems dynamically using state-space and transfer function approaches. - Analyze system stability and controllability through eigenstructure and matrix methods. - Design optimal and robust control systems using feedback strategies and optimization techniques. - Simplify high-order systems to manageable models without sacrificing key dynamic behaviors.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/","title":"Chapter 11: Linear Systems in Control Theory","text":""},{"location":"old-sections/section-3/chapter-11/chapter-11/#overview","title":"Overview","text":"<p>In this chapter, we connect the dots between the abstract world of linear algebra and real-world dynamic systems. Specifically, we'll see how state-space models use matrices to represent, analyze, and design control systems in electrical engineering.</p> <p>Understanding controllability and observability is crucial for engineers: if a system cannot be controlled or observed, then no amount of clever engineering will make it behave as desired. These ideas build directly on your earlier understanding of vector spaces, matrix rank, eigenvalues, and transformations.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#111-state-space-representation","title":"11.1 State-Space Representation","text":"<p>Imagine trying to describe every possible behavior of a system like a robot arm or an electric motor. Instead of writing down countless equations, we organize everything neatly into matrices and vectors.</p> <p>A state-space model expresses a system with two main equations:</p> \\[ \\dot{x}(t) = Ax(t) + Bu(t) \\\\\\\\ y(t) = Cx(t) + Du(t) \\] <p>Where: - \\(x(t)\\): State vector (describes the internal condition of the system) - \\(u(t)\\): Input vector (external signals we control) - \\(y(t)\\): Output vector (what we measure or observe) - \\(A\\), \\(B\\), \\(C\\), \\(D\\): Matrices that define how states and inputs relate to state changes and outputs</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#why-does-this-work","title":"Why does this work?","text":"<p>Matrix multiplication naturally captures how multiple inputs affect multiple outputs simultaneously. Instead of writing separate equations for every variable, matrices elegantly compress all the relationships into a compact form.</p> <p>Tip: Think of \\(A\\) as \"how the system evolves,\" \\(B\\) as \"how inputs affect the system,\" \\(C\\) as \"how we observe the system,\" and \\(D\\) as \"direct influence of inputs on outputs.\"</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#112-controllability","title":"11.2 Controllability","text":"<p>Controllability asks: \"Can we move the system to any desired state using available inputs?\"</p> <p>To answer this, we use the Controllability Matrix:</p> \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2B &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} \\] <ul> <li>If \\(\\mathcal{C}\\) has full rank (i.e., rank = number of states \\(n\\)), the system is controllable.</li> </ul>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#intuitive-view","title":"Intuitive View","text":"<p>Picture a video game controller. If certain buttons are broken, you might not be able to move your character anywhere you want. Similarly, if \\(\\mathcal{C}\\) is missing \"directions\" (lacks rank), you can't steer the system to any state.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#how-to-check-controllability","title":"How to check controllability","text":"<ol> <li>Form \\(\\mathcal{C}\\).</li> <li>Compute its rank.</li> <li>Compare to the number of states.</li> </ol> <p>Example: If \\(\\mathcal{C}\\) has rank 2 for a 2-state system, you're good. But if it only has rank 1, you can't reach every state.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#113-observability","title":"11.3 Observability","text":"<p>Observability asks: \"Can we deduce the internal states from measurements?\"</p> <p>To test this, use the Observability Matrix:</p> \\[ \\mathcal{O} = \\begin{bmatrix} C \\\\\\\\ CA \\\\\\\\ CA^2 \\\\\\\\ \\vdots \\\\\\\\ CA^{n-1} \\end{bmatrix} \\] <ul> <li>If \\(\\mathcal{O}\\) has full rank, the system is observable.</li> </ul>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#intuitive-view_1","title":"Intuitive View","text":"<p>Imagine trying to figure out the position of a hidden car by only watching its shadow. If the light source and shadow don't reveal everything, you're missing vital information \u2014 the system is not fully observable.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#how-to-check-observability","title":"How to check observability","text":"<ol> <li>Form \\(\\mathcal{O}\\).</li> <li>Compute its rank.</li> <li>Compare to the number of states.</li> </ol>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#114-kalman-decomposition","title":"11.4 Kalman Decomposition","text":"<p>Sometimes, parts of a system are controllable and observable, while others aren't. The Kalman Decomposition reorganizes the system into blocks that separate these parts.</p> <p>Why is this useful? - You can focus on controllable/observable parts and ignore the rest. - Simplifies design and analysis.</p> <p>Kalman Decomposition uses special coordinate transformations (like changing basis in linear algebra) to reveal these hidden structures.</p> <p>Note: Kalman Decomposition relies heavily on understanding matrix similarity transformations from earlier chapters!</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#115-transfer-function-representation","title":"11.5 Transfer Function Representation","text":"<p>While state-space models are great for multiple-input, multiple-output (MIMO) systems, sometimes it's easier to think about how inputs are transformed into outputs directly.</p> <p>The Transfer Function \\(G(s)\\) relates input to output in the Laplace domain:</p> \\[ G(s) = C(sI - A)^{-1}B + D \\] <ul> <li>\\(s\\) is a complex variable representing frequency.</li> <li>\\((sI - A)^{-1}\\) is the resolvent matrix that describes how system dynamics respond to inputs.</li> </ul> <p>How is this helpful? - Transfer functions give direct frequency response. - Easier to design filters and controllers for single-input, single-output (SISO) systems.</p> <p>Tip: Transfer Functions are like \"black box\" descriptions, while state-space models are \"white box\" internal blueprints.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#chapter-11-quiz","title":"Chapter 11 Quiz","text":""},{"location":"old-sections/section-3/chapter-11/chapter-11/#quiz-controllability","title":"Quiz: Controllability","text":"<p>Which matrix determines whether all states of a system can be influenced by the input?</p> <p>A. Observability Matrix B. Controllability Matrix C. Transfer Function Matrix D. State Transition Matrix</p> Show Answer <p>The correct answer is B. The Controllability Matrix evaluates if the system can reach any state using suitable input sequences. Full rank controllability ensures maximum maneuverability of the system.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#coming-up-next","title":"\u2728 Coming Up Next","text":"<p>In the next chapter, we'll build on this foundation by learning how to assess and design stability and feedback control mechanisms, ensuring that not only can we control a system but that it behaves well over time!</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/","title":"Chapter 12: System Stability and Feedback","text":""},{"location":"old-sections/section-3/chapter-12/chapter-12/#overview","title":"Overview","text":"<p>Building upon Chapter 11's foundation in state-space modeling, this chapter dives into ensuring systems not only move but behave well over time. We will explore how stability and feedback strategies are designed using linear algebra. These ideas are essential \u2014 a controllable system that is unstable is like a rocket ship you can steer but can't keep from exploding!</p> <p>Understanding how eigenvalues (from earlier eigenanalysis topics) control system behavior will become your main tool here.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#121-stability-of-state-space-systems","title":"12.1 Stability of State-Space Systems","text":"<p>In simple terms, a stable system tends to return to equilibrium after a disturbance.</p> <p>For a continuous-time system:</p> \\[ \\dot{x}(t) = Ax(t) \\] <p>the stability depends entirely on the eigenvalues of the matrix A.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#stability-conditions","title":"Stability Conditions","text":"<ul> <li>Stable: All eigenvalues have negative real parts.</li> <li>Unstable: Any eigenvalue has a positive real part.</li> <li>Marginally Stable: Eigenvalues are purely imaginary (on the imaginary axis), and no repeated eigenvalues.</li> </ul>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-eigenvalues","title":"Why Eigenvalues?","text":"<p>Each eigenvalue corresponds to a natural mode of the system. If the real part of an eigenvalue is positive, it describes a mode that grows exponentially over time \u2014 leading to instability.</p> <p>Tip: Think of eigenvalues as \"behavior seeds\" planted inside the system. If they are healthy (negative real parts), the system calms down. If they are poisonous (positive real parts), the system spirals out of control.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#122-feedback-control","title":"12.2 Feedback Control","text":"<p>Sometimes, a system isn't stable by itself \u2014 but we can add feedback to stabilize it!</p> <p>Feedback means using the output (or state) to influence the input.</p> <p>A simple state feedback control law is:</p> \\[  u(t) = -Kx(t) \\] <p>where \\(K\\) is the feedback gain matrix.</p> <p>Substituting this into the state equation gives:</p> \\[ \\dot{x}(t) = (A - BK)x(t) \\]"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-feedback-works","title":"Why Feedback Works","text":"<p>By choosing \\(K\\) wisely, we modify the system matrix from \\(A\\) to \\(A - BK\\). This new matrix can have different eigenvalues, meaning we can move the natural behaviors of the system toward stability.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#123-pole-placement","title":"12.3 Pole Placement","text":"<p>Pole placement is the art of designing \\(K\\) so that the closed-loop system \\(A - BK\\) has desired eigenvalues (poles).</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#procedure","title":"Procedure","text":"<ol> <li>Decide where you want the eigenvalues (e.g., all with large negative real parts for fast decay).</li> <li>Solve for \\(K\\) that achieves those eigenvalues.</li> </ol> <p>Key Requirement: - The system must be controllable (see Chapter 11) to place poles arbitrarily.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#how-it-connects","title":"How it connects","text":"<p>Remember: eigenvalues were first studied in basic matrix theory. Now, they determine real-world system behavior, and we actively design matrices to control them!</p> <p>Example: If you want a motor to settle to rest quickly after a bump, you design \\(K\\) to place poles far into the left half-plane.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#124-linear-quadratic-regulator-lqr","title":"12.4 Linear Quadratic Regulator (LQR)","text":"<p>Sometimes, just placing poles isn't enough \u2014 we want to optimize performance by balancing goals: - Make the system respond quickly - Avoid using huge control inputs (which could burn out hardware)</p> <p>The Linear Quadratic Regulator (LQR) solves this optimization problem:</p> <p>Minimize the cost function:</p> \\[ J = \\int_0^\\infty \\left( x(t)^T Q x(t) + u(t)^T R u(t) \\right) dt \\] <p>where: - \\(Q\\) penalizes bad states. - \\(R\\) penalizes control effort.</p> <p>The resulting optimal feedback gain \\(K\\) satisfies an equation called the Riccati equation.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-use-lqr","title":"Why use LQR?","text":"<ul> <li>Energy efficiency: Control effort is minimized.</li> <li>Performance guarantee: System stabilizes optimally according to a clear cost.</li> </ul> <p>Tip: LQR uses matrix optimization to find the \"best\" \\(K\\) rather than just \"any\" \\(K\\).</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#125-svd-applications-in-control","title":"12.5 SVD Applications in Control","text":"<p>The Singular Value Decomposition (SVD), introduced earlier, also plays a powerful role in control theory.</p> <p>SVD helps in: - Diagnosing poorly controllable or observable directions. - Designing robust controllers when certain inputs/outputs are weakly connected. - Understanding how sensitive a system is to disturbances.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#how-it-fits-in","title":"How it fits in","text":"<p>SVD breaks a matrix into \"stretching\" and \"rotating\" components \u2014 revealing hidden structure. When applying feedback, it helps identify which directions are easy or hard to control.</p> <p>Example: If an SVD shows a very small singular value, you know there are directions where the system reacts sluggishly \u2014 caution is needed in control design.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#chapter-12-quiz","title":"Chapter 12 Quiz","text":""},{"location":"old-sections/section-3/chapter-12/chapter-12/#quiz-feedback-control","title":"Quiz: Feedback Control","text":"<p>What is the primary goal of feedback control in a dynamic system?</p> <p>A. Increase computational load B. Stabilize the system and improve performance C. Make the system uncontrollable D. Remove all external inputs</p> Show Answer <p>The correct answer is B. Feedback control modifies the system dynamics to ensure stability, enhance performance, and meet design specifications even in the presence of disturbances or uncertainties.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#coming-up-next","title":"\u2728 Coming Up Next","text":"<p>In Chapter 13, we'll extend our control theory knowledge by learning how to model dynamic systems over time using matrix exponentiation and simplify complex systems without losing essential behavior.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/","title":"Chapter 13: Dynamic System Modeling","text":""},{"location":"old-sections/section-3/chapter-13/chapter-13/#overview","title":"Overview","text":"<p>In this chapter, we shift our focus from designing control systems to predicting and simplifying their behavior over time. We introduce powerful tools like matrix exponentiation to solve dynamic equations, explore Markov chains as discrete-time models, and learn how to reduce complex models while preserving critical dynamics.</p> <p>These concepts extend our earlier work on eigenvalues, matrix powers, and linear transformations into dynamic and probabilistic realms!</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#131-matrix-exponentiation-for-system-evolution","title":"13.1 Matrix Exponentiation for System Evolution","text":"<p>Consider the continuous-time system:</p> \\[ \\dot{x}(t) = Ax(t) \\] <p>We already know that \\(A\\)'s eigenvalues determine stability. But how exactly does \\(x(t)\\) evolve over time?</p> <p>The solution is given by the matrix exponential:</p> \\[ x(t) = e^{At}x(0) \\]"},{"location":"old-sections/section-3/chapter-13/chapter-13/#what-is-eat","title":"What is \\(e^{At}\\)?","text":"<ul> <li>It's defined similarly to the scalar exponential via a power series:</li> </ul> \\[ e^{At} = I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\dots \\] <ul> <li>It \"sums up\" all possible infinitesimal changes over time.</li> </ul> <p>Tip: Think of \\(e^{At}\\) as a \"super-transformation\" that smoothly evolves the state \\(x(0)\\) over time.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#why-matrix-exponentiation-works","title":"Why Matrix Exponentiation Works","text":"<p>Systems governed by linear differential equations can be thought of as being \"constantly nudged\" by \\(A\\). Matrix exponentiation mathematically captures this cumulative nudge.</p> <p>Example: In an RLC electrical circuit, the voltages and currents evolve over time according to matrix exponentials.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#132-markov-chains-and-matrix-powers","title":"13.2 Markov Chains and Matrix Powers","text":"<p>Dynamic systems aren't always continuous \u2014 sometimes they evolve in discrete steps, like a game moving piece by piece.</p> <p>A Markov chain models systems where: - The next state depends only on the current state. - Transitions are governed by probabilities.</p> <p>The evolution is:</p> \\[  x[k+1] = P x[k] \\] <p>where: - \\(x[k]\\) is the state probability vector at step \\(k\\). - \\(P\\) is the transition matrix (stochastic matrix: rows sum to 1).</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#how-to-predict-future-states","title":"How to Predict Future States","text":"<p>By taking powers of \\(P\\):</p> \\[  x[k] = P^k x[0] \\] <p>So \\(P^k\\) tells us how the system evolves after \\(k\\) steps.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#intuitive-view","title":"Intuitive View","text":"<p>Imagine flipping between different webpages. \\(P\\) tells you the chance of jumping from one page to another. Matrix powers predict where you'll likely end up after many clicks.</p> <p>Note: Eigenvalues and eigenvectors of \\(P\\) determine long-term behaviors, connecting back to our spectral analysis from earlier chapters!</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#133-model-reduction-techniques","title":"13.3 Model Reduction Techniques","text":"<p>Real-world systems can be huge \u2014 thousands of states! Simulating or controlling them exactly becomes impractical.</p> <p>Model reduction seeks to create a simpler system that behaves similarly to the original.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#common-strategies","title":"Common Strategies","text":"<ul> <li>Remove unimportant states: Find states that barely affect output and discard them.</li> <li>Approximate eigenvalues: Retain dominant modes (eigenvalues with slow decay) and ignore fast ones.</li> <li>Balanced Truncation: Find a coordinate system where controllability and observability are balanced, then trim small contributions.</li> </ul>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#why-reduce-models","title":"Why Reduce Models?","text":"<ul> <li>Faster simulations.</li> <li>Simpler controller designs.</li> <li>Easier to interpret system behavior.</li> </ul> <p>Tip: Model reduction is like summarizing a long novel into a short but faithful synopsis \u2014 you keep the important plots, lose the irrelevant details.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#134-preserving-key-dynamics","title":"13.4 Preserving Key Dynamics","text":"<p>When simplifying a system, we must ensure that essential behaviors (stability, main oscillations, dominant responses) are preserved.</p> <p>If not, we risk building controllers for a \"fake\" system that doesn't behave like the real one.</p> <p>This connects directly to: - Controllability and Observability: Reduced models should maintain important controllable and observable dynamics. - Spectral Properties: Critical eigenvalues should remain unchanged or closely approximated.</p> <p>Example: When reducing a robotic arm's control model, you can't discard modes that cause major swings \u2014 only tiny vibrations can be safely ignored.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#chapter-13-quiz","title":"Chapter 13 Quiz","text":""},{"location":"old-sections/section-3/chapter-13/chapter-13/#quiz-matrix-exponentiation","title":"Quiz: Matrix Exponentiation","text":"<p>Matrix exponentiation in dynamic systems is primarily used to:</p> <p>A. Compute eigenvalues of a matrix B. Solve time evolution equations C. Find the maximum rank of a matrix D. Perform matrix inversion</p> Show Answer <p>The correct answer is B. Matrix exponentiation solves the system evolution over time, describing how the system's state changes continuously according to the matrix dynamics.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#section-iii-conclusion","title":"\u2728 Section III Conclusion","text":"<p>You've now mastered the tools to model, analyze, and design dynamic systems using the power of linear algebra. Up next, we'll move into Signal Processing and Graph Theory, where you'll see matrices shape signals, images, and networks!</p>"},{"location":"old-sections/section-4/section-4/","title":"\ud83d\udcda Section IV: Signal Processing &amp; Graph Theory","text":"<p>Overview: This section highlights the powerful role of linear algebra in signal processing and graph theory, two cornerstone fields in electrical engineering, computer science, and data analysis. Students will learn to transform, analyze, and interpret signals and networks through matrix-based methods.</p>"},{"location":"old-sections/section-4/section-4/#chapter-14-fourier-and-cosine-transforms","title":"Chapter 14: Fourier and Cosine Transforms","text":"<ul> <li>Key Concepts: Fourier Transform and matrices, Discrete Cosine Transform (DCT), Fast Fourier Transform (FFT) matrix view.</li> <li>Focus: Understand how linear transformations encode frequency information and enable efficient signal processing.</li> <li>Skills: Apply matrix representations of Fourier techniques to analyze signals, compress data, and perform transformations in both time and frequency domains.</li> </ul>"},{"location":"old-sections/section-4/section-4/#chapter-15-graph-theory-and-linear-algebra","title":"Chapter 15: Graph Theory and Linear Algebra","text":"<ul> <li>Key Concepts: Graph adjacency matrices, incidence matrices, graph Laplacian matrices, spectral clustering.</li> <li>Focus: Represent and study graphs using matrices; apply linear algebraic techniques to understand network structure and clustering.</li> <li>Skills: Model complex networks, compute graph partitions, and explore eigenvalue-based methods for network analysis.</li> </ul>"},{"location":"old-sections/section-4/section-4/#chapter-16-applications-in-networks-and-flows","title":"Chapter 16: Applications in Networks and Flows","text":"<ul> <li>Key Concepts: Network flow problems, convolution as matrix multiplication, projections onto subspaces, reflection and rotation matrices (2D and 3D), cross product operations.</li> <li>Focus: Apply matrix techniques to dynamic systems, network flow optimization, and geometric transformations.</li> <li>Skills: Solve flow optimization problems, perform signal convolutions, and execute spatial transformations crucial for graphics, physics, and control.</li> </ul>"},{"location":"old-sections/section-4/section-4/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Transform signals efficiently using Fourier and cosine matrix representations. - Model and analyze networks and graphs through adjacency and Laplacian matrices. - Apply linear algebra techniques to practical problems in signal processing, optimization, and spatial modeling. - Interpret eigenvalues and eigenvectors as tools for clustering, filtering, and network dynamics.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/","title":"\ud83d\udcd6 Chapter 14: Fourier and Cosine Transforms","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#overview","title":"Overview","text":"<p>Signal processing fundamentally relies on understanding how a signal changes over time and across frequencies. In this chapter, you will explore how linear algebra provides the mathematical machinery for these transformations. We will particularly focus on how matrices represent the Fourier Transform, the Discrete Cosine Transform (DCT), and Fast Fourier Transform (FFT) techniques.</p> <p>You already know how matrices act as linear transformations on vectors from previous sections. Now, you will see how carefully chosen matrices can \"rotate\" a signal from the time domain to the frequency domain \u2014 revealing hidden patterns like periodicity and enabling powerful applications like signal compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#141-fourier-transform-and-matrices","title":"14.1 Fourier Transform and Matrices","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#what-is-the-fourier-transform","title":"What is the Fourier Transform?","text":"<p>The Fourier Transform expresses a signal as a sum of complex sinusoids (sines and cosines). It helps us answer the question:</p> <p>\"What frequencies are present in my signal, and how strong are they?\"</p> <p>Instead of looking at how a signal behaves over time, we want to look at its frequency content.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-does-the-fourier-matrix-work","title":"How Does the Fourier Matrix Work?","text":"<p>The Discrete Fourier Transform (DFT) can be represented as multiplication by a special matrix, called the Fourier Matrix \\(F_n\\) of size \\(n \\times n\\).</p> <p>Each entry of \\(F_n\\) is:</p> \\[ (F_n)_{jk} = \\omega_n^{jk} \\quad \\text{where} \\quad \\omega_n = e^{-2\\pi i/n} \\] <p>Here, \\(\\omega_n\\) is a \"primitive root of unity,\" a complex number representing one full circle of rotation split into \\(n\\) parts.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#intuitive-explanation","title":"Intuitive Explanation","text":"<p>Think of \\(\\omega_n\\) as a tiny clock hand. Multiplying by powers of \\(\\omega_n\\) spins vectors around the complex plane. The DFT sums these spins in a way that identifies the signal's dominant frequencies.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#key-properties-of-the-fourier-matrix","title":"Key Properties of the Fourier Matrix:","text":"<ul> <li>Unitary: \\(F_n^* F_n = nI\\) (where \\(F_n^*\\) is the conjugate transpose)</li> <li>Invertible: You can recover the original signal by applying \\(F_n^{-1}\\).</li> <li>Efficient: Although \\(F_n\\) is dense, its structure enables fast computation.</li> </ul>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#example","title":"Example","text":"<p>Given a simple signal: $$ \\mathbf{x} = [1, 0, -1, 0]^T $$ Applying the Fourier matrix transforms it into frequency components. The result tells us which \"pure vibrations\" build up this signal.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#142-discrete-cosine-transform-dct","title":"14.2 Discrete Cosine Transform (DCT)","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#what-is-the-dct","title":"What is the DCT?","text":"<p>While the Fourier Transform uses complex numbers (sines and cosines together), the Discrete Cosine Transform focuses only on cosine terms \u2014 making it entirely real-valued and often more efficient for practical applications.</p> <p>DCT is particularly important for compression, like how JPEG images reduce file size without losing visible quality.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-does-the-dct-matrix-work","title":"How Does the DCT Matrix Work?","text":"<p>The DCT transformation can also be seen as multiplication by a special matrix \\(C_n\\), where each element is a cosine of equally spaced angles.</p> \\[ (C_n)_{jk} = \\cos\\left( \\frac{\\pi}{n} \\left(j + \\frac{1}{2}\\right)k \\right) \\] <p>This formula ensures orthogonality (no redundant information) and compresses most energy into fewer components.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#intuitive-explanation_1","title":"Intuitive Explanation","text":"<p>Imagine drawing a signal as a squiggly line. The DCT breaks this line into a combination of \"smooth hills\" (cosine curves) \u2014 the fewer hills needed, the better the compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#why-use-dct-over-dft","title":"Why Use DCT Over DFT?","text":"<ul> <li>Efficiency: Works better for real, even-symmetric data.</li> <li>Compression: Most of the important information often gets concentrated in the first few coefficients.</li> </ul>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#143-fast-fourier-transform-fft","title":"14.3 Fast Fourier Transform (FFT)","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#why-do-we-need-fft","title":"Why Do We Need FFT?","text":"<p>Directly multiplying a vector by \\(F_n\\) takes \\(O(n^2)\\) operations, which becomes too slow for large \\(n\\). The Fast Fourier Transform (FFT) cleverly reduces this to \\(O(n \\log n)\\) by exploiting the symmetry of the Fourier matrix.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-fft-works","title":"How FFT Works","text":"<p>The idea behind FFT is divide and conquer: - Break the signal into even and odd parts. - Apply the Fourier transform recursively to smaller parts. - Combine the results efficiently using simple additions and multiplications.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#visual-analogy","title":"Visual Analogy","text":"<p>Imagine a giant jigsaw puzzle. Instead of solving it all at once, you first assemble smaller regions, then stitch them together. That saves a lot of work!</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#impact","title":"Impact","text":"<p>FFT is one of the most important algorithms in all of computer science and engineering. It powers everything from Wi-Fi and 5G networks to audio compression and digital photography.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#144-connecting-to-previous-topics","title":"14.4 Connecting to Previous Topics","text":"<p>In earlier chapters, you learned about: - Linear transformations: Matrix actions on vectors. - Eigenvalues and eigenvectors: Special vectors that reveal intrinsic properties of transformations.</p> <p>Here, the DFT and DCT are special types of linear transformations \u2014 and the matrices \\(F_n\\) and \\(C_n\\) have eigenproperties that make them extremely useful for diagonalizing convolution operators (used heavily in signal processing).</p> <p>Understanding how signals decompose into simpler parts echoes the ideas of basis change and projection you learned earlier.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#fourier-and-cosine-transforms","title":"Fourier and Cosine Transforms","text":"<p>What does the DCT (Discrete Cosine Transform) primarily help with in signal processing?</p> <p>A. Signal encryption B. Signal compression C. Noise amplification D. Increasing signal frequency  </p> Show Answer <p>The correct answer is B. The DCT is widely used for signal compression. In particular, it concentrates signal energy into fewer coefficients, which allows for efficient storage and transmission, such as in JPEG image compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Fourier Transforms use matrices to uncover frequency components hidden in time signals.</li> <li>DCT focuses only on cosine components, enabling highly efficient compression.</li> <li>FFT accelerates Fourier computations, enabling practical use of frequency analysis in large datasets and real-time applications.</li> </ul> <p>Ready for Chapter 15? We'll now see how matrices help us model and analyze networks in Graph Theory!</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/","title":"\ud83d\udcd6 Chapter 15: Graph Theory and Linear Algebra","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#overview","title":"Overview","text":"<p>Graph theory models relationships between objects, and linear algebra provides the powerful language to study these connections. In this chapter, you'll learn how matrices represent and analyze graphs, and how eigenvalues and eigenvectors reveal deep insights about network structure.</p> <p>By extending your knowledge of matrices, projections, and spectral theory, you will now be able to tackle problems like: - Finding important nodes in a network, - Partitioning a network into communities, - Modeling flow and connectivity in real-world systems.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#151-adjacency-and-incidence-matrices","title":"15.1 Adjacency and Incidence Matrices","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#adjacency-matrix","title":"Adjacency Matrix","text":"<p>An adjacency matrix \\(A\\) for a graph with \\(n\\) nodes is an \\(n \\times n\\) matrix where: - \\(A_{ij} = 1\\) if there is an edge from node \\(i\\) to node \\(j\\), - \\(A_{ij} = 0\\) otherwise.</p> <p>If the graph is undirected, \\(A\\) is symmetric.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#example","title":"Example","text":"<p>A simple graph: <pre><code>1 --- 2\n|     |\n3 --- 4\n</code></pre> Adjacency matrix: $$ A = \\begin{bmatrix}0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#incidence-matrix","title":"Incidence Matrix","text":"<p>An incidence matrix \\(B\\) for a graph with \\(n\\) nodes and \\(m\\) edges is an \\(n \\times m\\) matrix where: - Each column represents an edge. - Entries are \\(1\\), \\(-1\\), or \\(0\\), depending on whether a node is the start or end of the edge (for directed graphs).</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#152-graph-laplacians-and-their-properties","title":"15.2 Graph Laplacians and Their Properties","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#definition","title":"Definition","text":"<p>The graph Laplacian \\(L\\) is defined as: $$ L = D - A $$ where: - \\(D\\) is the degree matrix (diagonal, with node degrees), - \\(A\\) is the adjacency matrix.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#why-the-laplacian","title":"Why the Laplacian?","text":"<ul> <li>It captures both local (neighbor) and global (whole graph) structure.</li> <li>Laplacian eigenvalues reveal important properties like connectivity and clustering.</li> </ul>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#key-properties","title":"Key Properties:","text":"<ul> <li>The smallest eigenvalue is always \\(0\\).</li> <li>The number of zero eigenvalues = the number of connected components.</li> </ul>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#153-spectral-clustering","title":"15.3 Spectral Clustering","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#what-is-spectral-clustering","title":"What is Spectral Clustering?","text":"<p>Spectral clustering uses the eigenvectors of the Laplacian matrix to group nodes into clusters.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#how-it-works","title":"How It Works","text":"<ol> <li>Compute the Laplacian matrix \\(L\\).</li> <li>Find the eigenvectors corresponding to the smallest nonzero eigenvalues.</li> <li>Treat rows of these eigenvectors as new coordinates.</li> <li>Apply a simple clustering algorithm (like k-means) in this new space.</li> </ol>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#visual-analogy","title":"Visual Analogy","text":"<p>Imagine stretching a rubber band model of your graph. Natural clusters will \"pull apart\" \u2014 and that's what the eigenvectors show!</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#why-it-works","title":"Why It Works","text":"<p>The eigenvectors minimize a quantity called graph cut, ensuring that closely connected nodes stay together.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#154-connecting-to-previous-topics","title":"15.4 Connecting to Previous Topics","text":"<p>This chapter builds on: - Matrix representations: Adjacency and incidence matrices extend your matrix vocabulary. - Eigenvalues and eigenvectors: Understanding Laplacian spectra gives you a way to \"feel\" the structure of graphs. - Subspaces and projections: Spectral clustering is really a projection of graph data into meaningful subspaces.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#graph-theory-and-linear-algebra","title":"Graph Theory and Linear Algebra","text":"<p>What matrix is typically used to understand graph connectivity and clustering in spectral graph theory?</p> <p>A. Adjacency matrix B. Laplacian matrix C. Incidence matrix D. Degree matrix  </p> Show Answer <p>The correct answer is B. The Laplacian matrix captures the structure of a graph and its eigenvalues and eigenvectors provide critical information for clustering, connectivity analysis, and spectral graph theory applications.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Adjacency and incidence matrices encode graph connections.</li> <li>The Laplacian matrix reveals deep structural properties of graphs.</li> <li>Spectral methods use eigenvalues and eigenvectors to partition graphs and analyze network connectivity.</li> </ul> <p>Ready for Chapter 16? We'll explore network flows, convolutions, and spatial transformations next!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/","title":"\ud83d\udcd6 Chapter 16: Applications in Networks and Flows","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#overview","title":"Overview","text":"<p>In this final chapter of Section IV, we apply linear algebra to real-world problems in network flows, signal convolution, and spatial transformations. You'll see how matrices model dynamic systems, optimize resource distribution, and manipulate objects in physical space.</p> <p>Each concept builds on your understanding of matrices as powerful, flexible tools \u2014 not just static arrays of numbers, but active agents that move, combine, and optimize data.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#161-network-flow-problems","title":"16.1 Network Flow Problems","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#modeling-flows-with-matrices","title":"Modeling Flows with Matrices","text":"<p>Imagine a network of pipes, traffic intersections, or internet routers. We want to model how much \"stuff\" flows from point A to point B.</p> <p>We use incidence matrices and flow vectors: - Each row represents a node. - Each column represents an edge (connection).</p> <p>The key idea:</p> <p>Conservation Law: The total inflow minus outflow at each node must equal the external supply/demand.</p> <p>This gives rise to a system of linear equations: $$ B \\mathbf{f} = \\mathbf{s} $$ where: - \\(B\\) is the incidence matrix, - \\(\\mathbf{f}\\) is the flow vector, - \\(\\mathbf{s}\\) is the source/sink vector.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#solving-flow-problems","title":"Solving Flow Problems","text":"<p>You solve for \\(\\mathbf{f}\\) just like you solve any linear system \u2014 using methods such as LU decomposition, least squares, or iterative techniques when necessary.</p> <p>This highlights linear algebra's ability to model and optimize real-world systems!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#162-convolution-as-matrix-multiplication","title":"16.2 Convolution as Matrix Multiplication","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#what-is-convolution","title":"What is Convolution?","text":"<p>Convolution is a way of combining two signals to produce a third. It's crucial in fields like: - Image processing (blur, sharpen filters) - Audio effects (echo, reverb) - Engineering systems (impulse responses)</p> <p>Discrete convolution between two sequences can be expressed as matrix multiplication using a special kind of matrix called a Toeplitz matrix.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#toeplitz-matrix-structure","title":"Toeplitz Matrix Structure","text":"<p>A Toeplitz matrix has constant diagonals: $$ T = \\begin{bmatrix}t_0 &amp; 0 &amp; 0 \\\\ t_1 &amp; t_0 &amp; 0 \\\\ t_2 &amp; t_1 &amp; t_0\\end{bmatrix} $$</p> <p>Multiplying \\(T\\) by a vector \\(\\mathbf{x}\\) performs convolution!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#why-matrix-formulation","title":"Why Matrix Formulation?","text":"<ul> <li>Unified Framework: Convolution becomes just another matrix operation.</li> <li>Efficient Algorithms: Fast convolution methods often use matrix factorizations.</li> </ul>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#163-reflection-and-rotation-matrices-2d-and-3d","title":"16.3 Reflection and Rotation Matrices (2D and 3D)","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#geometric-transformations","title":"Geometric Transformations","text":"<p>Spatial transformations like rotation and reflection are elegantly expressed using matrices.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#reflection-across-a-line-2d","title":"Reflection Across a Line (2D)","text":"<p>The reflection matrix across a line at angle \\(\\theta\\) is: $$ R = \\begin{bmatrix}\\cos 2\\theta &amp; \\sin 2\\theta \\\\ \\sin 2\\theta &amp; -\\cos 2\\theta\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#rotation-2d","title":"Rotation (2D)","text":"<p>Rotation by angle \\(\\theta\\) counterclockwise: $$ \\text{Rotation} = \\begin{bmatrix}\\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#extension-to-3d","title":"Extension to 3D","text":"<p>In 3D, rotation matrices expand to \\(3 \\times 3\\) matrices, often around principal axes (X, Y, Z).</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Graphics: Moving objects in games and simulations.</li> <li>Physics: Modeling rigid body motion.</li> <li>Robotics: Calculating arm movements.</li> </ul> <p>Matrix formulations allow you to chain multiple transformations simply by multiplying matrices.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#164-cross-product-operations","title":"16.4 Cross Product Operations","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#what-is-the-cross-product","title":"What is the Cross Product?","text":"<p>Given two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^3\\), the cross product \\(\\mathbf{u} \\times \\mathbf{v}\\) produces a vector orthogonal to both.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#matrix-formulation","title":"Matrix Formulation","text":"<p>The cross product can be represented as a matrix multiplication: $$ \\mathbf{u} \\times \\mathbf{v} = \\mathbf{u} \\times \\mathbf{v} $$ where \\([\\mathbf{u}]_\\times\\) is the skew-symmetric matrix: $$ [\\mathbf{u}]_\\times = \\begin{bmatrix}0 &amp; -u_3 &amp; u_2 \\\\ u_3 &amp; 0 &amp; -u_1 \\\\ -u_2 &amp; u_1 &amp; 0\\end{bmatrix} $$</p> <p>This clever trick allows cross products to fit neatly into the world of linear transformations.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#165-connecting-to-previous-topics","title":"16.5 Connecting to Previous Topics","text":"<p>This chapter synthesizes ideas from: - Matrix multiplication and structure (Toeplitz, skew-symmetric matrices) - Linear systems (modeling network flows) - Geometric transformations (applying rotations and reflections)</p> <p>By now, you see that linear algebra underpins nearly every structure in signal processing, optimization, and spatial modeling.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#applications-in-networks-and-flows","title":"Applications in Networks and Flows","text":"<p>In network flow optimization, what does the Max-Flow Min-Cut Theorem primarily relate?</p> <p>A. The shortest path and the longest path B. Maximum network throughput and minimum cut capacity C. Maximum node degree and minimum spanning tree D. Signal-to-noise ratio  </p> Show Answer <p>The correct answer is B. The Max-Flow Min-Cut Theorem states that the maximum amount of flow passing from a source to a sink in a network equals the minimum capacity that, when removed, would disconnect the source from the sink.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Network flows are modeled and optimized using incidence matrices.</li> <li>Convolution operations can be framed as matrix multiplications.</li> <li>Geometric transformations (rotations, reflections) are elegantly expressed via matrices.</li> <li>Cross products fit neatly into matrix language via skew-symmetric matrices.</li> </ul> <p>Congratulations on completing Section IV! \ud83d\ude80 Next, you'll dive into applications of linear algebra in machine learning and data science!</p>"},{"location":"old-sections/section-5/section-5/","title":"\ud83d\udcda Section V: Data Science &amp; Machine Learning","text":"<p>Overview: This final section demonstrates how linear algebra forms the computational and conceptual foundation of modern data science and machine learning. Students will explore how matrices and vector spaces drive dimensionality reduction, model training, optimization, and kernel-based learning.</p>"},{"location":"old-sections/section-5/section-5/#chapter-17-principal-component-analysis-and-beyond","title":"Chapter 17: Principal Component Analysis and Beyond","text":"<ul> <li>Key Concepts: Covariance matrices, Principal Component Analysis (PCA), data compression via PCA, whitening transformations, low-rank matrix approximation, matrix completion.</li> <li>Focus: Reduce dimensionality and uncover structure in high-dimensional datasets using spectral techniques.</li> <li>Skills: Perform PCA, compress data while preserving variance, complete missing matrix entries in real-world datasets.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-18-machine-learning-foundations","title":"Chapter 18: Machine Learning Foundations","text":"<ul> <li>Key Concepts: Neural networks and linear layers, weight initialization using SVD, backpropagation and matrix calculus, gradient descent for solving linear systems, batch vs. stochastic methods.</li> <li>Focus: Understand how matrix operations power learning algorithms and neural network training.</li> <li>Skills: Construct and train linear models, differentiate matrix functions, optimize learning dynamics.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-19-advanced-optimization-techniques","title":"Chapter 19: Advanced Optimization Techniques","text":"<ul> <li>Key Concepts: Optimization landscapes, Hessians, Newton\u2019s method with matrices, convexity and linear functions.</li> <li>Focus: Explore curvature and convergence properties of loss functions using second-order information.</li> <li>Skills: Analyze critical points, perform Newton-based updates, and identify convexity in optimization problems.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-20-kernel-methods-and-collaborative-filtering","title":"Chapter 20: Kernel Methods and Collaborative Filtering","text":"<ul> <li>Key Concepts: Support Vector Machines (SVMs), kernel trick, positive semidefinite kernels, Gram matrices, collaborative filtering (matrix factorization), Eigenfaces for face recognition.</li> <li>Focus: Extend linear models to nonlinear settings via kernel methods; apply matrix factorization to recommendation systems and facial recognition.</li> <li>Skills: Use kernels to map data into higher dimensions, perform matrix factorization for prediction, and apply eigenfaces in image classification.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-21-specialized-topics-in-linear-algebra-applications","title":"Chapter 21: Specialized Topics in Linear Algebra Applications","text":"<ul> <li>Key Concepts: Spectral theorem, matrix perturbation theory, tensor basics, matrix calculus fundamentals, Kronecker product, vectorization, sparse matrix solvers, AI-driven applications.</li> <li>Focus: Investigate advanced applications and techniques bridging linear algebra with emerging fields in AI and big data.</li> <li>Skills: Apply theoretical tools in practical machine learning pipelines, use vectorization and sparse matrices for large-scale efficiency.</li> </ul>"},{"location":"old-sections/section-5/section-5/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Leverage linear algebra to perform feature extraction, dimensionality reduction, and matrix completion. - Implement learning algorithms grounded in matrix calculus and optimization. - Extend linear methods into nonlinear domains via kernels and support vector machines. - Apply advanced linear algebra in recommendation systems, neural networks, and AI-driven tasks.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/","title":"Chapter 17: Principal Component Analysis and Beyond","text":""},{"location":"old-sections/section-5/chapter-17/chapter-17/#introduction","title":"Introduction","text":"<p>Imagine standing in a massive library where every book represents a feature of your dataset. Some books are filled with crucial knowledge; others, not so much. Principal Component Analysis (PCA) helps us figure out which \"books\" contain the most useful information, allowing us to focus only on the essentials.</p> <p>In this chapter, we explore how PCA uses linear algebra \u2014 specifically eigenvalues and eigenvectors \u2014 to reduce the complexity of data while preserving its most important structures. We'll also touch on related ideas like whitening transformations, low-rank approximations, and matrix completion, expanding your toolkit for working with real-world, messy datasets.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#1-covariance-matrices","title":"1. Covariance Matrices","text":"<p>Before we dive into PCA, we must understand covariance matrices.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#what-is-covariance","title":"What is Covariance?","text":"<p>Covariance measures how two variables change together: - Positive covariance: Variables increase together. - Negative covariance: As one increases, the other decreases.</p> <p>Mathematically, for variables \\(X\\) and \\(Y\\):</p> \\[ \\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] \\] <p>where \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of \\(X\\) and \\(Y\\).</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#building-the-covariance-matrix","title":"Building the Covariance Matrix","text":"<p>Given a dataset with multiple features, the covariance matrix summarizes covariances between all pairs of features. For a data matrix \\(X\\) (after centering by subtracting the mean):</p> \\[ \\Sigma = \\frac{1}{n-1}X^T X \\] <p>Key Insight: The covariance matrix is symmetric and its structure tells us how features relate. Eigenvalues and eigenvectors of this matrix reveal the most important directions in the data.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#2-principal-component-analysis-pca","title":"2. Principal Component Analysis (PCA)","text":""},{"location":"old-sections/section-5/chapter-17/chapter-17/#intuitive-idea","title":"Intuitive Idea","text":"<p>PCA finds new axes (directions) where the data variance is maximized. Think of spinning a cloud of data points: PCA aligns a new coordinate system along the directions of greatest spread.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#how-it-works","title":"How It Works","text":"<ol> <li>Center the data: Subtract the mean from each feature.</li> <li>Compute the covariance matrix \\(\\Sigma\\).</li> <li>Find eigenvectors and eigenvalues:<ul> <li>Eigenvectors = Principal components (new axes)</li> <li>Eigenvalues = How much variance each principal component captures</li> </ul> </li> <li>Sort eigenvectors by eigenvalues in descending order.</li> <li>Project data onto the top \\(k\\) eigenvectors to reduce dimensionality.</li> </ol>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#why-pca-works","title":"Why PCA Works","text":"<ul> <li>Energy (variance) is preserved: We keep most of the \"action\" of the data.</li> <li>Orthogonality of principal components ensures that projections are independent and non-redundant.</li> </ul> <p>Quick Visualization</p> <p>Imagine shining a flashlight onto a 3D object. The resulting 2D shadow captures the shape. PCA finds the best way to cast that shadow to preserve maximum detail.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#3-whitening-transformations","title":"3. Whitening Transformations","text":"<p>PCA decorrelates features but may still have different variances along each axis. Whitening goes a step further: it scales the principal components so that they have unit variance.</p> <p>Mathematically:</p> \\[ X_{\\text{white}} = \\Lambda^{-1/2} U^T X \\] <p>where \\(U\\) contains the eigenvectors and \\(\\Lambda\\) is the diagonal matrix of eigenvalues.</p> <p>Use Cases: - Improve stability and convergence in machine learning algorithms. - Standardize features without introducing correlations.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#4-low-rank-matrix-approximation","title":"4. Low-Rank Matrix Approximation","text":"<p>Often, data can be well-approximated by a matrix with lower rank \u2014 fewer \"independent dimensions\" than the original.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#how-it-works_1","title":"How It Works","text":"<ul> <li>Singular Value Decomposition (SVD) decomposes a matrix into \\(U \\Sigma V^T\\).</li> <li>By keeping only the largest singular values and corresponding vectors, we build a lower-rank approximation.</li> </ul> <p>Movie Ratings Dataset</p> <p>Suppose a movie ratings matrix has missing entries. A low-rank approximation guesses missing ratings based on patterns in the data.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#5-matrix-completion","title":"5. Matrix Completion","text":"<p>Matrix completion fills missing values in a matrix, assuming the data lies in a low-dimensional space.</p> <ul> <li>Widely used in recommender systems (e.g., Netflix recommendations).</li> <li>Methods include minimizing the nuclear norm (sum of singular values) subject to constraints from known entries.</li> </ul> <p>The idea is that with only a few observations, if the data is simple enough (low-rank), we can guess the missing pieces accurately.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#summary","title":"Summary","text":"<ul> <li>Covariance matrices reveal relationships between features.</li> <li>PCA finds the directions of maximum variance to compress data.</li> <li>Whitening standardizes data after PCA.</li> <li>Low-rank approximation captures essential structure with fewer dimensions.</li> <li>Matrix completion guesses missing data using low-rank assumptions.</li> </ul> <p>These techniques are central to fields like data science, image compression, genomics, and more.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#quiz-question","title":"Quiz Question","text":"<p>Which matrix operation is central to finding the principal components in PCA?</p> <p>A. Matrix inversion B. Eigen decomposition of the covariance matrix C. LU decomposition D. QR factorization</p> Show Answer <p>The correct answer is B. PCA relies on the eigen decomposition of the data\u2019s covariance matrix to find directions (principal components) that capture the most variance.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/","title":"Chapter 18: Machine Learning Foundations","text":""},{"location":"old-sections/section-5/chapter-18/chapter-18/#introduction","title":"Introduction","text":"<p>Imagine teaching a robot how to recognize handwritten digits or detect objects in photos. How do you get it to learn patterns from numbers and pixels? The answer lies in linear algebra \u2014 particularly the way matrices represent transformations and learning steps.</p> <p>In this chapter, we build on your knowledge of matrices, eigenvalues, and matrix decompositions to understand the mathematical heart of machine learning. We'll explore linear layers, weight initialization, matrix calculus, and gradient descent \u2014 the essential \"training rituals\" behind intelligent systems.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#1-neural-networks-and-linear-layers","title":"1. Neural Networks and Linear Layers","text":"<p>At its core, a neural network is just a collection of matrix operations.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#linear-transformation","title":"Linear Transformation","text":"<p>Each layer in a network applies a matrix multiplication:</p> \\[ Z = W X + b \\] <p>where: - \\(W\\) = weight matrix - \\(X\\) = input vector - \\(b\\) = bias vector</p> <p>This operation transforms input data into a new space, hoping to make patterns easier to recognize.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#building-off-prior-knowledge","title":"Building Off Prior Knowledge","text":"<p>Remember PCA? PCA found new axes (principal components) that reveal structure. Similarly, neural networks learn new transformations (via \\(W\\)) that uncover hidden patterns in data.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#2-weight-initialization-using-svd","title":"2. Weight Initialization Using SVD","text":"<p>Training a neural network can fail if we start with poor weight values.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#the-problem","title":"The Problem","text":"<ul> <li>If weights are too large or small, the outputs can explode or vanish.</li> <li>This makes learning very slow or unstable.</li> </ul>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#the-solution","title":"The Solution","text":"<p>Use ideas from Singular Value Decomposition (SVD): - Initialize weights so that they are \"balanced\" across dimensions. - SVD helps ensure that initial transformations preserve variance, just like PCA!</p> <p>Common strategies: - Xavier Initialization: weights are scaled according to the number of input and output nodes. - He Initialization: scaled for ReLU activations.</p> <p>Why This Works</p> <p>Balanced weights prevent early layers from distorting the data too much, making learning smoother and faster.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#3-backpropagation-and-matrix-calculus","title":"3. Backpropagation and Matrix Calculus","text":"<p>To train a neural network, we must compute how the output error changes with respect to every weight \u2014 this is backpropagation.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#key-ideas","title":"Key Ideas","text":"<ul> <li>Use matrix calculus to compute gradients.</li> <li>Gradients tell us the direction and amount to adjust weights.</li> </ul> <p>If \\(L\\) is the loss function and \\(W\\) is the weight matrix:</p> \\[ \\frac{\\partial L}{\\partial W} \\] <p>computes how a small change in \\(W\\) changes the loss \\(L\\).</p> <p>Matrix calculus rules (like the chain rule) allow efficient computation over entire layers.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#4-gradient-descent-for-solving-linear-systems","title":"4. Gradient Descent for Solving Linear Systems","text":"<p>Gradient Descent is the workhorse of learning.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#how-it-works","title":"How It Works","text":"<ol> <li>Compute the gradient of the loss with respect to parameters.</li> <li>Update parameters in the negative direction of the gradient:</li> </ol> \\[ \\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta) \\] <p>where \\(\\eta\\) is the learning rate.</p> <p>In linear systems, this mirrors methods like the Conjugate Gradient for finding solutions iteratively.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#5-batch-vs-stochastic-methods","title":"5. Batch vs. Stochastic Methods","text":"<p>When updating weights, should we use the entire dataset or just parts?</p> <ul> <li>Batch Gradient Descent: Uses the whole dataset. More accurate but slow.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time. Faster but noisier.</li> <li>Mini-Batch Gradient Descent: A compromise \u2014 small groups of samples.</li> </ul> <p>Choosing the right method depends on dataset size, noise tolerance, and computational resources.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#summary","title":"Summary","text":"<ul> <li>Neural networks build on linear transformations (matrix multiplication).</li> <li>Weight initialization strategies use SVD concepts to stabilize early learning.</li> <li>Backpropagation applies matrix calculus to compute gradients.</li> <li>Gradient descent is an iterative method similar to solving linear systems.</li> <li>Batch, mini-batch, and stochastic updates balance speed and accuracy.</li> </ul> <p>Understanding these concepts is crucial for anyone wanting to master machine learning foundations!</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#quiz-question","title":"Quiz Question","text":"<p>In backpropagation, which mathematical concept is primarily used to compute how the loss changes with respect to weights?</p> <p>A. Matrix multiplication B. Matrix inverse C. Matrix calculus (derivatives) D. Kronecker product</p> Show Answer <p>The correct answer is C. Backpropagation requires calculating derivatives of the loss function with respect to model parameters using matrix calculus.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/","title":"Chapter 19: Advanced Optimization Techniques","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#introduction","title":"Introduction","text":"<p>Imagine climbing a mountain covered in fog, trying to find the highest peak (or the lowest valley if minimizing). You could walk uphill using just the steepness (gradient), but wouldn\u2019t it be faster if you also knew the shape of the land around you? That\u2019s the power of second-order optimization techniques like Newton's method.</p> <p>This chapter delves into the geometry of optimization landscapes, the role of curvature via the Hessian matrix, and how linear algebra unlocks faster and smarter ways to optimize machine learning models.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#1-optimization-landscapes","title":"1. Optimization Landscapes","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#visualizing-optimization","title":"Visualizing Optimization","text":"<p>An optimization landscape maps inputs to outputs (loss values). Picture a rolling 3D terrain: - Peaks = local maxima - Valleys = local minima</p> <p>Finding the lowest point is minimizing the loss function.</p> <p>Gradients tell us the direction of steepest descent, but curvature tells us how steep or flat the landscape is, helping us adjust our steps accordingly.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#building-from-earlier-chapters","title":"Building from Earlier Chapters","text":"<ul> <li>Gradient Descent (Chapter 18) uses first derivatives (gradients).</li> <li>Newton's Method (this chapter) adds second derivatives (curvature) to refine steps.</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#2-the-hessian-matrix","title":"2. The Hessian Matrix","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#what-is-the-hessian","title":"What is the Hessian?","text":"<p>The Hessian matrix captures second-order derivatives of a scalar function with respect to multiple variables.</p> <p>For a function \\(f(x_1, x_2, \\dots, x_n)\\):</p> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] <p>Key Properties: - Symmetric (if the function is smooth) - Describes local curvature</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#3-newtons-method-with-matrices","title":"3. Newton's Method with Matrices","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#how-it-works","title":"How It Works","text":"<p>Newton\u2019s update formula:</p> \\[ \\theta_{\\text{new}} = \\theta - H^{-1} \\nabla f(\\theta) \\] <ul> <li>\\(\\nabla f(\\theta)\\) = gradient (first derivative)</li> <li>\\(H\\) = Hessian (second derivative)</li> </ul> <p>Instead of just following the slope, we adjust steps using curvature to reach minima faster.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#why-it-works","title":"Why It Works","text":"<ul> <li>When near an optimum, Newton's method can converge quadratically (very fast).</li> <li>Corrects overly aggressive or overly timid steps that plain gradient descent would take.</li> </ul> <p>Trade-Offs</p> <p>Calculating and inverting the Hessian is expensive for high-dimensional data. Newton\u2019s method is powerful but often reserved for smaller problems or when approximations like quasi-Newton methods (e.g., BFGS) are available.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#4-convexity-and-linear-functions","title":"4. Convexity and Linear Functions","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#convexity","title":"Convexity","text":"<p>A function is convex if the line segment between any two points on the graph lies above or on the graph itself.</p> <p>In mathematical terms:</p> \\[ f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y) \\quad \\forall \\lambda \\in [0,1] \\]"},{"location":"old-sections/section-5/chapter-19/chapter-19/#why-convexity-matters","title":"Why Convexity Matters","text":"<ul> <li>No local minima trap: Every local minimum is a global minimum.</li> <li>Optimization algorithms (gradient descent, Newton's method) behave more predictably and converge faster.</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#identifying-convexity","title":"Identifying Convexity","text":"<ul> <li>A twice-differentiable function is convex if its Hessian is positive semidefinite (all eigenvalues \\( \\geq 0 \\)).</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#summary","title":"Summary","text":"<ul> <li>Optimization landscapes describe the shape of loss functions.</li> <li>The Hessian matrix encodes curvature information through second derivatives.</li> <li>Newton's method uses Hessians to achieve faster convergence.</li> <li>Convex functions are ideal because they guarantee global minima and stability.</li> </ul> <p>Understanding these optimization techniques opens doors to training more powerful machine learning models efficiently!</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#quiz-question","title":"Quiz Question","text":"<p>What is the main advantage of using Newton\u2019s method over gradient descent in optimization?</p> <p>A. It requires fewer matrix computations B. It uses curvature information to converge faster C. It avoids matrix inverses D. It minimizes computation time at every step</p> Show Answer <p>The correct answer is B. Newton\u2019s method uses second-order information (the Hessian) about the curvature of the function, allowing it to converge more quickly near the optimum.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/","title":"Chapter 20: Kernel Methods and Collaborative Filtering","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#introduction","title":"Introduction","text":"<p>Suppose you want to classify data that isn\u2019t neatly separated by a straight line (like distinguishing spirals or circles). What if we could \"lift\" the data into a higher dimension where it is separable by a line? Kernel methods make this possible \u2014 and they rely on clever applications of linear algebra!</p> <p>This chapter also introduces collaborative filtering, which uses matrix factorization to build recommendation systems, and explores real-world applications like Eigenfaces for facial recognition.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#1-support-vector-machines-svms-and-the-kernel-trick","title":"1. Support Vector Machines (SVMs) and the Kernel Trick","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#linear-svms","title":"Linear SVMs","text":"<p>In simple SVMs, we seek a hyperplane (a line in 2D, a plane in 3D, etc.) that best separates data into two classes by maximizing the margin between them.</p> <p>Mathematically, the hyperplane is defined as:</p> \\[ w^T x + b = 0 \\] <p>where: - \\(w\\) = weight vector (normal to the hyperplane) - \\(b\\) = bias</p> <p>Optimization aims to maximize the margin while correctly classifying as many points as possible.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#when-data-is-nonlinear","title":"When Data is Nonlinear","text":"<p>Often, data isn't linearly separable. To fix this, we can map data into a higher-dimensional space where separation is easier. But computing these higher-dimensional vectors explicitly is expensive!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#enter-the-kernel-trick","title":"Enter the Kernel Trick","text":"<p>Rather than computing transformations explicitly, the kernel trick computes the inner products in the higher-dimensional space directly using a kernel function \\(k(x, y)\\).</p> <p>Examples of kernels: - Linear Kernel: \\(k(x,y) = x^T y\\) - Polynomial Kernel: \\(k(x,y) = (x^T y + c)^d\\) - RBF (Gaussian) Kernel: \\(k(x,y) = \\exp\\left(-\\gamma \\|x-y\\|^2\\right)\\)</p> <p>Key Insight: You never need to know the mapping explicitly; you only need the inner products!</p> <p>Analogy</p> <p>Imagine trying to separate intertwined spaghetti noodles (complex data) on a plate. Instead of untangling them manually, you freeze them into ice, making them rigid and easier to separate. The kernel trick \"freezes\" complexity into manageable pieces.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#2-positive-semidefinite-kernels-and-gram-matrices","title":"2. Positive Semidefinite Kernels and Gram Matrices","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#positive-semidefinite-psd-kernels","title":"Positive Semidefinite (PSD) Kernels","text":"<p>A kernel function must generate a positive semidefinite Gram matrix: - A matrix \\(K\\) where each entry \\(K_{ij} = k(x_i, x_j)\\). - Positive semidefinite means that for any vector \\(z\\):</p> \\[ z^T K z \\geq 0 \\]"},{"location":"old-sections/section-5/chapter-20/chapter-20/#why-psd-matters","title":"Why PSD Matters","text":"<ul> <li>Ensures the kernel represents a valid inner product.</li> <li>Guarantees convexity in SVM optimization (critical for finding global minima).</li> </ul> <p>Important Reminder: PSD matrices have non-negative eigenvalues, linking back to your earlier understanding of eigenvalues from Chapters 5 and 6!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#3-collaborative-filtering-and-matrix-factorization","title":"3. Collaborative Filtering and Matrix Factorization","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#what-is-collaborative-filtering","title":"What is Collaborative Filtering?","text":"<p>Imagine a movie platform like Netflix. How do we recommend movies you haven't seen yet? - Collaborative Filtering predicts your preferences based on patterns in your and others' ratings.</p> <p>This leads to a large, partially-filled user-item matrix where entries are known ratings.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#matrix-factorization","title":"Matrix Factorization","text":"<p>The idea is to factorize the user-item matrix \\(M\\) into two low-rank matrices:</p> \\[ M \\approx U V^T \\] <p>where: - \\(U\\) = user factors (preferences) - \\(V\\) = item factors (attributes)</p> <p>The product \\(U V^T\\) reconstructs known ratings and fills in the missing ones.</p> <p>Optimization typically minimizes the reconstruction error:</p> \\[ \\min_{U, V} \\sum_{(i,j) \\in \\text{known}} (M_{ij} - U_i^T V_j)^2 \\]"},{"location":"old-sections/section-5/chapter-20/chapter-20/#4-eigenfaces-for-face-recognition","title":"4. Eigenfaces for Face Recognition","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#eigenfaces-method","title":"Eigenfaces Method","text":"<ul> <li>Treat grayscale face images as large vectors.</li> <li>Compute the covariance matrix of the training faces.</li> <li>Perform eigen decomposition to find principal components (\"Eigenfaces\").</li> <li>Represent each face as a linear combination of a few Eigenfaces.</li> </ul> <p>Key Intuition: Faces live in a much smaller \"face space\" than the full pixel space!</p> <p>This is a direct application of PCA (Chapter 17) to image classification.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#summary","title":"Summary","text":"<ul> <li>SVMs maximize margins for classification and use the kernel trick to handle nonlinear data.</li> <li>Kernels must create positive semidefinite Gram matrices to guarantee valid optimization.</li> <li>Collaborative filtering uses low-rank matrix factorization for recommendation systems.</li> <li>Eigenfaces apply PCA concepts to facial recognition by finding a lower-dimensional \"face space.\"</li> </ul> <p>These techniques are foundational for modern machine learning applications ranging from Netflix recommendations to facial ID systems!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#quiz-question","title":"Quiz Question","text":"<p>Which concept allows Support Vector Machines to operate in a higher-dimensional space without explicitly computing the mapping?</p> <p>A. Matrix decomposition B. Kernel trick C. Eigen decomposition D. SVD factorization</p> Show Answer <p>The correct answer is B. The kernel trick enables operations in higher-dimensional feature spaces without explicitly computing the transformations.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/","title":"Chapter 21: Specialized Topics in Linear Algebra Applications","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#introduction","title":"Introduction","text":"<p>In previous chapters, we have built a strong foundation in linear algebra techniques powering data science and machine learning. Now, we venture into specialized and advanced applications where linear algebra intersects with modern AI, large-scale systems, and emerging technologies.</p> <p>In this chapter, we study the spectral theorem, matrix perturbation theory, tensor operations, Kronecker products, vectorization techniques, and sparse matrix solvers \u2014 all critical tools for working at scale and with cutting-edge AI systems.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#1-spectral-theorem","title":"1. Spectral Theorem","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#what-it-says","title":"What It Says","text":"<p>If a matrix \\(A\\) is symmetric, it can be diagonalized by an orthogonal matrix:</p> \\[ A = Q \\Lambda Q^T \\] <p>where: - \\(Q\\) = matrix of orthonormal eigenvectors - \\(\\Lambda\\) = diagonal matrix of eigenvalues</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Simplifies computations dramatically.</li> <li>Powers techniques like PCA, SVD, and spectral clustering.</li> </ul> <p>Key Intuition: Symmetric matrices are \"nice\" \u2014 their eigenvectors form an orthonormal basis, making transformations purely scaling operations along those directions.</p> <p>Vibrations of a String</p> <p>In physics, modes of vibration of a string correspond to eigenvectors of a matrix describing the system \u2014 spectral decomposition reveals natural resonances.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#2-matrix-perturbation-theory","title":"2. Matrix Perturbation Theory","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#the-problem","title":"The Problem","text":"<p>Real-world data is noisy. Even small changes to a matrix (perturbations) can affect eigenvalues, eigenvectors, and solutions.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#what-we-study","title":"What We Study","text":"<ul> <li>How sensitive are eigenvalues/eigenvectors to small changes?</li> <li>How robust are algorithms against floating-point errors?</li> </ul>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications","title":"Applications","text":"<ul> <li>Understanding stability in machine learning models.</li> <li>Ensuring numerical safety in simulations.</li> </ul> <p>Connection to Previous Learning: This extends your understanding of eigenanalysis (Chapter 6) into practical, imperfect settings where exact numbers can't be trusted.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#3-tensor-basics","title":"3. Tensor Basics","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#beyond-matrices","title":"Beyond Matrices","text":"<p>A tensor generalizes matrices to higher dimensions: - Scalars (0D tensors) - Vectors (1D tensors) - Matrices (2D tensors) - Higher-order arrays (3D+, e.g., video data)</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#why-tensors-matter","title":"Why Tensors Matter","text":"<p>Modern AI models (like deep learning networks) often handle tensor inputs: - Images: (Height, Width, Channels) - Videos: (Time, Height, Width, Channels)</p> <p>Tensors allow for richer representations of data structures.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#4-matrix-calculus-fundamentals","title":"4. Matrix Calculus Fundamentals","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#matrix-derivatives","title":"Matrix Derivatives","text":"<p>Extends standard calculus to matrix-valued functions.</p> <p>Examples: - Gradient of a scalar function with respect to a vector. - Derivative of matrix products (using chain rules).</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications_1","title":"Applications","text":"<ul> <li>Training deep neural networks.</li> <li>Optimizing functions over matrix variables.</li> </ul> <p>Connection to Earlier Chapters: You already saw matrix calculus in backpropagation (Chapter 18); here, we formalize the broader theory.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#5-kronecker-product-and-vectorization","title":"5. Kronecker Product and Vectorization","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#kronecker-product","title":"Kronecker Product","text":"<p>Given matrices \\(A\\) and \\(B\\):</p> \\[ A \\otimes B \\] <p>produces a large block matrix \u2014 every element of \\(A\\) multiplied by the entire matrix \\(B\\).</p> <p>Usage: - Modeling structured systems. - Compactly representing large linear operations.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#vectorization","title":"Vectorization","text":"<p>\"Flatten\" a matrix into a long column vector by stacking its columns.</p> <p>Notation:</p> \\[ \\text{vec}(A) \\] <p>Key Identity:</p> \\[ \\text{vec}(ABC) = (C^T \\otimes A) \\text{vec}(B) \\] <p>Vectorization allows matrix equations to be manipulated as large linear systems \u2014 a powerful trick for optimization and computation.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#6-sparse-matrix-solvers","title":"6. Sparse Matrix Solvers","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#sparse-matrices","title":"Sparse Matrices","text":"<p>Matrices where most entries are zero.</p> <p>Examples: - Social network graphs. - Recommendation systems.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#solving-sparse-systems","title":"Solving Sparse Systems","text":"<p>Standard methods (Gaussian elimination) are inefficient for sparse systems. Specialized solvers: - Store only non-zero entries. - Use iterative methods (like Conjugate Gradient, GMRES).</p> <p>Benefits: - Saves memory. - Speeds up computation dramatically.</p> <p>Connection to Previous Learning: This builds on your knowledge of iterative methods (Chapter 8) with a focus on real-world scalability.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications-in-ai-driven-fields","title":"Applications in AI-Driven Fields","text":"<ul> <li>Large language models (LLMs) use massive sparse tensors.</li> <li>Computer vision uses tensor decomposition.</li> <li>Recommender systems leverage matrix factorization and sparse solvers.</li> <li>Scientific computing relies on perturbation analysis to ensure reliable simulations.</li> </ul>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#summary","title":"Summary","text":"<ul> <li>The spectral theorem enables powerful decompositions.</li> <li>Matrix perturbation theory helps manage errors.</li> <li>Tensors model rich, multi-dimensional data.</li> <li>Matrix calculus generalizes derivatives for optimization.</li> <li>Kronecker products and vectorization simplify large linear operations.</li> <li>Sparse matrix solvers enable efficient computation at massive scale.</li> </ul> <p>These specialized tools bridge linear algebra to real-world AI, data science, and large-scale system design.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#quiz-question","title":"Quiz Question","text":"<p>What is a major advantage of using sparse matrix solvers in large-scale machine learning applications?</p> <p>A. They reduce memory usage and computational time B. They increase the density of the matrix C. They ensure perfect numerical accuracy D. They eliminate the need for eigenvalues</p> Show Answer <p>The correct answer is A. Sparse matrix solvers exploit the zero patterns in matrices to save memory and accelerate computations, which is crucial for handling large-scale data.</p>"},{"location":"prompts/cover-image/","title":"Cover image","text":"<p>Please generate a wide-landscape cover image for my new course on applied linear algebra. The format must be a 1.91:1 width:height ratio image for use on social media previews.  Please the title \"Linear Algebra\" in the center and put a montage of images around the title that you infer from the full course description below.</p> <p>Here is the course description:</p>"},{"location":"prompts/cover-image/#course-description-applied-linear-algebra-for-ai-and-machine-learning","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"prompts/cover-image/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"prompts/cover-image/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"prompts/cover-image/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"prompts/cover-image/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"prompts/cover-image/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"prompts/cover-image/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"prompts/cover-image/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"prompts/cover-image/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"prompts/cover-image/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"prompts/cover-image/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"prompts/cover-image/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"prompts/cover-image/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"prompts/cover-image/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"prompts/cover-image/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"prompts/cover-image/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"prompts/cover-image/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"prompts/cover-image/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"prompts/cover-image/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"prompts/cover-image/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"prompts/cover-image/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"prompts/cover-image/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"prompts/cover-image/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"prompts/cover-image/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"prompts/cover-image/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"prompts/cover-image/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"prompts/cover-image/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"prompts/cover-image/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"prompts/cover-image/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"prompts/cover-image/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"prompts/cover-image/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"prompts/cover-image/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"prompts/cover-image/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"prompts/cover-image/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"prompts/cover-image/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"prompts/cover-image/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"prompts/logo/","title":"Logo","text":"<p>Create a logo using minimalistic geometry for my new textbook on linear algebra. It should be usable as a favicon.</p>"},{"location":"sims/basis-coordinate-visualizer/","title":"Basis and Coordinate System Visualizer","text":"<p>Run the Basis and Coordinate System Visualizer Fullscreen</p>"},{"location":"sims/basis-coordinate-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates that the same geometric vector can have different coordinate representations depending on which basis you use. The left panel shows the standard basis (e\u2081, e\u2082) while the right panel shows a custom basis (b\u2081, b\u2082) that you can modify.</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p>"},{"location":"sims/basis-coordinate-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Vector: In either panel, drag the green vector endpoint to move it</li> <li>Drag Basis Vectors: In the right panel, drag the endpoints of b\u2081 (red) and b\u2082 (blue) to change the custom basis</li> <li>Use Presets: Click preset buttons to see common basis configurations:</li> <li>Standard: b\u2081 = (1, 0), b\u2082 = (0, 1) - matches the standard basis</li> <li>Rotated 45\u00b0: Basis rotated by 45 degrees</li> <li>Skewed: Non-orthogonal basis vectors</li> <li>Stretched: Basis vectors with different lengths</li> <li>Toggle Options:</li> <li>Show Grid: Toggle grid lines in both panels</li> <li>Show Projections: Toggle dashed projection lines</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/basis-coordinate-visualizer/#coordinate-representation","title":"Coordinate Representation","text":"<p>The same geometric vector v has different coordinates in different bases: - In standard basis: v = (x, y) means v = x\u00b7e\u2081 + y\u00b7e\u2082 - In custom basis: [v]_B = (c\u2081, c\u2082) means v = c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082</p>"},{"location":"sims/basis-coordinate-visualizer/#the-equation","title":"The Equation","text":"<p>The coordinates [v]_B satisfy: \\(\\(\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2\\)\\)</p>"},{"location":"sims/basis-coordinate-visualizer/#change-of-basis","title":"Change of Basis","text":"<p>When you change the basis vectors, the grid lines in the right panel change to follow the new basis directions. The coordinates change, but the geometric vector stays the same!</p>"},{"location":"sims/basis-coordinate-visualizer/#important-observations","title":"Important Observations","text":"<ol> <li>Same Vector, Different Numbers: Moving to a stretched basis makes coordinates smaller</li> <li>Grid Deformation: The grid follows the basis vectors</li> <li>Parallel Basis Warning: If b\u2081 and b\u2082 become parallel, coordinates become undefined</li> <li>Verification: The formula c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082 = v is shown at the bottom</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/basis-coordinate-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/basis-coordinate-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/basis-coordinate-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/basis-coordinate-visualizer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/basis-coordinate-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics</li> <li>Linear combinations</li> <li>Understanding of basis vectors</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Start with \"Standard\" preset and observe both panels show same coordinates</li> <li> <p>Move the vector and verify coordinates match</p> </li> <li> <p>Rotated Basis Investigation (5 min):</p> </li> <li>Click \"Rotated 45\u00b0\"</li> <li>Notice how coordinates change but vector stays the same</li> <li> <p>Find a vector where custom coordinates are simpler than standard</p> </li> <li> <p>Skewed Basis Exploration (5 min):</p> </li> <li>Click \"Skewed\"</li> <li>Observe how grid lines are no longer perpendicular</li> <li> <p>Verify the linear combination formula still works</p> </li> <li> <p>Custom Basis Creation (10 min):</p> </li> <li>Drag b\u2081 and b\u2082 to create your own basis</li> <li>Find a basis where a specific vector has integer coordinates</li> <li>Try to make b\u2081 and b\u2082 parallel and observe the \"undefined\" warning</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the same vector have different coordinates in different bases?</li> <li>What happens geometrically when you stretch a basis vector?</li> <li>Why do parallel basis vectors make coordinates undefined?</li> <li>How would you convert coordinates from one basis to another?</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a vector and a basis, calculate the coordinates by hand and verify</li> <li>Explain why the grid deforms when the basis changes</li> <li>Find a basis that makes a given vector have coordinates (1, 1)</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Change of basis - Excellent visual explanation</li> <li>Khan Academy - Change of basis</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.5.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 4.4.</li> </ol>"},{"location":"sims/block-matrix/","title":"Block Matrix Partitioning","text":"<p>Run the Block Matrix MicroSim Fullscreen</p> <p>Edit the Block Matrix MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/block-matrix/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/block-matrix/#description","title":"Description","text":"<p>A block matrix (or partitioned matrix) views a matrix as an array of smaller matrices called blocks or submatrices. This MicroSim lets you interactively partition an 8\u00d78 matrix and see how the blocks are formed.</p> <p>Key Features:</p> <ul> <li>Draggable Partitions: Drag the red (horizontal) and blue (vertical) handles to resize blocks</li> <li>Color-Coded Blocks: Each of the four blocks (A, B, C, D) has a distinct color</li> <li>Preset Patterns: Choose from 2\u00d72, row, column, or asymmetric partitions</li> <li>Dimension Display: See the dimensions of each block update in real-time</li> </ul>"},{"location":"sims/block-matrix/#block-matrix-notation","title":"Block Matrix Notation","text":"<p>A matrix M partitioned into four blocks is written:</p> \\[M = \\begin{bmatrix} A &amp; B \\\\ C &amp; D \\end{bmatrix}\\] <p>where A, B, C, D are submatrices with compatible dimensions:</p> <ul> <li>A is (top rows) \u00d7 (left columns)</li> <li>B is (top rows) \u00d7 (right columns)</li> <li>C is (bottom rows) \u00d7 (left columns)</li> <li>D is (bottom rows) \u00d7 (right columns)</li> </ul>"},{"location":"sims/block-matrix/#why-block-matrices","title":"Why Block Matrices?","text":""},{"location":"sims/block-matrix/#parallel-computation","title":"Parallel Computation","text":"<p>Independent blocks can be processed on different cores or machines.</p>"},{"location":"sims/block-matrix/#structured-algorithms","title":"Structured Algorithms","text":"<p>Many algorithms exploit block structure: - Block LU decomposition - Strassen's matrix multiplication - Hierarchical matrices (H-matrices)</p>"},{"location":"sims/block-matrix/#conceptual-clarity","title":"Conceptual Clarity","text":"<p>Complex systems naturally decompose into interacting subsystems that correspond to blocks.</p>"},{"location":"sims/block-matrix/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/block-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Partition a matrix into blocks of specified dimensions</li> <li>Write block matrix notation for a given partition</li> <li>Identify valid block decompositions for matrix operations</li> <li>Explain how block structure enables efficient computation</li> </ol>"},{"location":"sims/block-matrix/#exploration-activity-5-minutes","title":"Exploration Activity (5 minutes)","text":"<ol> <li>Default Partition: Observe the symmetric 4\u00d74 / 4\u00d74 split</li> <li>Drag Partitions: Move handles to create different block sizes</li> <li>Try Presets: Compare row partition vs column partition</li> <li>Check Dimensions: Verify that block dimensions sum to 8</li> </ol>"},{"location":"sims/block-matrix/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>What happens if you partition a matrix for block multiplication but the inner dimensions don't match?</li> <li>When would you choose row partition vs 2\u00d72 blocks?</li> <li>How does block structure relate to parallel computing?</li> </ul>"},{"location":"sims/block-matrix/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Block matrices in context</li> <li>Matrix Computations - Golub and Van Loan (Chapter 1)</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/","title":"Dot Product and Cross Product Visualizer","text":"<p>Run the Dot Product and Cross Product Visualizer Fullscreen</p> <p>Edit the Dot Product and Cross Product Visualizer Using the p5.js Editor</p>"},{"location":"sims/dot-cross-product-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand the geometric meaning of both the dot product and cross product. The dot product view shows how vectors project onto each other and how the angle between vectors affects the result. The cross product view demonstrates the perpendicular vector and its relationship to parallelogram area.</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p>"},{"location":"sims/dot-cross-product-visualizer/#how-to-use","title":"How to Use","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product-view-2d","title":"Dot Product View (2D)","text":"<ol> <li>Drag Vectors: Click and drag the endpoints of vectors u (blue) and v (red)</li> <li>Observe Projection: The purple line shows the projection of v onto u</li> <li>Watch the Angle: The orange arc shows the angle \u03b8 between vectors</li> <li>See the Formula: The panel shows how u\u00b7v = |u||v|cos(\u03b8)</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#cross-product-view-3d","title":"Cross Product View (3D)","text":"<ol> <li>Toggle View: Click \"Show Cross Product (3D)\" to switch views</li> <li>Rotate Scene: Click and drag to rotate the 3D view</li> <li>Observe Parallelogram: The yellow area shows the parallelogram formed by u and v</li> <li>See Result Vector: The green vector u\u00d7v is perpendicular to both u and v</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#controls","title":"Controls","text":"<ul> <li>Show Projection: Toggle the projection visualization</li> <li>Show Parallelogram: Toggle the parallelogram in 3D view</li> <li>Show Formula: Toggle the step-by-step formula calculation</li> <li>Animate Angle Sweep: Watch how the products change as angle varies from 0\u00b0 to 180\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product","title":"Dot Product","text":"<ul> <li>Projection: The dot product relates to how much one vector projects onto another</li> <li>Angle Relationship: \\(\\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)</li> <li>Perpendicularity: When \u03b8 = 90\u00b0, the dot product is zero</li> <li>Sign: Positive when angle &lt; 90\u00b0, negative when angle &gt; 90\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#cross-product","title":"Cross Product","text":"<ul> <li>Perpendicular Result: u\u00d7v is perpendicular to both u and v</li> <li>Area: |u\u00d7v| equals the area of the parallelogram formed by u and v</li> <li>Right-Hand Rule: The direction follows the right-hand rule</li> <li>Only in 3D: The cross product is only defined for 3D vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Dot Product: \\(\\(\\mathbf{u} \\cdot \\mathbf{v} = u_x v_x + u_y v_y = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)\\)</p> <p>Cross Product: \\(\\(\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_y v_z - u_z v_y \\\\ u_z v_x - u_x v_z \\\\ u_x v_y - u_y v_x \\end{bmatrix}\\)\\)</p> \\[|\\mathbf{u} \\times \\mathbf{v}| = |\\mathbf{u}||\\mathbf{v}|\\sin\\theta\\]"},{"location":"sims/dot-cross-product-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/dot-cross-product-visualizer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/dot-cross-product-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/dot-cross-product-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/dot-cross-product-visualizer/#duration","title":"Duration","text":"<p>25-30 minutes</p>"},{"location":"sims/dot-cross-product-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics (magnitude, direction, components)</li> <li>Trigonometry (cosine, sine)</li> <li>Basic understanding of perpendicularity</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Dot Product Exploration (10 min):</li> <li>Start with u = (3, 0) and v = (2, 0) (same direction)</li> <li>Rotate v to 90\u00b0 and observe dot product becomes zero</li> <li>Continue to 180\u00b0 and see negative dot product</li> <li> <p>Run animation to see the full sweep</p> </li> <li> <p>Projection Investigation (5 min):</p> </li> <li>Observe how the projection length changes with angle</li> <li>Find configurations where projection equals |v|</li> <li> <p>Find configurations where projection is negative</p> </li> <li> <p>Cross Product Exploration (10 min):</p> </li> <li>Switch to 3D view</li> <li>Observe the green cross product vector</li> <li>Rotate view to verify it's perpendicular to both u and v</li> <li> <p>Change vectors and observe parallelogram area changes</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Toggle between views for the same vectors</li> <li>Compare how dot product and cross product magnitude change with angle</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the dot product zero when vectors are perpendicular?</li> <li>What does the sign of the dot product tell you about the angle?</li> <li>Why does the cross product only exist in 3D?</li> <li>How can you use dot product to find the angle between two vectors?</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given two vectors, predict whether the dot product is positive, negative, or zero</li> <li>Calculate the area of a parallelogram using the cross product</li> <li>Find a vector perpendicular to two given vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Dot products and duality</li> <li>3Blue1Brown - Cross products</li> <li>Khan Academy - Cross Product Introduction</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.2-1.3.</li> </ol>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>Open Learning Graph Viewer</p> <p>This interactive viewer allows you to explore the learning graph for Applied Linear Algebra for AI and Machine Learning.</p>"},{"location":"sims/graph-viewer/#features","title":"Features","text":"<ul> <li>Search: Type in the search box to find specific concepts</li> <li>Category Filtering: Use checkboxes to show/hide concept categories</li> <li>Interactive Navigation: Click and drag to explore, scroll to zoom</li> <li>Statistics: View real-time counts of visible nodes and edges</li> </ul>"},{"location":"sims/graph-viewer/#using-the-viewer","title":"Using the Viewer","text":"<ol> <li> <p>Search for Concepts: Start typing in the search box to find concepts. Click on a result to focus on that node.</p> </li> <li> <p>Filter by Category: Use the category checkboxes in the sidebar to show or hide groups of related concepts. Use \"Check All\" or \"Uncheck All\" for bulk operations.</p> </li> <li> <p>Navigate the Graph:</p> </li> <li>Drag to pan around the graph</li> <li>Scroll to zoom in and out</li> <li> <p>Click on a node to select it and highlight its connections</p> </li> <li> <p>View Statistics: The sidebar shows counts of visible nodes, edges, and foundational concepts.</p> </li> </ol>"},{"location":"sims/graph-viewer/#graph-structure","title":"Graph Structure","text":"<ul> <li>Foundational Concepts: Prerequisites with no dependencies</li> <li>Advanced Concepts: Topics that build on multiple prerequisites</li> <li>Edges: Arrows point from a concept to its prerequisites</li> </ul>"},{"location":"sims/graph-viewer/#launch-the-viewer","title":"Launch the Viewer","text":""},{"location":"sims/linear-combination-explorer/","title":"Linear Combination Explorer","text":"<p>Run the Linear Combination Explorer Fullscreen</p>"},{"location":"sims/linear-combination-explorer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand linear combinations by allowing them to adjust scalar coefficients and observe how the result vector changes. The challenge mode tests students' ability to find the right coefficients to reach a target point.</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p>"},{"location":"sims/linear-combination-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Coefficients: Use the c\u2081 and c\u2082 sliders to change the scalar multipliers</li> <li>Drag Basis Vectors: Click and drag the endpoints of v\u2081 (red) and v\u2082 (blue) to change their directions</li> <li>Observe the Result: The green arrow shows c\u2081v\u2081 + c\u2082v\u2082</li> <li>See Tip-to-Tail: Enable \"Show Components\" to see how the scaled vectors add tip-to-tail</li> <li>Challenge Mode: Click \"New Challenge\" to get a target point (yellow star), then find the coefficients to reach it</li> <li>Get Help: Click \"Show Solution\" to see the answer</li> </ol>"},{"location":"sims/linear-combination-explorer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/linear-combination-explorer/#linear-combination","title":"Linear Combination","text":"\\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\] <p>A linear combination is a sum of scalar multiples of vectors. Any vector in the plane can be written as a linear combination of two non-parallel vectors.</p>"},{"location":"sims/linear-combination-explorer/#span","title":"Span","text":"<p>The span of vectors is the set of all possible linear combinations. For two non-parallel vectors in 2D, the span is the entire plane.</p>"},{"location":"sims/linear-combination-explorer/#parallel-vectors","title":"Parallel Vectors","text":"<p>When v\u2081 and v\u2082 are parallel (one is a scalar multiple of the other), their span collapses to a line. The visualization shows this with a warning.</p>"},{"location":"sims/linear-combination-explorer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/linear-combination-explorer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/linear-combination-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/linear-combination-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/linear-combination-explorer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/linear-combination-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication</li> <li>Basic understanding of coordinate systems</li> </ul>"},{"location":"sims/linear-combination-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Set c\u2081 = 1, c\u2082 = 0 and observe: result equals v\u2081</li> <li>Set c\u2081 = 0, c\u2082 = 1 and observe: result equals v\u2082</li> <li> <p>Set c\u2081 = 1, c\u2082 = 1 and observe: tip-to-tail addition</p> </li> <li> <p>Coefficient Investigation (5 min):</p> </li> <li>What happens when c\u2081 = -1?</li> <li>Find coefficients that put the result at (0, 0)</li> <li> <p>Make the result point in the opposite direction of v\u2081</p> </li> <li> <p>Challenge Mode (10 min):</p> </li> <li>Click \"New Challenge\" to get a target</li> <li>Try to reach the target by adjusting only c\u2081 and c\u2082</li> <li>Record how many attempts it takes</li> <li> <p>After solving, verify by checking the math</p> </li> <li> <p>Span Investigation (5 min):</p> </li> <li>Drag v\u2082 to be parallel to v\u2081</li> <li>Notice the \"span is a line\" warning</li> <li>Try to reach targets outside the line - impossible!</li> </ol>"},{"location":"sims/linear-combination-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why can any point in the plane be reached with two non-parallel vectors?</li> <li>What happens to the span when the vectors become parallel?</li> <li>Is the linear combination c\u2081v\u2081 + c\u2082v\u2082 the same as c\u2082v\u2082 + c\u2081v\u2081?</li> <li>How would you find the coefficients algebraically?</li> </ol>"},{"location":"sims/linear-combination-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, calculate the coefficients by solving a system of equations</li> <li>Explain why two parallel vectors only span a line</li> <li>Predict whether a given point is reachable with given basis vectors</li> </ul>"},{"location":"sims/linear-combination-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors - Excellent visual introduction</li> <li>Khan Academy - Linear Combinations and Span</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.3.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 1.3.</li> </ol>"},{"location":"sims/matrix-basic-ops/","title":"Matrix Addition and Scalar Multiplication","text":"<p>Run the Matrix Basic Operations MicroSim Fullscreen</p> <p>Edit the Matrix Basic Operations MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-basic-ops/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-basic-ops/#description","title":"Description","text":"<p>This MicroSim provides hands-on practice with the two most fundamental matrix operations: addition and scalar multiplication. Both operations are element-wise, meaning they operate on corresponding entries independently.</p> <p>Key Features:</p> <ul> <li>Dual Operations: Switch between matrix addition (A + B = C) and scalar multiplication (k \u00d7 A = C)</li> <li>Visual Layout: Three matrices displayed with operation symbols for clear understanding</li> <li>Step-Through Mode: Click \"Step\" to highlight each calculation sequentially</li> <li>Adjustable Scalar: Slider controls the scalar value from -3 to 3 for multiplication</li> <li>Formula Display: Shows the mathematical formula and current calculation</li> </ul>"},{"location":"sims/matrix-basic-ops/#how-it-works","title":"How It Works","text":""},{"location":"sims/matrix-basic-ops/#matrix-addition","title":"Matrix Addition","text":"<p>For two matrices A and B of the same dimensions, their sum C is computed entry-by-entry:</p> \\[c_{ij} = a_{ij} + b_{ij}\\]"},{"location":"sims/matrix-basic-ops/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>For a scalar k and matrix A, the product C multiplies each entry by k:</p> \\[c_{ij} = k \\cdot a_{ij}\\]"},{"location":"sims/matrix-basic-ops/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-basic-ops/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Calculate the sum of two matrices by adding corresponding entries</li> <li>Multiply a matrix by a scalar by multiplying each entry</li> <li>Recognize that both operations preserve matrix dimensions</li> <li>Understand the element-wise nature of these operations</li> </ol>"},{"location":"sims/matrix-basic-ops/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Start with Addition: Observe how each entry in C equals the sum of corresponding entries in A and B</li> <li>Use Step Mode: Click \"Step\" repeatedly to see each calculation highlighted in sequence</li> <li>Switch to Scalar Multiply: Change to scalar multiplication and adjust the slider</li> <li>Explore Edge Cases: What happens when k = 0? When k = -1?</li> </ol>"},{"location":"sims/matrix-basic-ops/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Dimension Requirement: For addition, matrices must have the same dimensions</li> <li>Commutativity: Matrix addition is commutative (A + B = B + A)</li> <li>Scalar Distribution: k(A + B) = kA + kB</li> </ul>"},{"location":"sims/matrix-basic-ops/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If A[2,3] = 4 and B[2,3] = -2, what is C[2,3] in A + B?</li> <li>If k = -2 and A[1,1] = 5, what is C[1,1] in kA?</li> <li>Does the order of matrices matter for addition? Why or why not?</li> </ol>"},{"location":"sims/matrix-basic-ops/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix addition and scalar multiplication in context</li> <li>Linear Algebra and Its Applications - Lay, Lay, and McDonald</li> </ul>"},{"location":"sims/matrix-inverse/","title":"Matrix Inverse Explorer","text":"<p>Run the Matrix Inverse Explorer MicroSim Fullscreen</p> <p>Edit the Matrix Inverse Explorer MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-inverse/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-inverse/#description","title":"Description","text":"<p>The matrix inverse generalizes division to matrices. For a square matrix A, its inverse A\u207b\u00b9 (if it exists) satisfies AA\u207b\u00b9 = A\u207b\u00b9A = I, where I is the identity matrix. This MicroSim lets you explore matrix inversion interactively.</p> <p>Key Features:</p> <ul> <li>Real-Time Computation: See the inverse update instantly for random matrices</li> <li>Verification Display: Watch AA\u207b\u00b9 = I computed live</li> <li>Determinant Indicator: Color-coded display shows invertibility status</li> <li>Singularity Exploration: Make matrices singular or approach singularity smoothly</li> <li>Formula Display: See the 2\u00d72 inverse formula applied</li> </ul>"},{"location":"sims/matrix-inverse/#the-22-inverse-formula","title":"The 2\u00d72 Inverse Formula","text":"<p>For a 2\u00d72 matrix:</p> \\[A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\] <p>The inverse is:</p> \\[A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\] <p>Key insight: The inverse exists if and only if det(A) = ad - bc \u2260 0.</p>"},{"location":"sims/matrix-inverse/#singular-matrices","title":"Singular Matrices","text":"<p>A matrix is singular (not invertible) when:</p> <ul> <li>det(A) = 0</li> <li>The columns are linearly dependent</li> <li>The matrix maps some non-zero vector to zero</li> </ul> <p>The MicroSim shows this by turning red when you click \"Make Singular\" or slide toward singularity.</p>"},{"location":"sims/matrix-inverse/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-inverse/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Apply the 2\u00d72 inverse formula to compute A\u207b\u00b9</li> <li>Verify that AA\u207b\u00b9 = I for invertible matrices</li> <li>Identify singular matrices by their zero determinant</li> <li>Explain why singular matrices have no inverse</li> </ol>"},{"location":"sims/matrix-inverse/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Start with Default: Observe the invertible matrix and verify AA\u207b\u00b9 = I</li> <li>Click Randomize: Generate new matrices and check determinants</li> <li>Make Singular: Click the button and observe the warning</li> <li>Approach Singularity: Use the slider to see determinant approach zero</li> </ol>"},{"location":"sims/matrix-inverse/#discussion-points","title":"Discussion Points","text":"<ul> <li>What happens to the inverse entries as det(A) approaches zero?</li> <li>Why does \"Make Singular\" make row 2 proportional to row 1?</li> <li>How does this connect to solving systems of equations?</li> </ul>"},{"location":"sims/matrix-inverse/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For A = [[3, 1], [2, 1]], compute det(A) and A\u207b\u00b9.</li> <li>Why can't you divide by a matrix the way you divide by a number?</li> <li>If det(A) = 0.001, is A technically invertible? Is it practically invertible?</li> </ol>"},{"location":"sims/matrix-inverse/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix inverse in context</li> <li>Khan Academy: Inverse Matrices - Video explanation</li> </ul>"},{"location":"sims/matrix-multiplication/","title":"Matrix Multiplication Visualizer","text":"<p>Run the Matrix Multiplication MicroSim Fullscreen</p> <p>Edit the Matrix Multiplication MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-multiplication/main.html\" height=\"407px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-multiplication/#about-this-microsim","title":"About This MicroSim","text":"<p>Matrix multiplication is the most important and nuanced matrix operation in linear algebra\u2014and arguably the most misunderstood! This interactive MicroSim transforms an abstract algorithm into a visual, step-by-step experience that makes the row-by-column dot product process crystal clear. Watch as matrices come alive with color-coded highlighting, animated calculations, and real-time feedback that turns confusion into understanding.</p> <p>Key Features:</p> <ul> <li>Color-Coded Highlighting: The current row of A glows blue while the matching column of B glows green, showing exactly which vectors combine to form each result</li> <li>Step-by-Step Animation: Watch each element-wise multiplication happen in real-time with the current operation highlighted in yellow</li> <li>Live Running Sum: See the dot product accumulate as each term is added, building intuition for how entries are computed</li> <li>Operations Counter: Displays the total number of multiplications and additions required, reinforcing computational complexity concepts</li> <li>Auto-Play Mode: Sit back and watch the entire multiplication unfold automatically at adjustable speeds</li> <li>Flexible Dimensions: Experiment with 2\u00d72, 2\u00d73, 3\u00d72, and 3\u00d73 matrices to understand dimension compatibility rules</li> </ul>"},{"location":"sims/matrix-multiplication/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/matrix-multiplication/#controls","title":"Controls","text":"Control Function A Dimension Selector Choose the dimensions of matrix A (2\u00d72, 2\u00d73, 3\u00d72, or 3\u00d73) B Dimension Selector Choose the dimensions of matrix B (options depend on A's columns) Reset Generate new random matrices and restart the visualization First/Next Multiplication Step through the calculation one multiplication at a time Auto/Stop Toggle automatic playback of the entire multiplication Animation Speed Slider Adjust playback speed (Slower on left, Faster on right)"},{"location":"sims/matrix-multiplication/#understanding-the-calculation-display","title":"Understanding the Calculation Display","text":"<p>The MicroSim displays the dot product calculation using special notation to show progress:</p> Notation Meaning Description \\(3 \\times 2\\) Completed This multiplication has been computed and added to the running sum \\([4 \\times 5]\\) Current This is the multiplication about to be performed (square brackets) \\((2 \\times 1)\\) Pending This multiplication is waiting to be computed (parentheses) <p>For example, when computing \\(c_{12}\\) with a 3-element dot product:</p> \\[3 \\times 2 + [4 \\times 5] + (2 \\times 1)\\] \\[\\underbrace{3 \\times 2}_{\\text{Done}} + \\underbrace{[4 \\times 5]}_{\\text{Current}} + \\underbrace{(2 \\times 1)}_{\\text{Future}}\\]"},{"location":"sims/matrix-multiplication/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Start Fresh: Click Reset to generate new random matrices</li> <li>Observe the Setup: Notice how matrix A (blue-tinted) and B (green-tinted) are displayed with their dimensions shown below</li> <li>Click First Multiplication: The first row of A and first column of B light up, showing which elements combine</li> <li>Watch the Calculation: The equation area shows the dot product formula with the current term in brackets</li> <li>See the Running Sum: After each multiplication, the running sum updates in red</li> <li>Continue Stepping: Click Next Multiplication to process each term until the entry is complete</li> <li>Move to Next Entry: Once \\(c_{11}\\) is computed (shown in gold), the visualization moves to \\(c_{12}\\)</li> <li>Try Auto Mode: Click Auto to watch the entire process animate automatically</li> </ol>"},{"location":"sims/matrix-multiplication/#tips-for-learning","title":"Tips for Learning","text":"<ul> <li>Predict Before Clicking: Before each step, mentally calculate what the running sum will be</li> <li>Slow Down: Use the Animation Speed slider to slow down auto-play for careful observation</li> <li>Change Dimensions: Try different matrix sizes to see how the number of operations changes</li> <li>Count Operations: Notice how the \"Number of Operations\" line updates when you change dimensions</li> </ul>"},{"location":"sims/matrix-multiplication/#how-matrix-multiplication-works","title":"How Matrix Multiplication Works","text":"<p>For matrices A (m\u00d7n) and B (n\u00d7p), the result C is an (m\u00d7p) matrix where:</p> \\[c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}\\] <p>Each entry \\(c_{ij}\\) is the dot product of row \\(i\\) of A with column \\(j\\) of B.</p>"},{"location":"sims/matrix-multiplication/#dimension-compatibility-rule","title":"Dimension Compatibility Rule","text":"<p>The number of columns in A must equal the number of rows in B:</p> Matrix A Matrix B Result C Valid? 2\u00d73 3\u00d72 2\u00d72 Yes 3\u00d72 3\u00d74 \u2014 No 2\u00d72 2\u00d72 2\u00d72 Yes"},{"location":"sims/matrix-multiplication/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-multiplication/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain the row-by-column dot product process for computing each entry</li> <li>Determine the dimensions of the result matrix given input dimensions</li> <li>Identify when two matrices can be multiplied (dimension compatibility)</li> <li>Calculate individual entries of a matrix product</li> </ol>"},{"location":"sims/matrix-multiplication/#guided-exploration-7-10-minutes","title":"Guided Exploration (7-10 minutes)","text":"<ol> <li>Watch One Entry: Click \"First Multiplication\" then \"Next Multiplication\" repeatedly to see \\(c_{11}\\) computed step by step</li> <li>Observe the Notation: Notice how completed terms have no brackets, the current term has square brackets, and future terms have parentheses</li> <li>Use Auto-Play: Click \"Auto\" to watch the entire multiplication animate automatically</li> <li>Change Dimensions: Try 3\u00d73 matrices to see more computation steps and higher operation counts</li> <li>Predict Before Clicking: Before each step, predict what the running sum will become</li> </ol>"},{"location":"sims/matrix-multiplication/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Not Commutative: Unlike regular multiplication, AB \u2260 BA in general</li> <li>Dimension Flow: (m\u00d7n) \u00d7 (n\u00d7p) \u2192 (m\u00d7p) \u2014 the inner dimensions must match</li> <li>Computational Cost: Each entry requires n multiplications and n-1 additions</li> </ul>"},{"location":"sims/matrix-multiplication/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For A (2\u00d73) and B (3\u00d74), what are the dimensions of C = AB?</li> <li>How many multiplication operations are needed to compute one entry of C?</li> <li>If A[1,2] = 3 and B[2,1] = 4, what is their contribution to C[1,1]?</li> </ol>"},{"location":"sims/matrix-multiplication/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix multiplication in context</li> <li>3Blue1Brown: Matrix Multiplication - Visual intuition for matrix multiplication</li> </ul>"},{"location":"sims/neural-network-layer/","title":"Neural Network Layer","text":"<p>Run the Neural Network Layer MicroSim Fullscreen</p> <p>Edit the Neural Network Layer MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/neural-network-layer/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/neural-network-layer/#description","title":"Description","text":"<p>This MicroSim visualizes how a fully connected neural network layer is implemented as matrix-vector multiplication. Each layer computes:</p> \\[h = \\sigma(Wx + b)\\] <p>where:</p> <ul> <li>x is the input vector (left neurons)</li> <li>W is the weight matrix (connection lines)</li> <li>b is the bias vector (optional, shown as nodes above outputs)</li> <li>\u03c3 is the activation function (ReLU, sigmoid, tanh, or none)</li> <li>h is the output vector (right neurons)</li> </ul> <p>Key Features:</p> <ul> <li>Visual Weights: Connection thickness shows weight magnitude; blue = positive, red = negative</li> <li>Activation Functions: Compare ReLU, sigmoid, tanh, or linear (none)</li> <li>Adjustable Architecture: Change the number of inputs and outputs</li> <li>Bias Toggle: Show or hide bias terms</li> <li>Random Initialization: Generate new weights or inputs</li> </ul>"},{"location":"sims/neural-network-layer/#the-matrix-view","title":"The Matrix View","text":"<p>The weight matrix W has dimensions (outputs \u00d7 inputs). Each row of W corresponds to one output neuron and contains the weights for all connections to that neuron.</p> <p>For 3 inputs and 2 outputs:</p> \\[W = \\begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\\\ w_{21} &amp; w_{22} &amp; w_{23} \\end{bmatrix}\\] <p>The output is computed as:</p> \\[h_i = \\sigma\\left(\\sum_{j=1}^{n} w_{ij} x_j + b_i\\right)\\]"},{"location":"sims/neural-network-layer/#activation-functions","title":"Activation Functions","text":"Function Formula Range Properties None \u03c3(z) = z (-\u221e, \u221e) Linear, no nonlinearity ReLU \u03c3(z) = max(0, z) [0, \u221e) Sparse activation, fast Sigmoid \u03c3(z) = 1/(1+e^(-z)) (0, 1) Smooth, probability interpretation Tanh \u03c3(z) = tanh(z) (-1, 1) Zero-centered, smooth"},{"location":"sims/neural-network-layer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/neural-network-layer/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain how a neural network layer implements matrix-vector multiplication</li> <li>Describe the role of weights, biases, and activation functions</li> <li>Calculate the output of a simple layer by hand</li> <li>Connect linear algebra concepts to deep learning</li> </ol>"},{"location":"sims/neural-network-layer/#guided-exploration-7-10-minutes","title":"Guided Exploration (7-10 minutes)","text":"<ol> <li>Start Simple: Set inputs=2, outputs=2, activation=none, no bias</li> <li>Observe Weights: Click \"Random W\" and watch connection changes</li> <li>Add Nonlinearity: Switch to ReLU and note how negative pre-activations become 0</li> <li>Enable Bias: Toggle bias on and observe the bias nodes appear</li> </ol>"},{"location":"sims/neural-network-layer/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Why do we need activation functions? (Without them, the whole network is just one big linear transformation)</li> <li>What does a large positive weight vs large negative weight mean visually?</li> <li>How many parameters (weights + biases) does this layer have?</li> </ul>"},{"location":"sims/neural-network-layer/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For a layer with 4 inputs and 3 outputs, what are the dimensions of W?</li> <li>If all weights are positive, can an output ever be negative (with ReLU)?</li> <li>How many total parameters does a 10-input, 5-output layer have (with bias)?</li> </ol>"},{"location":"sims/neural-network-layer/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Neural network layers as matrix operations</li> <li>Deep Learning Book - Goodfellow, Bengio, and Courville</li> </ul>"},{"location":"sims/norm-comparison-visualizer/","title":"Norm Comparison Visualizer","text":"<p>Run the Norm Comparison Visualizer Fullscreen</p>"},{"location":"sims/norm-comparison-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand different ways to measure vector \"length\" or distance. By showing the unit \"circles\" for L1, L2, and L-infinity norms simultaneously, students can see how each norm defines what it means for a vector to have \"length 1.\"</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p>"},{"location":"sims/norm-comparison-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Point: Click and drag the black point to any location on the grid</li> <li>Observe Norm Values: The info panel shows all three norm values for the current point</li> <li>Compare Unit Shapes:</li> <li>Blue Circle: L2 norm (Euclidean) - points at distance 1 from origin</li> <li>Green Diamond: L1 norm (Manhattan) - points with L1 distance 1 from origin</li> <li>Orange Square: L\u221e norm (Maximum) - points with L\u221e distance 1 from origin</li> <li>Toggle Norms: Use checkboxes to show/hide each norm's unit shape</li> <li>Adjust Radius: Use the slider to see how the shapes scale</li> <li>Animate: Watch the point move around the L2 unit circle</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/norm-comparison-visualizer/#l2-norm-euclidean","title":"L2 Norm (Euclidean)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_2 = \\sqrt{x^2 + y^2}\\)\\) - Standard \"straight-line\" distance - Unit shape is a circle - Used in: Least squares regression, ridge regularization</p>"},{"location":"sims/norm-comparison-visualizer/#l1-norm-manhattan","title":"L1 Norm (Manhattan)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_1 = |x| + |y|\\)\\) - Distance measured along axes (like city blocks) - Unit shape is a diamond (rotated square) - Used in: Lasso regularization, sparse solutions</p>"},{"location":"sims/norm-comparison-visualizer/#l-norm-maximum","title":"L\u221e Norm (Maximum)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_\\infty = \\max(|x|, |y|)\\)\\) - Maximum absolute component value - Unit shape is a square - Used in: Constraining maximum deviation</p>"},{"location":"sims/norm-comparison-visualizer/#why-different-norms-matter","title":"Why Different Norms Matter","text":"<ul> <li>For (3, 4): L1 = 7, L2 = 5, L\u221e = 4</li> <li>The same point can have very different \"distances\" depending on which norm you use</li> <li>In machine learning, L1 promotes sparse solutions while L2 distributes weight evenly</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/norm-comparison-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/norm-comparison-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/norm-comparison-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or machine learning</p>"},{"location":"sims/norm-comparison-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/norm-comparison-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of vectors and distance</li> <li>Familiarity with absolute value</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Drag the point to various locations</li> <li>Observe how the three norm values change differently</li> <li> <p>Find points where two norms give the same value</p> </li> <li> <p>Unit Shape Analysis (5 min):</p> </li> <li>Turn off two norms and examine one at a time</li> <li>Drag the point onto each boundary</li> <li> <p>Notice the indicator when point is \"on boundary\"</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Find a point where L1 &gt; L2 &gt; L\u221e</li> <li>Find a point where all three norms are equal (hint: on an axis)</li> <li> <p>Predict which norm will be largest for a given point</p> </li> <li> <p>Application Discussion (5 min):</p> </li> <li>Why might we use L1 in machine learning?</li> <li>What does it mean geometrically for L1 to \"promote sparsity\"?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the L1 unit \"circle\" a diamond shape?</li> <li>For what points are all three norms equal?</li> <li>Which norm gives the smallest value for most points? Why?</li> <li>How does the relationship between norms change as points move farther from the origin?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Calculate all three norms for a given vector</li> <li>Predict which shape a point is inside/outside of</li> <li>Explain why L1 regularization promotes sparse solutions</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#references","title":"References","text":"<ol> <li>Understanding Different Norms - Wolfram MathWorld</li> <li>L1 vs L2 Regularization - StatQuest</li> <li>Why L1 promotes sparsity - Cross Validated discussion</li> <li>Boyd, S. &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.</li> </ol>"},{"location":"sims/orthogonal-transform/","title":"Orthogonal Matrix Transformation","text":"<p>Run the Orthogonal Transform MicroSim Fullscreen</p> <p>Edit the Orthogonal Transform MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/orthogonal-transform/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/orthogonal-transform/#description","title":"Description","text":"<p>An orthogonal matrix Q satisfies Q^T Q = I, meaning its transpose equals its inverse. This special property makes orthogonal matrices preserve lengths and angles\u2014they represent rotations and reflections that transform shapes without distortion.</p> <p>Key Features:</p> <ul> <li>Real-Time Rotation: Drag the angle slider to see the unit square rotate smoothly</li> <li>Length Preservation: Toggle \"Lengths\" to verify that |Qv| = |v| for all vectors</li> <li>Angle Preservation: Toggle \"Angles\" to see that angles between vectors are maintained</li> <li>Reflection Toggle: Click \"Reflect\" to add a reflection (changes det(Q) from +1 to -1)</li> <li>Live Matrix Display: Watch the rotation matrix entries update as cos(\u03b8) and sin(\u03b8)</li> </ul>"},{"location":"sims/orthogonal-transform/#the-rotation-matrix","title":"The Rotation Matrix","text":"<p>A 2D rotation by angle \u03b8 is represented by:</p> \\[Q = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\] <p>Properties:</p> <ul> <li>Orthogonal: Q^T Q = Q Q^T = I</li> <li>Determinant: det(Q) = 1 (rotation) or -1 (reflection)</li> <li>Preserves Lengths: ||Qx|| = ||x|| for all vectors x</li> <li>Preserves Angles: The angle between Qx and Qy equals the angle between x and y</li> </ul>"},{"location":"sims/orthogonal-transform/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/orthogonal-transform/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain why orthogonal matrices preserve lengths and angles</li> <li>Write the 2D rotation matrix for any angle \u03b8</li> <li>Distinguish between rotation (det = +1) and reflection (det = -1)</li> <li>Verify that Q^T Q = I for rotation matrices</li> </ol>"},{"location":"sims/orthogonal-transform/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Rotate the Square: Move the slider from 0\u00b0 to 90\u00b0 and observe the shape transformation</li> <li>Check Lengths: Enable \"Lengths\" and verify the sample vectors maintain their magnitude</li> <li>Check Angles: Enable \"Angles\" and verify the angle between vectors is preserved</li> <li>Add Reflection: Click \"Reflect\" and observe how the square flips</li> </ol>"},{"location":"sims/orthogonal-transform/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Why does the unit square maintain its shape during rotation?</li> <li>What happens to the determinant when reflection is added?</li> <li>How does this relate to preserving area?</li> </ul>"},{"location":"sims/orthogonal-transform/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the rotation matrix for \u03b8 = 90\u00b0?</li> <li>If a rotation matrix has det = -1, is it a pure rotation?</li> <li>Why is Q^(-1) = Q^T computationally advantageous?</li> </ol>"},{"location":"sims/orthogonal-transform/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Orthogonal matrices in context</li> <li>3Blue1Brown: Linear Transformations - Visual understanding of transformations</li> </ul>"},{"location":"sims/row-column-vectors/","title":"Row and Column Vectors","text":"<p>Run the Row and Column Vectors MicroSim Fullscreen</p> <p>Edit the Row and Column Vectors MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/row-column-vectors/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/row-column-vectors/#description","title":"Description","text":"<p>This MicroSim provides a side-by-side comparison of row vectors and column vectors, the two fundamental vector orientations in linear algebra. Understanding the distinction between these vector types is essential for matrix operations, as the orientation determines how vectors interact with matrices during multiplication.</p> <p>Key Features:</p> <ul> <li>Visual Comparison: Row vectors are displayed horizontally (1\u00d7n), while column vectors are displayed vertically (m\u00d71)</li> <li>Color Coding: Blue for row vectors, green for column vectors helps reinforce the distinction</li> <li>Dynamic Dimensions: Adjust the number of elements (2-6) to see how both vector types scale</li> <li>Dimension Annotations: Toggle display of dimension labels (1\u00d7n and m\u00d71) to reinforce notation</li> </ul>"},{"location":"sims/row-column-vectors/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/row-column-vectors/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Visually distinguish between row vectors and column vectors</li> <li>Correctly identify the dimensions of row vectors (1\u00d7n) and column vectors (m\u00d71)</li> <li>Understand that the same values can be arranged as either a row or column vector</li> <li>Recognize how vector orientation affects matrix compatibility in multiplication</li> </ol>"},{"location":"sims/row-column-vectors/#warm-up-activity-2-minutes","title":"Warm-up Activity (2 minutes)","text":"<p>Ask students: \"If I have 4 numbers, how many different ways can I arrange them in a rectangle?\" Let them discover that a 1\u00d74 row and 4\u00d71 column are two valid arrangements.</p>"},{"location":"sims/row-column-vectors/#guided-exploration-5-minutes","title":"Guided Exploration (5 minutes)","text":"<ol> <li>Start with the default 4 elements</li> <li>Point out that both vectors contain the same number of values</li> <li>Toggle the dimension annotations to show 1\u00d74 for the row and 4\u00d71 for the column</li> <li>Click \"Randomize\" several times to show that values change but structure remains</li> </ol>"},{"location":"sims/row-column-vectors/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Notation: Row vectors are written horizontally: \\([r_1, r_2, r_3, r_4]\\)</li> <li>Transpose Relationship: A row vector transposed becomes a column vector</li> <li>Matrix Multiplication: A row vector (1\u00d7n) can multiply a column vector (n\u00d71) to produce a scalar (dot product)</li> </ul>"},{"location":"sims/row-column-vectors/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What are the dimensions of a row vector with 5 elements?</li> <li>If you transpose a 1\u00d76 row vector, what are the resulting dimensions?</li> <li>Can you multiply a 1\u00d73 row vector by a 3\u00d71 column vector? What is the result's dimension?</li> </ol>"},{"location":"sims/row-column-vectors/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Row and column vectors in context</li> <li>Linear Algebra and Its Applications - Lay, Lay, and McDonald</li> </ul>"},{"location":"sims/sparse-dense-matrices/","title":"Sparse vs Dense Matrices","text":"<p>Run the Sparse vs Dense MicroSim Fullscreen</p> <p>Edit the Sparse vs Dense MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/sparse-dense-matrices/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/sparse-dense-matrices/#description","title":"Description","text":"<p>Matrices are classified by their distribution of zero entries. This MicroSim provides a visual comparison of dense matrices (mostly non-zero) and sparse matrices (mostly zero), helping students understand why sparsity matters for computation.</p> <p>Key Features:</p> <ul> <li>Visual Comparison: Side-by-side matrix visualization with color intensity showing values</li> <li>Sparsity Patterns: Choose from random, diagonal, banded, or block patterns</li> <li>Storage Statistics: Real-time comparison of memory requirements</li> <li>Adjustable Parameters: Control matrix size and sparsity level</li> </ul>"},{"location":"sims/sparse-dense-matrices/#dense-vs-sparse","title":"Dense vs Sparse","text":"Property Dense Matrix Sparse Matrix Zero entries Few Many (typically &gt;90%) Storage O(n\u00b2) O(nnz) Storage format 2D array CSR, CSC, COO Example Covariance matrices Graph adjacency <p>where nnz = number of non-zero entries.</p>"},{"location":"sims/sparse-dense-matrices/#why-sparsity-matters","title":"Why Sparsity Matters","text":""},{"location":"sims/sparse-dense-matrices/#storage-efficiency","title":"Storage Efficiency","text":"<p>A 10,000 \u00d7 10,000 dense matrix requires 800 MB (100M entries \u00d7 8 bytes). The same matrix with 99% sparsity needs only ~16 MB in sparse format.</p>"},{"location":"sims/sparse-dense-matrices/#computational-speed","title":"Computational Speed","text":"<p>Operations that skip zeros are much faster:</p> <ul> <li>Matrix-vector multiply: O(nnz) vs O(n\u00b2)</li> <li>Solving linear systems: specialized algorithms exist</li> </ul>"},{"location":"sims/sparse-dense-matrices/#real-world-examples","title":"Real-World Examples","text":"<ul> <li>Graph adjacency: Most nodes connect to few others</li> <li>Document-term matrices: Each document uses few of all possible words</li> <li>Finite elements: Local interactions create banded structures</li> </ul>"},{"location":"sims/sparse-dense-matrices/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/sparse-dense-matrices/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Distinguish sparse and dense matrices visually</li> <li>Estimate storage requirements for both types</li> <li>Recognize common sparsity patterns in applications</li> <li>Explain why sparse storage formats save memory</li> </ol>"},{"location":"sims/sparse-dense-matrices/#exploration-activity-5-minutes","title":"Exploration Activity (5 minutes)","text":"<ol> <li>Compare Visually: Note the color distribution in both panels</li> <li>Try Patterns: Select diagonal, banded, and block patterns</li> <li>Increase Size: Slide to larger matrices and observe storage savings</li> <li>Adjust Sparsity: See how storage ratio changes with sparsity level</li> </ol>"},{"location":"sims/sparse-dense-matrices/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>At what sparsity level does sparse storage become advantageous?</li> <li>Why do graph adjacency matrices tend to be sparse?</li> <li>What real-world data would produce a banded matrix?</li> </ul>"},{"location":"sims/sparse-dense-matrices/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Sparse and dense matrices in context</li> <li>SciPy Sparse Matrix Documentation - Python sparse matrix formats</li> </ul>"},{"location":"sims/special-matrices/","title":"Special Matrix Types Gallery","text":"<p>Run the Special Matrix Types MicroSim Fullscreen</p> <p>Edit the Special Matrix Types MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/special-matrices/main.html\" height=\"542px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/special-matrices/#description","title":"Description","text":"<p>Many matrices have special structures that simplify computation or carry geometric meaning. This gallery displays four fundamental matrix types side-by-side, helping students recognize their distinctive patterns.</p> <p>Featured Matrix Types:</p> Type Pattern Key Property Identity 1s on diagonal, 0s elsewhere AI = IA = A Diagonal Non-zeros only on diagonal Easy powers: D^k has d_i^k Upper Triangular Zeros below diagonal Back substitution Lower Triangular Zeros above diagonal Forward substitution <p>Interactive Features:</p> <ul> <li>Adjustable Size: Change matrix dimensions from 3\u00d73 to 6\u00d76</li> <li>Toggle Zeros: Show or hide zero entries to focus on structure</li> <li>Click to Randomize: Click any matrix card to generate new random values</li> </ul>"},{"location":"sims/special-matrices/#why-these-matrices-matter","title":"Why These Matrices Matter","text":""},{"location":"sims/special-matrices/#identity-matrix-i","title":"Identity Matrix (I)","text":"<p>The multiplicative identity for matrices. Multiplying any matrix by I leaves it unchanged\u2014like multiplying a number by 1.</p>"},{"location":"sims/special-matrices/#diagonal-matrices","title":"Diagonal Matrices","text":"<p>Store information efficiently (only n values for an n\u00d7n matrix). Powers, inverses, and eigenvalues are trivial to compute.</p>"},{"location":"sims/special-matrices/#triangular-matrices","title":"Triangular Matrices","text":"<p>Enable efficient equation solving. LU decomposition factors any matrix into L (lower) and U (upper) triangular components.</p>"},{"location":"sims/special-matrices/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/special-matrices/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify the visual pattern of each special matrix type</li> <li>State the defining property of each type</li> <li>Recognize these patterns when they appear in larger problems</li> </ol>"},{"location":"sims/special-matrices/#quick-recognition-drill-3-minutes","title":"Quick Recognition Drill (3 minutes)","text":"<ol> <li>Display the gallery at different sizes</li> <li>Toggle zeros off and ask students to identify each type by structure alone</li> <li>Click to randomize and verify the pattern holds regardless of specific values</li> </ol>"},{"location":"sims/special-matrices/#discussion-points","title":"Discussion Points","text":"<ul> <li>Why is the identity matrix always the same regardless of random values?</li> <li>How does triangular structure help in solving equations?</li> <li>What's the relationship between diagonal and identity matrices?</li> </ul>"},{"location":"sims/special-matrices/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Special matrix types in context</li> <li>MIT OCW 18.06: Linear Algebra - Gilbert Strang's course</li> </ul>"},{"location":"sims/symmetric-matrix/","title":"Symmetric Matrix","text":"<p>Run the Symmetric Matrix MicroSim Fullscreen</p> <p>Edit the Symmetric Matrix MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/symmetric-matrix/main.html\" height=\"520px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/symmetric-matrix/#description","title":"Description","text":"<p>This MicroSim provides an interactive visualization of symmetric matrices, a fundamental concept in linear algebra. A matrix is symmetric when it equals its own transpose, meaning the element at position (i,j) equals the element at position (j,i) for all indices.</p> <p>Key Features:</p> <ul> <li>Visual Symmetry: Color coding highlights the relationship between the upper triangle (blue), lower triangle (green), and diagonal elements (tan)</li> <li>Dynamic Size: Adjust the matrix dimensions from 2\u00d72 to 10\u00d710 using the slider</li> <li>Random Generation: The Regenerate button creates new random symmetric matrices with values 0-9</li> <li>Index Labels: Row and column indices help identify element positions</li> </ul>"},{"location":"sims/symmetric-matrix/#symmetry-property","title":"Symmetry Property","text":"<p>A matrix \\(A\\) is symmetric if and only if:</p> \\[A = A^T\\] <p>Which means for all valid indices \\(i\\) and \\(j\\):</p> \\[a_{ij} = a_{ji}\\]"},{"location":"sims/symmetric-matrix/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/symmetric-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify whether a given matrix is symmetric by visual inspection</li> <li>Understand that symmetric matrices are equal to their transpose</li> <li>Recognize that only square matrices can be symmetric</li> <li>Observe that the diagonal elements have no symmetry constraint</li> </ol>"},{"location":"sims/symmetric-matrix/#warm-up-activity-2-minutes","title":"Warm-up Activity (2 minutes)","text":"<p>Ask students: \"If you fold a square matrix along its main diagonal, which elements would overlap?\" Let them discover that \\(a_{ij}\\) overlaps with \\(a_{ji}\\).</p>"},{"location":"sims/symmetric-matrix/#guided-exploration-5-minutes","title":"Guided Exploration (5 minutes)","text":"<ol> <li>Start with a small 3\u00d73 matrix</li> <li>Point out that blue cells in the upper triangle have matching green cells in the lower triangle</li> <li>Click \"Regenerate\" several times to see that the symmetry property always holds</li> <li>Increase the size to 6\u00d76 and observe the same pattern</li> </ol>"},{"location":"sims/symmetric-matrix/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Transpose Relationship: \\(A^T\\) is obtained by swapping rows and columns, so \\(A = A^T\\) means this swap leaves the matrix unchanged</li> <li>Degrees of Freedom: An \\(n\u00d7n\\) symmetric matrix has only \\(\\frac{n(n+1)}{2}\\) independent values (upper triangle + diagonal)</li> <li>Applications: Covariance matrices, distance matrices, and adjacency matrices of undirected graphs are always symmetric</li> </ul>"},{"location":"sims/symmetric-matrix/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>How many unique values determine a 5\u00d75 symmetric matrix?</li> <li>If \\(a_{23} = 7\\) in a symmetric matrix, what is \\(a_{32}\\)?</li> <li>Can a 3\u00d74 matrix be symmetric? Why or why not?</li> </ol>"},{"location":"sims/symmetric-matrix/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix properties</li> <li>Special Matrices - Other important matrix types</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/","title":"2D and 3D Vector Visualization","text":"<p>Run the 2D and 3D Vector Visualization Fullscreen</p> <p>You can embed this MicroSim in your own webpage using the following iframe code:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-2d-3d-visualizer/main.html\"\n        height=\"562px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-2d-3d-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization helps students understand how vector components determine position and direction in both 2D and 3D coordinate systems. Students can manipulate the x, y, and z components using sliders and observe how the vector changes in real-time.</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p>"},{"location":"sims/vector-2d-3d-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Components: Use the X, Y, and Z sliders to change the vector components (range: -5 to 5)</li> <li>Switch Views: Click \"Switch to 3D\" to toggle between 2D and 3D visualization modes</li> <li>Toggle Projections: Enable/disable dashed projection lines that show how the vector projects onto each axis</li> <li>Toggle Labels: Show or hide component labels and axis labels</li> <li>Rotate 3D View: In 3D mode, click and drag on the canvas to rotate the view</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Components: How x, y (and z in 3D) values determine the vector endpoint</li> <li>Vector Magnitude: The length of the vector, calculated as \\(\\|v\\| = \\sqrt{x^2 + y^2}\\) in 2D or \\(\\|v\\| = \\sqrt{x^2 + y^2 + z^2}\\) in 3D</li> <li>Coordinate Axes: The standard basis vectors along x, y, and z directions</li> <li>Projection: How a vector projects onto coordinate planes and axes</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-2d-3d-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-2d-3d-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-2d-3d-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of Cartesian coordinate systems</li> <li>Basic knowledge of what vectors represent (magnitude and direction)</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Have students explore the 2D view first, adjusting x and y sliders to see how vectors change</li> <li>Pattern Recognition (5 min): Ask students to find vectors with the same magnitude but different directions</li> <li>3D Extension (5 min): Switch to 3D view and explore how the z-component adds a third dimension</li> <li>Projection Analysis (5 min): Enable projections and discuss how vectors decompose into components</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What happens to the magnitude when you double all components?</li> <li>Can two different vectors have the same magnitude? Give examples.</li> <li>How do the projection lines help you understand vector components?</li> <li>What is the geometric interpretation when x=0 or y=0?</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Ask students to create a vector with a specific magnitude</li> <li>Have students predict the direction before adjusting sliders</li> <li>Quiz on calculating magnitudes from given components</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Vectors, what even are they? - Excellent visual introduction to vectors</li> <li>Khan Academy - Introduction to Vectors</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.</li> </ol>"},{"location":"sims/vector-operations-playground/","title":"Vector Operations Playground","text":"<p>Run the Vector Operations Playground Fullscreen</p>"},{"location":"sims/vector-operations-playground/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive playground allows students to explore vector operations by directly manipulating vectors and observing the results in real-time. Students can perform vector addition, subtraction, and scalar multiplication while seeing both the geometric and numerical representations.</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p>"},{"location":"sims/vector-operations-playground/#how-to-use","title":"How to Use","text":"<ol> <li>Drag Vectors: Click and drag the circular endpoints of vectors u (blue) and v (red) to position them anywhere on the grid</li> <li>Select Operation: Use the radio buttons to choose between:</li> <li>Add: Shows u + v (green result vector)</li> <li>Subtract: Shows u - v (green result vector)</li> <li>Scalar \u00d7: Shows c\u00b7u where c is controlled by the slider</li> <li>Adjust Scalar: When in scalar multiplication mode, use the slider to change the scalar value from -3 to 3</li> <li>Toggle Visualizations:</li> <li>Parallelogram: Shows the parallelogram construction for addition</li> <li>Components: Shows projection lines to the axes</li> <li>Animate: Click to see a step-by-step animation of the operation</li> <li>Reset: Return all vectors to their default positions</li> </ol>"},{"location":"sims/vector-operations-playground/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Addition: The parallelogram rule and tip-to-tail method</li> <li>Vector Subtraction: Finding the difference vector u - v</li> <li>Scalar Multiplication: How scalars stretch, shrink, or reverse vectors</li> <li>Component Operations: How operations work on individual components</li> </ul>"},{"location":"sims/vector-operations-playground/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Addition: \\(\\mathbf{u} + \\mathbf{v} = (u_x + v_x, u_y + v_y)\\)</p> <p>Subtraction: \\(\\mathbf{u} - \\mathbf{v} = (u_x - v_x, u_y - v_y)\\)</p> <p>Scalar Multiplication: \\(c\\mathbf{u} = (c \\cdot u_x, c \\cdot u_y)\\)</p>"},{"location":"sims/vector-operations-playground/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-operations-playground/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-operations-playground/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-operations-playground/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-operations-playground/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/vector-operations-playground/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of 2D coordinate systems</li> <li>Basic knowledge of vectors as arrows with magnitude and direction</li> </ul>"},{"location":"sims/vector-operations-playground/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Let students freely drag vectors and observe how operations change</li> <li>Addition Investigation (5 min):</li> <li>Enable parallelogram view</li> <li>Ask students to verify the parallelogram rule geometrically</li> <li>Have them predict the sum before moving vectors</li> <li>Subtraction Investigation (5 min):</li> <li>Switch to subtraction mode</li> <li>Explore how u - v relates to the vector from v's tip to u's tip</li> <li>Scalar Multiplication (5 min):</li> <li>Vary the scalar from -3 to 3</li> <li>Observe what happens at c = 0, c = 1, c = -1</li> <li>Synthesis (5 min): Combine concepts to solve problems</li> </ol>"},{"location":"sims/vector-operations-playground/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What is the geometric meaning of the parallelogram in vector addition?</li> <li>If u + v = w, what is u - v geometrically?</li> <li>What happens to the direction of a vector when multiplied by a negative scalar?</li> <li>Can two different pairs of vectors have the same sum?</li> </ol>"},{"location":"sims/vector-operations-playground/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, find u and v that add to reach it</li> <li>Predict the result of an operation before seeing it</li> <li>Find a scalar that makes cu equal to a specific vector</li> </ul>"},{"location":"sims/vector-operations-playground/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors</li> <li>Khan Academy - Vector Addition</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press. Chapter 1.</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/","title":"Vector Space Axiom Explorer","text":"<p>Run the Vector Space Axiom Explorer Fullscreen</p> <p>Edit the Vector Space Axiom Explorer Using the p5.js Editor</p>"},{"location":"sims/vector-space-axiom-explorer/#about-this-infographic","title":"About This Infographic","text":"<p>This interactive infographic helps students learn and remember the ten vector space axioms. The axioms are organized into two groups: five for vector addition and five for scalar multiplication. Click on each axiom card to see its definition and a concrete example.</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p>"},{"location":"sims/vector-space-axiom-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over the central hub to see what a vector space is and examples of vector spaces and fields</li> <li>Click on axiom cards to expand and see:</li> <li>Full definition of the axiom</li> <li>A concrete numerical example in R\u00b2</li> <li>Track your progress with the counter at the bottom showing how many axioms you've viewed</li> <li>A checkmark appears on viewed axioms</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#the-ten-vector-space-axioms","title":"The Ten Vector Space Axioms","text":"<p>For a set V to be a vector space over a field F, it must satisfy:</p>"},{"location":"sims/vector-space-axiom-explorer/#addition-axioms-1-5","title":"Addition Axioms (1-5)","text":"<ol> <li>Closure: u + v \u2208 V</li> <li>Commutativity: u + v = v + u</li> <li>Associativity: (u + v) + w = u + (v + w)</li> <li>Identity: v + 0 = v</li> <li>Inverse: v + (-v) = 0</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#scalar-multiplication-axioms-6-10","title":"Scalar Multiplication Axioms (6-10)","text":"<ol> <li>Closure: cv \u2208 V</li> <li>Distributivity (vectors): c(u + v) = cu + cv</li> <li>Distributivity (scalars): (c + d)v = cv + dv</li> <li>Associativity: c(dv) = (cd)v</li> <li>Identity: 1\u00b7v = v</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#why-these-axioms-matter","title":"Why These Axioms Matter","text":"<p>These ten axioms are the foundation of linear algebra. Any set that satisfies all ten axioms is a vector space, which means all theorems about vector spaces apply to it. This includes:</p> <ul> <li>R^n (n-dimensional real space)</li> <li>Polynomials of degree \u2264 n</li> <li>Matrices of a given size</li> <li>Continuous functions on an interval</li> <li>Solutions to homogeneous differential equations</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#embedding-this-infographic","title":"Embedding This Infographic","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-space-axiom-explorer/main.html\"\n        height=\"750px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-space-axiom-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-space-axiom-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/vector-space-axiom-explorer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-space-axiom-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication concepts</li> <li>Familiarity with R\u00b2 as an example vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Introduction (3 min):</li> <li>Hover over the central hub to understand what a vector space is</li> <li> <p>Note the two types of operations: addition and scalar multiplication</p> </li> <li> <p>Addition Axioms (5 min):</p> </li> <li>Click through all five addition axioms</li> <li>Work through each example mentally or on paper</li> <li> <p>Note how they formalize intuitive properties</p> </li> <li> <p>Scalar Multiplication Axioms (5 min):</p> </li> <li>Click through all five scalar multiplication axioms</li> <li>Compare distributivity axioms 7 and 8</li> <li> <p>Understand why the identity axiom uses 1</p> </li> <li> <p>Verification Exercise (5 min):</p> </li> <li>Given a candidate vector space (e.g., 2\u00d72 matrices)</li> <li>Check that all ten axioms hold</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we need all ten axioms? What would go wrong if one was missing?</li> <li>Can you think of a set with addition that violates one of these axioms?</li> <li>Why is the additive identity (zero vector) unique?</li> <li>What's the difference between the two distributivity axioms?</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>List all ten axioms from memory</li> <li>Given a set and operations, verify which axioms hold</li> <li>Explain why a given set is NOT a vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Abstract vector spaces</li> <li>Khan Academy - Vector Spaces</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.1.</li> <li>Axler, S. (2015). Linear Algebra Done Right (3rd ed.). Chapter 1.</li> </ol>"}]}