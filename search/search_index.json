{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linear Algebra","text":"<p>Course Description</p> <p>In Linear Algebra, you won\u2019t just learn matrices and vectors\u2014you'll experience them in action. This course empowers undergraduate students in Computer Science and Artificial Intelligence to develop a deep, functional understanding of linear algebra\u2019s essential role in modern technology.</p>"},{"location":"about/","title":"About This Course","text":""},{"location":"about/#why-linear-algebra-matters-now-more-than-ever","title":"Why Linear Algebra Matters Now More Than Ever","text":"<p>Welcome to one of the most exciting and relevant mathematics courses you will ever take! Linear algebra is not just another math requirement to check off your list\u2014it is the mathematical language that powers the modern world of artificial intelligence, machine learning, computer graphics, data science, robotics, and so much more.</p> <p>Every time you interact with a large language model like ChatGPT or Claude, linear algebra is working behind the scenes. When your phone recognizes your face, when Netflix recommends your next favorite show, when autonomous vehicles navigate city streets, and when medical imaging systems detect diseases\u2014linear algebra makes it all possible.</p>"},{"location":"about/#the-foundation-of-modern-ai","title":"The Foundation of Modern AI","text":"<p>At the heart of every neural network lies matrix multiplication. The attention mechanisms that make transformers so powerful? Matrix operations. The embeddings that capture the meaning of words and images? Vectors in high-dimensional spaces. Principal component analysis for dimensionality reduction? Eigenvalue decomposition. The gradient descent algorithms that train every deep learning model? Linear algebra from start to finish.</p> <p>Understanding linear algebra gives you the superpower to truly comprehend how AI systems work\u2014not as mysterious black boxes, but as elegant mathematical structures you can reason about, debug, and improve.</p>"},{"location":"about/#learning-through-interactive-visualization","title":"Learning Through Interactive Visualization","text":"<p>This course takes a fundamentally different approach to teaching linear algebra. Instead of drowning in abstract notation, you will build deep intuition through over 150 interactive MicroSimulations. These browser-based visualizations let you experiment with vectors, matrices, transformations, and decompositions in real-time.</p> <p>Watch eigenvectors remain unchanged as transformations stretch space around them. See how SVD decomposes images into fundamental components. Explore how gradient descent navigates loss landscapes. Manipulate quaternions and witness how they elegantly solve the gimbal lock problem. These are not passive animations\u2014they are hands-on laboratories where you control the parameters and discover the concepts yourself.</p>"},{"location":"about/#you-will-have-fun","title":"You Will Have Fun","text":"<p>Yes, you read that right. This course is designed to be genuinely enjoyable. The MicroSims turn abstract concepts into interactive playgrounds. The connections to real-world AI applications give every topic immediate relevance. And the satisfaction of truly understanding how modern technology works is deeply rewarding.</p> <p>Whether you aspire to build the next breakthrough in AI, create stunning computer graphics, develop autonomous systems, or simply want to understand the mathematical foundations of our increasingly algorithmic world\u2014this course will give you the tools, the intuition, and the confidence to succeed.</p> <p>Let's explore the beautiful mathematics that shapes our digital future!</p>"},{"location":"about/#background","title":"Background","text":"<p>In January of 2025, I was invited to lead a group of four students in a senior design project. These were all students in their senior year at the University of Minnesota Department of Electrical Engineering and Computer Design. Each student team member was assigned to create an intelligent textbook for either a class they took or a subject they were interested in. One of these students selected Linear Algebra. This course is a fork of that textbook, but it has been redesigned significantly to focus on the application of linear algebra in machine learning.</p> <p>Our tools have also matured since January 2025. This version of our intelligent textbook was generated using Claude Code Skills. We also put a much stronger focus on creating high-quality MicroSims that bring abstract concepts to life.</p> <p>\u2014 Dan McCreary, January 2025</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"course-description/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"course-description/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"course-description/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"course-description/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"course-description/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"course-description/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"course-description/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"course-description/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"course-description/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"course-description/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"course-description/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"course-description/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"course-description/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"course-description/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"course-description/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"course-description/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"course-description/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"course-description/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"course-description/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"course-description/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"course-description/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"course-description/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"course-description/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"course-description/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"course-description/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"course-description/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"course-description/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"course-description/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"course-description/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"course-description/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"course-description/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"faq/","title":"Applied Linear Algebra for AI and Machine Learning FAQ","text":"<p>This FAQ addresses common questions about the course content, concepts, and applications. Questions are organized by category to help you find answers quickly.</p>"},{"location":"faq/#getting-started-questions","title":"Getting Started Questions","text":""},{"location":"faq/#what-is-this-course-about","title":"What is this course about?","text":"<p>This course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. You'll develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life. The course covers vectors, matrices, linear transformations, eigenvalues, matrix decompositions, and their applications in neural networks, generative AI, image processing, and autonomous systems.</p>"},{"location":"faq/#who-is-this-course-designed-for","title":"Who is this course designed for?","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul> <p>The material is presented at a college undergraduate level, making it accessible to anyone with the prerequisites who wants to understand the mathematics behind modern AI systems.</p>"},{"location":"faq/#what-are-the-prerequisites-for-this-course","title":"What are the prerequisites for this course?","text":"<p>To succeed in this course, you should have:</p> <ul> <li>College Algebra or equivalent: Familiarity with functions, equations, and basic mathematical notation</li> <li>Basic programming experience: Python is recommended but not required</li> <li>Familiarity with calculus concepts: Understanding of derivatives and integrals at a conceptual level</li> </ul> <p>You don't need prior exposure to linear algebra\u2014this course starts from the fundamentals and builds up systematically.</p>"},{"location":"faq/#how-is-the-course-structured","title":"How is the course structured?","text":"<p>The course is divided into four major parts spanning 15 chapters:</p> <ol> <li>Part 1: Foundations of Linear Algebra (Chapters 1-4): Vectors, matrices, systems of equations, and linear transformations</li> <li>Part 2: Advanced Matrix Theory (Chapters 5-8): Determinants, eigenvalues, matrix decompositions, and abstract vector spaces</li> <li>Part 3: Linear Algebra in Machine Learning (Chapters 9-12): ML foundations, neural networks, generative AI, and optimization</li> <li>Part 4: Computer Vision and Autonomous Systems (Chapters 13-15): Image processing, 3D geometry, and sensor fusion</li> </ol> <p>Each chapter includes interactive MicroSims to reinforce concepts through hands-on exploration.</p>"},{"location":"faq/#how-do-i-use-the-interactive-microsims","title":"How do I use the interactive MicroSims?","text":"<p>MicroSims are browser-based interactive simulations that let you visualize and experiment with linear algebra concepts. To use them:</p> <ol> <li>Navigate to the MicroSims section in the sidebar</li> <li>Select a simulation relevant to what you're studying</li> <li>Use the sliders, buttons, and controls to adjust parameters</li> <li>Observe how changes affect the visualization in real-time</li> <li>Connect the visual behavior to the mathematical concepts you're learning</li> </ol> <p>No software installation is required\u2014all MicroSims run directly in your web browser.</p>"},{"location":"faq/#why-is-linear-algebra-important-for-ai-and-machine-learning","title":"Why is linear algebra important for AI and machine learning?","text":"<p>Linear algebra is the mathematical language in which modern AI systems are written. Understanding it enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically inside them</li> <li>Optimize performance by choosing efficient matrix operations and representations</li> <li>Innovate by seeing new ways to apply linear algebra concepts to novel problems</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundational concepts</li> </ul> <p>From the matrix multiplications in neural networks to the transformations in computer vision, virtually every AI algorithm relies heavily on linear algebra operations.</p>"},{"location":"faq/#how-long-does-it-take-to-complete-each-chapter","title":"How long does it take to complete each chapter?","text":"<p>Each chapter is designed for approximately one week of study, including:</p> <ul> <li>Reading the chapter content (2-3 hours)</li> <li>Working through examples and exercises (2-3 hours)</li> <li>Exploring interactive MicroSims (1-2 hours)</li> <li>Completing practice problems (2-3 hours)</li> </ul> <p>The entire course spans 15 weeks at this pace, though self-study learners can adjust their schedule as needed.</p>"},{"location":"faq/#what-software-do-i-need","title":"What software do I need?","text":"<p>For reading the textbook and using MicroSims, you only need a modern web browser. For hands-on programming exercises, you'll benefit from:</p> <ul> <li>Python 3.x with the following libraries:</li> <li>NumPy: For numerical computations and array operations</li> <li>Matplotlib: For creating visualizations</li> <li>scikit-learn: For machine learning examples</li> <li>Optional: GPU access for deep learning exercises in later chapters</li> </ul> <p>All code examples in the textbook use Python with NumPy.</p>"},{"location":"faq/#how-can-i-check-my-understanding-of-the-material","title":"How can I check my understanding of the material?","text":"<p>Each chapter provides multiple ways to assess your understanding:</p> <ul> <li>Concept check questions embedded throughout the text</li> <li>Interactive MicroSims where you can test your predictions</li> <li>Practice problems with varying difficulty levels</li> <li>The glossary for reviewing terminology</li> <li>Quiz questions for self-assessment</li> </ul> <p>Working through these resources actively, rather than passively reading, is the key to building deep understanding.</p>"},{"location":"faq/#where-can-i-get-help-if-im-stuck","title":"Where can I get help if I'm stuck?","text":"<p>If you're struggling with a concept:</p> <ol> <li>Review the relevant glossary definitions for terminology clarity</li> <li>Use the MicroSims to build geometric intuition</li> <li>Re-read prerequisite material if foundational concepts are unclear</li> <li>Check the learning graph to ensure you've covered prerequisite concepts</li> <li>For textbook issues, report problems on the GitHub Issues page</li> </ol>"},{"location":"faq/#core-concept-questions","title":"Core Concept Questions","text":""},{"location":"faq/#what-is-the-difference-between-a-scalar-and-a-vector","title":"What is the difference between a scalar and a vector?","text":"<p>A scalar is a single numerical value representing magnitude only (like temperature or mass). A vector is an ordered collection of scalars that represents both magnitude and direction. While the scalar 5 tells you \"how much,\" the vector (3, 4) tells you \"how much and in which direction.\"</p> <p>Example: Speed of 60 mph is a scalar; velocity of 60 mph heading northeast is represented as a vector with components in the x and y directions.</p> <p>See also: Chapter 1: Vectors and Vector Spaces</p>"},{"location":"faq/#what-is-a-matrix-and-how-does-it-relate-to-vectors","title":"What is a matrix and how does it relate to vectors?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. You can think of a matrix as:</p> <ul> <li>A collection of column vectors side by side</li> <li>A collection of row vectors stacked vertically</li> <li>A representation of a linear transformation</li> </ul> <p>A matrix with dimensions m\u00d7n has m rows and n columns. When you multiply a matrix by a vector, you're applying a linear transformation that maps the input vector to an output vector.</p> <p>Example: A 3\u00d72 matrix contains 3 rows and 2 columns, and can be viewed as 2 column vectors in 3-dimensional space.</p> <p>See also: Chapter 2: Matrices and Matrix Operations</p>"},{"location":"faq/#what-does-it-mean-for-vectors-to-be-linearly-independent","title":"What does it mean for vectors to be linearly independent?","text":"<p>Vectors are linearly independent if no vector in the set can be written as a linear combination of the others. Equivalently, the only way to combine them to get the zero vector is with all zero coefficients.</p> <p>Example: The vectors (1, 0) and (0, 1) are linearly independent because neither is a multiple of the other. However, (1, 2) and (2, 4) are linearly dependent because (2, 4) = 2\u00b7(1, 2).</p> <p>Linear independence is crucial because independent vectors provide \"new directions\" in space, while dependent vectors are redundant.</p>"},{"location":"faq/#what-is-a-basis-and-why-is-it-important","title":"What is a basis and why is it important?","text":"<p>A basis is a set of linearly independent vectors that span an entire vector space. Every vector in the space can be written as a unique linear combination of basis vectors. The number of vectors in a basis equals the dimension of the space.</p> <p>The basis is important because it provides a coordinate system for the vector space. The standard basis in 3D consists of the unit vectors along each axis: (1,0,0), (0,1,0), and (0,0,1).</p> <p>Example: Any point in 3D space can be written as x(1,0,0) + y(0,1,0) + z(0,0,1) where (x, y, z) are the coordinates.</p>"},{"location":"faq/#what-is-the-dot-product-and-what-does-it-tell-us","title":"What is the dot product and what does it tell us?","text":"<p>The dot product (also called inner product) of two vectors produces a scalar value computed as the sum of products of corresponding components:</p> \\[\\mathbf{a} \\cdot \\mathbf{b} = a_1b_1 + a_2b_2 + \\ldots + a_nb_n\\] <p>Geometrically, the dot product equals |a||b|cos(\u03b8) where \u03b8 is the angle between the vectors. This tells us:</p> <ul> <li>If the dot product is positive, vectors point in similar directions</li> <li>If the dot product is zero, vectors are perpendicular (orthogonal)</li> <li>If the dot product is negative, vectors point in opposite directions</li> </ul> <p>Example: The dot product of (1, 2) and (3, 4) is 1\u00d73 + 2\u00d74 = 11.</p>"},{"location":"faq/#what-is-a-linear-transformation","title":"What is a linear transformation?","text":"<p>A linear transformation is a function between vector spaces that preserves vector addition and scalar multiplication. If T is a linear transformation, then:</p> <ul> <li>T(u + v) = T(u) + T(v) for all vectors u and v</li> <li>T(cv) = cT(v) for all vectors v and scalars c</li> </ul> <p>Every linear transformation can be represented by a matrix. Common examples include rotations, reflections, scaling, shearing, and projections.</p> <p>Example: Rotating a 2D vector by 45\u00b0 is a linear transformation represented by the rotation matrix [[cos(45\u00b0), -sin(45\u00b0)], [sin(45\u00b0), cos(45\u00b0)]].</p> <p>See also: Chapter 4: Linear Transformations</p>"},{"location":"faq/#what-is-the-determinant-and-what-does-it-tell-us","title":"What is the determinant and what does it tell us?","text":"<p>The determinant is a scalar value computed from a square matrix that tells us:</p> <ol> <li>Whether the matrix is invertible (nonzero determinant = invertible)</li> <li>The volume scaling factor of the associated transformation</li> <li>The orientation change (negative determinant = orientation flip)</li> </ol> <p>For a 2\u00d72 matrix [[a, b], [c, d]], the determinant is ad - bc.</p> <p>Example: A rotation matrix always has determinant 1 (preserves area and orientation). A reflection matrix has determinant -1 (preserves area but flips orientation).</p> <p>See also: Chapter 5: Determinants and Matrix Properties</p>"},{"location":"faq/#what-are-eigenvalues-and-eigenvectors","title":"What are eigenvalues and eigenvectors?","text":"<p>An eigenvector of a matrix A is a nonzero vector v that, when transformed by A, points in the same direction (or exactly opposite)\u2014it only gets scaled by a factor \u03bb called the eigenvalue:</p> \\[A\\mathbf{v} = \\lambda\\mathbf{v}\\] <p>Eigenvectors reveal the \"natural directions\" of a transformation where the transformation acts as simple scaling rather than rotation or shearing.</p> <p>Example: For a horizontal stretch matrix that doubles the x-coordinate, any vector along the x-axis is an eigenvector with eigenvalue 2, and any vector along the y-axis is an eigenvector with eigenvalue 1.</p> <p>See also: Chapter 6: Eigenvalues and Eigenvectors</p>"},{"location":"faq/#what-is-singular-value-decomposition-svd","title":"What is Singular Value Decomposition (SVD)?","text":"<p>SVD decomposes any matrix A into three matrices: A = U\u03a3V^T where:</p> <ul> <li>U contains left singular vectors (orthonormal columns)</li> <li>\u03a3 is a diagonal matrix of singular values (non-negative, decreasing)</li> <li>V^T contains right singular vectors (orthonormal rows)</li> </ul> <p>SVD reveals the fundamental structure of any matrix and enables:</p> <ul> <li>Low-rank approximation (keeping only largest singular values)</li> <li>Image compression</li> <li>Pseudoinverse computation</li> <li>Dimensionality reduction</li> </ul> <p>Example: Truncating an image's SVD to keep only the 50 largest singular values can reduce storage by 90% while maintaining recognizable quality.</p> <p>See also: Chapter 7: Matrix Decompositions</p>"},{"location":"faq/#what-is-principal-component-analysis-pca","title":"What is Principal Component Analysis (PCA)?","text":"<p>PCA is a technique that finds the directions of maximum variance in data by computing eigenvectors of the covariance matrix. The first principal component points in the direction of greatest variance, the second in the direction of greatest remaining variance (perpendicular to the first), and so on.</p> <p>PCA is used for:</p> <ul> <li>Dimensionality reduction (keeping top k components)</li> <li>Data visualization (projecting high-dimensional data to 2D or 3D)</li> <li>Feature extraction (finding the most informative directions)</li> <li>Noise reduction (removing low-variance components)</li> </ul> <p>Example: Applying PCA to face images produces \"eigenfaces\"\u2014the principal components that capture the most variation in facial appearance.</p> <p>See also: Chapter 9: Machine Learning Foundations</p>"},{"location":"faq/#how-do-neural-networks-use-linear-algebra","title":"How do neural networks use linear algebra?","text":"<p>Neural networks are fundamentally composed of linear algebra operations:</p> <ul> <li>Weight matrices connect layers through matrix-vector multiplication</li> <li>Bias vectors add constant offsets to layer outputs</li> <li>Forward propagation chains matrix multiplications with nonlinear activations</li> <li>Backpropagation uses chain rule with Jacobian matrices to compute gradients</li> <li>Batch processing uses matrix-matrix multiplication for efficiency</li> </ul> <p>Each layer computes: output = activation(W\u00b7input + b) where W is the weight matrix and b is the bias vector.</p> <p>Example: A layer connecting 100 inputs to 50 outputs uses a 50\u00d7100 weight matrix containing 5,000 learnable parameters.</p> <p>See also: Chapter 10: Neural Networks and Deep Learning</p>"},{"location":"faq/#what-is-the-attention-mechanism-in-transformers","title":"What is the attention mechanism in transformers?","text":"<p>The attention mechanism computes weighted combinations of values based on the relevance between queries and keys. Given Query (Q), Key (K), and Value (V) matrices:</p> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\] <p>This allows each position in a sequence to \"attend to\" relevant positions elsewhere. The dot product Q\u00b7K^T measures similarity, softmax normalizes to weights, and the weighted sum of V produces the output.</p> <p>Multi-head attention runs multiple attention operations in parallel, allowing the model to attend to different types of relationships simultaneously.</p> <p>See also: Chapter 11: Generative AI and LLMs</p>"},{"location":"faq/#what-is-a-kalman-filter","title":"What is a Kalman filter?","text":"<p>A Kalman filter is an optimal algorithm for estimating the state of a system from noisy measurements. It works in two steps:</p> <ol> <li>Predict: Use a system model to predict the next state</li> <li>Update: Correct the prediction using new measurements</li> </ol> <p>The Kalman gain determines how much to trust the prediction versus the measurement. The filter optimally combines both sources of information based on their uncertainties.</p> <p>Example: A GPS receiver uses Kalman filtering to estimate position by fusing satellite measurements (accurate but slow) with inertial sensors (fast but drifts).</p> <p>See also: Chapter 15: Autonomous Systems and Sensor Fusion</p>"},{"location":"faq/#technical-detail-questions","title":"Technical Detail Questions","text":""},{"location":"faq/#what-is-the-difference-between-l1-l2-and-l-infinity-norms","title":"What is the difference between L1, L2, and L-infinity norms?","text":"<p>These are different ways to measure vector length:</p> Norm Formula Geometric Interpretation L1 (Manhattan) $|v|_1 = \\sum v_i L2 (Euclidean) \\(\\|v\\|_2 = \\sqrt{\\sum v_i^2}\\) Straight-line distance L\u221e (Max) $|v|_\\infty = \\max v_i <p>Example: For vector (3, -4), L1 = 7, L2 = 5, L\u221e = 4.</p>"},{"location":"faq/#what-is-the-difference-between-a-symmetric-and-orthogonal-matrix","title":"What is the difference between a symmetric and orthogonal matrix?","text":"<p>A symmetric matrix equals its own transpose: A = A^T. This means the element in row i, column j equals the element in row j, column i. Symmetric matrices have real eigenvalues and orthogonal eigenvectors.</p> <p>An orthogonal matrix has columns (and rows) that are orthonormal: Q^TQ = QQ^T = I. This means Q^(-1) = Q^T. Orthogonal matrices preserve lengths and angles\u2014rotations and reflections are orthogonal.</p> <p>Example: Covariance matrices are symmetric. Rotation matrices are orthogonal.</p>"},{"location":"faq/#what-is-the-difference-between-rank-and-nullity","title":"What is the difference between rank and nullity?","text":"<p>The rank of a matrix is the dimension of its column space\u2014the number of linearly independent columns. The nullity is the dimension of its null space\u2014the number of independent vectors that map to zero.</p> <p>The Rank-Nullity Theorem states: rank(A) + nullity(A) = number of columns.</p> <p>Example: A 3\u00d75 matrix with rank 3 has nullity 5 - 3 = 2, meaning two free variables exist in the solution to Ax = 0.</p>"},{"location":"faq/#what-is-the-condition-number-and-why-does-it-matter","title":"What is the condition number and why does it matter?","text":"<p>The condition number of a matrix is the ratio of its largest to smallest singular value. It measures how sensitive solutions are to small changes in input:</p> <ul> <li>Condition number \u2248 1: Well-conditioned (stable)</li> <li>Condition number &gt; 10^10: Ill-conditioned (numerically unstable)</li> </ul> <p>An ill-conditioned matrix amplifies rounding errors, potentially making computed solutions unreliable.</p> <p>Example: A matrix with condition number 10^6 can amplify input errors by up to a million times in the output.</p>"},{"location":"faq/#what-is-the-difference-between-row-echelon-form-and-reduced-row-echelon-form","title":"What is the difference between row echelon form and reduced row echelon form?","text":"<p>Row echelon form (REF): - Leading entries (pivots) are 1 - Each pivot is to the right of the pivot above - Rows of all zeros are at the bottom - Entries below each pivot are zero</p> <p>Reduced row echelon form (RREF) adds: - Each pivot is the only nonzero entry in its column</p> <p>RREF makes reading solutions easier but requires more computation to achieve.</p>"},{"location":"faq/#what-is-the-difference-between-qr-and-lu-decomposition","title":"What is the difference between QR and LU decomposition?","text":"Feature LU Decomposition QR Decomposition Form A = LU (Lower \u00d7 Upper triangular) A = QR (Orthogonal \u00d7 Upper triangular) Matrix type Square, some need pivoting Any matrix Stability May need partial pivoting Inherently stable Primary use Solving linear systems Least squares, eigenvalue algorithms Computation Generally faster More stable for ill-conditioned problems"},{"location":"faq/#what-is-the-pseudoinverse","title":"What is the pseudoinverse?","text":"<p>The pseudoinverse A^+ generalizes matrix inversion to non-square and singular matrices. It's computed from SVD as:</p> \\[A^+ = V\\Sigma^+U^T\\] <p>where \u03a3^+ is formed by taking reciprocals of nonzero singular values.</p> <p>The pseudoinverse solves least squares problems: x = A^+b minimizes ||Ax - b||.</p>"},{"location":"faq/#what-is-the-difference-between-gradient-descent-and-newtons-method","title":"What is the difference between gradient descent and Newton's method?","text":"Feature Gradient Descent Newton's Method Uses First derivatives (gradient) First and second derivatives (Hessian) Step direction Steepest descent Newton step using curvature Convergence Linear (slow near minimum) Quadratic (fast near minimum) Per-iteration cost Low (gradient only) High (Hessian inversion) Robustness Works far from minimum May diverge far from minimum <p>For large-scale problems, quasi-Newton methods like BFGS approximate the Hessian without computing it explicitly.</p>"},{"location":"faq/#what-is-the-difference-between-convolution-and-correlation-in-image-processing","title":"What is the difference between convolution and correlation in image processing?","text":"<p>Convolution flips the kernel before sliding it across the image. Correlation does not flip the kernel. For symmetric kernels, they're identical.</p> <p>Mathematically, convolution is associative (order doesn't matter for multiple filters), which is important for neural network design.</p> <p>In practice, most deep learning frameworks implement correlation but call it \"convolution.\"</p>"},{"location":"faq/#what-are-homogeneous-coordinates","title":"What are homogeneous coordinates?","text":"<p>Homogeneous coordinates add an extra dimension to represent points. A 2D point (x, y) becomes (x, y, 1) in homogeneous coordinates. This enables:</p> <ul> <li>Representing translations as matrix multiplication</li> <li>Unified treatment of affine and projective transformations</li> <li>Representing points at infinity</li> <li>Simplifying perspective projection</li> </ul> <p>To convert back: (x, y, w) \u2192 (x/w, y/w)</p> <p>Example: Translation, which is not a linear transformation in Cartesian coordinates, becomes a matrix multiplication in homogeneous coordinates.</p>"},{"location":"faq/#common-challenge-questions","title":"Common Challenge Questions","text":""},{"location":"faq/#why-do-i-get-different-results-when-multiplying-matrices-in-different-orders","title":"Why do I get different results when multiplying matrices in different orders?","text":"<p>Matrix multiplication is not commutative: AB \u2260 BA in general. The order matters because:</p> <ul> <li>A applies to the result of B, not the other way around</li> <li>Dimensions may not even allow reverse multiplication</li> <li>Geometrically, applying transformation A then B differs from B then A</li> </ul> <p>Example: Rotating then scaling gives a different result than scaling then rotating.</p>"},{"location":"faq/#how-do-i-know-if-a-system-of-equations-has-a-unique-solution-no-solution-or-infinitely-many-solutions","title":"How do I know if a system of equations has a unique solution, no solution, or infinitely many solutions?","text":"<p>Analyze the augmented matrix after row reduction:</p> Condition Solution Type Pivot in every column (of coefficient matrix) Unique solution Pivot in last column (constant column) No solution Fewer pivots than variables Infinitely many solutions <p>The rank of the coefficient matrix compared to the augmented matrix determines solvability.</p>"},{"location":"faq/#why-does-my-matrix-inversion-give-numerical-errors","title":"Why does my matrix inversion give numerical errors?","text":"<p>Numerical errors in matrix inversion occur when:</p> <ol> <li>Matrix is singular or near-singular: Small pivots cause division by tiny numbers</li> <li>Poor conditioning: Large condition number amplifies rounding errors</li> <li>Accumulated errors: Long computation chains compound small errors</li> </ol> <p>Solutions: - Use LU or QR decomposition instead of explicit inversion - Apply partial pivoting - Use higher precision arithmetic for critical applications - Reformulate the problem to avoid explicit inversion</p>"},{"location":"faq/#how-do-i-handle-non-square-matrices","title":"How do I handle non-square matrices?","text":"<p>Non-square matrices can't be inverted directly, but you can:</p> <ul> <li>For m\u00d7n with m &gt; n (overdetermined): Use pseudoinverse or least squares</li> <li>For m\u00d7n with m &lt; n (underdetermined): Solution has free variables; use minimum norm solution</li> <li>For any case: SVD works on all matrices and provides the pseudoinverse</li> </ul>"},{"location":"faq/#why-do-eigenvalue-computations-sometimes-give-complex-numbers","title":"Why do eigenvalue computations sometimes give complex numbers?","text":"<p>Complex eigenvalues occur when a real matrix includes rotational components. For example, a pure rotation matrix in 2D has eigenvalues cos(\u03b8) \u00b1 i\u00b7sin(\u03b8).</p> <p>Complex eigenvalues always come in conjugate pairs for real matrices. They indicate oscillatory behavior in dynamical systems.</p>"},{"location":"faq/#how-do-i-choose-the-right-matrix-decomposition","title":"How do I choose the right matrix decomposition?","text":"Problem Best Decomposition Solve Ax = b (general) LU with pivoting Solve Ax = b (symmetric positive definite) Cholesky Least squares QR Eigenvalues/vectors Use specialized eigenvalue algorithms Dimensionality reduction SVD or eigendecomposition Low-rank approximation Truncated SVD Numerical stability critical QR or SVD"},{"location":"faq/#why-is-gradient-descent-slow-for-some-problems","title":"Why is gradient descent slow for some problems?","text":"<p>Gradient descent can be slow when:</p> <ol> <li>Ill-conditioning: Different dimensions have very different scales</li> <li>Saddle points: Gradient is small but not at a minimum</li> <li>Plateaus: Loss surface is nearly flat</li> <li>Learning rate issues: Too small = slow; too large = oscillation</li> </ol> <p>Solutions: Use adaptive methods (Adam, RMSprop), apply preconditioning, or normalize features.</p>"},{"location":"faq/#how-do-i-debug-dimension-mismatch-errors-in-neural-networks","title":"How do I debug dimension mismatch errors in neural networks?","text":"<p>Common dimension mismatch causes:</p> <ol> <li>Matrix multiplication: Inner dimensions must match (m\u00d7n times n\u00d7p)</li> <li>Batch dimension confusion: First dimension is usually batch size</li> <li>Flattening errors: Wrong reshape before fully connected layers</li> <li>Convolution output: Calculate output size using (input - kernel + 2\u00d7padding)/stride + 1</li> </ol> <p>Trace dimensions through each layer systematically to find the mismatch.</p>"},{"location":"faq/#best-practice-questions","title":"Best Practice Questions","text":""},{"location":"faq/#when-should-i-use-sparse-matrix-representations","title":"When should I use sparse matrix representations?","text":"<p>Use sparse matrices when:</p> <ul> <li>More than 90% of entries are zero</li> <li>Matrix is large (thousands of rows/columns)</li> <li>Memory is constrained</li> <li>Operations preserve sparsity</li> </ul> <p>Common sparse formats include CSR (fast row slicing), CSC (fast column slicing), and COO (fast construction).</p> <p>Example: A 10,000\u00d710,000 matrix with only 50,000 nonzero entries uses 99.95% less memory in sparse format.</p>"},{"location":"faq/#how-do-i-choose-the-number-of-principal-components-to-keep","title":"How do I choose the number of principal components to keep?","text":"<p>Common approaches:</p> <ol> <li>Variance threshold: Keep components explaining 95% or 99% of total variance</li> <li>Scree plot: Look for an \"elbow\" where variance explained drops sharply</li> <li>Cross-validation: Choose k that minimizes reconstruction error on held-out data</li> <li>Domain knowledge: Keep components that have interpretable meaning</li> </ol> <p>There's no universal rule\u2014the best choice depends on your specific application.</p>"},{"location":"faq/#what-regularization-strength-should-i-use","title":"What regularization strength should I use?","text":"<p>Finding the right regularization strength (\u03bb) typically requires:</p> <ol> <li>Cross-validation: Try multiple values and evaluate on validation set</li> <li>Grid search: Systematically explore a range (often logarithmic: 0.001, 0.01, 0.1, 1, 10)</li> <li>Domain knowledge: Larger \u03bb when you expect simpler relationships</li> <li>Monitoring: Watch for underfitting (\u03bb too large) or overfitting (\u03bb too small)</li> </ol> <p>Start with \u03bb = 1 and adjust based on validation performance.</p>"},{"location":"faq/#how-should-i-normalize-features-before-applying-linear-algebra-algorithms","title":"How should I normalize features before applying linear algebra algorithms?","text":"<p>Common normalization strategies:</p> Method Formula When to Use Standardization (x - \u03bc) / \u03c3 Features with different scales; PCA Min-Max (x - min) / (max - min) Bounded output needed (0-1) L2 Normalization x / Batch Normalization Layer-wise during training Deep neural networks <p>Always apply the same transformation to training and test data.</p>"},{"location":"faq/#how-do-i-handle-missing-data-in-matrix-computations","title":"How do I handle missing data in matrix computations?","text":"<p>Options for missing data:</p> <ol> <li>Imputation: Fill with mean, median, or predicted values</li> <li>Matrix completion: Use low-rank methods to estimate missing entries</li> <li>Mask and ignore: Weight valid entries only in loss computation</li> <li>Drop rows/columns: If missingness is sparse and random</li> </ol> <p>For recommendation systems, matrix completion methods specifically designed for sparse matrices work well.</p>"},{"location":"faq/#whats-the-best-way-to-implement-matrix-operations-efficiently","title":"What's the best way to implement matrix operations efficiently?","text":"<p>For efficient matrix operations:</p> <ol> <li>Use optimized libraries: NumPy, BLAS, LAPACK, cuBLAS for GPU</li> <li>Avoid explicit loops: Vectorize operations</li> <li>Consider memory layout: Row-major vs column-major affects cache performance</li> <li>Batch operations: Process multiple inputs simultaneously</li> <li>Exploit structure: Use specialized algorithms for symmetric, sparse, or banded matrices</li> <li>Avoid unnecessary copies: Use in-place operations when possible</li> </ol>"},{"location":"faq/#how-should-i-choose-between-different-attention-mechanisms","title":"How should I choose between different attention mechanisms?","text":"Mechanism Complexity Best For Dot-product O(n\u00b2d) Standard transformer, moderate sequences Multi-head O(n\u00b2d) Learning multiple relationship types Linear O(nd\u00b2) Very long sequences Sparse O(nd) Extremely long sequences with local patterns Cross-attention O(nm d) Different-length source and target <p>For most applications, multi-head dot-product attention works well.</p>"},{"location":"faq/#advanced-topic-questions","title":"Advanced Topic Questions","text":""},{"location":"faq/#how-does-lora-reduce-the-cost-of-fine-tuning-large-language-models","title":"How does LoRA reduce the cost of fine-tuning large language models?","text":"<p>Low-Rank Adaptation (LoRA) decomposes weight updates as the product of two small matrices: \u0394W = BA where B is (d \u00d7 r) and A is (r \u00d7 k) with rank r &lt;&lt; min(d, k).</p> <p>Instead of updating millions of parameters in the original weight matrix, LoRA only trains the small A and B matrices. This reduces:</p> <ul> <li>Trainable parameters by 10-100x</li> <li>Memory requirements during training</li> <li>Storage for multiple fine-tuned models (only store A and B)</li> </ul> <p>Example: For a 1000\u00d71000 weight matrix, using rank r=8 reduces parameters from 1,000,000 to 16,000.</p>"},{"location":"faq/#what-is-the-relationship-between-svd-and-eigendecomposition","title":"What is the relationship between SVD and eigendecomposition?","text":"<p>For a matrix A:</p> <ul> <li>A^T A has eigenvalues \u03c3\u00b2 (squared singular values) and eigenvectors V</li> <li>A A^T has eigenvalues \u03c3\u00b2 and eigenvectors U</li> <li>The singular values of A are the square roots of eigenvalues of A^T A</li> </ul> <p>For symmetric matrices, SVD and eigendecomposition are essentially the same, with singular values being absolute values of eigenvalues.</p>"},{"location":"faq/#how-does-convolution-in-neural-networks-differ-from-mathematical-convolution","title":"How does convolution in neural networks differ from mathematical convolution?","text":"Aspect Mathematical Convolution Neural Network \"Convolution\" Kernel flip Yes No (technically correlation) Dimensions Continuous or 1D discrete 2D, 3D with channels Kernel Fixed Learned from data Goal Signal processing Feature extraction <p>The terminology is historical\u2014neural network convolution is mathematically cross-correlation, but the learned kernels make the distinction practically irrelevant.</p>"},{"location":"faq/#how-do-quaternions-avoid-gimbal-lock","title":"How do quaternions avoid gimbal lock?","text":"<p>Gimbal lock occurs with Euler angles when two rotation axes align, losing a degree of freedom. Quaternions avoid this by:</p> <ul> <li>Representing rotations as 4D unit vectors (avoiding singularities)</li> <li>Using a different mathematical structure without axis-angle limitations</li> <li>Enabling smooth interpolation (SLERP) between orientations</li> </ul> <p>The trade-off is less intuitive representation\u2014quaternion components don't directly correspond to roll, pitch, yaw angles.</p>"},{"location":"faq/#what-makes-slam-computationally-challenging","title":"What makes SLAM computationally challenging?","text":"<p>SLAM (Simultaneous Localization and Mapping) is challenging because:</p> <ol> <li>Chicken-and-egg problem: Need position to build map, need map to determine position</li> <li>Growing state: Map size grows over time, increasing computation</li> <li>Loop closure: Recognizing previously visited locations requires global matching</li> <li>Real-time constraints: Must process sensor data fast enough for navigation</li> <li>Uncertainty management: Probabilistic state estimation with correlated errors</li> </ol> <p>Modern SLAM systems use sparse representations, keyframe-based approaches, and graph optimization to manage complexity.</p>"},{"location":"faq/#how-do-i-design-a-custom-loss-function-using-matrix-operations","title":"How do I design a custom loss function using matrix operations?","text":"<p>When designing custom loss functions:</p> <ol> <li>Express in matrix form: Vectorize to enable batch computation</li> <li>Ensure differentiability: Gradient must exist for backpropagation</li> <li>Consider numerical stability: Avoid log(0), division by zero</li> <li>Check convexity: Convex losses are easier to optimize</li> <li>Match the problem: Classification \u2192 cross-entropy; regression \u2192 MSE or Huber</li> </ol> <p>Example: Weighted least squares: L = (y - Xw)^T D (y - Xw) where D is a diagonal weight matrix.</p>"},{"location":"faq/#what-are-the-trade-offs-between-different-sensor-fusion-approaches","title":"What are the trade-offs between different sensor fusion approaches?","text":"Approach Pros Cons Kalman Filter Optimal for linear Gaussian Assumes linearity and Gaussian noise Extended Kalman Handles nonlinearity Linearization errors, may diverge Particle Filter Any distribution Computationally expensive Factor Graph Handles complex relationships Complex implementation Neural Fusion Learns optimal fusion Requires training data, less interpretable <p>Choose based on your system's characteristics and computational constraints.</p>"},{"location":"faq/#how-can-i-verify-my-linear-algebra-implementations-are-correct","title":"How can I verify my linear algebra implementations are correct?","text":"<p>Testing strategies:</p> <ol> <li>Identity checks: Does multiplying by identity return input?</li> <li>Inverse checks: Does AA^(-1) = I?</li> <li>Orthogonality checks: Is Q^T Q = I for orthogonal Q?</li> <li>Numerical comparison: Compare with NumPy/SciPy results</li> <li>Known solutions: Test on problems with analytical solutions</li> <li>Property preservation: Do transformations preserve expected properties?</li> <li>Gradient checking: Compare analytical gradients with numerical differences</li> </ol>"},{"location":"faq/#what-emerging-applications-of-linear-algebra-should-i-be-aware-of","title":"What emerging applications of linear algebra should I be aware of?","text":"<p>Active areas applying linear algebra include:</p> <ul> <li>Quantum computing: Quantum states are vectors; operations are unitary matrices</li> <li>Graph neural networks: Message passing as sparse matrix operations</li> <li>Neural radiance fields (NeRF): 3D scene representation using transformations</li> <li>Diffusion models: Noise addition/removal as matrix operations</li> <li>Mixture of experts: Sparse gating with linear combinations</li> <li>State space models: Efficient alternatives to attention using matrix structure</li> </ul> <p>Understanding linear algebra fundamentals prepares you for these advancing fields.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":"<p>This glossary contains 300 terms from the Applied Linear Algebra for AI and Machine Learning course, organized alphabetically with ISO 11179-compliant definitions.</p>"},{"location":"glossary/#2d-rotation","title":"2D Rotation","text":"<p>A linear transformation that rotates vectors in a two-dimensional plane around the origin by a specified angle.</p> <p>Example: Rotating a point (1, 0) by 90 degrees counterclockwise yields (0, 1).</p> <p>See also: 3D Rotation, Rotation Matrix</p>"},{"location":"glossary/#2d-vector","title":"2D Vector","text":"<p>An ordered pair of real numbers representing a point or direction in a two-dimensional plane.</p> <p>Example: The vector (3, 4) represents a point 3 units along the x-axis and 4 units along the y-axis.</p> <p>See also: 3D Vector, N-Dimensional Vector</p>"},{"location":"glossary/#2x2-determinant","title":"2x2 Determinant","text":"<p>The determinant of a 2\u00d72 matrix, calculated as ad - bc for matrix [[a, b], [c, d]].</p> <p>Example: For matrix [[3, 2], [1, 4]], the determinant is 3\u00d74 - 2\u00d71 = 10.</p> <p>See also: 3x3 Determinant, Determinant</p>"},{"location":"glossary/#3d-coordinate-system","title":"3D Coordinate System","text":"<p>A reference frame using three perpendicular axes (typically x, y, z) to specify positions in three-dimensional space.</p> <p>Example: The point (2, 3, 5) is located 2 units along x, 3 units along y, and 5 units up the z-axis.</p> <p>See also: Homogeneous Coordinates, Coordinate System</p>"},{"location":"glossary/#3d-rotation","title":"3D Rotation","text":"<p>A linear transformation that rotates vectors in three-dimensional space around a specified axis by a given angle.</p> <p>Example: Rotating a vector around the z-axis preserves its z-component while rotating its x and y components.</p> <p>See also: 2D Rotation, Euler Angles, Quaternion Rotation</p>"},{"location":"glossary/#3d-vector","title":"3D Vector","text":"<p>An ordered triple of real numbers representing a point or direction in three-dimensional space.</p> <p>Example: The vector (1, 2, 3) points from the origin to a location one unit along x, two along y, and three along z.</p> <p>See also: 2D Vector, N-Dimensional Vector</p>"},{"location":"glossary/#3x3-determinant","title":"3x3 Determinant","text":"<p>The determinant of a 3\u00d73 matrix, computed using cofactor expansion along any row or column.</p> <p>Example: Using the rule of Sarrus or cofactor expansion to compute the determinant of a 3\u00d73 rotation matrix yields 1.</p> <p>See also: 2x2 Determinant, Cofactor Expansion</p>"},{"location":"glossary/#abstract-vector-space","title":"Abstract Vector Space","text":"<p>A set of elements (vectors) together with operations of addition and scalar multiplication satisfying the vector space axioms.</p> <p>Example: The set of all polynomials of degree at most n forms an abstract vector space with standard polynomial addition.</p> <p>See also: Vector Space, Vector Space Axioms</p>"},{"location":"glossary/#activation-function","title":"Activation Function","text":"<p>A nonlinear function applied element-wise to the output of a neural network layer, introducing nonlinearity into the model.</p> <p>Example: ReLU returns max(0, x), allowing neural networks to learn complex, nonlinear patterns in data.</p> <p>See also: ReLU, Sigmoid, Tanh, Softmax</p>"},{"location":"glossary/#adam-optimizer","title":"Adam Optimizer","text":"<p>An adaptive learning rate optimization algorithm combining momentum and RMSprop for efficient gradient descent.</p> <p>Adam adjusts the learning rate for each parameter individually based on estimates of first and second moments of gradients.</p> <p>Example: Training a deep neural network with Adam typically converges faster than standard SGD.</p> <p>See also: SGD, Momentum, RMSprop</p>"},{"location":"glossary/#algebraic-multiplicity","title":"Algebraic Multiplicity","text":"<p>The number of times an eigenvalue appears as a root of the characteristic polynomial.</p> <p>Example: If \u03bb = 2 is a repeated root appearing twice in the characteristic polynomial, its algebraic multiplicity is 2.</p> <p>See also: Eigenvalue, Geometric Multiplicity, Characteristic Polynomial</p>"},{"location":"glossary/#attention-mechanism","title":"Attention Mechanism","text":"<p>A neural network component that computes weighted combinations of values based on learned relevance scores between queries and keys.</p> <p>Attention allows models to focus on relevant parts of the input when producing each part of the output.</p> <p>Example: In machine translation, attention helps the model focus on specific source words when generating each target word.</p> <p>See also: Self-Attention, Cross-Attention, Multi-Head Attention</p>"},{"location":"glossary/#attention-score","title":"Attention Score","text":"<p>A scalar value indicating the relevance between a query vector and a key vector, typically computed as their scaled dot product.</p> <p>Example: Higher attention scores mean the model considers those key-value pairs more relevant for the current query.</p> <p>See also: Attention Weights, Query Matrix, Key Matrix</p>"},{"location":"glossary/#attention-weights","title":"Attention Weights","text":"<p>Normalized attention scores (typically via softmax) that determine how much each value contributes to the output.</p> <p>Example: If a word has attention weight 0.7 for another word, it strongly influences how that word is processed.</p> <p>See also: Attention Score, Softmax, Value Matrix</p>"},{"location":"glossary/#augmented-matrix","title":"Augmented Matrix","text":"<p>A matrix formed by appending the constant terms of a system of linear equations as an additional column to the coefficient matrix.</p> <p>Example: For the system 2x + 3y = 5 and x - y = 1, the augmented matrix is [[2, 3, 5], [1, -1, 1]].</p> <p>See also: Matrix Equation Form, Gaussian Elimination</p>"},{"location":"glossary/#back-substitution","title":"Back Substitution","text":"<p>A technique for solving triangular systems of equations by solving for variables in reverse order, starting from the last equation.</p> <p>Example: In a row echelon form system, you first find the last variable, then substitute it back to find the others.</p> <p>See also: Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#backpropagation","title":"Backpropagation","text":"<p>An algorithm for computing gradients of the loss function with respect to neural network weights by applying the chain rule layer by layer.</p> <p>Backpropagation efficiently computes gradients by propagating error signals backward through the network.</p> <p>Example: During training, backpropagation calculates how much each weight contributed to the prediction error.</p> <p>See also: Forward Propagation, Chain Rule Matrices, Gradient Descent</p>"},{"location":"glossary/#basic-variable","title":"Basic Variable","text":"<p>A variable in a system of linear equations that corresponds to a pivot column in the reduced row echelon form.</p> <p>Example: In a system with two pivot columns for x and y, these are basic variables while z (if present without a pivot) is free.</p> <p>See also: Free Variable, Pivot Column</p>"},{"location":"glossary/#basis-transition-matrix","title":"Basis Transition Matrix","text":"<p>A matrix that converts vector coordinates from one basis representation to another.</p> <p>Example: To express a vector given in standard basis using a custom basis, multiply by the appropriate transition matrix.</p> <p>See also: Change of Basis, Basis Vector</p>"},{"location":"glossary/#basis-vector","title":"Basis Vector","text":"<p>A vector that is part of a basis, which is a linearly independent set that spans the entire vector space.</p> <p>Example: In 2D, the vectors (1, 0) and (0, 1) form the standard basis.</p> <p>See also: Standard Basis, Linear Independence, Span</p>"},{"location":"glossary/#batch-gradient-descent","title":"Batch Gradient Descent","text":"<p>An optimization method that computes the gradient using the entire training dataset before updating parameters.</p> <p>Example: After processing all 10,000 training examples, batch gradient descent makes one parameter update.</p> <p>See also: SGD, Mini-Batch SGD, Learning Rate</p>"},{"location":"glossary/#batch-normalization","title":"Batch Normalization","text":"<p>A technique that normalizes layer inputs across a mini-batch to accelerate training and improve stability.</p> <p>Example: Before applying the activation function, batch normalization centers and scales the values using mini-batch statistics.</p> <p>See also: Layer Normalization, Neural Network Layer</p>"},{"location":"glossary/#bfgs-algorithm","title":"BFGS Algorithm","text":"<p>A quasi-Newton optimization method that approximates the inverse Hessian matrix to find the minimum of a function efficiently.</p> <p>BFGS builds an approximation of curvature information without computing second derivatives explicitly.</p> <p>Example: BFGS is commonly used for training logistic regression and other convex optimization problems.</p> <p>See also: Quasi-Newton Method, Hessian Matrix, Newton's Method</p>"},{"location":"glossary/#bias-vector","title":"Bias Vector","text":"<p>A vector of constant terms added to the weighted sum in each layer of a neural network, allowing shifts in the activation function.</p> <p>Example: Adding a bias of 0.5 to all neurons allows the network to output non-zero values even when all inputs are zero.</p> <p>See also: Weight Matrix, Neural Network Layer</p>"},{"location":"glossary/#block-matrix","title":"Block Matrix","text":"<p>A matrix partitioned into rectangular submatrices (blocks) that can be manipulated as individual elements.</p> <p>Example: A 4\u00d74 matrix can be viewed as a 2\u00d72 block matrix where each block is a 2\u00d72 matrix.</p> <p>See also: Matrix, Sparse Matrix</p>"},{"location":"glossary/#blur-filter","title":"Blur Filter","text":"<p>A convolution kernel that averages neighboring pixel values to reduce image sharpness and noise.</p> <p>Example: A 3\u00d73 averaging filter replaces each pixel with the mean of its 9 neighbors.</p> <p>See also: Image Convolution, Sharpen Filter, Image Filter</p>"},{"location":"glossary/#bounding-box","title":"Bounding Box","text":"<p>A rectangular region defined by coordinates that encloses an object of interest in an image or 3D space.</p> <p>Example: An object detector outputs bounding boxes as (x_min, y_min, x_max, y_max) coordinates around detected cars.</p> <p>See also: Object Detection, Object Tracking</p>"},{"location":"glossary/#camera-calibration","title":"Camera Calibration","text":"<p>The process of estimating camera parameters (intrinsic and extrinsic) to accurately map 3D world points to 2D image coordinates.</p> <p>Example: Using a checkerboard pattern to determine focal length, principal point, and lens distortion coefficients.</p> <p>See also: Camera Matrix, Intrinsic Parameters, Extrinsic Parameters</p>"},{"location":"glossary/#camera-matrix","title":"Camera Matrix","text":"<p>A matrix that maps 3D world coordinates to 2D image coordinates, combining intrinsic and extrinsic parameters.</p> <p>Example: The 3\u00d74 camera matrix projects a 3D point onto the image plane using perspective projection.</p> <p>See also: Projection Matrix, Intrinsic Parameters, Extrinsic Parameters</p>"},{"location":"glossary/#cauchy-schwarz-inequality","title":"Cauchy-Schwarz Inequality","text":"<p>A fundamental inequality stating that the absolute value of an inner product is at most the product of the norms.</p> <p>Example: For any vectors u and v, |\u27e8u, v\u27e9| \u2264 ||u|| \u00b7 ||v||, with equality when vectors are parallel.</p> <p>See also: Inner Product, Norm from Inner Product</p>"},{"location":"glossary/#chain-rule-matrices","title":"Chain Rule Matrices","text":"<p>Matrix representations of derivative chains used in backpropagation to compute gradients through composed functions.</p> <p>Example: The gradient flows backward through layers as products of Jacobian matrices.</p> <p>See also: Backpropagation, Gradient Vector</p>"},{"location":"glossary/#change-of-basis","title":"Change of Basis","text":"<p>The process of expressing vectors in terms of a different set of basis vectors than the original representation.</p> <p>Example: Converting from Cartesian coordinates to polar-like basis vectors requires a change of basis operation.</p> <p>See also: Basis Transition Matrix, Basis Vector</p>"},{"location":"glossary/#characteristic-equation","title":"Characteristic Equation","text":"<p>An equation obtained by setting the characteristic polynomial equal to zero, whose roots are the eigenvalues.</p> <p>Example: For a 2\u00d72 matrix, solving det(A - \u03bbI) = 0 gives the characteristic equation \u03bb\u00b2 - trace(A)\u03bb + det(A) = 0.</p> <p>See also: Characteristic Polynomial, Eigenvalue</p>"},{"location":"glossary/#characteristic-polynomial","title":"Characteristic Polynomial","text":"<p>A polynomial whose roots are the eigenvalues of a matrix, computed as det(A - \u03bbI).</p> <p>Example: For a 3\u00d73 matrix, the characteristic polynomial is a cubic in \u03bb.</p> <p>See also: Characteristic Equation, Eigenvalue, Determinant</p>"},{"location":"glossary/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>A factorization of a positive definite matrix into the product of a lower triangular matrix and its transpose.</p> <p>Cholesky decomposition is computationally efficient and numerically stable for solving systems with positive definite matrices.</p> <p>Example: A covariance matrix can be decomposed as A = LL^T, where L is lower triangular.</p> <p>See also: Positive Definite Matrix, LU Decomposition</p>"},{"location":"glossary/#codomain","title":"Codomain","text":"<p>The set of all possible output values of a function or transformation.</p> <p>Example: For a transformation from R\u00b2 to R\u00b3, the codomain is R\u00b3.</p> <p>See also: Domain, Image, Range</p>"},{"location":"glossary/#cofactor","title":"Cofactor","text":"<p>The signed minor of a matrix element, used in computing determinants and the adjugate matrix.</p> <p>Example: The cofactor of element a\u2081\u2082 is (-1)^(1+2) times the minor M\u2081\u2082.</p> <p>See also: Minor, Cofactor Expansion</p>"},{"location":"glossary/#cofactor-expansion","title":"Cofactor Expansion","text":"<p>A method for computing the determinant by expanding along a row or column using minors and cofactors.</p> <p>Example: Expanding a 3\u00d73 determinant along the first row: det(A) = a\u2081\u2081C\u2081\u2081 + a\u2081\u2082C\u2081\u2082 + a\u2081\u2083C\u2081\u2083.</p> <p>See also: Determinant, Minor, Cofactor</p>"},{"location":"glossary/#color-space-transform","title":"Color Space Transform","text":"<p>A linear transformation that converts image data between different color representations.</p> <p>Example: Converting from RGB to YUV separates luminance from color information using a matrix transformation.</p> <p>See also: RGB Image, Image Matrix</p>"},{"location":"glossary/#column-space","title":"Column Space","text":"<p>The set of all possible linear combinations of the column vectors of a matrix, also called the range.</p> <p>Example: If a 3\u00d72 matrix has linearly independent columns, its column space is a 2D plane in R\u00b3.</p> <p>See also: Range, Row Space, Rank</p>"},{"location":"glossary/#column-vector","title":"Column Vector","text":"<p>A matrix with a single column, representing a vector as a vertical array of numbers.</p> <p>Example: The column vector [1; 2; 3] has three rows and one column.</p> <p>See also: Row Vector, Vector</p>"},{"location":"glossary/#compact-svd","title":"Compact SVD","text":"<p>A form of singular value decomposition that retains only the non-zero singular values and corresponding vectors.</p> <p>Example: For a rank-r matrix, compact SVD uses r singular values instead of the full min(m,n).</p> <p>See also: SVD, Full SVD, Truncated SVD</p>"},{"location":"glossary/#complex-eigenvalue","title":"Complex Eigenvalue","text":"<p>An eigenvalue that has a nonzero imaginary component, occurring in conjugate pairs for real matrices.</p> <p>Example: A 2D rotation matrix has complex eigenvalues cos(\u03b8) \u00b1 i\u00b7sin(\u03b8).</p> <p>See also: Eigenvalue, Characteristic Polynomial</p>"},{"location":"glossary/#composition-of-transforms","title":"Composition of Transforms","text":"<p>The result of applying one linear transformation followed by another, represented by matrix multiplication.</p> <p>Example: Rotating then scaling is represented by multiplying the rotation matrix by the scaling matrix.</p> <p>See also: Linear Transformation, Transformation Matrix</p>"},{"location":"glossary/#condition-number","title":"Condition Number","text":"<p>A measure of how sensitive a matrix computation is to small changes in input, defined as the ratio of largest to smallest singular values.</p> <p>Large condition numbers indicate ill-conditioned matrices prone to numerical instability.</p> <p>Example: A condition number of 10\u2076 means errors can be amplified by up to a million times.</p> <p>See also: Singular Value, Numerical Stability</p>"},{"location":"glossary/#constrained-optimization","title":"Constrained Optimization","text":"<p>Optimization of an objective function subject to equality or inequality constraints on the variables.</p> <p>Example: Minimizing cost while meeting quality requirements is a constrained optimization problem.</p> <p>See also: Lagrange Multiplier, KKT Conditions</p>"},{"location":"glossary/#convex-function","title":"Convex Function","text":"<p>A function where any line segment between two points on its graph lies above or on the graph.</p> <p>Convex functions have a single global minimum, making optimization straightforward.</p> <p>Example: f(x) = x\u00b2 is convex because the parabola curves upward everywhere.</p> <p>See also: Convexity, Hessian Matrix</p>"},{"location":"glossary/#convexity","title":"Convexity","text":"<p>The property of a set or function ensuring that any weighted average of two elements remains within the set or below the function graph.</p> <p>Example: A convex optimization problem has no local minima that are not also global minima.</p> <p>See also: Convex Function, Constrained Optimization</p>"},{"location":"glossary/#convolution-kernel","title":"Convolution Kernel","text":"<p>A small matrix of weights used in convolution operations to detect features or patterns in input data.</p> <p>Example: A 3\u00d73 edge detection kernel highlights boundaries between regions of different intensities.</p> <p>See also: Convolutional Layer, Image Convolution</p>"},{"location":"glossary/#convolutional-layer","title":"Convolutional Layer","text":"<p>A neural network layer that applies learnable convolution kernels to detect local patterns in input data.</p> <p>Example: Early convolutional layers might detect edges, while deeper layers detect complex features like faces.</p> <p>See also: Convolution Kernel, Stride, Padding</p>"},{"location":"glossary/#coordinate-system","title":"Coordinate System","text":"<p>A system that uses numerical values (coordinates) to uniquely specify the position of points in a space.</p> <p>Example: The Cartesian coordinate system uses perpendicular x and y axes to locate points in a plane.</p> <p>See also: 3D Coordinate System, Basis Vector</p>"},{"location":"glossary/#correlation-matrix","title":"Correlation Matrix","text":"<p>A matrix of pairwise Pearson correlation coefficients between features, with values ranging from -1 to 1.</p> <p>Example: A correlation matrix shows which stock prices tend to move together.</p> <p>See also: Covariance Matrix, Feature Matrix</p>"},{"location":"glossary/#cosine-similarity","title":"Cosine Similarity","text":"<p>A measure of similarity between two vectors computed as the cosine of the angle between them.</p> <p>Example: Two parallel vectors have cosine similarity of 1, while orthogonal vectors have similarity of 0.</p> <p>See also: Dot Product, Semantic Similarity</p>"},{"location":"glossary/#covariance-matrix","title":"Covariance Matrix","text":"<p>A symmetric matrix containing covariances between pairs of variables, with variances on the diagonal.</p> <p>The covariance matrix captures how features vary together in a dataset.</p> <p>Example: A 3\u00d73 covariance matrix for height, weight, and age shows how these variables co-vary.</p> <p>See also: Correlation Matrix, PCA</p>"},{"location":"glossary/#cramers-rule","title":"Cramers Rule","text":"<p>A formula for solving systems of linear equations using ratios of determinants.</p> <p>Example: For a 2\u00d72 system, x = det(A_x)/det(A), where A_x replaces the first column with the constant terms.</p> <p>See also: Determinant, System of Equations</p>"},{"location":"glossary/#cross-attention","title":"Cross-Attention","text":"<p>An attention mechanism where queries come from one sequence and keys/values come from a different sequence.</p> <p>Example: In translation, cross-attention allows the decoder to attend to relevant parts of the encoded source sentence.</p> <p>See also: Self-Attention, Attention Mechanism</p>"},{"location":"glossary/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<p>A loss function measuring the difference between predicted probability distributions and true labels.</p> <p>Example: For classification, cross-entropy heavily penalizes confident wrong predictions.</p> <p>See also: Loss Function, Softmax</p>"},{"location":"glossary/#cross-product","title":"Cross Product","text":"<p>A binary operation on two 3D vectors that produces a vector perpendicular to both inputs.</p> <p>Example: The cross product of (1, 0, 0) and (0, 1, 0) is (0, 0, 1).</p> <p>See also: Dot Product, Vector</p>"},{"location":"glossary/#data-matrix","title":"Data Matrix","text":"<p>A matrix where rows represent observations (samples) and columns represent features (variables).</p> <p>Example: A data matrix for 1000 patients with 50 measurements has dimensions 1000\u00d750.</p> <p>See also: Feature Matrix, Design Matrix</p>"},{"location":"glossary/#deep-network","title":"Deep Network","text":"<p>A neural network with multiple hidden layers, enabling learning of hierarchical feature representations.</p> <p>Example: A 50-layer convolutional network can learn increasingly abstract features for image classification.</p> <p>See also: Hidden Layer, Neural Network Layer</p>"},{"location":"glossary/#dense-matrix","title":"Dense Matrix","text":"<p>A matrix in which most elements are non-zero, typically stored as a full 2D array.</p> <p>Example: A random matrix with normally distributed entries is typically dense.</p> <p>See also: Sparse Matrix, Matrix</p>"},{"location":"glossary/#design-matrix","title":"Design Matrix","text":"<p>A matrix of input features used in regression, with rows as observations and columns as predictors.</p> <p>Example: In linear regression with 100 samples and 5 features, the design matrix is 100\u00d75.</p> <p>See also: Data Matrix, Linear Regression</p>"},{"location":"glossary/#determinant","title":"Determinant","text":"<p>A scalar value computed from a square matrix that indicates whether the matrix is invertible and measures volume scaling.</p> <p>Example: A matrix with determinant zero is singular and cannot be inverted.</p> <p>See also: 2x2 Determinant, 3x3 Determinant, Singular Matrix</p>"},{"location":"glossary/#determinant-properties","title":"Determinant Properties","text":"<p>Rules governing how determinants behave under matrix operations, including row operations and multiplication.</p> <p>Example: The determinant of a product equals the product of determinants: det(AB) = det(A)\u00b7det(B).</p> <p>See also: Determinant, Multiplicative Property</p>"},{"location":"glossary/#diagonal-form","title":"Diagonal Form","text":"<p>A diagonal matrix that a matrix can be transformed into through diagonalization, with eigenvalues on the diagonal.</p> <p>Example: If A = PDP\u207b\u00b9 where D is diagonal, then D is the diagonal form of A.</p> <p>See also: Diagonalization, Eigenvalue</p>"},{"location":"glossary/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A square matrix where all non-diagonal elements are zero.</p> <p>Example: The matrix [[3, 0], [0, 5]] is diagonal with entries 3 and 5.</p> <p>See also: Identity Matrix, Triangular Matrix</p>"},{"location":"glossary/#diagonalization","title":"Diagonalization","text":"<p>The process of expressing a matrix as A = PDP\u207b\u00b9, where D is diagonal and P contains eigenvectors.</p> <p>Example: Diagonalizing a symmetric matrix simplifies computing matrix powers: A^n = PD^nP\u207b\u00b9.</p> <p>See also: Eigenvalue, Eigenvector, Diagonal Form</p>"},{"location":"glossary/#dimension-of-space","title":"Dimension of Space","text":"<p>The number of vectors in any basis for a vector space, indicating the degrees of freedom.</p> <p>Example: R\u00b3 has dimension 3 because any basis requires exactly three vectors.</p> <p>See also: Vector Space, Basis Vector</p>"},{"location":"glossary/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Techniques that reduce the number of features while preserving important information in the data.</p> <p>Example: Using PCA to reduce 100 features to 10 principal components that capture 95% of variance.</p> <p>See also: PCA, Truncated SVD</p>"},{"location":"glossary/#domain","title":"Domain","text":"<p>The set of all possible input values for a function or transformation.</p> <p>Example: A transformation from R\u00b3 to R\u00b2 has domain R\u00b3.</p> <p>See also: Codomain, Range</p>"},{"location":"glossary/#dominant-eigenvalue","title":"Dominant Eigenvalue","text":"<p>The eigenvalue with the largest absolute value, which determines the long-term behavior of iterative matrix powers.</p> <p>Example: In power iteration, multiplying by a matrix repeatedly converges to the direction of the dominant eigenvector.</p> <p>See also: Eigenvalue, Power Iteration</p>"},{"location":"glossary/#dot-product","title":"Dot Product","text":"<p>A binary operation on two vectors that produces a scalar, computed as the sum of componentwise products.</p> <p>Example: The dot product of (1, 2, 3) and (4, 5, 6) is 1\u00d74 + 2\u00d75 + 3\u00d76 = 32.</p> <p>See also: Inner Product, Cosine Similarity</p>"},{"location":"glossary/#edge-detection","title":"Edge Detection","text":"<p>Image processing techniques that identify boundaries between regions with different intensities or colors.</p> <p>Example: The Sobel operator detects edges by computing intensity gradients in horizontal and vertical directions.</p> <p>See also: Sobel Operator, Image Convolution</p>"},{"location":"glossary/#eigen-equation","title":"Eigen Equation","text":"<p>The fundamental equation Av = \u03bbv that defines eigenvalues \u03bb and eigenvectors v of a matrix A.</p> <p>Example: If Av = 3v, then v is an eigenvector with eigenvalue 3.</p> <p>See also: Eigenvalue, Eigenvector</p>"},{"location":"glossary/#eigendecomposition","title":"Eigendecomposition","text":"<p>The representation of a matrix as a product of its eigenvector matrix, diagonal eigenvalue matrix, and inverse eigenvector matrix.</p> <p>Example: For a symmetric matrix, eigendecomposition is A = Q\u039bQ^T where Q is orthogonal.</p> <p>See also: Diagonalization, Eigenvalue, Eigenvector</p>"},{"location":"glossary/#eigenspace","title":"Eigenspace","text":"<p>The set of all eigenvectors corresponding to a particular eigenvalue, together with the zero vector.</p> <p>Example: For a 3\u00d73 matrix with eigenvalue 2 having geometric multiplicity 2, the eigenspace is a plane.</p> <p>See also: Eigenvector, Geometric Multiplicity</p>"},{"location":"glossary/#eigenvalue","title":"Eigenvalue","text":"<p>A scalar \u03bb such that Av = \u03bbv for some nonzero vector v, indicating a direction scaled by the transformation.</p> <p>Example: A 2D rotation has complex eigenvalues, indicating no real directions are preserved.</p> <p>See also: Eigenvector, Characteristic Polynomial</p>"},{"location":"glossary/#eigenvector","title":"Eigenvector","text":"<p>A nonzero vector v such that Av = \u03bbv for some scalar \u03bb, indicating a direction unchanged by the transformation except for scaling.</p> <p>Example: For a horizontal stretch matrix, any vector along the x-axis is an eigenvector.</p> <p>See also: Eigenvalue, Eigenspace</p>"},{"location":"glossary/#embedding","title":"Embedding","text":"<p>A learned vector representation that maps discrete objects (like words or items) to continuous vector space.</p> <p>Embeddings capture semantic relationships where similar items have similar vector representations.</p> <p>Example: Word embeddings place \"king\" and \"queen\" close together in vector space.</p> <p>See also: Embedding Space, Word Embedding</p>"},{"location":"glossary/#embedding-space","title":"Embedding Space","text":"<p>A continuous vector space where embedded objects reside, with geometric relationships encoding semantic meaning.</p> <p>Example: In an embedding space, arithmetic like \"king - man + woman \u2248 queen\" can hold.</p> <p>See also: Embedding, Latent Space</p>"},{"location":"glossary/#epipolar-geometry","title":"Epipolar Geometry","text":"<p>The geometric relationship between two camera views of the same 3D scene, constraining point correspondences.</p> <p>Example: A point in one image must lie on an epipolar line in the other image.</p> <p>See also: Stereo Vision, Triangulation</p>"},{"location":"glossary/#euclidean-distance","title":"Euclidean Distance","text":"<p>The straight-line distance between two points, computed as the L2 norm of their difference.</p> <p>Example: The Euclidean distance between (0, 0) and (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L2 Norm, Vector Magnitude</p>"},{"location":"glossary/#euler-angles","title":"Euler Angles","text":"<p>Three angles (typically roll, pitch, yaw) that describe the orientation of a rigid body in 3D space.</p> <p>Example: An aircraft's orientation is often described as 10\u00b0 roll, 5\u00b0 pitch, and 30\u00b0 yaw.</p> <p>See also: Gimbal Lock, Quaternion, 3D Rotation</p>"},{"location":"glossary/#extended-kalman-filter","title":"Extended Kalman Filter","text":"<p>A nonlinear extension of the Kalman filter that linearizes the system model around the current estimate.</p> <p>Example: EKF is used when sensor measurements are nonlinear functions of the state, like bearing angles.</p> <p>See also: Kalman Filter, State Estimation</p>"},{"location":"glossary/#extrinsic-parameters","title":"Extrinsic Parameters","text":"<p>Camera parameters describing the position and orientation of the camera in world coordinates.</p> <p>Example: Extrinsic parameters include a rotation matrix and translation vector from world to camera frame.</p> <p>See also: Intrinsic Parameters, Camera Matrix</p>"},{"location":"glossary/#feature-detection","title":"Feature Detection","text":"<p>Algorithms that identify distinctive points or regions in images that can be tracked or matched.</p> <p>Example: SIFT and ORB algorithms detect corner-like features that are robust to rotation and scale.</p> <p>See also: Edge Detection, Homography</p>"},{"location":"glossary/#feature-matrix","title":"Feature Matrix","text":"<p>A matrix organizing feature values where rows represent samples and columns represent individual features.</p> <p>Example: A feature matrix for housing data might have columns for square footage, bedrooms, and price.</p> <p>See also: Feature Vector, Data Matrix</p>"},{"location":"glossary/#feature-vector","title":"Feature Vector","text":"<p>A vector containing the numerical feature values that describe a single data sample.</p> <p>Example: A feature vector for an image might contain pixel intensities or extracted features like edges.</p> <p>See also: Feature Matrix, Embedding</p>"},{"location":"glossary/#forward-propagation","title":"Forward Propagation","text":"<p>The process of computing outputs by passing inputs through each layer of a neural network sequentially.</p> <p>Example: In forward propagation, input data flows through hidden layers to produce a prediction.</p> <p>See also: Backpropagation, Neural Network Layer</p>"},{"location":"glossary/#four-subspaces","title":"Four Subspaces","text":"<p>The column space, row space, null space, and left null space of a matrix, with fundamental relationships.</p> <p>Example: The column space and left null space are orthogonal complements in the output space.</p> <p>See also: Column Space, Row Space, Null Space, Left Null Space</p>"},{"location":"glossary/#fourier-transform","title":"Fourier Transform","text":"<p>A transformation that decomposes a signal into its constituent frequencies using sinusoidal basis functions.</p> <p>Example: The Fourier transform of an image reveals its frequency content for filtering or compression.</p> <p>See also: Frequency Domain, Image Compression</p>"},{"location":"glossary/#free-variable","title":"Free Variable","text":"<p>A variable in a system of linear equations that can take any value, corresponding to a non-pivot column.</p> <p>Example: In a system with infinitely many solutions, free variables parameterize the solution set.</p> <p>See also: Basic Variable, Infinite Solutions</p>"},{"location":"glossary/#frequency-domain","title":"Frequency Domain","text":"<p>A representation of data in terms of frequency components rather than spatial or temporal values.</p> <p>Example: High-frequency components in an image correspond to edges and fine details.</p> <p>See also: Fourier Transform, Image Compression</p>"},{"location":"glossary/#full-svd","title":"Full SVD","text":"<p>The complete singular value decomposition with U, \u03a3, and V^T matrices of full dimensions.</p> <p>Example: For an m\u00d7n matrix, full SVD has U as m\u00d7m, \u03a3 as m\u00d7n, and V^T as n\u00d7n.</p> <p>See also: SVD, Compact SVD, Truncated SVD</p>"},{"location":"glossary/#function","title":"Function","text":"<p>A rule that assigns exactly one output value to each input value from the domain.</p> <p>Example: f(x) = x\u00b2 is a function mapping each real number to its square.</p> <p>See also: Domain, Codomain, Linear Transformation</p>"},{"location":"glossary/#gaussian-elimination","title":"Gaussian Elimination","text":"<p>An algorithm for transforming a matrix to row echelon form using elementary row operations.</p> <p>Example: Gaussian elimination converts an augmented matrix to a form where back substitution can solve the system.</p> <p>See also: Row Operations, Row Echelon Form</p>"},{"location":"glossary/#geometric-multiplicity","title":"Geometric Multiplicity","text":"<p>The dimension of the eigenspace corresponding to an eigenvalue, equal to the number of linearly independent eigenvectors.</p> <p>Example: If an eigenvalue has geometric multiplicity 2, there are two independent eigenvector directions.</p> <p>See also: Algebraic Multiplicity, Eigenspace</p>"},{"location":"glossary/#gimbal-lock","title":"Gimbal Lock","text":"<p>A phenomenon where two rotation axes become aligned, causing loss of one degree of rotational freedom.</p> <p>Example: When pitch reaches 90\u00b0, roll and yaw rotations become equivalent, losing independent control.</p> <p>See also: Euler Angles, Quaternion</p>"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"<p>An iterative optimization algorithm that moves parameters in the direction opposite to the gradient to minimize a function.</p> <p>Example: Training neural networks uses gradient descent to reduce the loss function over many iterations.</p> <p>See also: Gradient Vector, Learning Rate, SGD</p>"},{"location":"glossary/#gradient-vector","title":"Gradient Vector","text":"<p>A vector of partial derivatives indicating the direction and rate of steepest increase of a function.</p> <p>Example: The gradient of f(x, y) = x\u00b2 + y\u00b2 at point (1, 2) is (2, 4).</p> <p>See also: Gradient Descent, Hessian Matrix</p>"},{"location":"glossary/#gram-schmidt-process","title":"Gram-Schmidt Process","text":"<p>An algorithm for orthonormalizing a set of vectors by sequentially removing projections onto previously processed vectors.</p> <p>Example: Starting with vectors (1, 1) and (1, 2), Gram-Schmidt produces orthonormal vectors.</p> <p>See also: Orthonormal Basis, QR Decomposition</p>"},{"location":"glossary/#gram-schmidt-qr","title":"Gram-Schmidt QR","text":"<p>QR decomposition computed using the Gram-Schmidt orthogonalization process.</p> <p>Example: The Q matrix from Gram-Schmidt contains orthonormal columns spanning the column space of A.</p> <p>See also: Gram-Schmidt Process, QR Decomposition, Householder QR</p>"},{"location":"glossary/#grayscale-image","title":"Grayscale Image","text":"<p>A digital image represented by a single channel matrix of intensity values, typically ranging from 0 (black) to 255 (white).</p> <p>Example: A 512\u00d7512 grayscale image is stored as a single 512\u00d7512 matrix of pixel values.</p> <p>See also: Image Matrix, RGB Image</p>"},{"location":"glossary/#hessian-matrix","title":"Hessian Matrix","text":"<p>A square matrix of second partial derivatives describing the local curvature of a multivariable function.</p> <p>Example: For f(x, y), the Hessian is [[\u2202\u00b2f/\u2202x\u00b2, \u2202\u00b2f/\u2202x\u2202y], [\u2202\u00b2f/\u2202y\u2202x, \u2202\u00b2f/\u2202y\u00b2]].</p> <p>See also: Gradient Vector, Newton's Method</p>"},{"location":"glossary/#hidden-layer","title":"Hidden Layer","text":"<p>A neural network layer between the input and output layers that learns intermediate representations.</p> <p>Example: A network with two hidden layers of 64 neurons each can learn hierarchical features.</p> <p>See also: Neural Network Layer, Deep Network</p>"},{"location":"glossary/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>An extended coordinate system using n+1 coordinates to represent points in n-dimensional space, enabling uniform treatment of transformations.</p> <p>Example: The 2D point (3, 4) becomes (3, 4, 1) in homogeneous coordinates.</p> <p>See also: Projection Matrix, SE3 Transform</p>"},{"location":"glossary/#homogeneous-system","title":"Homogeneous System","text":"<p>A system of linear equations where all constant terms are zero, always having at least the trivial solution.</p> <p>Example: The system 2x + 3y = 0 and x - y = 0 is homogeneous with solution x = y = 0.</p> <p>See also: Trivial Solution, Null Space</p>"},{"location":"glossary/#homography","title":"Homography","text":"<p>A projective transformation that maps points between two planes, represented by a 3\u00d73 matrix.</p> <p>Example: A homography can correct perspective distortion when photographing a document at an angle.</p> <p>See also: Perspective Projection, Feature Detection</p>"},{"location":"glossary/#householder-qr","title":"Householder QR","text":"<p>QR decomposition using Householder reflections, which is numerically more stable than Gram-Schmidt.</p> <p>Example: Householder QR is preferred for solving least squares problems due to better numerical properties.</p> <p>See also: QR Decomposition, Gram-Schmidt QR</p>"},{"location":"glossary/#identity-matrix","title":"Identity Matrix","text":"<p>A square diagonal matrix with ones on the main diagonal and zeros elsewhere, acting as the multiplicative identity.</p> <p>Example: Multiplying any matrix by the identity matrix of compatible size returns the original matrix.</p> <p>See also: Diagonal Matrix, Matrix Inverse</p>"},{"location":"glossary/#image","title":"Image","text":"<p>The set of all output values that a function or transformation actually produces from its domain.</p> <p>Example: For f(x) = x\u00b2, the image is all non-negative real numbers.</p> <p>See also: Range, Codomain</p>"},{"location":"glossary/#image-compression","title":"Image Compression","text":"<p>Techniques that reduce the storage size of images while preserving visual quality, often using matrix decompositions.</p> <p>Example: SVD-based compression retains only the largest singular values, approximating the image with fewer parameters.</p> <p>See also: Truncated SVD, Low-Rank Approximation</p>"},{"location":"glossary/#image-convolution","title":"Image Convolution","text":"<p>An operation that applies a kernel to each position in an image, computing weighted sums of neighboring pixels.</p> <p>Example: Convolving with a 3\u00d73 averaging kernel blurs the image by smoothing local variations.</p> <p>See also: Convolution Kernel, Image Filter</p>"},{"location":"glossary/#image-filter","title":"Image Filter","text":"<p>A convolution kernel designed for specific image processing tasks like blurring, sharpening, or edge detection.</p> <p>Example: A Gaussian filter smooths images while preserving edges better than simple averaging.</p> <p>See also: Image Convolution, Blur Filter, Sharpen Filter</p>"},{"location":"glossary/#image-matrix","title":"Image Matrix","text":"<p>A numerical representation of a digital image as a matrix (grayscale) or tensor (color) of pixel values.</p> <p>Example: An 800\u00d7600 RGB image is stored as a 800\u00d7600\u00d73 tensor of values.</p> <p>See also: Grayscale Image, RGB Image, Image Tensor</p>"},{"location":"glossary/#image-tensor","title":"Image Tensor","text":"<p>A multi-dimensional array representing image data, typically with dimensions for height, width, and color channels.</p> <p>Example: A batch of 32 RGB images of size 224\u00d7224 forms a 32\u00d7224\u00d7224\u00d73 tensor.</p> <p>See also: Image Matrix, Tensor</p>"},{"location":"glossary/#infinite-solutions","title":"Infinite Solutions","text":"<p>A condition where a system of linear equations has infinitely many solutions, forming a line, plane, or higher-dimensional set.</p> <p>Example: Parallel planes that coincide have infinitely many intersection points.</p> <p>See also: Solution Set, Free Variable</p>"},{"location":"glossary/#inner-product","title":"Inner Product","text":"<p>A generalization of the dot product that defines angles and lengths in abstract vector spaces.</p> <p>Example: The standard inner product in R^n is the dot product \u27e8u, v\u27e9 = \u03a3u_i v_i.</p> <p>See also: Dot Product, Inner Product Space</p>"},{"location":"glossary/#inner-product-space","title":"Inner Product Space","text":"<p>A vector space equipped with an inner product operation satisfying linearity, symmetry, and positive-definiteness.</p> <p>Example: R^n with the dot product is an inner product space.</p> <p>See also: Inner Product, Vector Space</p>"},{"location":"glossary/#interpolation","title":"Interpolation","text":"<p>Creating intermediate values or states between known data points using mathematical techniques.</p> <p>Example: Linear interpolation between embeddings generates semantically meaningful intermediate representations.</p> <p>See also: Latent Space, Embedding</p>"},{"location":"glossary/#intrinsic-parameters","title":"Intrinsic Parameters","text":"<p>Camera parameters describing internal properties like focal length, principal point, and lens distortion.</p> <p>Example: A camera with focal length 50mm and sensor size 36mm has specific intrinsic parameters.</p> <p>See also: Extrinsic Parameters, Camera Matrix</p>"},{"location":"glossary/#invertible-matrix","title":"Invertible Matrix","text":"<p>A square matrix that has an inverse, equivalent to having a nonzero determinant and full rank.</p> <p>Example: The matrix [[1, 2], [3, 4]] is invertible because its determinant 1\u00d74 - 2\u00d73 = -2 \u2260 0.</p> <p>See also: Matrix Inverse, Singular Matrix, Determinant</p>"},{"location":"glossary/#invertible-transform","title":"Invertible Transform","text":"<p>A linear transformation that can be reversed, mapping each output uniquely back to its input.</p> <p>Example: Rotation is invertible\u2014rotating by angle \u03b8 can be reversed by rotating by -\u03b8.</p> <p>See also: Linear Transformation, Matrix Inverse</p>"},{"location":"glossary/#kalman-filter","title":"Kalman Filter","text":"<p>An optimal recursive algorithm for estimating the state of a linear dynamic system from noisy measurements.</p> <p>The Kalman filter combines predictions from a system model with sensor measurements, weighting by their uncertainties.</p> <p>Example: GPS receivers use Kalman filters to estimate position by fusing measurements over time.</p> <p>See also: State Estimation, Kalman Gain, Extended Kalman Filter</p>"},{"location":"glossary/#kalman-gain","title":"Kalman Gain","text":"<p>A matrix that determines how much the state estimate should be updated based on the measurement residual.</p> <p>Example: High Kalman gain means trusting measurements more; low gain means trusting predictions more.</p> <p>See also: Kalman Filter, Update Step</p>"},{"location":"glossary/#kernel","title":"Kernel","text":"<p>The set of all vectors mapped to zero by a linear transformation, also called the null space.</p> <p>Example: For a projection matrix onto a line, the kernel is the orthogonal complement of that line.</p> <p>See also: Null Space, Rank-Nullity Theorem</p>"},{"location":"glossary/#key-matrix","title":"Key Matrix","text":"<p>A learned matrix that transforms input embeddings into key vectors used for computing attention scores.</p> <p>Example: In transformers, the key matrix creates representations that queries match against.</p> <p>See also: Query Matrix, Value Matrix, Attention Mechanism</p>"},{"location":"glossary/#kkt-conditions","title":"KKT Conditions","text":"<p>Necessary conditions for optimality in constrained optimization problems with both equality and inequality constraints.</p> <p>KKT conditions generalize Lagrange multiplier conditions to handle inequality constraints.</p> <p>Example: At an optimal point, KKT conditions require zero gradient and complementary slackness for active constraints.</p> <p>See also: Lagrange Multiplier, Constrained Optimization</p>"},{"location":"glossary/#l1-norm","title":"L1 Norm","text":"<p>The sum of absolute values of vector components, also called the Manhattan distance or taxicab norm.</p> <p>Example: The L1 norm of vector (3, -4, 2) is |3| + |-4| + |2| = 9.</p> <p>See also: L2 Norm, L-Infinity Norm</p>"},{"location":"glossary/#l2-norm","title":"L2 Norm","text":"<p>The square root of the sum of squared components, equal to the Euclidean length of a vector.</p> <p>Example: The L2 norm of vector (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L1 Norm, Vector Magnitude, Euclidean Distance</p>"},{"location":"glossary/#l-infinity-norm","title":"L-Infinity Norm","text":"<p>The maximum absolute value among all vector components.</p> <p>Example: The L-infinity norm of vector (3, -7, 2) is max(|3|, |-7|, |2|) = 7.</p> <p>See also: L1 Norm, L2 Norm</p>"},{"location":"glossary/#lagrange-multiplier","title":"Lagrange Multiplier","text":"<p>A scalar variable introduced to convert a constrained optimization problem into an unconstrained one.</p> <p>Example: To minimize f(x) subject to g(x) = 0, solve \u2207f = \u03bb\u2207g where \u03bb is the Lagrange multiplier.</p> <p>See also: Constrained Optimization, KKT Conditions</p>"},{"location":"glossary/#lasso-regression","title":"Lasso Regression","text":"<p>Linear regression with L1 regularization, promoting sparse solutions by shrinking some coefficients to exactly zero.</p> <p>Example: Lasso with 100 features might zero out 80 of them, performing automatic feature selection.</p> <p>See also: Ridge Regression, Regularization</p>"},{"location":"glossary/#latent-space","title":"Latent Space","text":"<p>A lower-dimensional space containing learned representations that capture meaningful variations in data.</p> <p>Example: A variational autoencoder learns a latent space where each dimension controls a meaningful image attribute.</p> <p>See also: Embedding Space, Dimensionality Reduction</p>"},{"location":"glossary/#layer-normalization","title":"Layer Normalization","text":"<p>A technique that normalizes activations across features for each individual sample, independent of batch size.</p> <p>Example: Layer normalization is preferred over batch normalization for transformers and recurrent networks.</p> <p>See also: Batch Normalization, Neural Network Layer</p>"},{"location":"glossary/#learning-rate","title":"Learning Rate","text":"<p>A hyperparameter controlling the step size in gradient-based optimization.</p> <p>Example: A learning rate of 0.001 means each gradient update moves parameters by 0.001 times the gradient.</p> <p>See also: Gradient Descent, SGD</p>"},{"location":"glossary/#least-squares-problem","title":"Least Squares Problem","text":"<p>An optimization problem seeking to minimize the sum of squared differences between observed and predicted values.</p> <p>Example: Fitting a line to scattered points by minimizing the sum of squared vertical distances.</p> <p>See also: Normal Equations, Linear Regression</p>"},{"location":"glossary/#left-null-space","title":"Left Null Space","text":"<p>The null space of a matrix transpose, containing all vectors orthogonal to the row space.</p> <p>Example: Vectors in the left null space of A satisfy A^T x = 0.</p> <p>See also: Null Space, Four Subspaces</p>"},{"location":"glossary/#left-singular-vector","title":"Left Singular Vector","text":"<p>A column of the U matrix in singular value decomposition, representing a direction in the output space.</p> <p>Example: Left singular vectors form an orthonormal basis for the column space of the matrix.</p> <p>See also: SVD, Right Singular Vector</p>"},{"location":"glossary/#lidar-point-cloud","title":"LIDAR Point Cloud","text":"<p>A set of 3D points measured by a laser scanner, representing the geometry of the surrounding environment.</p> <p>Example: Autonomous vehicles use LIDAR point clouds to detect obstacles and map road surfaces.</p> <p>See also: Point Cloud, Sensor Fusion</p>"},{"location":"glossary/#linear-combination","title":"Linear Combination","text":"<p>A sum of vectors multiplied by scalar coefficients.</p> <p>Example: 2\u00b7(1, 0) + 3\u00b7(0, 1) = (2, 3) is a linear combination of the standard basis vectors.</p> <p>See also: Span, Linear Independence</p>"},{"location":"glossary/#linear-dependence","title":"Linear Dependence","text":"<p>A property of a set of vectors where at least one vector can be written as a linear combination of the others.</p> <p>Example: Vectors (1, 2), (2, 4), and (3, 6) are linearly dependent because (3, 6) = 3\u00b7(1, 2).</p> <p>See also: Linear Independence, Span</p>"},{"location":"glossary/#linear-equation","title":"Linear Equation","text":"<p>An equation where variables appear only to the first power with constant coefficients.</p> <p>Example: 3x + 2y - z = 7 is a linear equation in three variables.</p> <p>See also: System of Equations, Matrix Equation Form</p>"},{"location":"glossary/#linear-independence","title":"Linear Independence","text":"<p>A property of a set of vectors where no vector can be written as a linear combination of the others.</p> <p>Example: Vectors (1, 0) and (0, 1) are linearly independent in R\u00b2.</p> <p>See also: Linear Dependence, Basis Vector</p>"},{"location":"glossary/#linear-regression","title":"Linear Regression","text":"<p>A method for modeling the relationship between variables by fitting a linear equation to observed data.</p> <p>Example: Predicting house prices from square footage using a line: price = m \u00d7 sqft + b.</p> <p>See also: Least Squares Problem, Design Matrix</p>"},{"location":"glossary/#linear-transformation","title":"Linear Transformation","text":"<p>A function between vector spaces that preserves vector addition and scalar multiplication.</p> <p>Example: A rotation is a linear transformation because rotating a sum equals the sum of rotations.</p> <p>See also: Transformation Matrix, Matrix</p>"},{"location":"glossary/#localization","title":"Localization","text":"<p>The process of determining an agent's position and orientation within a known map or environment.</p> <p>Example: A robot uses sensor data to estimate it is at coordinates (10, 5) facing north.</p> <p>See also: SLAM, Mapping, State Estimation</p>"},{"location":"glossary/#lora","title":"LoRA","text":"<p>Low-Rank Adaptation, a technique for efficiently fine-tuning large language models by learning low-rank updates to weight matrices.</p> <p>Example: LoRA reduces fine-tuning parameters from billions to millions by decomposing updates as BA where B and A are small matrices.</p> <p>See also: Weight Matrix, Low-Rank Approximation</p>"},{"location":"glossary/#loss-function","title":"Loss Function","text":"<p>A function measuring the discrepancy between model predictions and true values, guiding optimization.</p> <p>Example: Mean squared error loss penalizes large prediction errors quadratically.</p> <p>See also: Cross-Entropy Loss, Gradient Descent</p>"},{"location":"glossary/#low-rank-approximation","title":"Low-Rank Approximation","text":"<p>An approximation of a matrix using a product of smaller matrices, capturing the most significant patterns.</p> <p>Example: Truncating SVD to k singular values gives the best rank-k approximation in Frobenius norm.</p> <p>See also: Truncated SVD, Matrix Rank</p>"},{"location":"glossary/#lower-triangular","title":"Lower Triangular","text":"<p>A matrix where all entries above the main diagonal are zero.</p> <p>Example: The matrix [[2, 0, 0], [3, 1, 0], [4, 5, 6]] is lower triangular.</p> <p>See also: Upper Triangular, Triangular Matrix</p>"},{"location":"glossary/#lu-decomposition","title":"LU Decomposition","text":"<p>Factorization of a matrix into the product of a lower triangular and an upper triangular matrix.</p> <p>Example: LU decomposition enables efficient solving of multiple systems with the same coefficient matrix.</p> <p>See also: Cholesky Decomposition, Gaussian Elimination</p>"},{"location":"glossary/#mapping","title":"Mapping","text":"<p>The process of building a representation of the environment from sensor observations.</p> <p>Example: A robot creates a 2D occupancy grid showing walls and open spaces as it explores.</p> <p>See also: SLAM, Localization</p>"},{"location":"glossary/#matrix","title":"Matrix","text":"<p>A rectangular array of numbers arranged in rows and columns, representing linear transformations or data.</p> <p>Example: A 2\u00d73 matrix has 2 rows and 3 columns, containing 6 entries.</p> <p>See also: Matrix Dimensions, Vector</p>"},{"location":"glossary/#matrix-addition","title":"Matrix Addition","text":"<p>Element-wise addition of two matrices with the same dimensions.</p> <p>Example: [[1, 2], [3, 4]] + [[5, 6], [7, 8]] = [[6, 8], [10, 12]].</p> <p>See also: Matrix, Matrix Scalar Multiply</p>"},{"location":"glossary/#matrix-dimensions","title":"Matrix Dimensions","text":"<p>The number of rows and columns in a matrix, written as m\u00d7n where m is rows and n is columns.</p> <p>Example: A 4\u00d73 matrix has 4 rows and 3 columns.</p> <p>See also: Matrix, Row Vector, Column Vector</p>"},{"location":"glossary/#matrix-entry","title":"Matrix Entry","text":"<p>A single element of a matrix, identified by its row and column position.</p> <p>Example: In matrix A, the entry a\u2082\u2083 is in row 2 and column 3.</p> <p>See also: Matrix, Matrix Notation</p>"},{"location":"glossary/#matrix-equation-form","title":"Matrix Equation Form","text":"<p>Expression of a system of linear equations as a single matrix equation Ax = b.</p> <p>Example: The system 2x + y = 5 and x - y = 1 becomes [[2, 1], [1, -1]]\u00b7[x, y]^T = [5, 1]^T.</p> <p>See also: System of Equations, Augmented Matrix</p>"},{"location":"glossary/#matrix-factorization","title":"Matrix Factorization","text":"<p>Decomposition of a matrix into a product of simpler matrices revealing structure or enabling computation.</p> <p>Example: Factoring a rating matrix into user and item matrices enables recommendation systems.</p> <p>See also: LU Decomposition, SVD</p>"},{"location":"glossary/#matrix-inverse","title":"Matrix Inverse","text":"<p>A matrix A\u207b\u00b9 such that AA\u207b\u00b9 = A\u207b\u00b9A = I, existing only for square invertible matrices.</p> <p>Example: For [[a, b], [c, d]], the inverse is (1/(ad-bc))\u00b7[[d, -b], [-c, a]].</p> <p>See also: Invertible Matrix, Identity Matrix</p>"},{"location":"glossary/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>An operation combining two matrices by taking dot products of rows of the first with columns of the second.</p> <p>Example: A (2\u00d73) matrix times a (3\u00d74) matrix yields a (2\u00d74) matrix.</p> <p>See also: Matrix-Vector Product, Composition of Transforms</p>"},{"location":"glossary/#matrix-notation","title":"Matrix Notation","text":"<p>Conventions for writing matrices, including bracket notation and subscript indexing.</p> <p>Example: A matrix A with element a_{ij} in row i and column j is written as A = [a_{ij}].</p> <p>See also: Matrix, Matrix Entry</p>"},{"location":"glossary/#matrix-rank","title":"Matrix Rank","text":"<p>The maximum number of linearly independent rows or columns in a matrix.</p> <p>Example: A 5\u00d73 matrix has rank at most 3, the smaller of its dimensions.</p> <p>See also: Rank, Column Space, Row Space</p>"},{"location":"glossary/#matrix-scalar-multiply","title":"Matrix Scalar Multiply","text":"<p>Multiplication of every entry of a matrix by a scalar value.</p> <p>Example: 3\u00b7[[1, 2], [3, 4]] = [[3, 6], [9, 12]].</p> <p>See also: Scalar Multiplication, Matrix</p>"},{"location":"glossary/#matrix-transpose","title":"Matrix Transpose","text":"<p>The matrix obtained by interchanging rows and columns of the original matrix.</p> <p>Example: The transpose of [[1, 2, 3], [4, 5, 6]] is [[1, 4], [2, 5], [3, 6]].</p> <p>See also: Symmetric Matrix, Transpose Determinant</p>"},{"location":"glossary/#matrix-vector-product","title":"Matrix-Vector Product","text":"<p>Multiplication of a matrix by a vector, producing a new vector.</p> <p>Example: [[1, 2], [3, 4]]\u00b7[5, 6]^T = [17, 39]^T.</p> <p>See also: Matrix Multiplication, Linear Transformation</p>"},{"location":"glossary/#measurement-vector","title":"Measurement Vector","text":"<p>A vector containing sensor observations at a given time step, used for updating state estimates.</p> <p>Example: A measurement vector might contain GPS position and compass heading readings.</p> <p>See also: State Vector, Kalman Filter</p>"},{"location":"glossary/#mini-batch-sgd","title":"Mini-Batch SGD","text":"<p>Stochastic gradient descent using small batches of training examples to estimate the gradient.</p> <p>Example: With batch size 32, gradients are computed from 32 samples before each parameter update.</p> <p>See also: SGD, Batch Gradient Descent</p>"},{"location":"glossary/#minor","title":"Minor","text":"<p>The determinant of a submatrix formed by deleting one row and one column from a matrix.</p> <p>Example: For a 3\u00d73 matrix, M\u2081\u2082 is the determinant of the 2\u00d72 submatrix with row 1 and column 2 removed.</p> <p>See also: Cofactor, Cofactor Expansion</p>"},{"location":"glossary/#momentum","title":"Momentum","text":"<p>An optimization technique that accelerates gradient descent by accumulating a velocity vector of past gradients.</p> <p>Example: Momentum helps overcome local minima and oscillations by adding inertia to parameter updates.</p> <p>See also: SGD, Adam Optimizer</p>"},{"location":"glossary/#motion-planning","title":"Motion Planning","text":"<p>Computing a sequence of configurations or trajectories to move from start to goal while avoiding obstacles.</p> <p>Example: A robotic arm plans a collision-free path from its current position to grasp an object.</p> <p>See also: Path Planning, Trajectory Optimization</p>"},{"location":"glossary/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Parallel attention mechanisms that attend to different representation subspaces, capturing diverse relationships.</p> <p>Example: With 8 attention heads, a transformer can simultaneously focus on syntax, semantics, and other aspects.</p> <p>See also: Attention Mechanism, Self-Attention</p>"},{"location":"glossary/#multiplicative-property","title":"Multiplicative Property","text":"<p>The property that the determinant of a product equals the product of determinants: det(AB) = det(A)\u00b7det(B).</p> <p>Example: If det(A) = 2 and det(B) = 3, then det(AB) = 6.</p> <p>See also: Determinant, Determinant Properties</p>"},{"location":"glossary/#n-dimensional-vector","title":"N-Dimensional Vector","text":"<p>An ordered list of n real numbers representing a point or direction in n-dimensional space.</p> <p>Example: A 100-dimensional feature vector in machine learning might represent an image or document.</p> <p>See also: 2D Vector, 3D Vector, Vector</p>"},{"location":"glossary/#neural-network-layer","title":"Neural Network Layer","text":"<p>A computational unit in a neural network that transforms input through weights, biases, and activation functions.</p> <p>Example: A dense layer with 128 neurons applies a 128-dimensional linear transformation followed by activation.</p> <p>See also: Hidden Layer, Activation Function</p>"},{"location":"glossary/#neuron-model","title":"Neuron Model","text":"<p>A mathematical abstraction of a biological neuron, computing a weighted sum of inputs plus bias through an activation.</p> <p>Example: A neuron computes output = \u03c3(w\u2081x\u2081 + w\u2082x\u2082 + ... + b) where \u03c3 is the activation function.</p> <p>See also: Perceptron, Activation Function</p>"},{"location":"glossary/#newtons-method","title":"Newton's Method","text":"<p>An optimization algorithm using second-order derivatives to take optimal steps toward a function's minimum.</p> <p>Example: Newton's method converges quadratically near the optimum but requires computing the Hessian.</p> <p>See also: Hessian Matrix, Quasi-Newton Method, BFGS Algorithm</p>"},{"location":"glossary/#no-solution","title":"No Solution","text":"<p>A condition where a system of linear equations has no values satisfying all equations simultaneously.</p> <p>Example: The system x + y = 1 and x + y = 2 has no solution because the planes are parallel.</p> <p>See also: Solution Set, Infinite Solutions</p>"},{"location":"glossary/#non-uniform-scaling","title":"Non-Uniform Scaling","text":"<p>Scaling a transformation that uses different scale factors along different axes.</p> <p>Example: Scaling by 2 in x and 0.5 in y stretches horizontally while compressing vertically.</p> <p>See also: Uniform Scaling, Scaling Matrix</p>"},{"location":"glossary/#normal-equations","title":"Normal Equations","text":"<p>The system A^T A x = A^T b derived from the least squares problem, giving the optimal solution.</p> <p>Example: Solving the normal equations finds the best-fit line coefficients for linear regression.</p> <p>See also: Least Squares Problem, Pseudoinverse</p>"},{"location":"glossary/#norm-from-inner-product","title":"Norm from Inner Product","text":"<p>A vector norm derived from an inner product as ||v|| = \u221a\u27e8v, v\u27e9.</p> <p>Example: The Euclidean norm is induced by the standard dot product: ||v|| = \u221a(v \u00b7 v).</p> <p>See also: Inner Product, L2 Norm</p>"},{"location":"glossary/#null-space","title":"Null Space","text":"<p>The set of all vectors mapped to zero by a matrix, representing the homogeneous solution set.</p> <p>Example: The null space of [[1, 2], [2, 4]] is all multiples of (-2, 1).</p> <p>See also: Kernel, Rank-Nullity Theorem</p>"},{"location":"glossary/#nullity","title":"Nullity","text":"<p>The dimension of the null space, indicating the number of free variables in the homogeneous solution.</p> <p>Example: A 4\u00d75 matrix with rank 3 has nullity 5 - 3 = 2.</p> <p>See also: Null Space, Rank-Nullity Theorem</p>"},{"location":"glossary/#numerical-rank","title":"Numerical Rank","text":"<p>The number of singular values above a tolerance threshold, accounting for floating-point errors.</p> <p>Example: A matrix might have theoretical rank 10 but numerical rank 8 due to tiny singular values from noise.</p> <p>See also: Matrix Rank, Condition Number</p>"},{"location":"glossary/#numerical-stability","title":"Numerical Stability","text":"<p>The property of an algorithm being resistant to accumulation and amplification of rounding errors.</p> <p>Example: Householder QR is more numerically stable than Gram-Schmidt for solving least squares problems.</p> <p>See also: Condition Number, Partial Pivoting</p>"},{"location":"glossary/#object-detection","title":"Object Detection","text":"<p>Computer vision tasks that identify and localize objects of interest within images or video.</p> <p>Example: YOLO detects cars, pedestrians, and traffic signs in autonomous driving applications.</p> <p>See also: Bounding Box, Object Tracking</p>"},{"location":"glossary/#object-tracking","title":"Object Tracking","text":"<p>Following the movement of detected objects across consecutive frames in video sequences.</p> <p>Example: Kalman filters predict object positions between frames to maintain identity through occlusions.</p> <p>See also: Object Detection, Kalman Filter</p>"},{"location":"glossary/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>A square matrix whose columns (and rows) are orthonormal, satisfying Q^T Q = I.</p> <p>Example: A rotation matrix is orthogonal because rotating preserves lengths and angles.</p> <p>See also: Orthonormal Basis, Matrix Inverse</p>"},{"location":"glossary/#orthogonal-projection","title":"Orthogonal Projection","text":"<p>Mapping a vector onto a subspace by dropping the perpendicular component.</p> <p>Example: Projecting (3, 4) onto the x-axis gives (3, 0).</p> <p>See also: Projection, Projection onto Subspace</p>"},{"location":"glossary/#orthogonal-vectors","title":"Orthogonal Vectors","text":"<p>Vectors whose inner product is zero, meaning they are perpendicular.</p> <p>Example: Vectors (1, 1) and (1, -1) are orthogonal because 1\u00d71 + 1\u00d7(-1) = 0.</p> <p>See also: Orthogonality, Dot Product</p>"},{"location":"glossary/#orthogonality","title":"Orthogonality","text":"<p>The property of being perpendicular, characterized by zero inner product between vectors.</p> <p>Orthogonality is fundamental to many algorithms including QR decomposition and principal component analysis.</p> <p>Example: Eigenvectors of a symmetric matrix corresponding to different eigenvalues are orthogonal.</p> <p>See also: Orthogonal Vectors, Orthonormal Basis</p>"},{"location":"glossary/#orthonormal-basis","title":"Orthonormal Basis","text":"<p>A basis consisting of mutually orthogonal unit vectors.</p> <p>Example: The standard basis {(1, 0, 0), (0, 1, 0), (0, 0, 1)} is orthonormal in R\u00b3.</p> <p>See also: Orthonormal Set, Gram-Schmidt Process</p>"},{"location":"glossary/#orthonormal-set","title":"Orthonormal Set","text":"<p>A set of vectors that are mutually orthogonal and all have unit length.</p> <p>Example: The vectors (1, 0) and (0, 1) form an orthonormal set in R\u00b2.</p> <p>See also: Orthonormal Basis, Unit Vector</p>"},{"location":"glossary/#padding","title":"Padding","text":"<p>Adding extra values (typically zeros) around input data to control output dimensions in convolution.</p> <p>Example: Zero-padding a 5\u00d75 image to 7\u00d77 before 3\u00d73 convolution produces a 5\u00d75 output.</p> <p>See also: Convolutional Layer, Stride</p>"},{"location":"glossary/#partial-pivoting","title":"Partial Pivoting","text":"<p>Row swapping during Gaussian elimination to place the largest magnitude element in the pivot position.</p> <p>Example: Partial pivoting improves numerical stability by avoiding division by small numbers.</p> <p>See also: Gaussian Elimination, LU Decomposition</p>"},{"location":"glossary/#path-planning","title":"Path Planning","text":"<p>Determining a route through space from start to goal, typically avoiding obstacles.</p> <p>Example: A* algorithm finds shortest paths on navigation graphs for robotic path planning.</p> <p>See also: Motion Planning, Trajectory Optimization</p>"},{"location":"glossary/#pca","title":"PCA","text":"<p>Principal Component Analysis, a technique that finds orthogonal directions of maximum variance in data.</p> <p>PCA reduces dimensionality while preserving as much variance as possible.</p> <p>Example: PCA on face images extracts eigenfaces, the principal components of facial variation.</p> <p>See also: Principal Component, Variance Explained, Dimensionality Reduction</p>"},{"location":"glossary/#perceptron","title":"Perceptron","text":"<p>The simplest neural network unit, computing a weighted sum of inputs passed through a step function.</p> <p>Example: A perceptron can learn to classify linearly separable patterns like AND and OR functions.</p> <p>See also: Neuron Model, Activation Function</p>"},{"location":"glossary/#perspective-projection","title":"Perspective Projection","text":"<p>Mapping 3D points to 2D image coordinates using perspective geometry where distant objects appear smaller.</p> <p>Example: Parallel lines in 3D appear to converge at a vanishing point in perspective projection.</p> <p>See also: Projection Matrix, Camera Matrix</p>"},{"location":"glossary/#pivot-column","title":"Pivot Column","text":"<p>A column in a matrix that contains a pivot position (leading 1 in row echelon form).</p> <p>Example: In reduced row echelon form, pivot columns correspond to basic variables.</p> <p>See also: Pivot Position, Basic Variable</p>"},{"location":"glossary/#pivot-position","title":"Pivot Position","text":"<p>The location of a leading entry (first nonzero element) in a row of an echelon form matrix.</p> <p>Example: After Gaussian elimination, pivot positions identify which variables are determined.</p> <p>See also: Pivot Column, Row Echelon Form</p>"},{"location":"glossary/#point-cloud","title":"Point Cloud","text":"<p>A set of data points in 3D space, typically from LIDAR or depth sensors, representing surface geometry.</p> <p>Example: A point cloud of a room might contain millions of points representing walls, furniture, and objects.</p> <p>See also: LIDAR Point Cloud, 3D Coordinate System</p>"},{"location":"glossary/#pooling-layer","title":"Pooling Layer","text":"<p>A neural network layer that reduces spatial dimensions by aggregating values within local regions.</p> <p>Example: Max pooling takes the maximum value in each 2\u00d72 region, reducing resolution by half.</p> <p>See also: Convolutional Layer, Stride</p>"},{"location":"glossary/#position-encoding","title":"Position Encoding","text":"<p>Fixed or learned vectors added to embeddings that convey sequence position information.</p> <p>Example: Sinusoidal position encodings allow transformers to understand word order in sentences.</p> <p>See also: Transformer Architecture, Embedding</p>"},{"location":"glossary/#positive-definite-matrix","title":"Positive Definite Matrix","text":"<p>A symmetric matrix where x^T Ax &gt; 0 for all nonzero vectors x, having all positive eigenvalues.</p> <p>Example: Covariance matrices of non-degenerate distributions are positive definite.</p> <p>See also: Cholesky Decomposition, Eigenvalue</p>"},{"location":"glossary/#power-iteration","title":"Power Iteration","text":"<p>An iterative algorithm for computing the dominant eigenvalue and eigenvector of a matrix.</p> <p>Example: Multiplying a random vector by a matrix repeatedly and normalizing converges to the dominant eigenvector.</p> <p>See also: Dominant Eigenvalue, Eigenvalue</p>"},{"location":"glossary/#prediction-step","title":"Prediction Step","text":"<p>The Kalman filter phase that projects the state estimate forward using the system dynamics model.</p> <p>Example: The prediction step estimates where a tracked vehicle will be at the next time step.</p> <p>See also: Update Step, Kalman Filter</p>"},{"location":"glossary/#principal-component","title":"Principal Component","text":"<p>A direction in data space corresponding to maximum variance, computed as an eigenvector of the covariance matrix.</p> <p>Example: The first principal component of height and weight data might point along the \"body size\" direction.</p> <p>See also: PCA, Variance Explained</p>"},{"location":"glossary/#projection","title":"Projection","text":"<p>A linear transformation that maps vectors onto a subspace, reducing dimension.</p> <p>Example: Projecting 3D points onto the xy-plane removes the z-coordinate.</p> <p>See also: Orthogonal Projection, Projection Matrix</p>"},{"location":"glossary/#projection-matrix","title":"Projection Matrix","text":"<p>A matrix P satisfying P\u00b2 = P, used to project vectors onto a subspace.</p> <p>Example: The projection matrix onto a line through unit vector u is P = uu^T.</p> <p>See also: Projection, Orthogonal Projection</p>"},{"location":"glossary/#projection-onto-subspace","title":"Projection onto Subspace","text":"<p>Mapping a vector to the closest point in a subspace, minimizing the distance from the original vector.</p> <p>Example: Projecting a 3D vector onto a plane gives the point in the plane nearest to the vector.</p> <p>See also: Orthogonal Projection, Least Squares Problem</p>"},{"location":"glossary/#pseudoinverse","title":"Pseudoinverse","text":"<p>A generalization of the matrix inverse to non-square or singular matrices, computed from SVD as A\u207a = V\u03a3\u207aU^T.</p> <p>Example: The pseudoinverse solves the least squares problem: x = A\u207ab minimizes ||Ax - b||.</p> <p>See also: SVD, Least Squares Problem</p>"},{"location":"glossary/#quaternion","title":"Quaternion","text":"<p>A four-component hypercomplex number used to represent 3D rotations without gimbal lock.</p> <p>Example: A unit quaternion q = (cos(\u03b8/2), sin(\u03b8/2)\u00b7axis) represents rotation by angle \u03b8 around the given axis.</p> <p>See also: Quaternion Rotation, Euler Angles</p>"},{"location":"glossary/#quaternion-rotation","title":"Quaternion Rotation","text":"<p>Rotation of a 3D vector using quaternion multiplication, avoiding gimbal lock and enabling smooth interpolation.</p> <p>Example: Quaternion rotation is computed as v' = qvq where q is the quaternion conjugate.</p> <p>See also: Quaternion, 3D Rotation</p>"},{"location":"glossary/#query-matrix","title":"Query Matrix","text":"<p>A learned matrix that transforms input embeddings into query vectors for computing attention.</p> <p>Example: In self-attention, each token's query asks \"what information should I gather?\"</p> <p>See also: Key Matrix, Value Matrix, Attention Mechanism</p>"},{"location":"glossary/#quasi-newton-method","title":"Quasi-Newton Method","text":"<p>Optimization methods that approximate second-order information without explicit Hessian computation.</p> <p>Example: BFGS builds a Hessian approximation from gradients observed during optimization.</p> <p>See also: BFGS Algorithm, Newton's Method</p>"},{"location":"glossary/#range","title":"Range","text":"<p>The set of all output vectors that a linear transformation can produce, equivalent to the column space.</p> <p>Example: For a 3\u00d72 matrix with linearly independent columns, the range is a 2D plane in R\u00b3.</p> <p>See also: Column Space, Image</p>"},{"location":"glossary/#rank","title":"Rank","text":"<p>The dimension of the column space (or row space) of a matrix, indicating its linear independence structure.</p> <p>Example: A 5\u00d75 matrix with rank 3 has 3 linearly independent columns.</p> <p>See also: Matrix Rank, Column Space</p>"},{"location":"glossary/#rank-nullity-theorem","title":"Rank-Nullity Theorem","text":"<p>The theorem stating that rank plus nullity equals the number of columns: rank(A) + nullity(A) = n.</p> <p>Example: A 4\u00d76 matrix with rank 4 has nullity 6 - 4 = 2.</p> <p>See also: Rank, Nullity</p>"},{"location":"glossary/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>A matrix form where each pivot is 1, pivots are the only nonzero entries in their columns, and rows are ordered by pivot position.</p> <p>Example: [[1, 0, 2], [0, 1, -1]] is in reduced row echelon form.</p> <p>See also: Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#reflection-matrix","title":"Reflection Matrix","text":"<p>A matrix representing a linear transformation that mirrors vectors across a line, plane, or hyperplane.</p> <p>Example: The matrix [[1, 0], [0, -1]] reflects vectors across the x-axis.</p> <p>See also: Linear Transformation, Transformation Matrix</p>"},{"location":"glossary/#regularization","title":"Regularization","text":"<p>A technique adding penalty terms to loss functions to prevent overfitting by constraining model complexity.</p> <p>Example: L2 regularization adds \u03bb||w||\u00b2 to the loss, shrinking weights toward zero.</p> <p>See also: Ridge Regression, Lasso Regression</p>"},{"location":"glossary/#relu","title":"ReLU","text":"<p>Rectified Linear Unit, an activation function returning max(0, x), popular in deep learning for its simplicity and effectiveness.</p> <p>Example: ReLU(5) = 5 and ReLU(-3) = 0, providing nonlinearity while avoiding vanishing gradients for positive values.</p> <p>See also: Activation Function, Sigmoid, Tanh</p>"},{"location":"glossary/#rgb-image","title":"RGB Image","text":"<p>A color image represented by three channel matrices for red, green, and blue intensity values.</p> <p>Example: Each pixel in an RGB image has three values, like (255, 128, 0) for orange.</p> <p>See also: Grayscale Image, Image Matrix</p>"},{"location":"glossary/#ridge-regression","title":"Ridge Regression","text":"<p>Linear regression with L2 regularization, shrinking coefficients toward zero to prevent overfitting.</p> <p>Example: Ridge regression with \u03bb = 0.1 adds 0.1\u00d7||w||\u00b2 to the least squares loss.</p> <p>See also: Lasso Regression, Regularization</p>"},{"location":"glossary/#right-singular-vector","title":"Right Singular Vector","text":"<p>A column of the V matrix in singular value decomposition, representing a direction in the input space.</p> <p>Example: Right singular vectors form an orthonormal basis for the row space of the matrix.</p> <p>See also: SVD, Left Singular Vector</p>"},{"location":"glossary/#rigid-body-transform","title":"Rigid Body Transform","text":"<p>A transformation preserving distances and angles, consisting of rotation and translation.</p> <p>Example: Moving and rotating a robot arm's end effector is a rigid body transformation.</p> <p>See also: SE3 Transform, Rotation Matrix</p>"},{"location":"glossary/#rmsprop","title":"RMSprop","text":"<p>An adaptive learning rate optimizer that divides the learning rate by a running average of squared gradients.</p> <p>Example: RMSprop adapts to different gradient magnitudes, learning slowly for large gradients and quickly for small ones.</p> <p>See also: Adam Optimizer, SGD</p>"},{"location":"glossary/#rotation-matrix","title":"Rotation Matrix","text":"<p>A square orthogonal matrix with determinant 1 that rotates vectors by a fixed angle around a fixed axis.</p> <p>Example: A 2D rotation by angle \u03b8 is represented by [[cos(\u03b8), -sin(\u03b8)], [sin(\u03b8), cos(\u03b8)]].</p> <p>See also: 2D Rotation, 3D Rotation, Orthogonal Matrix</p>"},{"location":"glossary/#row-addition","title":"Row Addition","text":"<p>An elementary row operation adding a scalar multiple of one row to another row.</p> <p>Example: Subtracting 2 times row 1 from row 2 eliminates the leading entry in row 2.</p> <p>See also: Row Operations, Gaussian Elimination</p>"},{"location":"glossary/#row-echelon-form","title":"Row Echelon Form","text":"<p>A matrix form where each row's leading entry is to the right of the row above, and all-zero rows are at the bottom.</p> <p>Example: [[2, 3, 1], [0, 4, 2], [0, 0, 5]] is in row echelon form.</p> <p>See also: Reduced Row Echelon Form, Gaussian Elimination</p>"},{"location":"glossary/#row-operations","title":"Row Operations","text":"<p>Elementary transformations on matrix rows: swapping rows, scaling rows, or adding multiples of rows.</p> <p>Example: Row operations transform a matrix to row echelon form without changing the solution set.</p> <p>See also: Row Swap, Row Scaling, Row Addition</p>"},{"location":"glossary/#row-scaling","title":"Row Scaling","text":"<p>An elementary row operation multiplying all entries in a row by a nonzero scalar.</p> <p>Example: Multiplying row 2 by 1/4 makes the leading entry 1.</p> <p>See also: Row Operations, Gaussian Elimination</p>"},{"location":"glossary/#row-space","title":"Row Space","text":"<p>The set of all linear combinations of the row vectors of a matrix.</p> <p>Example: For a 3\u00d72 matrix with linearly independent rows, the row space is a 3D subspace of R\u00b3.</p> <p>See also: Column Space, Four Subspaces</p>"},{"location":"glossary/#row-swap","title":"Row Swap","text":"<p>An elementary row operation exchanging two rows of a matrix.</p> <p>Example: Swapping rows 1 and 2 places the row with larger leading entry on top.</p> <p>See also: Row Operations, Partial Pivoting</p>"},{"location":"glossary/#row-vector","title":"Row Vector","text":"<p>A matrix with a single row, representing a vector as a horizontal array of numbers.</p> <p>Example: The row vector [1, 2, 3] has one row and three columns.</p> <p>See also: Column Vector, Vector</p>"},{"location":"glossary/#scalar","title":"Scalar","text":"<p>A single numerical value, as opposed to a vector or matrix.</p> <p>Example: In the expression 3v, the number 3 is a scalar multiplying vector v.</p> <p>See also: Scalar Multiplication, Vector</p>"},{"location":"glossary/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiplication of a vector by a scalar, scaling all components by that factor.</p> <p>Example: 3 \u00d7 (1, 2, 3) = (3, 6, 9).</p> <p>See also: Scalar, Vector</p>"},{"location":"glossary/#scaling-matrix","title":"Scaling Matrix","text":"<p>A diagonal matrix that scales vectors by different factors along different axes.</p> <p>Example: The diagonal matrix [[2, 0], [0, 3]] scales x by 2 and y by 3.</p> <p>See also: Uniform Scaling, Non-Uniform Scaling</p>"},{"location":"glossary/#scree-plot","title":"Scree Plot","text":"<p>A graph of eigenvalues or variance explained in decreasing order, used to determine how many components to retain.</p> <p>Example: A scree plot showing a sharp \"elbow\" at component 5 suggests retaining 5 principal components.</p> <p>See also: PCA, Variance Explained</p>"},{"location":"glossary/#se3-transform","title":"SE3 Transform","text":"<p>The Special Euclidean group in 3D, representing rigid body transformations as rotation plus translation.</p> <p>Example: An SE3 transform combines a 3\u00d73 rotation matrix and a translation vector into a 4\u00d74 homogeneous matrix.</p> <p>See also: Rigid Body Transform, Homogeneous Coordinates</p>"},{"location":"glossary/#self-attention","title":"Self-Attention","text":"<p>An attention mechanism where queries, keys, and values all derive from the same sequence.</p> <p>Example: In self-attention, each word attends to all other words in the same sentence.</p> <p>See also: Attention Mechanism, Cross-Attention</p>"},{"location":"glossary/#semantic-similarity","title":"Semantic Similarity","text":"<p>The degree to which two pieces of text or concepts share meaning, measured by embedding proximity.</p> <p>Example: \"Happy\" and \"joyful\" have high semantic similarity, reflected in similar embedding vectors.</p> <p>See also: Cosine Similarity, Embedding</p>"},{"location":"glossary/#sensor-fusion","title":"Sensor Fusion","text":"<p>Combining data from multiple sensors to achieve more accurate and reliable state estimates.</p> <p>Example: Fusing GPS, IMU, and camera data improves vehicle localization compared to any single sensor.</p> <p>See also: Kalman Filter, LIDAR Point Cloud</p>"},{"location":"glossary/#sgd","title":"SGD","text":"<p>Stochastic Gradient Descent, an optimization algorithm that updates parameters using gradients from random single samples.</p> <p>Example: SGD makes many small updates per epoch rather than one large update, introducing beneficial noise.</p> <p>See also: Batch Gradient Descent, Mini-Batch SGD</p>"},{"location":"glossary/#sharpen-filter","title":"Sharpen Filter","text":"<p>A convolution kernel that enhances edges and details by emphasizing high-frequency components.</p> <p>Example: A sharpening kernel subtracts blurred versions from the original, boosting differences.</p> <p>See also: Image Convolution, Blur Filter</p>"},{"location":"glossary/#shear-matrix","title":"Shear Matrix","text":"<p>A transformation matrix that skews shapes by shifting points parallel to an axis by an amount proportional to their distance from it.</p> <p>Example: A horizontal shear slides the top of a square right while keeping the bottom fixed, creating a parallelogram.</p> <p>See also: Transformation Matrix, Linear Transformation</p>"},{"location":"glossary/#sigmoid","title":"Sigmoid","text":"<p>An S-shaped activation function mapping real values to the range (0, 1), defined as \u03c3(x) = 1/(1 + e^(-x)).</p> <p>Example: Sigmoid outputs 0.5 at x = 0, approaching 1 for large positive x and 0 for large negative x.</p> <p>See also: Activation Function, Tanh</p>"},{"location":"glossary/#signed-area","title":"Signed Area","text":"<p>The determinant of a 2\u00d72 matrix, representing the oriented area of the parallelogram spanned by two vectors.</p> <p>Example: Vectors (2, 0) and (0, 3) span a parallelogram with signed area 2\u00d73 - 0\u00d70 = 6.</p> <p>See also: 2x2 Determinant, Volume Scaling Factor</p>"},{"location":"glossary/#similar-matrices","title":"Similar Matrices","text":"<p>Matrices A and B related by B = P\u207b\u00b9AP for some invertible P, sharing eigenvalues and determinant.</p> <p>Example: A matrix and its diagonalization are similar, connected by the eigenvector matrix.</p> <p>See also: Diagonalization, Eigenvalue</p>"},{"location":"glossary/#singular-matrix","title":"Singular Matrix","text":"<p>A square matrix with determinant zero, having no inverse and reduced rank.</p> <p>Example: [[1, 2], [2, 4]] is singular because row 2 is twice row 1.</p> <p>See also: Determinant, Invertible Matrix</p>"},{"location":"glossary/#singular-value","title":"Singular Value","text":"<p>A non-negative value on the diagonal of \u03a3 in SVD, equal to the square root of an eigenvalue of A^T A.</p> <p>Example: The largest singular value indicates the maximum scaling factor applied by the matrix.</p> <p>See also: SVD, Left Singular Vector, Right Singular Vector</p>"},{"location":"glossary/#slam","title":"SLAM","text":"<p>Simultaneous Localization and Mapping, algorithms that build a map while tracking the agent's position within it.</p> <p>Example: A robot vacuum uses SLAM to create a floor plan while cleaning, avoiding revisiting areas.</p> <p>See also: Localization, Mapping</p>"},{"location":"glossary/#sobel-operator","title":"Sobel Operator","text":"<p>A convolution kernel for edge detection computing horizontal and vertical intensity gradients.</p> <p>Example: The Sobel x-kernel [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]] detects vertical edges.</p> <p>See also: Edge Detection, Image Convolution</p>"},{"location":"glossary/#softmax","title":"Softmax","text":"<p>An activation function that converts a vector of values into a probability distribution summing to 1.</p> <p>Example: Softmax([1, 2, 3]) \u2248 [0.09, 0.24, 0.67], emphasizing the largest value.</p> <p>See also: Activation Function, Attention Weights</p>"},{"location":"glossary/#solution-set","title":"Solution Set","text":"<p>The set of all variable assignments satisfying a system of equations.</p> <p>Example: A system might have a solution set that is a single point, a line, a plane, or empty.</p> <p>See also: Unique Solution, Infinite Solutions, No Solution</p>"},{"location":"glossary/#span","title":"Span","text":"<p>The set of all possible linear combinations of a given set of vectors.</p> <p>Example: The span of (1, 0) and (0, 1) is all of R\u00b2.</p> <p>See also: Linear Combination, Basis Vector</p>"},{"location":"glossary/#sparse-matrix","title":"Sparse Matrix","text":"<p>A matrix where most elements are zero, stored using specialized formats for efficiency.</p> <p>Example: A 10000\u00d710000 matrix with only 50000 nonzero entries is 99.95% sparse.</p> <p>See also: Dense Matrix, Matrix</p>"},{"location":"glossary/#spectral-theorem","title":"Spectral Theorem","text":"<p>A theorem stating that symmetric matrices have orthonormal eigenvectors and real eigenvalues, enabling orthogonal diagonalization.</p> <p>Example: Any real symmetric matrix A can be written as Q\u039bQ^T where Q is orthogonal.</p> <p>See also: Symmetric Eigenvalues, Eigendecomposition</p>"},{"location":"glossary/#standard-basis","title":"Standard Basis","text":"<p>The set of unit vectors along coordinate axes, forming the default basis for R^n.</p> <p>Example: In R\u00b3, the standard basis is {(1, 0, 0), (0, 1, 0), (0, 0, 1)}.</p> <p>See also: Basis Vector, Coordinate System</p>"},{"location":"glossary/#standardization","title":"Standardization","text":"<p>Transforming features to have zero mean and unit variance for consistent scaling.</p> <p>Example: Standardized values are computed as z = (x - \u03bc)/\u03c3 for each feature.</p> <p>See also: Feature Matrix, PCA</p>"},{"location":"glossary/#state-estimation","title":"State Estimation","text":"<p>The process of inferring hidden system states from noisy observations using probabilistic models.</p> <p>Example: Estimating a drone's actual position from GPS measurements with known error characteristics.</p> <p>See also: Kalman Filter, State Vector</p>"},{"location":"glossary/#state-vector","title":"State Vector","text":"<p>A vector containing all variables needed to describe the current state of a dynamic system.</p> <p>Example: A tracking state vector might be [x, y, vx, vy] for position and velocity.</p> <p>See also: State Estimation, Kalman Filter</p>"},{"location":"glossary/#stereo-vision","title":"Stereo Vision","text":"<p>Depth perception using two cameras, analogous to human binocular vision.</p> <p>Example: Stereo matching finds corresponding points in left and right images to compute depth.</p> <p>See also: Triangulation, Epipolar Geometry</p>"},{"location":"glossary/#stride","title":"Stride","text":"<p>The step size by which a convolution kernel moves across the input, affecting output dimensions.</p> <p>Example: With stride 2, the kernel moves 2 pixels at a time, reducing output size by half.</p> <p>See also: Convolutional Layer, Padding</p>"},{"location":"glossary/#subspace","title":"Subspace","text":"<p>A subset of a vector space that is itself a vector space under the same operations.</p> <p>Example: The xy-plane in R\u00b3 is a subspace: any linear combination of xy-plane vectors stays in the plane.</p> <p>See also: Vector Space, Span</p>"},{"location":"glossary/#svd","title":"SVD","text":"<p>Singular Value Decomposition, a factorization A = U\u03a3V^T revealing the fundamental structure of any matrix.</p> <p>SVD is one of the most important decompositions in applied linear algebra, with applications in dimensionality reduction, compression, and pseudoinverse computation.</p> <p>Example: Truncating SVD provides the best low-rank matrix approximation.</p> <p>See also: Singular Value, Left Singular Vector, Right Singular Vector</p>"},{"location":"glossary/#symmetric-eigenvalues","title":"Symmetric Eigenvalues","text":"<p>The property that symmetric matrices have only real eigenvalues (no complex values).</p> <p>Example: The symmetric matrix [[1, 2], [2, 1]] has real eigenvalues 3 and -1.</p> <p>See also: Spectral Theorem, Symmetric Matrix</p>"},{"location":"glossary/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A square matrix equal to its transpose, meaning a_{ij} = a_{ji} for all entries.</p> <p>Example: Covariance matrices are symmetric because Cov(X, Y) = Cov(Y, X).</p> <p>See also: Matrix Transpose, Spectral Theorem</p>"},{"location":"glossary/#system-of-equations","title":"System of Equations","text":"<p>A collection of linear equations that must all be satisfied simultaneously.</p> <p>Example: The system 2x + y = 5, x - y = 1 has two equations in two unknowns.</p> <p>See also: Linear Equation, Matrix Equation Form</p>"},{"location":"glossary/#tanh","title":"Tanh","text":"<p>Hyperbolic tangent activation function mapping values to (-1, 1), defined as tanh(x) = (e^x - e^(-x))/(e^x + e^(-x)).</p> <p>Example: Tanh is centered at zero unlike sigmoid, making it often preferable for hidden layers.</p> <p>See also: Activation Function, Sigmoid, ReLU</p>"},{"location":"glossary/#tensor","title":"Tensor","text":"<p>A multi-dimensional array generalizing scalars, vectors, and matrices to higher dimensions.</p> <p>Example: A 3D tensor with shape (batch_size, height, width) stores a batch of grayscale images.</p> <p>See also: Tensor Operations, Image Tensor</p>"},{"location":"glossary/#tensor-operations","title":"Tensor Operations","text":"<p>Mathematical operations on tensors including element-wise operations, contractions, and products.</p> <p>Example: Batched matrix multiplication multiplies corresponding matrices across the batch dimension.</p> <p>See also: Tensor, Matrix Multiplication</p>"},{"location":"glossary/#trajectory-optimization","title":"Trajectory Optimization","text":"<p>Computing optimal paths through state space that minimize cost while satisfying dynamics and constraints.</p> <p>Example: Planning a fuel-efficient trajectory for a spacecraft to reach Mars.</p> <p>See also: Path Planning, Motion Planning</p>"},{"location":"glossary/#transformation-matrix","title":"Transformation Matrix","text":"<p>A matrix that represents a linear transformation, mapping input vectors to output vectors.</p> <p>Example: A 2D rotation by 45\u00b0 has transformation matrix [[\u221a2/2, -\u221a2/2], [\u221a2/2, \u221a2/2]].</p> <p>See also: Linear Transformation, Rotation Matrix</p>"},{"location":"glossary/#transformer-architecture","title":"Transformer Architecture","text":"<p>A neural network architecture using self-attention to process sequences in parallel without recurrence.</p> <p>Transformers are the foundation of modern large language models and many vision systems.</p> <p>Example: GPT models use transformer decoders to generate text one token at a time.</p> <p>See also: Self-Attention, Multi-Head Attention, Position Encoding</p>"},{"location":"glossary/#transpose-determinant","title":"Transpose Determinant","text":"<p>The property that the determinant of a transpose equals the original determinant: det(A^T) = det(A).</p> <p>Example: This property allows cofactor expansion along any row or column.</p> <p>See also: Determinant, Matrix Transpose</p>"},{"location":"glossary/#triangular-matrix","title":"Triangular Matrix","text":"<p>A square matrix where all entries either above or below the main diagonal are zero.</p> <p>Example: Triangular systems can be solved efficiently using back or forward substitution.</p> <p>See also: Upper Triangular, Lower Triangular</p>"},{"location":"glossary/#triangulation","title":"Triangulation","text":"<p>Computing 3D point positions from multiple 2D observations using geometric constraints.</p> <p>Example: With two calibrated cameras, triangulation determines the 3D location of a visible feature.</p> <p>See also: Stereo Vision, Epipolar Geometry</p>"},{"location":"glossary/#trivial-solution","title":"Trivial Solution","text":"<p>The zero solution x = 0 that always satisfies a homogeneous system of equations.</p> <p>Example: The system 2x + 3y = 0 always has the trivial solution x = 0, y = 0.</p> <p>See also: Homogeneous System, Null Space</p>"},{"location":"glossary/#truncated-svd","title":"Truncated SVD","text":"<p>Singular value decomposition keeping only the k largest singular values and corresponding vectors.</p> <p>Example: Truncated SVD with k = 100 reduces a million-dimensional space to 100 dimensions.</p> <p>See also: SVD, Low-Rank Approximation, Dimensionality Reduction</p>"},{"location":"glossary/#uniform-scaling","title":"Uniform Scaling","text":"<p>Scaling by the same factor in all directions, preserving shape while changing size.</p> <p>Example: Uniform scaling by factor 2 doubles all distances, making a circle twice as large.</p> <p>See also: Non-Uniform Scaling, Scaling Matrix</p>"},{"location":"glossary/#unique-solution","title":"Unique Solution","text":"<p>A system of equations having exactly one solution.</p> <p>Example: Two non-parallel lines in 2D have a unique intersection point.</p> <p>See also: Solution Set, No Solution, Infinite Solutions</p>"},{"location":"glossary/#unit-vector","title":"Unit Vector","text":"<p>A vector with magnitude (length) equal to one.</p> <p>Example: The unit vector in the direction of (3, 4) is (3/5, 4/5) = (0.6, 0.8).</p> <p>See also: Vector Normalization, Vector Magnitude</p>"},{"location":"glossary/#update-step","title":"Update Step","text":"<p>The Kalman filter phase that corrects the prediction using new measurements and the Kalman gain.</p> <p>Example: When a GPS measurement arrives, the update step adjusts the position estimate.</p> <p>See also: Prediction Step, Kalman Gain</p>"},{"location":"glossary/#upper-triangular","title":"Upper Triangular","text":"<p>A matrix where all entries below the main diagonal are zero.</p> <p>Example: The matrix [[1, 2, 3], [0, 4, 5], [0, 0, 6]] is upper triangular.</p> <p>See also: Lower Triangular, Triangular Matrix</p>"},{"location":"glossary/#value-matrix","title":"Value Matrix","text":"<p>A learned matrix that transforms input embeddings into value vectors aggregated by attention weights.</p> <p>Example: Values contain the information that attention mechanisms choose to pass forward.</p> <p>See also: Query Matrix, Key Matrix, Attention Mechanism</p>"},{"location":"glossary/#variance-explained","title":"Variance Explained","text":"<p>The proportion of total data variance captured by principal components, guiding dimensionality reduction decisions.</p> <p>Example: If the first 10 components explain 95% of variance, 90% of features can be discarded.</p> <p>See also: PCA, Scree Plot</p>"},{"location":"glossary/#vector","title":"Vector","text":"<p>An ordered list of numbers representing magnitude and direction in a space.</p> <p>Example: The velocity (5, 3) represents movement of 5 units/s in x and 3 units/s in y.</p> <p>See also: 2D Vector, 3D Vector, Vector Space</p>"},{"location":"glossary/#vector-addition","title":"Vector Addition","text":"<p>Adding two vectors by summing their corresponding components.</p> <p>Example: (1, 2) + (3, 4) = (4, 6).</p> <p>See also: Vector, Vector Subtraction</p>"},{"location":"glossary/#vector-magnitude","title":"Vector Magnitude","text":"<p>The length of a vector, computed as the square root of the sum of squared components.</p> <p>Example: The magnitude of vector (3, 4) is \u221a(9 + 16) = 5.</p> <p>See also: L2 Norm, Unit Vector</p>"},{"location":"glossary/#vector-normalization","title":"Vector Normalization","text":"<p>Dividing a vector by its magnitude to produce a unit vector with the same direction.</p> <p>Example: Normalizing (3, 4) gives (3/5, 4/5) with magnitude 1.</p> <p>See also: Unit Vector, Vector Magnitude</p>"},{"location":"glossary/#vector-notation","title":"Vector Notation","text":"<p>Conventions for writing vectors, including bold letters, arrows, or component lists.</p> <p>Example: The vector v = (v\u2081, v\u2082, v\u2083) can also be written as v or with an arrow above.</p> <p>See also: Vector, Matrix Notation</p>"},{"location":"glossary/#vector-space","title":"Vector Space","text":"<p>A collection of vectors with defined addition and scalar multiplication operations satisfying axioms.</p> <p>Example: R\u00b2 with standard vector addition and scalar multiplication is a vector space.</p> <p>See also: Abstract Vector Space, Vector Space Axioms</p>"},{"location":"glossary/#vector-space-axioms","title":"Vector Space Axioms","text":"<p>The eight properties that addition and scalar multiplication must satisfy for a set to be a vector space.</p> <p>Example: The axioms include commutativity, associativity, and the existence of zero and inverse vectors.</p> <p>See also: Vector Space, Abstract Vector Space</p>"},{"location":"glossary/#vector-subtraction","title":"Vector Subtraction","text":"<p>Subtracting two vectors by computing the difference of corresponding components.</p> <p>Example: (5, 7) - (2, 3) = (3, 4).</p> <p>See also: Vector Addition, Vector</p>"},{"location":"glossary/#volume-scaling-factor","title":"Volume Scaling Factor","text":"<p>The absolute value of a determinant, indicating how much a transformation scales volumes.</p> <p>Example: A matrix with determinant 3 triples the volume of any region it transforms.</p> <p>See also: Determinant, Signed Area</p>"},{"location":"glossary/#weight-matrix","title":"Weight Matrix","text":"<p>A matrix of learnable parameters connecting layers in a neural network.</p> <p>Example: A layer connecting 100 inputs to 50 outputs has a 50\u00d7100 weight matrix.</p> <p>See also: Bias Vector, Neural Network Layer</p>"},{"location":"glossary/#word-embedding","title":"Word Embedding","text":"<p>A vector representation of a word learned from text data, capturing semantic meaning.</p> <p>Example: Word2Vec learns that \"king\" - \"man\" + \"woman\" \u2248 \"queen\" in embedding space.</p> <p>See also: Embedding, Semantic Similarity</p>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"license/#commercial-licensing","title":"Commercial Licensing","text":"<p>Commercial rights are reserved by the copyright holder. For commercial licensing, publication inquiries, or permission to use this work in commercial contexts, please contact Dan McCreary on LinkedIn.</p>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 15 chapters covering 300 concepts across four major parts.</p>"},{"location":"chapters/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":"<ol> <li> <p>Vectors and Vector Spaces - Introduces vectors as fundamental building blocks, covering operations, norms, and vector space theory.</p> </li> <li> <p>Matrices and Matrix Operations - Covers matrix notation, special types, and core operations including multiplication and inverse.</p> </li> <li> <p>Systems of Linear Equations - Methods for representing and solving linear systems using elimination and matrix techniques.</p> </li> <li> <p>Linear Transformations - How matrices represent geometric transformations including rotation, scaling, and projection.</p> </li> </ol>"},{"location":"chapters/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":"<ol> <li> <p>Determinants and Matrix Properties - Determinant computation, properties, and geometric interpretation.</p> </li> <li> <p>Eigenvalues and Eigenvectors - Eigentheory fundamentals including characteristic polynomials and diagonalization.</p> </li> <li> <p>Matrix Decompositions - Matrix factorization methods including LU, QR, Cholesky, and SVD.</p> </li> <li> <p>Vector Spaces and Inner Products - Abstract inner product spaces, orthogonality, and least squares.</p> </li> </ol>"},{"location":"chapters/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":"<ol> <li> <p>Machine Learning Foundations - Data representation, PCA, linear regression, and gradient descent.</p> </li> <li> <p>Neural Networks and Deep Learning - Deep learning architecture, weight matrices, and backpropagation.</p> </li> <li> <p>Generative AI and Large Language Models - Embeddings, attention mechanisms, and transformer architecture.</p> </li> <li> <p>Optimization and Learning Algorithms - Optimization algorithms for training machine learning models.</p> </li> </ol>"},{"location":"chapters/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":"<ol> <li> <p>Image Processing and Computer Vision - Image representation, convolution, filtering, and feature detection.</p> </li> <li> <p>3D Geometry and Transformations - Three-dimensional geometry, quaternions, and camera models.</p> </li> <li> <p>Autonomous Systems and Sensor Fusion - Kalman filtering, SLAM, and autonomous navigation.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>This textbook is designed for sequential learning - each chapter builds on concepts from previous chapters. The dependency structure ensures that prerequisite knowledge is always introduced before it's needed. While you can jump ahead to specific topics of interest, completing earlier chapters first will provide the strongest foundation.</p> <p>Each chapter includes a list of concepts covered, allowing you to track your progress through the learning graph. Interactive microsimulations throughout the book help reinforce abstract mathematical concepts with visual, hands-on exploration.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/","title":"Vectors and Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#summary","title":"Summary","text":"<p>This chapter introduces vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces. By the end of this chapter, you will understand how to perform vector operations, compute norms and distances, and work with the abstract concepts of span, linear independence, and basis vectors that form the foundation for all subsequent topics.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description: College Algebra and familiarity with basic calculus concepts.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#introduction","title":"Introduction","text":"<p>Linear algebra provides the mathematical foundation for modern artificial intelligence, machine learning, and computer vision. At the heart of this mathematical framework lies the vector\u2014an elegant structure that represents both magnitude and direction. Whether you're representing a data point in a machine learning model, describing the velocity of an autonomous vehicle, or encoding the semantic meaning of a word, vectors serve as your fundamental tool.</p> <p>This chapter establishes the foundational concepts you'll build upon throughout this course. We begin with the simple distinction between scalars and vectors, then progressively develop your understanding through vector operations, norms, and the powerful abstractions of vector spaces.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalars-and-vectors","title":"Scalars and Vectors","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-scalars","title":"Understanding Scalars","text":"<p>A scalar is a single numerical value that represents a quantity with magnitude only. Scalars are the numbers you've worked with throughout your mathematical education\u2014they can be positive, negative, or zero. In the context of linear algebra, scalars typically come from a field (usually the real numbers \\(\\mathbb{R}\\) or complex numbers \\(\\mathbb{C}\\)).</p> <p>Examples of scalars include:</p> <ul> <li>Temperature: 72\u00b0F</li> <li>Mass: 5.2 kg</li> <li>Speed: 60 mph</li> <li>Count: 42 items</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#what-is-a-vector","title":"What is a Vector?","text":"<p>A vector is an ordered collection of scalars that represents both magnitude and direction. While scalars tell us \"how much,\" vectors tell us \"how much and in which direction.\" Geometrically, we can visualize a vector as an arrow in space\u2014the length represents magnitude, and the arrowhead indicates direction.</p> <p>More formally, a vector in \\(n\\)-dimensional space is an ordered \\(n\\)-tuple of real numbers:</p> \\[\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\] <p>where each \\(v_i\\) is a scalar component of the vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-notation","title":"Vector Notation","text":"<p>Mathematical texts use several conventions for denoting vectors. Throughout this course, we use the following:</p> Notation Description Example Bold lowercase Standard textbook notation \\(\\mathbf{v}\\), \\(\\mathbf{u}\\), \\(\\mathbf{w}\\) Arrow above Handwritten notation \\(\\vec{v}\\), \\(\\vec{u}\\) Column matrix Computational notation \\(\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) Row matrix Alternative notation \\([v_1 \\quad v_2]\\) Component form Explicit values \\((3, 4)\\) or \\(\\langle 3, 4 \\rangle\\) <p>Notation in Code</p> <p>In programming contexts like NumPy, vectors are typically represented as one-dimensional arrays: <code>v = np.array([3, 4, 5])</code>.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vectors-in-different-dimensions","title":"Vectors in Different Dimensions","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#2d-vectors","title":"2D Vectors","text":"<p>A 2D vector exists in a two-dimensional plane and consists of two components. We write a 2D vector as:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}\\] <p>where \\(v_x\\) is the horizontal component and \\(v_y\\) is the vertical component. Geometrically, this represents an arrow from the origin to the point \\((v_x, v_y)\\).</p> <p>Common applications of 2D vectors include:</p> <ul> <li>Position coordinates on a screen</li> <li>Velocity in a plane</li> <li>Force components in 2D physics simulations</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#3d-vectors","title":"3D Vectors","text":"<p>A 3D vector extends into three-dimensional space with three components:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix}\\] <p>The additional \\(z\\)-component allows us to represent quantities in our three-dimensional world, such as the position of a drone or the direction of a camera in virtual reality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-2d-and-3d-vector-visualization","title":"Diagram: 2D and 3D Vector Visualization","text":"2D and 3D Vector Visualization MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, visualize</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p> <p>Canvas layout: - Left panel (500px): 2D vector visualization with coordinate grid - Right panel (400px): 3D vector visualization with rotatable axes - Bottom controls: Component sliders and toggle switches</p> <p>Visual elements: - 2D coordinate grid with x and y axes, grid lines at unit intervals - 3D coordinate system with x, y, z axes (perspective view) - Vector arrow from origin to point, with distinct arrowhead - Dashed projection lines from vector tip to each axis - Component labels showing current values - Origin point clearly marked</p> <p>Interactive controls: - Slider: x-component (-5 to 5, default 3) - Slider: y-component (-5 to 5, default 4) - Slider: z-component (-5 to 5, default 2) for 3D view - Toggle: Show/hide projection lines - Toggle: Show/hide component labels - Button: Switch between 2D and 3D views - For 3D: Mouse drag to rotate view</p> <p>Default parameters: - 2D vector: (3, 4) - 3D vector: (3, 4, 2) - Projection lines: visible - Component labels: visible</p> <p>Behavior: - As sliders change, vector arrow updates in real-time - Projection lines dynamically connect to axis intersections - Magnitude value updates and displays - 3D view allows full rotation with mouse drag</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#n-dimensional-vectors","title":"N-Dimensional Vectors","text":"<p>An N-dimensional vector generalizes to any number of dimensions:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\] <p>While we cannot visualize vectors beyond three dimensions geometrically, the mathematical operations extend naturally. In machine learning, it's common to work with vectors having hundreds or thousands of dimensions:</p> <ul> <li>Word embeddings: 300-dimensional vectors representing word meanings</li> <li>Image features: 2048-dimensional vectors from convolutional neural networks</li> <li>User preference vectors: 100+ dimensions in recommendation systems</li> </ul> <p>The power of linear algebra is that the same operations and theorems apply regardless of dimensionality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#basic-vector-operations","title":"Basic Vector Operations","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-addition","title":"Vector Addition","text":"<p>Vector addition combines two vectors of the same dimension by adding their corresponding components:</p> \\[\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\\] <p>Geometrically, vector addition follows the parallelogram rule or the tip-to-tail method. If you place the tail of \\(\\mathbf{v}\\) at the tip of \\(\\mathbf{u}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) points from the tail of \\(\\mathbf{u}\\) to the tip of \\(\\mathbf{v}\\).</p> <p>Vector addition has the following properties:</p> <ul> <li>Commutative: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associative: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Scalar multiplication multiplies each component of a vector by a scalar value:</p> \\[c \\cdot \\mathbf{v} = c \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} c \\cdot v_1 \\\\ c \\cdot v_2 \\\\ \\vdots \\\\ c \\cdot v_n \\end{bmatrix}\\] <p>The geometric effect of scalar multiplication:</p> <ul> <li>\\(c &gt; 1\\): Stretches the vector (increases magnitude)</li> <li>\\(0 &lt; c &lt; 1\\): Shrinks the vector (decreases magnitude)</li> <li>\\(c = 0\\): Produces the zero vector</li> <li>\\(c &lt; 0\\): Reverses direction and scales magnitude</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-subtraction","title":"Vector Subtraction","text":"<p>Vector subtraction is defined in terms of addition and scalar multiplication:</p> \\[\\mathbf{u} - \\mathbf{v} = \\mathbf{u} + (-1)\\mathbf{v}\\] <p>This yields component-wise subtraction:</p> \\[\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_n - v_n \\end{bmatrix}\\]"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-operations-playground","title":"Diagram: Vector Operations Playground","text":"Vector Operations Playground MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, practice, demonstrate</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p> <p>Canvas layout: - Main area (600px): Coordinate grid showing vectors - Right panel (200px): Controls and result display - Bottom: Operation selector and calculation display</p> <p>Visual elements: - Coordinate grid with axes from -10 to 10 - Vector u (blue arrow) with adjustable endpoint - Vector v (red arrow) with adjustable endpoint - Result vector (green arrow) showing operation result - Parallelogram outline when showing addition (dashed lines) - Component labels on each vector - Magnitude display for each vector</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Radio buttons: Addition, Subtraction, Scalar Multiply - Slider: Scalar value c (-3 to 3, default 2) for scalar multiplication - Checkbox: Show parallelogram construction - Checkbox: Show component breakdown - Button: Animate operation step-by-step - Button: Reset to defaults</p> <p>Default parameters: - Vector u: (3, 2) - Vector v: (1, 4) - Operation: Addition - Scalar: 2 - Show parallelogram: true</p> <p>Behavior: - Dragging vector endpoints updates all calculations in real-time - Operation result vector updates immediately - Step-by-step animation shows geometric construction - Component breakdown displays numerical computation - Parallelogram appears for addition to show geometric interpretation</p> <p>Implementation: p5.js with drag-and-drop interaction</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-products","title":"Vector Products","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#the-dot-product","title":"The Dot Product","text":"<p>The dot product (also called the inner product or scalar product) takes two vectors of the same dimension and returns a scalar. For vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\):</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n\\] <p>The geometric interpretation reveals the dot product's significance:</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos\\theta\\] <p>where \\(\\theta\\) is the angle between the vectors and \\(\\|\\cdot\\|\\) denotes magnitude. This relationship connects algebraic computation with geometric meaning.</p> <p>Key properties and insights:</p> Condition Geometric Meaning Dot Product Value \\(\\theta = 0\u00b0\\) Vectors point same direction Maximum positive \\(\\theta = 90\u00b0\\) Vectors are perpendicular Zero \\(\\theta = 180\u00b0\\) Vectors point opposite directions Maximum negative <p>The dot product has important applications:</p> <ul> <li>Computing angles between vectors</li> <li>Projecting one vector onto another</li> <li>Calculating work in physics (force \u00b7 displacement)</li> <li>Measuring similarity between feature vectors</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-cross-product","title":"The Cross Product","text":"<p>The cross product is defined only for 3D vectors and produces a vector perpendicular to both input vectors:</p> \\[\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_2 v_3 - u_3 v_2 \\\\ u_3 v_1 - u_1 v_3 \\\\ u_1 v_2 - u_2 v_1 \\end{bmatrix}\\] <p>The magnitude of the cross product equals the area of the parallelogram formed by the two vectors:</p> \\[\\|\\mathbf{u} \\times \\mathbf{v}\\| = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\sin\\theta\\] <p>The direction follows the right-hand rule: if you curl your fingers from \\(\\mathbf{u}\\) toward \\(\\mathbf{v}\\), your thumb points in the direction of \\(\\mathbf{u} \\times \\mathbf{v}\\).</p> <p>Important properties of the cross product:</p> <ul> <li>Anti-commutative: \\(\\mathbf{u} \\times \\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u})\\)</li> <li>Not associative: \\((\\mathbf{u} \\times \\mathbf{v}) \\times \\mathbf{w} \\neq \\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w})\\)</li> <li>Self-cross is zero: \\(\\mathbf{v} \\times \\mathbf{v} = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-dot-product-and-cross-product-visualizer","title":"Diagram: Dot Product and Cross Product Visualizer","text":"Dot Product and Cross Product Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare, differentiate</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p> <p>Canvas layout: - Left panel (400px): 2D view for dot product with projection visualization - Right panel (400px): 3D view for cross product with parallelogram and result vector - Bottom panel: Numerical results and formulas</p> <p>Visual elements: - 2D panel: Two vectors, projection line, angle arc, shaded projection component - 3D panel: Two vectors, cross product result vector (perpendicular), parallelogram surface - Angle measurement display with theta symbol - Area calculation display for parallelogram - Formula display showing computation steps</p> <p>Interactive controls: - Draggable vector endpoints in both panels - Slider: Angle between vectors (0\u00b0 to 180\u00b0) - Toggle: Show projection in 2D - Toggle: Show parallelogram in 3D - Toggle: Show right-hand rule animation - Button: Animate angle sweep from 0\u00b0 to 180\u00b0 - 3D rotation via mouse drag</p> <p>Default parameters: - Vector u: (3, 2) in 2D, (3, 2, 0) in 3D - Vector v: (4, 1) in 2D, (1, 4, 0) in 3D - Show projection: true - Show parallelogram: true</p> <p>Behavior: - Dot product value updates with cos(theta) relationship visible - When vectors perpendicular, dot product display highlights zero - Cross product vector length changes with parallelogram area - Right-hand rule animation shows finger curl and thumb direction - Numerical display shows step-by-step calculation</p> <p>Implementation: p5.js with WEBGL for 3D panel</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#magnitude-and-norms","title":"Magnitude and Norms","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-magnitude","title":"Vector Magnitude","text":"<p>The magnitude (or length) of a vector measures how far the vector extends from the origin. For a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\), the magnitude is:</p> \\[\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This formula is the generalization of the Pythagorean theorem to \\(n\\) dimensions. For a 2D vector \\((3, 4)\\), the magnitude is \\(\\sqrt{9 + 16} = 5\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-norms","title":"Understanding Norms","text":"<p>A norm is a function that assigns a non-negative length to each vector in a vector space. Different norms measure \"length\" differently, and each has applications in machine learning and optimization.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#l2-norm-euclidean-norm","title":"L2 Norm (Euclidean Norm)","text":"<p>The L2 norm is the standard Euclidean distance from the origin:</p> \\[\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This is the magnitude we computed above. It's the \"straight-line\" distance and is used extensively in:</p> <ul> <li>Least squares regression</li> <li>Euclidean distance calculations</li> <li>Ridge regularization (L2 regularization)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l1-norm-manhattan-norm","title":"L1 Norm (Manhattan Norm)","text":"<p>The L1 norm sums the absolute values of components:</p> \\[\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^{n} |v_i|\\] <p>Named after the grid-like street layout of Manhattan, this norm measures distance as if you could only travel along axes. It's used in:</p> <ul> <li>Lasso regularization (L1 regularization)</li> <li>Robust statistics</li> <li>Sparse signal recovery</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l-infinity-norm-maximum-norm","title":"L-Infinity Norm (Maximum Norm)","text":"<p>The L-infinity norm returns the maximum absolute component value:</p> \\[\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\\] <p>This norm is useful when you need to constrain the maximum deviation in any single dimension.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-norm-comparison-visualizer","title":"Diagram: Norm Comparison Visualizer","text":"Norm Comparison Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p> <p>Canvas layout: - Main area (600px): Coordinate grid with unit \"circles\" for each norm - Right panel (200px): Numerical values and controls</p> <p>Visual elements: - Coordinate grid from -2 to 2 on both axes - Unit circle for L2 norm (actual circle, blue) - Unit \"circle\" for L1 norm (diamond/rhombus shape, green) - Unit \"circle\" for L-infinity norm (square, orange) - Point showing selected vector with lines to origin - Distance measurements for each norm displayed</p> <p>Interactive controls: - Draggable point to select vector - Checkboxes: Show L1, L2, L-infinity unit shapes (all on by default) - Slider: Adjust norm \"radius\" to see scaled shapes - Toggle: Animate point around each unit shape - Display: Current coordinates and all three norm values</p> <p>Default parameters: - Selected point: (0.6, 0.8) - All three norm shapes visible - Radius: 1</p> <p>Behavior: - As point is dragged, all three norm values update - Unit shapes clearly show different \"distance = 1\" definitions - Points on each unit shape highlight when norm equals 1 - Animation moves point around each shape showing equal norm path</p> <p>Implementation: p5.js with parametric curve drawing</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#euclidean-distance","title":"Euclidean Distance","text":"<p>The Euclidean distance between two vectors measures the straight-line separation:</p> \\[d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2}\\] <p>This fundamental metric appears throughout machine learning:</p> <ul> <li>k-Nearest Neighbors classification</li> <li>K-means clustering</li> <li>Distance-based similarity measures</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#unit-vectors-and-normalization","title":"Unit Vectors and Normalization","text":"<p>A unit vector has magnitude exactly 1. Any non-zero vector can be converted to a unit vector through normalization:</p> \\[\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\] <p>The resulting vector \\(\\hat{\\mathbf{v}}\\) (pronounced \"v-hat\") points in the same direction as \\(\\mathbf{v}\\) but has unit length. Normalization is essential for:</p> <ul> <li>Comparing vector directions without magnitude bias</li> <li>Creating orthonormal bases</li> <li>Preparing features for machine learning algorithms</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations-and-span","title":"Linear Combinations and Span","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations","title":"Linear Combinations","text":"<p>A linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) is a sum of scalar multiples:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\] <p>where \\(c_1, c_2, \\ldots, c_k\\) are scalars. Linear combinations allow us to create new vectors from existing ones and form the foundation for understanding vector spaces.</p> <p>Example: Given \\(\\mathbf{v}_1 = (1, 0)\\) and \\(\\mathbf{v}_2 = (0, 1)\\), the linear combination \\(3\\mathbf{v}_1 + 2\\mathbf{v}_2 = (3, 2)\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-span-of-vectors","title":"The Span of Vectors","text":"<p>The span of a set of vectors is the collection of all possible linear combinations of those vectors:</p> \\[\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k : c_i \\in \\mathbb{R}\\}\\] <p>Geometrically, the span represents all points reachable by combining the vectors:</p> <ul> <li>The span of one non-zero vector is a line through the origin</li> <li>The span of two non-parallel vectors in 3D is a plane through the origin</li> <li>The span of three non-coplanar vectors in 3D is all of \\(\\mathbb{R}^3\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-linear-combination-explorer","title":"Diagram: Linear Combination Explorer","text":"Linear Combination Explorer MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: use, demonstrate, calculate</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p> <p>Canvas layout: - Main area (550px): 2D coordinate grid with vectors and span visualization - Right panel (250px): Controls for coefficients and target challenges</p> <p>Visual elements: - Coordinate grid from -6 to 6 - Two basis vectors v1 (red) and v2 (blue) with adjustable directions - Scaled versions c1v1 (light red) and c2v2 (light blue) - Result vector as sum (green, thick arrow) - Shaded region showing span (when two vectors span a plane) - Target point for challenges (yellow star) - Trail showing path as coefficients change</p> <p>Interactive controls: - Slider: Coefficient c1 (-3 to 3, step 0.1) - Slider: Coefficient c2 (-3 to 3, step 0.1) - Draggable endpoints to adjust v1 and v2 directions - Toggle: Show span region (shaded area) - Toggle: Show component arrows tip-to-tail - Button: Random target challenge - Button: Show solution for current target - Display: Current linear combination equation</p> <p>Default parameters: - v1: (2, 1) - v2: (1, 2) - c1: 1, c2: 1 - Show span: true - Show components: true</p> <p>Behavior: - Adjusting c1/c2 sliders moves result vector smoothly - Target challenges require finding correct coefficients - When vectors are parallel, span collapses to a line (visual feedback) - Component arrows show tip-to-tail construction of sum</p> <p>Implementation: p5.js with slider controls</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence-and-dependence","title":"Linear Independence and Dependence","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence","title":"Linear Independence","text":"<p>A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is linearly independent if the only solution to:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>is \\(c_1 = c_2 = \\cdots = c_k = 0\\). In other words, no vector in the set can be written as a linear combination of the others.</p> <p>Geometric interpretation:</p> <ul> <li>Two vectors are linearly independent if they are not parallel</li> <li>Three vectors in 3D are linearly independent if they don't all lie in the same plane</li> <li>Linearly independent vectors point in \"genuinely different\" directions</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-dependence","title":"Linear Dependence","text":"<p>Vectors are linearly dependent if at least one can be expressed as a linear combination of the others. This occurs when there exist scalars, not all zero, such that:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>Example of dependence: The vectors \\(\\mathbf{v}_1 = (1, 2)\\), \\(\\mathbf{v}_2 = (2, 4)\\), and \\(\\mathbf{v}_3 = (3, 6)\\) are linearly dependent because \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) and \\(\\mathbf{v}_3 = 3\\mathbf{v}_1\\). They all lie on the same line.</p> Property Linearly Independent Linearly Dependent Unique representation Each point in span has unique coefficients Multiple ways to reach same point Redundancy No redundant vectors At least one vector is redundant Span efficiency Minimal set to span a subspace Could remove vectors without reducing span"},{"location":"chapters/01-vectors-and-vector-spaces/#basis-and-coordinate-systems","title":"Basis and Coordinate Systems","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#basis-vectors","title":"Basis Vectors","text":"<p>A basis for a vector space is a set of linearly independent vectors that span the entire space. Every vector in the space can be written uniquely as a linear combination of basis vectors.</p> <p>A basis provides:</p> <ul> <li>A coordinate system for the space</li> <li>A way to represent any vector as a list of coefficients</li> <li>The minimum number of vectors needed to describe the space</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-standard-basis","title":"The Standard Basis","text":"<p>The standard basis for \\(\\mathbb{R}^n\\) consists of vectors with a single 1 and all other components 0:</p> <p>For \\(\\mathbb{R}^2\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>For \\(\\mathbb{R}^3\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>The standard basis vectors align with the coordinate axes, making them intuitive for visualization and computation.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#coordinate-systems","title":"Coordinate Systems","text":"<p>A coordinate system assigns a unique tuple of numbers to each point in space. When we choose a basis \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n\\}\\), any vector \\(\\mathbf{v}\\) can be written as:</p> \\[\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2 + \\cdots + c_n\\mathbf{b}_n\\] <p>The coefficients \\((c_1, c_2, \\ldots, c_n)\\) are the coordinates of \\(\\mathbf{v}\\) with respect to this basis. Different bases give different coordinate representations of the same vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-basis-and-coordinate-system-visualizer","title":"Diagram: Basis and Coordinate System Visualizer","text":"Basis and Coordinate System Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret, compare</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p> <p>Canvas layout: - Left panel (400px): Standard basis grid with vector - Right panel (400px): Custom basis grid with same vector - Bottom: Coordinate comparison display</p> <p>Visual elements: - Standard basis: Traditional x-y grid with e1, e2 as unit vectors - Custom basis: Skewed grid based on chosen basis vectors b1, b2 - Same geometric vector shown in both coordinate systems - Grid lines parallel to basis vectors in each panel - Coordinate labels showing (x,y) in standard and (c1,c2) in custom - Dashed lines from vector tip to axes showing coordinates</p> <p>Interactive controls: - Draggable point to select vector (synced between panels) - Draggable endpoints for custom basis vectors b1 and b2 - Preset buttons: Standard, Rotated 45\u00b0, Skewed, Stretched - Toggle: Show grid lines - Toggle: Show coordinate projections - Animation: Morph from standard to custom basis</p> <p>Default parameters: - Vector: (3, 2) in standard basis - Custom basis: b1 = (2, 0), b2 = (1, 1) - Grid lines: visible - Projections: visible</p> <p>Behavior: - Moving point updates coordinates in both systems - Changing custom basis redraws right panel grid - Coordinates update in real-time showing transformation - Morph animation shows how grid deforms between bases</p> <p>Implementation: p5.js with matrix transformation for grid rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-spaces","title":"Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#definition-of-a-vector-space","title":"Definition of a Vector Space","text":"<p>A vector space is a collection of objects called vectors that satisfies ten axioms regarding addition and scalar multiplication. For a set \\(V\\) to be a vector space over a field \\(F\\) (typically \\(\\mathbb{R}\\)), it must satisfy:</p> <p>Addition axioms:</p> <ol> <li>Closure: \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</li> <li>Commutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associativity: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: There exists \\(\\mathbf{0}\\) such that \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: For each \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ol> <p>Scalar multiplication axioms:</p> <ol> <li>Closure: \\(c\\mathbf{v} \\in V\\)</li> <li>Distributivity (vectors): \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> <li>Distributivity (scalars): \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</li> <li>Associativity: \\(c(d\\mathbf{v}) = (cd)\\mathbf{v}\\)</li> <li>Identity: \\(1\\mathbf{v} = \\mathbf{v}\\)</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#examples-of-vector-spaces","title":"Examples of Vector Spaces","text":"<p>The most familiar vector space is \\(\\mathbb{R}^n\\), but vector spaces are more general:</p> Vector Space Elements Operations \\(\\mathbb{R}^n\\) n-tuples of real numbers Component-wise addition/scaling Polynomials of degree \u2264 n Polynomials \\(a_0 + a_1x + \\cdots + a_nx^n\\) Add coefficients, scale coefficients Continuous functions on \\([a,b]\\) Functions \\(f: [a,b] \\to \\mathbb{R}\\) \\((f+g)(x) = f(x) + g(x)\\) \\(m \\times n\\) matrices Matrices with \\(m\\) rows, \\(n\\) columns Matrix addition, scalar multiplication"},{"location":"chapters/01-vectors-and-vector-spaces/#dimension-of-a-vector-space","title":"Dimension of a Vector Space","text":"<p>The dimension of a vector space is the number of vectors in any basis. This count is always the same regardless of which basis you choose\u2014a fundamental theorem of linear algebra.</p> <ul> <li>\\(\\mathbb{R}^2\\) has dimension 2</li> <li>\\(\\mathbb{R}^3\\) has dimension 3</li> <li>The space of polynomials of degree \u2264 3 has dimension 4 (basis: \\(\\{1, x, x^2, x^3\\}\\))</li> </ul> <p>Dimension tells us the \"degrees of freedom\" in a vector space\u2014how many independent directions exist.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-space-axiom-explorer","title":"Diagram: Vector Space Axiom Explorer","text":"Vector Space Axiom Explorer Infographic <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize, list</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p> <p>Layout: Central hub showing \"Vector Space\" with two branches for Addition and Scalar Multiplication axioms</p> <p>Visual elements: - Central node: \"Vector Space V over field F\" - Left branch (blue theme): Five addition axiom nodes - Right branch (green theme): Five scalar multiplication axiom nodes - Each axiom node shows: name, symbolic form, geometric mini-visualization - Connection lines showing axiom groupings</p> <p>Interactive elements: - Hover over axiom node: Tooltip shows full definition and concrete example - Click axiom node: Animates a 2D example demonstrating the axiom - Hover over \"V\": Shows examples of vector spaces - Hover over \"F\": Shows examples of fields (R, C) - Progress tracker: Checkmarks for viewed axioms</p> <p>Hover text content: - Closure (add): \"For any u, v in V, the sum u + v is also in V\" - Commutativity: \"Order doesn't matter: u + v = v + u\" - Associativity (add): \"Grouping doesn't matter: (u + v) + w = u + (v + w)\" - Additive identity: \"The zero vector 0 exists: v + 0 = v\" - Additive inverse: \"Every vector has an opposite: v + (-v) = 0\" - Closure (scalar): \"For any scalar c and v in V, cv is also in V\" - Distributivity (vectors): \"c(u + v) = cu + cv\" - Distributivity (scalars): \"(c + d)v = cv + dv\" - Associativity (scalar): \"c(dv) = (cd)v\" - Scalar identity: \"1v = v\"</p> <p>Visual style: Modern node-and-edge layout with pastel colors</p> <p>Implementation: HTML/CSS/JavaScript with SVG nodes and CSS transitions</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#applications-in-ai-and-machine-learning","title":"Applications in AI and Machine Learning","text":"<p>The concepts introduced in this chapter form the foundation for understanding machine learning algorithms:</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#feature-vectors","title":"Feature Vectors","text":"<p>Machine learning represents data points as vectors. A sample with features \\((x_1, x_2, \\ldots, x_n)\\) becomes a vector in \\(\\mathbb{R}^n\\). Vector operations enable:</p> <ul> <li>Distance-based classification: k-NN uses Euclidean distance between feature vectors</li> <li>Similarity measures: Cosine similarity uses the dot product to compare vectors</li> <li>Centering data: Subtracting the mean vector centers the dataset at the origin</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#word-embeddings","title":"Word Embeddings","text":"<p>Natural language processing represents words as dense vectors (typically 100-300 dimensions). These embedding vectors capture semantic meaning:</p> <ul> <li>Similar words have nearby vectors (small Euclidean distance)</li> <li>Analogies appear as vector arithmetic: \\(\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\)</li> <li>The dot product measures semantic similarity</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#neural-network-parameters","title":"Neural Network Parameters","text":"<p>Neural networks store learned knowledge in weight matrices and bias vectors. Forward propagation consists of:</p> <ol> <li>Linear combination: \\(\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}\\)</li> <li>Nonlinear activation: \\(\\mathbf{a} = \\sigma(\\mathbf{z})\\)</li> </ol> <p>Understanding vector spaces helps interpret what neural networks learn and how they transform data.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#summary_1","title":"Summary","text":"<p>This chapter established the fundamental concepts of linear algebra that underpin all subsequent topics:</p> <p>Core Concepts:</p> <ul> <li>Scalars are single numbers; vectors are ordered collections with magnitude and direction</li> <li>Vectors exist in spaces of any dimension, from 2D and 3D to high-dimensional feature spaces</li> <li>Basic operations include addition, scalar multiplication, and subtraction</li> </ul> <p>Products and Measurements:</p> <ul> <li>The dot product returns a scalar measuring projection and angle</li> <li>The cross product (3D only) returns a perpendicular vector</li> <li>Norms (L1, L2, L-infinity) measure vector length in different ways</li> <li>Normalization creates unit vectors for direction comparison</li> </ul> <p>Abstract Structures:</p> <ul> <li>Linear combinations create new vectors from scaled sums</li> <li>The span is all reachable points through linear combinations</li> <li>Linear independence means no vector is redundant</li> <li>A basis provides a coordinate system for a vector space</li> <li>The dimension counts basis vectors (degrees of freedom)</li> </ul> <p>These concepts provide the vocabulary and tools for the matrix operations, transformations, and decompositions explored in subsequent chapters.</p> Self-Check: Can you answer these questions? <ol> <li>What is the geometric interpretation of the dot product \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\)?</li> <li>If vectors \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), \\(\\mathbf{v}_3\\) are linearly dependent, what does this mean geometrically in 3D?</li> <li>Why must a basis for \\(\\mathbb{R}^3\\) contain exactly three vectors?</li> <li>How does the L1 norm differ from the L2 norm when measuring the vector \\((3, 4)\\)?</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/","title":"Quiz: Vectors and Vector Spaces","text":"<p>Test your understanding of vectors, vector operations, and vector space fundamentals.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#1-what-is-a-vector-in-the-context-of-linear-algebra","title":"1. What is a vector in the context of linear algebra?","text":"<ol> <li>A single number representing magnitude</li> <li>An ordered collection of numbers representing magnitude and direction</li> <li>A matrix with only one row</li> <li>A geometric shape with four sides</li> </ol> Show Answer <p>The correct answer is B. A vector is an ordered collection of numbers (components) that represents both magnitude and direction. Unlike scalars which are single numbers, vectors have multiple components that together define a quantity in multi-dimensional space.</p> <p>Concept Tested: Vector</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#2-which-operation-produces-a-scalar-result-from-two-vectors","title":"2. Which operation produces a scalar result from two vectors?","text":"<ol> <li>Vector addition</li> <li>Scalar multiplication</li> <li>Dot product</li> <li>Cross product</li> </ol> Show Answer <p>The correct answer is C. The dot product (also called inner product or scalar product) of two vectors produces a scalar value. It is computed as the sum of the products of corresponding components: \\(\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i\\).</p> <p>Concept Tested: Dot Product</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#3-what-does-the-l2-norm-euclidean-norm-of-a-vector-measure","title":"3. What does the L2 norm (Euclidean norm) of a vector measure?","text":"<ol> <li>The number of non-zero components</li> <li>The sum of all component values</li> <li>The length or magnitude of the vector</li> <li>The angle between the vector and the x-axis</li> </ol> Show Answer <p>The correct answer is C. The L2 norm (Euclidean norm) measures the length or magnitude of a vector in Euclidean space. It is computed as \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^n v_i^2}\\), which corresponds to the geometric distance from the origin to the vector's endpoint.</p> <p>Concept Tested: Euclidean Norm</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#4-two-vectors-are-orthogonal-when-their-dot-product-equals","title":"4. Two vectors are orthogonal when their dot product equals:","text":"<ol> <li>One</li> <li>Zero</li> <li>Negative one</li> <li>Their product of magnitudes</li> </ol> Show Answer <p>The correct answer is B. Two vectors are orthogonal (perpendicular) when their dot product equals zero. This is because \\(\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos\\theta\\), and when \\(\\theta = 90\u00b0\\), \\(\\cos(90\u00b0) = 0\\).</p> <p>Concept Tested: Orthogonality</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#5-what-is-a-unit-vector","title":"5. What is a unit vector?","text":"<ol> <li>A vector with all components equal to one</li> <li>A vector with magnitude equal to one</li> <li>The first vector in a basis</li> <li>A vector pointing in the positive x-direction</li> </ol> Show Answer <p>The correct answer is B. A unit vector is a vector with magnitude (norm) equal to one. Any non-zero vector can be converted to a unit vector by dividing it by its magnitude: \\(\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\). Unit vectors indicate direction without magnitude.</p> <p>Concept Tested: Unit Vector</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#6-what-is-a-linear-combination-of-vectors","title":"6. What is a linear combination of vectors?","text":"<ol> <li>Adding vectors without any scaling</li> <li>Multiplying all vectors together</li> <li>Scaling vectors by scalars and then adding them</li> <li>Taking the dot product of multiple vectors</li> </ol> Show Answer <p>The correct answer is C. A linear combination of vectors involves scaling each vector by a scalar coefficient and then summing the results: \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n\\). This is a fundamental operation in linear algebra used to create new vectors from existing ones.</p> <p>Concept Tested: Linear Combination</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#7-a-set-of-vectors-is-linearly-independent-if","title":"7. A set of vectors is linearly independent if:","text":"<ol> <li>All vectors are orthogonal to each other</li> <li>No vector can be written as a linear combination of the others</li> <li>All vectors have the same magnitude</li> <li>The vectors point in different directions</li> </ol> Show Answer <p>The correct answer is B. Vectors are linearly independent if no vector in the set can be expressed as a linear combination of the others. Equivalently, the only solution to \\(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_n\\mathbf{v}_n = \\mathbf{0}\\) is \\(c_1 = c_2 = \\cdots = c_n = 0\\).</p> <p>Concept Tested: Linear Independence</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#8-given-vectors-mathbfa-3-4-and-mathbfb-1-2-what-is-mathbfa-cdot-mathbfb","title":"8. Given vectors \\(\\mathbf{a} = [3, 4]\\) and \\(\\mathbf{b} = [1, 2]\\), what is \\(\\mathbf{a} \\cdot \\mathbf{b}\\)?","text":"<ol> <li>7</li> <li>10</li> <li>11</li> <li>14</li> </ol> Show Answer <p>The correct answer is C. The dot product is computed as \\(\\mathbf{a} \\cdot \\mathbf{b} = (3)(1) + (4)(2) = 3 + 8 = 11\\). The dot product sums the products of corresponding components.</p> <p>Concept Tested: Dot Product</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#9-the-span-of-a-set-of-vectors-is","title":"9. The span of a set of vectors is:","text":"<ol> <li>The largest vector in the set</li> <li>The set of all possible linear combinations of those vectors</li> <li>The number of vectors in the set</li> <li>The sum of all vector magnitudes</li> </ol> Show Answer <p>The correct answer is B. The span of a set of vectors is the set of all vectors that can be created through linear combinations of those vectors. It represents all points reachable by scaling and adding the original vectors.</p> <p>Concept Tested: Span</p>"},{"location":"chapters/01-vectors-and-vector-spaces/quiz/#10-what-is-the-dimension-of-a-vector-space","title":"10. What is the dimension of a vector space?","text":"<ol> <li>The largest component value among all vectors</li> <li>The number of vectors in the space</li> <li>The number of vectors in a basis for the space</li> <li>The magnitude of the longest vector</li> </ol> Show Answer <p>The correct answer is C. The dimension of a vector space is the number of vectors in any basis for that space. All bases of a given vector space have the same number of vectors, which defines the dimension. For example, \\(\\mathbb{R}^3\\) has dimension 3.</p> <p>Concept Tested: Dimension</p>"},{"location":"chapters/02-matrices-and-matrix-operations/","title":"Matrices and Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#summary","title":"Summary","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations. You will learn about matrix notation, various special matrix types including diagonal, triangular, symmetric, and orthogonal matrices, and master core operations like multiplication, transpose, and inverse. These concepts form the computational backbone of all linear algebra applications.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"chapters/02-matrices-and-matrix-operations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#introduction-to-matrices","title":"Introduction to Matrices","text":"<p>In the previous chapter, we explored vectors as the fundamental objects of linear algebra\u2014ordered lists of numbers representing points or directions in space. Now we extend this foundation to matrices, rectangular arrays of numbers that organize multiple vectors into a single mathematical object. Matrices are ubiquitous in modern computing: they represent images as pixel grids, encode neural network weights, store graph adjacency relationships, and transform coordinates in computer graphics.</p> <p>The matrix perspective transforms our understanding of linear systems. Rather than thinking of equations individually, we can represent entire systems compactly and manipulate them using matrix operations. This algebraic framework enables efficient computation on modern hardware, where matrix operations are highly optimized through libraries like NumPy, TensorFlow, and PyTorch.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#what-is-a-matrix","title":"What is a Matrix?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. We denote matrices with bold uppercase letters such as \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), or \\(\\mathbf{M}\\). The numbers within a matrix are called entries or elements.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-definition","title":"Matrix Definition","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(a_{ij}\\) is the entry in row \\(i\\) and column \\(j\\)</li> <li>\\(m\\) is the number of rows</li> <li>\\(n\\) is the number of columns</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-notation-and-dimensions","title":"Matrix Notation and Dimensions","text":"<p>The dimensions of a matrix describe its shape. An \\(m \\times n\\) matrix (read \"m by n\") has \\(m\\) rows and \\(n\\) columns. The first number always indicates rows, and the second indicates columns.</p> Dimensions Description Example Use Case \\(3 \\times 3\\) Square matrix Rotation in 3D \\(28 \\times 28\\) Square matrix MNIST digit image \\(m \\times n\\) Rectangular Data matrix with \\(m\\) samples, \\(n\\) features \\(1 \\times n\\) Row vector Single observation \\(m \\times 1\\) Column vector Single feature across samples <p>Using standard matrix notation, we refer to individual entries with subscripts. The entry \\(a_{ij}\\) (or \\(A_{ij}\\)) denotes the element in row \\(i\\) and column \\(j\\). This indexing convention\u2014row first, column second\u2014is consistent across mathematics and most programming languages.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#row-and-column-vectors","title":"Row and Column Vectors","text":"<p>Matrices with only one row or one column receive special names. A row vector is a \\(1 \\times n\\) matrix containing \\(n\\) elements arranged horizontally:</p> <p>\\(\\mathbf{r} = \\begin{bmatrix} r_1 &amp; r_2 &amp; \\cdots &amp; r_n \\end{bmatrix}\\)</p> <p>A column vector is an \\(m \\times 1\\) matrix containing \\(m\\) elements arranged vertically:</p> <p>\\(\\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix}\\)</p> <p>In machine learning contexts, column vectors typically represent individual data points or feature vectors, while row vectors represent observations in a data matrix. This distinction matters because matrix multiplication requires compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-row-and-column-vector-visualization","title":"Diagram: Row and Column Vector Visualization","text":"Row and Column Vector Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: compare, contrast</p> <p>Learning Objective: Help students visually distinguish between row vectors (horizontal) and column vectors (vertical), understanding how their orientation affects matrix operations.</p> <p>Canvas layout: - Main drawing area showing both vector types side by side - Controls below for adjusting vector dimensions</p> <p>Visual elements: - Left side: A row vector displayed horizontally with labeled entries - Right side: A column vector displayed vertically with labeled entries - Color coding: row vector in blue, column vector in green - Grid background showing the row/column structure - Dimension labels showing \"1 \u00d7 n\" for row and \"m \u00d7 1\" for column</p> <p>Interactive controls: - Slider: Number of elements (2-6) - Toggle: Show/hide dimension annotations - Button: Randomize values</p> <p>Default parameters: - Elements: 4 - Values: random integers 1-9</p> <p>Behavior: - Adjusting element count updates both vectors simultaneously - Dimension labels update dynamically - Values displayed inside each cell</p> <p>Implementation: p5.js</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-entries-and-indexing","title":"Matrix Entries and Indexing","text":"<p>Each matrix entry \\(a_{ij}\\) occupies a specific position determined by its row index \\(i\\) and column index \\(j\\). Understanding this indexing system is essential for implementing matrix algorithms and interpreting matrix operations.</p> <p>Consider a \\(3 \\times 4\\) matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\end{bmatrix}\\)</p> <p>In this example:</p> <ul> <li>\\(a_{11} = 1\\) (first row, first column)</li> <li>\\(a_{23} = 7\\) (second row, third column)</li> <li>\\(a_{32} = 10\\) (third row, second column)</li> </ul> <p>Zero-Based vs One-Based Indexing</p> <p>Mathematical notation uses one-based indexing (starting from 1), while programming languages like Python and JavaScript use zero-based indexing (starting from 0). In NumPy, accessing element \\(a_{23}\\) requires <code>A[1, 2]</code>.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#viewing-matrices-as-collections-of-vectors","title":"Viewing Matrices as Collections of Vectors","text":"<p>A powerful perspective views matrices as collections of vectors. An \\(m \\times n\\) matrix can be interpreted as:</p> <ul> <li>\\(n\\) column vectors of dimension \\(m\\), or</li> <li>\\(m\\) row vectors of dimension \\(n\\)</li> </ul> <p>This dual interpretation underlies many matrix operations. When we multiply a matrix by a vector, we're computing a linear combination of the matrix's column vectors. When we compute the transpose, we're swapping between row and column interpretations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#basic-matrix-operations","title":"Basic Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition","title":"Matrix Addition","text":"<p>Matrix addition combines two matrices of identical dimensions by adding corresponding entries. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) matrices, their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is also an \\(m \\times n\\) matrix where each entry is:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition-formula","title":"Matrix Addition Formula","text":"<p>\\(c_{ij} = a_{ij} + b_{ij}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in the sum matrix</li> <li>\\(a_{ij}\\) is the corresponding entry in \\(\\mathbf{A}\\)</li> <li>\\(b_{ij}\\) is the corresponding entry in \\(\\mathbf{B}\\)</li> </ul> <p>Matrix addition is both commutative (\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)) and associative (\\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\)).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Matrix scalar multiplication multiplies every entry of a matrix by a single number (scalar). If \\(k\\) is a scalar and \\(\\mathbf{A}\\) is a matrix, then \\(k\\mathbf{A}\\) has entries:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication-formula","title":"Scalar Multiplication Formula","text":"<p>\\((k\\mathbf{A})_{ij} = k \\cdot a_{ij}\\)</p> <p>where:</p> <ul> <li>\\(k\\) is the scalar multiplier</li> <li>\\(a_{ij}\\) is the original matrix entry</li> </ul> <p>Scalar multiplication scales all entries uniformly. In neural networks, this operation appears when applying learning rates to gradient matrices during backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-addition-and-scalar-multiplication","title":"Diagram: Matrix Addition and Scalar Multiplication","text":"Matrix Addition and Scalar Multiplication Interactive <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to practice matrix addition and scalar multiplication interactively, reinforcing the element-wise nature of these operations.</p> <p>Canvas layout: - Top section: Two input matrices A and B displayed side by side - Middle: Result matrix C with operation indicator - Bottom: Control panel</p> <p>Visual elements: - 3\u00d73 matrices displayed as grids with editable cells - Color highlighting showing corresponding entries during addition - Animation showing values flowing into result matrix - Operation symbol (+, \u00d7) displayed between matrices</p> <p>Interactive controls: - Radio buttons: Select operation (Addition / Scalar Multiply) - Slider: Scalar value k (for scalar multiplication, range -3 to 3) - Button: Randomize matrices - Button: Step through calculation - Toggle: Show/hide calculation details</p> <p>Default parameters: - Matrix size: 3\u00d73 - Operation: Addition - Scalar: 2 - Values: small integers (-5 to 5)</p> <p>Behavior: - Clicking a cell allows value editing - Result updates in real-time as inputs change - Step mode highlights each entry calculation sequentially - Animation shows entry-by-entry computation</p> <p>Implementation: p5.js with editable input fields</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication is the most important and nuanced matrix operation. Unlike addition, matrix multiplication has specific dimension requirements and is not commutative.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product","title":"Matrix-Vector Product","text":"<p>The matrix-vector product multiplies an \\(m \\times n\\) matrix by an \\(n \\times 1\\) column vector, producing an \\(m \\times 1\\) column vector. This operation represents applying a linear transformation to a vector.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product-formula","title":"Matrix-Vector Product Formula","text":"<p>\\(\\mathbf{y} = \\mathbf{A}\\mathbf{x}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix</li> <li>\\(\\mathbf{x}\\) is an \\(n \\times 1\\) column vector</li> <li>\\(\\mathbf{y}\\) is the resulting \\(m \\times 1\\) column vector</li> </ul> <p>Each entry of the result is the dot product of a row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\):</p> <p>\\(y_i = \\sum_{j=1}^{n} a_{ij} x_j = a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n\\)</p> <p>Alternatively, the matrix-vector product can be viewed as a linear combination of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\mathbf{A}\\mathbf{x} = x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n\\)</p> <p>where \\(\\mathbf{a}_j\\) denotes the \\(j\\)-th column of \\(\\mathbf{A}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-matrix-multiplication","title":"Matrix-Matrix Multiplication","text":"<p>Matrix multiplication extends the matrix-vector product. The product of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B}\\) is an \\(m \\times p\\) matrix \\(\\mathbf{C}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication-formula","title":"Matrix Multiplication Formula","text":"<p>\\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in row \\(i\\), column \\(j\\) of the product</li> <li>The sum runs over the shared dimension \\(n\\)</li> <li>Each entry requires \\(n\\) multiplications and \\(n-1\\) additions</li> </ul> <p>The dimension compatibility rule states: the number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\).</p> Matrix A Matrix B Product C Valid? \\(2 \\times 3\\) \\(3 \\times 4\\) \\(2 \\times 4\\) Yes \\(3 \\times 2\\) \\(3 \\times 4\\) \u2014 No \\(4 \\times 4\\) \\(4 \\times 4\\) \\(4 \\times 4\\) Yes \\(1 \\times 5\\) \\(5 \\times 1\\) \\(1 \\times 1\\) Yes (scalar) <p>Matrix Multiplication is Not Commutative</p> <p>In general, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). In fact, if \\(\\mathbf{A}\\mathbf{B}\\) exists, \\(\\mathbf{B}\\mathbf{A}\\) may not even be defined (different dimensions). Even for square matrices where both products exist, they typically differ.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-multiplication-visualizer","title":"Diagram: Matrix Multiplication Visualizer","text":"Matrix Multiplication Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand the row-by-column computation process in matrix multiplication by animating each entry calculation.</p> <p>Canvas layout: - Left: Matrix A (highlighted rows) - Center: Matrix B (highlighted columns) - Right: Result matrix C (entries fill in as computed) - Bottom: Control panel and calculation display</p> <p>Visual elements: - Matrix A with current row highlighted in blue - Matrix B with current column highlighted in green - Result matrix C with current entry position highlighted in yellow - Dot product calculation shown step by step below matrices - Running sum displayed during computation</p> <p>Interactive controls: - Dropdown: Matrix A dimensions (2\u00d72, 2\u00d73, 3\u00d72, 3\u00d73) - Dropdown: Matrix B dimensions (matching first dimension of result) - Button: \"Next Entry\" - advance to next calculation - Button: \"Auto Play\" - animate all calculations - Slider: Animation speed (200ms - 2000ms per step) - Button: Reset - Toggle: Show column interpretation (linear combination view)</p> <p>Default parameters: - Matrix A: 2\u00d73 - Matrix B: 3\u00d72 - Animation speed: 800ms - Values: small integers (1-5)</p> <p>Behavior: - Highlight corresponding row of A and column of B - Show element-wise multiplication with + signs - Display running sum as computation proceeds - Fill in result entry when complete - Move to next entry automatically or on button click - Column interpretation mode shows result as linear combination</p> <p>Implementation: p5.js with animation states here</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-multiplication","title":"Properties of Matrix Multiplication","text":"<p>Matrix multiplication satisfies several important algebraic properties:</p> <ul> <li>Associativity: \\((\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\)</li> <li>Distributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\)</li> <li>Scalar compatibility: \\(k(\\mathbf{A}\\mathbf{B}) = (k\\mathbf{A})\\mathbf{B} = \\mathbf{A}(k\\mathbf{B})\\)</li> <li>Non-commutativity: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\) in general</li> </ul> <p>These properties enable powerful algebraic manipulations while requiring careful attention to the order of operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#the-matrix-transpose","title":"The Matrix Transpose","text":"<p>The matrix transpose operation flips a matrix over its diagonal, converting rows to columns and vice versa. For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-definition","title":"Transpose Definition","text":"<p>\\((\\mathbf{A}^T)_{ij} = a_{ji}\\)</p> <p>where:</p> <ul> <li>The entry in row \\(i\\), column \\(j\\) of \\(\\mathbf{A}^T\\) equals the entry in row \\(j\\), column \\(i\\) of \\(\\mathbf{A}\\)</li> </ul> <p>For example:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix}\\)</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-properties","title":"Transpose Properties","text":"<p>The transpose operation satisfies these properties:</p> <ul> <li>\\((\\mathbf{A}^T)^T = \\mathbf{A}\\) (double transpose returns original)</li> <li>\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\) (transpose distributes over addition)</li> <li>\\((k\\mathbf{A})^T = k\\mathbf{A}^T\\) (scalars pass through)</li> <li>\\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) (order reverses for products!)</li> </ul> <p>The last property is particularly important: when transposing a product, the order of the matrices reverses. This \"reverse order law\" appears frequently in derivations involving neural network backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#special-matrix-types","title":"Special Matrix Types","text":"<p>Many matrices have special structures that simplify computation or carry geometric meaning. Recognizing these types enables algorithmic optimizations and deeper understanding.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#identity-matrix","title":"Identity Matrix","text":"<p>The identity matrix \\(\\mathbf{I}_n\\) is the multiplicative identity for \\(n \\times n\\) matrices. It has ones on the diagonal and zeros elsewhere:</p> <p>\\(\\mathbf{I}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>For any matrix \\(\\mathbf{A}\\) with compatible dimensions:</p> <p>\\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\)</p> <p>The identity matrix represents the \"do nothing\" transformation\u2014it maps every vector to itself.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A diagonal matrix has nonzero entries only on its main diagonal. All off-diagonal entries are zero:</p> <p>\\(\\mathbf{D} = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 \\\\ 0 &amp; d_2 &amp; 0 \\\\ 0 &amp; 0 &amp; d_3 \\end{bmatrix}\\)</p> <p>Diagonal matrices have special computational properties:</p> <ul> <li>Multiplication: Multiplying by a diagonal matrix scales rows or columns</li> <li>Powers: \\(\\mathbf{D}^k\\) has entries \\(d_i^k\\)</li> <li>Inverse: \\(\\mathbf{D}^{-1}\\) has entries \\(1/d_i\\) (if all \\(d_i \\neq 0\\))</li> </ul> <p>In machine learning, diagonal matrices appear in batch normalization and as covariance matrices for independent features.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#triangular-matrices","title":"Triangular Matrices","text":"<p>A triangular matrix has zeros on one side of the diagonal.</p> <p>An upper triangular matrix has zeros below the diagonal:</p> <p>\\(\\mathbf{U} = \\begin{bmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\\\ 0 &amp; u_{22} &amp; u_{23} \\\\ 0 &amp; 0 &amp; u_{33} \\end{bmatrix}\\)</p> <p>A lower triangular matrix has zeros above the diagonal:</p> <p>\\(\\mathbf{L} = \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 \\\\ l_{21} &amp; l_{22} &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; l_{33} \\end{bmatrix}\\)</p> <p>Triangular matrices are computationally advantageous:</p> <ul> <li>Solving \\(\\mathbf{L}\\mathbf{x} = \\mathbf{b}\\) uses forward substitution (start from top)</li> <li>Solving \\(\\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) uses back substitution (start from bottom)</li> <li>Both require only \\(O(n^2)\\) operations instead of \\(O(n^3)\\)</li> </ul> <p>LU decomposition factors any matrix into lower and upper triangular components, enabling efficient system solving.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-special-matrix-types-gallery","title":"Diagram: Special Matrix Types Gallery","text":"Special Matrix Types Interactive Gallery <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize</p> <p>Learning Objective: Help students visually identify and distinguish between different special matrix types (identity, diagonal, upper/lower triangular) by their structural patterns.</p> <p>Layout: Grid of matrix visualizations with interactive selection</p> <p>Visual elements: - 4\u00d74 grid showing four matrix types simultaneously - Each matrix displayed as a colored grid:   - Identity: ones on diagonal (gold), zeros elsewhere (light gray)   - Diagonal: colored diagonal entries, zeros elsewhere   - Upper triangular: colored upper triangle including diagonal, zeros below   - Lower triangular: colored lower triangle including diagonal, zeros above - Structural pattern highlighted with shading - Labels below each matrix type</p> <p>Interactive features: - Click on a matrix type to enlarge and see detailed properties - Hover over entries to see position (i,j) and value - Toggle: Show/hide zero entries - Slider: Matrix size (3\u00d73 to 6\u00d76) - Button: Show random example values</p> <p>Information panels (on click): - Definition of the matrix type - Key properties - Computational advantages - Common applications</p> <p>Color scheme: - Non-zero entries: blue gradient based on value - Zero entries: light gray or hidden - Diagonal: highlighted in gold - Structural regions: subtle background shading</p> <p>Implementation: HTML/CSS/JavaScript with SVG or Canvas</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#symmetric-matrix","title":"Symmetric Matrix","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-symmetric-matrix","title":"Diagram: Symmetric Matrix","text":"<p>A symmetric matrix equals its own transpose: \\(\\mathbf{A} = \\mathbf{A}^T\\). This means \\(a_{ij} = a_{ji}\\) for all entries\u2014the matrix is mirror-symmetric across the diagonal.</p> <p>\\(\\mathbf{S} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{bmatrix}\\)</p> <p>Symmetric matrices arise naturally in many contexts:</p> <ul> <li>Covariance matrices in statistics</li> <li>Adjacency matrices for undirected graphs</li> <li>Hessian matrices (second derivatives) in optimization</li> <li>Gram matrices \\(\\mathbf{A}^T\\mathbf{A}\\) from any matrix \\(\\mathbf{A}\\)</li> </ul> <p>Symmetric matrices have remarkable spectral properties: they always have real eigenvalues and orthogonal eigenvectors, enabling powerful decomposition methods.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>An orthogonal matrix \\(\\mathbf{Q}\\) satisfies \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I}\\). Equivalently, its inverse equals its transpose: \\(\\mathbf{Q}^{-1} = \\mathbf{Q}^T\\).</p> <p>The columns of an orthogonal matrix form an orthonormal set\u2014they are mutually perpendicular unit vectors. Orthogonal matrices represent rotations and reflections that preserve lengths and angles.</p> <p>Properties of orthogonal matrices:</p> <ul> <li>Length preservation: \\(\\|\\mathbf{Q}\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) for all vectors \\(\\mathbf{x}\\)</li> <li>Angle preservation: The angle between \\(\\mathbf{Q}\\mathbf{x}\\) and \\(\\mathbf{Q}\\mathbf{y}\\) equals the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)</li> <li>Determinant: \\(\\det(\\mathbf{Q}) = \\pm 1\\) (rotation if \\(+1\\), reflection if \\(-1\\))</li> <li>Easy inversion: Computing \\(\\mathbf{Q}^{-1}\\) requires only a transpose</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-orthogonal-matrix-transformation","title":"Diagram: Orthogonal Matrix Transformation","text":"Orthogonal Matrix Transformation Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Demonstrate that orthogonal matrices preserve lengths and angles by visualizing how rotation matrices transform shapes without distortion.</p> <p>Canvas layout: - Main area: 2D coordinate plane with original and transformed shapes - Right panel: Matrix display and controls</p> <p>Visual elements: - Coordinate grid with x and y axes - Original shape (unit square or set of vectors) in blue - Transformed shape in red (semi-transparent overlay) - Length indicators showing preservation - Angle arc showing angle preservation between vectors - 2\u00d72 rotation matrix displayed with current angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (0\u00b0 to 360\u00b0) - Checkbox: Show length comparison - Checkbox: Show angle comparison - Dropdown: Shape to transform (square, triangle, circle of vectors) - Button: Add reflection (multiply by reflection matrix) - Display: Matrix values updating with angle</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: unit square - Show lengths: true - Show angles: false</p> <p>Behavior: - Shape smoothly rotates as angle slider changes - Length indicators show |Qx| = |x| - Angle arcs update to show angle preservation - Matrix entries update: cos(\u03b8), -sin(\u03b8), sin(\u03b8), cos(\u03b8) - Reflection button flips across an axis</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse","title":"Matrix Inverse","text":"<p>The matrix inverse generalizes division to matrices. For a square matrix \\(\\mathbf{A}\\), its inverse \\(\\mathbf{A}^{-1}\\) (if it exists) satisfies:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse-definition","title":"Matrix Inverse Definition","text":"<p>\\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(n \\times n\\) square matrix</li> <li>\\(\\mathbf{A}^{-1}\\) is the inverse matrix</li> <li>\\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#invertible-matrices","title":"Invertible Matrices","text":"<p>A matrix is invertible (also called nonsingular or non-degenerate) if its inverse exists. Not all matrices are invertible\u2014those without inverses are called singular.</p> <p>Conditions for invertibility (all equivalent):</p> <ul> <li>\\(\\mathbf{A}^{-1}\\) exists</li> <li>\\(\\det(\\mathbf{A}) \\neq 0\\)</li> <li>The columns of \\(\\mathbf{A}\\) are linearly independent</li> <li>The rows of \\(\\mathbf{A}\\) are linearly independent</li> <li>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\)</li> <li>\\(\\mathbf{A}\\) has full rank</li> </ul> <p>For a 2\u00d72 matrix, the inverse has a closed form:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#22-matrix-inverse-formula","title":"2\u00d72 Matrix Inverse Formula","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\implies \\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(ad - bc\\) is the determinant of \\(\\mathbf{A}\\)</li> <li>The inverse exists only if \\(ad - bc \\neq 0\\)</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-inverse","title":"Properties of Matrix Inverse","text":"<p>When inverses exist, they satisfy these properties:</p> <ul> <li>\\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\)</li> <li>\\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\)</li> <li>\\((k\\mathbf{A})^{-1} = \\frac{1}{k}\\mathbf{A}^{-1}\\) for \\(k \\neq 0\\)</li> <li>\\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) (order reverses!)</li> </ul> <p>The inverse enables solving linear systems: if \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), then \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\).</p> <p>Computational Practice</p> <p>While mathematically elegant, computing \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly is usually avoided in practice. Instead, we solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) directly using methods like LU decomposition, which are more numerically stable and efficient.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-inverse-explorer","title":"Diagram: Matrix Inverse Explorer","text":"Matrix Inverse Interactive Explorer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to explore matrix inversion for 2\u00d72 and 3\u00d73 matrices, verify the inverse property AA\u207b\u00b9 = I, and understand when matrices are singular.</p> <p>Canvas layout: - Left: Input matrix A (editable) - Center: Computed inverse A\u207b\u00b9 (or \"Singular\" warning) - Right: Verification showing AA\u207b\u00b9 = I - Bottom: Controls and determinant display</p> <p>Visual elements: - Editable matrix cells with color-coded entries - Determinant value prominently displayed - Color indicator: green for invertible, red for singular/near-singular - Verification matrix showing identity (or near-identity for numerical precision) - Warning icon when determinant is near zero</p> <p>Interactive controls: - Matrix size toggle: 2\u00d72 / 3\u00d73 - Editable cells for matrix entries - Button: Randomize matrix - Button: Make singular (set det = 0) - Slider: Approach singularity (smoothly varies toward singular) - Toggle: Show calculation steps</p> <p>Default parameters: - Size: 2\u00d72 - Initial matrix: [[2, 1], [1, 1]] (invertible)</p> <p>Behavior: - Real-time inverse computation as entries change - Determinant updates continuously - Verification matrix computes A \u00d7 A\u207b\u00b9 - Near-singular matrices show numerical instability - Step-by-step mode shows cofactor/adjugate method for 2\u00d72</p> <p>Implementation: p5.js with matrix computation library</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#sparse-and-dense-matrices","title":"Sparse and Dense Matrices","text":"<p>Matrices are classified by their distribution of zero and nonzero entries.</p> <p>A dense matrix has few zero entries relative to its total size. Most entries contain meaningful nonzero values. Dense matrices require \\(O(mn)\\) storage and \\(O(mn)\\) operations for most computations.</p> <p>A sparse matrix has many zero entries\u2014typically the number of nonzero entries is \\(O(n)\\) or \\(O(n \\log n)\\) rather than \\(O(n^2)\\) for an \\(n \\times n\\) matrix. Sparse matrices arise naturally in:</p> <ul> <li>Graph adjacency matrices: Most nodes connect to few others</li> <li>Document-term matrices: Each document uses few of all possible words</li> <li>Finite element methods: Local interactions produce banded structures</li> </ul> Property Dense Matrix Sparse Matrix Storage \\(O(mn)\\) \\(O(\\text{nnz})\\) Zero entries Few Many (typically &gt;90%) Storage format 2D array CSR, CSC, COO Multiplication Standard algorithms Specialized sparse algorithms Examples Covariance matrices Adjacency matrices <p>where \\(\\text{nnz}\\) denotes the number of nonzero entries.</p> <p>Sparse matrix formats like Compressed Sparse Row (CSR) store only nonzero values along with their positions, dramatically reducing memory requirements and enabling faster operations by skipping zero computations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-sparse-vs-dense-matrix-visualization","title":"Diagram: Sparse vs Dense Matrix Visualization","text":"Sparse vs Dense Matrix Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Help students understand the structural difference between sparse and dense matrices and appreciate why sparsity enables computational efficiency.</p> <p>Layout: Side-by-side matrix visualizations with statistics</p> <p>Visual elements: - Left panel: Dense matrix (most cells filled with color) - Right panel: Sparse matrix (few colored cells, most white/gray) - Color intensity indicates value magnitude - Zero entries shown as white or light gray - Statistics overlay:   - Total entries   - Nonzero entries   - Sparsity percentage   - Memory comparison (dense vs sparse storage)</p> <p>Interactive features: - Slider: Matrix size (10\u00d710 to 100\u00d7100) - Slider: Sparsity level (10% to 99% zeros) - Dropdown: Sparse pattern (random, diagonal, banded, block) - Toggle: Show storage comparison bar chart - Hover: Show value at position</p> <p>Example data: - Dense: Random values in most cells - Sparse: Graph adjacency pattern or banded structure</p> <p>Statistics display: - Memory ratio: \"Sparse uses X% of dense storage\" - Multiplication speedup estimate</p> <p>Implementation: HTML Canvas or p5.js with efficient rendering</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrices","title":"Block Matrices","text":"<p>A block matrix (or partitioned matrix) is a matrix viewed as an array of smaller matrices called blocks or submatrices. Block structure often reflects natural problem decomposition.</p> <p>\\(\\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), \\(\\mathbf{D}\\) are matrices of compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrix-operations","title":"Block Matrix Operations","text":"<p>Block matrices support \"block-wise\" operations that mirror scalar operations:</p> <p>Block Addition: If two matrices have the same block structure:</p> <p>\\(\\begin{bmatrix} \\mathbf{A}_1 &amp; \\mathbf{B}_1 \\\\ \\mathbf{C}_1 &amp; \\mathbf{D}_1 \\end{bmatrix} + \\begin{bmatrix} \\mathbf{A}_2 &amp; \\mathbf{B}_2 \\\\ \\mathbf{C}_2 &amp; \\mathbf{D}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_1 + \\mathbf{A}_2 &amp; \\mathbf{B}_1 + \\mathbf{B}_2 \\\\ \\mathbf{C}_1 + \\mathbf{C}_2 &amp; \\mathbf{D}_1 + \\mathbf{D}_2 \\end{bmatrix}\\)</p> <p>Block Multiplication: With compatible block dimensions:</p> <p>\\(\\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\begin{bmatrix} \\mathbf{E} \\\\ \\mathbf{F} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}\\mathbf{E} + \\mathbf{B}\\mathbf{F} \\\\ \\mathbf{C}\\mathbf{E} + \\mathbf{D}\\mathbf{F} \\end{bmatrix}\\)</p> <p>Block structure enables:</p> <ul> <li>Parallel computation on independent blocks</li> <li>Efficient algorithms exploiting structure</li> <li>Conceptual clarity in complex systems</li> <li>Memory-efficient storage when blocks have special properties</li> </ul> <p>In deep learning, weight matrices are often organized in blocks corresponding to different layers or attention heads.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-block-matrix-structure","title":"Diagram: Block Matrix Structure","text":"Block Matrix Partitioning Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, classify</p> <p>Learning Objective: Help students understand how matrices can be partitioned into blocks and how block operations mirror element-wise operations at a higher level.</p> <p>Canvas layout: - Main area: Large matrix with adjustable partition lines - Right panel: Block extraction and operation display</p> <p>Visual elements: - 8\u00d78 matrix displayed as a grid - Draggable horizontal and vertical partition lines - Distinct colors for each block region - Block labels (A, B, C, D, etc.) overlaid on regions - Extracted blocks shown separately on right</p> <p>Interactive controls: - Drag partition lines to resize blocks - Dropdown: Example partitioning (2\u00d72 blocks, row partition, column partition) - Toggle: Show block dimensions - Button: Demonstrate block multiplication - Checkbox: Show compatibility requirements</p> <p>Default parameters: - Matrix size: 8\u00d78 - Initial partition: 4\u00d74 blocks (2\u00d72 block structure) - Values: integers 1-9</p> <p>Behavior: - Partition lines snap to integer positions - Block colors update as partitioning changes - Dimension labels update dynamically - Block multiplication demo shows step-by-step block operations</p> <p>Implementation: p5.js with draggable elements</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#applications-in-machine-learning-and-ai","title":"Applications in Machine Learning and AI","text":"<p>The matrix concepts covered in this chapter form the computational foundation of modern machine learning systems.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#data-representation","title":"Data Representation","text":"<p>Machine learning datasets are naturally represented as matrices:</p> <ul> <li>Design matrix \\(\\mathbf{X}\\): Each row is a sample, each column is a feature</li> <li>Weight matrix \\(\\mathbf{W}\\): Neural network parameters connecting layers</li> <li>Embedding matrix \\(\\mathbf{E}\\): Word or token embeddings in NLP</li> </ul> <p>For a dataset with \\(m\\) samples and \\(n\\) features, the design matrix is \\(m \\times n\\). Feature scaling, normalization, and transformation all use matrix operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#neural-network-layers","title":"Neural Network Layers","text":"<p>A fully connected neural network layer performs:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{W}\\) is the weight matrix</li> <li>\\(\\mathbf{x}\\) is the input vector</li> <li>\\(\\mathbf{b}\\) is the bias vector</li> <li>\\(\\sigma\\) is a nonlinear activation function</li> </ul> <p>Batch processing extends this to:</p> <p>\\(\\mathbf{H} = \\sigma(\\mathbf{X}\\mathbf{W}^T + \\mathbf{b})\\)</p> <p>where \\(\\mathbf{X}\\) contains multiple input vectors as rows, enabling parallel computation of many samples.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>Transformer models compute attention using three matrices:</p> <ul> <li>Query matrix \\(\\mathbf{Q}\\): What we're looking for</li> <li>Key matrix \\(\\mathbf{K}\\): What's available to match</li> <li>Value matrix \\(\\mathbf{V}\\): What we retrieve</li> </ul> <p>The attention computation:</p> <p>\\(\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\\)</p> <p>relies entirely on matrix multiplications and transposes covered in this chapter.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-neural-network-layer-matrix-operations","title":"Diagram: Neural Network Layer Matrix Operations","text":"Neural Network Layer Forward Pass Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: implement, demonstrate</p> <p>Learning Objective: Show how matrix-vector multiplication implements a neural network layer, connecting abstract matrix operations to concrete deep learning computations.</p> <p>Canvas layout: - Left: Input vector visualization - Center: Weight matrix with connection lines - Right: Output vector (pre and post activation) - Bottom: Controls and formula display</p> <p>Visual elements: - Input neurons (circles) with values - Weight matrix displayed as a heatmap - Connection lines from inputs to outputs (thickness = weight magnitude) - Output neurons showing weighted sums - Activation function visualization (ReLU, sigmoid curve) - Formula: h = \u03c3(Wx + b) displayed with current values</p> <p>Interactive controls: - Slider: Number of inputs (2-6) - Slider: Number of outputs (2-6) - Dropdown: Activation function (none, ReLU, sigmoid, tanh) - Button: Randomize weights - Button: Randomize input - Toggle: Show bias term - Toggle: Show matrix multiplication step-by-step</p> <p>Default parameters: - Inputs: 3 - Outputs: 2 - Activation: ReLU - Random weights in [-1, 1] - Random inputs in [0, 1]</p> <p>Behavior: - Changing input values updates outputs in real-time - Connection line colors indicate positive (blue) vs negative (red) weights - Step-by-step mode highlights each row-vector dot product - Activation function applied visually to each output</p> <p>Implementation: p5.js with animation</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter introduced matrices as the fundamental data structures of linear algebra with broad applications in AI and machine learning.</p> <p>Core Concepts:</p> <ul> <li>A matrix is a rectangular array of numbers with dimensions \\(m \\times n\\)</li> <li>Matrix entries are indexed by row and column: \\(a_{ij}\\)</li> <li>Row vectors (\\(1 \\times n\\)) and column vectors (\\(m \\times 1\\)) are special cases</li> </ul> <p>Fundamental Operations:</p> <ul> <li>Matrix addition adds corresponding entries (requires same dimensions)</li> <li>Scalar multiplication scales all entries by a constant</li> <li>Matrix-vector product \\(\\mathbf{A}\\mathbf{x}\\) produces a linear combination of columns</li> <li>Matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) requires matching inner dimensions</li> <li>Transpose \\(\\mathbf{A}^T\\) flips rows and columns</li> </ul> <p>Special Matrix Types:</p> <ul> <li>Identity matrix \\(\\mathbf{I}\\): multiplicative identity</li> <li>Diagonal matrix: nonzeros only on diagonal</li> <li>Triangular matrices: zeros above or below diagonal</li> <li>Symmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)</li> <li>Orthogonal matrix: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\) (preserves lengths/angles)</li> </ul> <p>Inverse and Structure:</p> <ul> <li>Matrix inverse \\(\\mathbf{A}^{-1}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\)</li> <li>Invertible matrices have nonzero determinant and linearly independent columns</li> <li>Sparse matrices have few nonzero entries; dense matrices have many</li> <li>Block matrices partition into submatrices for structured computation</li> </ul> <p>Key Properties to Remember:</p> <ul> <li>Matrix multiplication is NOT commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)</li> <li>Product transpose reverses order: \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\)</li> <li>Inverse product reverses order: \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)</li> </ul> <p>These operations form the computational vocabulary of machine learning, from basic linear regression to advanced transformer architectures.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#exercises","title":"Exercises","text":"Exercise 1: Matrix Dimensions <p>Given matrices \\(\\mathbf{A}\\) (3\u00d74), \\(\\mathbf{B}\\) (4\u00d72), and \\(\\mathbf{C}\\) (3\u00d72), which products are defined? What are their dimensions?</p> <ul> <li>\\(\\mathbf{A}\\mathbf{B}\\)</li> <li>\\(\\mathbf{B}\\mathbf{A}\\)</li> <li>\\(\\mathbf{A}\\mathbf{C}\\)</li> <li>\\(\\mathbf{A}\\mathbf{B} + \\mathbf{C}\\)</li> </ul> Exercise 2: Transpose Properties <p>Prove that \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) for 2\u00d72 matrices by computing both sides with generic entries.</p> Exercise 3: Symmetric Matrices <p>Show that for any matrix \\(\\mathbf{A}\\), both \\(\\mathbf{A}^T\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^T\\) are symmetric matrices.</p> Exercise 4: Orthogonal Matrix Verification <p>Verify that the following matrix is orthogonal and determine if it represents a rotation or reflection:</p> <p>\\(\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix}\\)</p> Exercise 5: Block Multiplication <p>Compute the product of the following block matrices:</p> <p>\\(\\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ \\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ -\\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\) is any 2\u00d72 matrix.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/","title":"Quiz: Matrices and Matrix Operations","text":"<p>Test your understanding of matrices, matrix operations, and their properties.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#1-what-is-the-result-of-multiplying-an-m-times-n-matrix-by-an-n-times-p-matrix","title":"1. What is the result of multiplying an \\(m \\times n\\) matrix by an \\(n \\times p\\) matrix?","text":"<ol> <li>An \\(m \\times p\\) matrix</li> <li>An \\(n \\times n\\) matrix</li> <li>An \\(m \\times n\\) matrix</li> <li>A scalar value</li> </ol> Show Answer <p>The correct answer is A. When multiplying an \\(m \\times n\\) matrix by an \\(n \\times p\\) matrix, the result is an \\(m \\times p\\) matrix. The inner dimensions (\\(n\\)) must match, and the outer dimensions (\\(m\\) and \\(p\\)) determine the output size.</p> <p>Concept Tested: Matrix Multiplication</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#2-which-property-does-matrix-multiplication-not-have","title":"2. Which property does matrix multiplication NOT have?","text":"<ol> <li>Associativity: \\((AB)C = A(BC)\\)</li> <li>Distributivity: \\(A(B + C) = AB + AC\\)</li> <li>Commutativity: \\(AB = BA\\)</li> <li>Identity: \\(AI = IA = A\\)</li> </ol> Show Answer <p>The correct answer is C. Matrix multiplication is not commutative in general\u2014\\(AB \\neq BA\\) for most matrices. The order of multiplication matters because each product represents a different sequence of transformations.</p> <p>Concept Tested: Matrix Multiplication Properties</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#3-what-is-the-transpose-of-a-matrix","title":"3. What is the transpose of a matrix?","text":"<ol> <li>The matrix with all elements negated</li> <li>The matrix with rows and columns interchanged</li> <li>The inverse of the matrix</li> <li>The matrix multiplied by itself</li> </ol> Show Answer <p>The correct answer is B. The transpose of a matrix is obtained by interchanging its rows and columns. If \\(A\\) is an \\(m \\times n\\) matrix, then \\(A^T\\) is an \\(n \\times m\\) matrix where \\((A^T)_{ij} = A_{ji}\\).</p> <p>Concept Tested: Matrix Transpose</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#4-a-square-matrix-a-is-invertible-if-and-only-if","title":"4. A square matrix \\(A\\) is invertible if and only if:","text":"<ol> <li>It is symmetric</li> <li>Its determinant is non-zero</li> <li>All diagonal elements are positive</li> <li>It has more rows than columns</li> </ol> Show Answer <p>The correct answer is B. A square matrix is invertible (has an inverse) if and only if its determinant is non-zero. A zero determinant indicates the matrix is singular and maps some non-zero vectors to zero, making the transformation irreversible.</p> <p>Concept Tested: Matrix Inverse</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#5-what-is-the-identity-matrix","title":"5. What is the identity matrix?","text":"<ol> <li>A matrix with all ones</li> <li>A matrix with all zeros</li> <li>A square matrix with ones on the diagonal and zeros elsewhere</li> <li>A matrix that equals its transpose</li> </ol> Show Answer <p>The correct answer is C. The identity matrix \\(I\\) is a square matrix with ones on the main diagonal and zeros elsewhere. It is the multiplicative identity for matrices: \\(AI = IA = A\\) for any compatible matrix \\(A\\).</p> <p>Concept Tested: Identity Matrix</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#6-if-a-is-a-3-times-2-matrix-and-b-is-a-2-times-4-matrix-what-are-the-dimensions-of-ab","title":"6. If \\(A\\) is a \\(3 \\times 2\\) matrix and \\(B\\) is a \\(2 \\times 4\\) matrix, what are the dimensions of \\(AB\\)?","text":"<ol> <li>\\(2 \\times 2\\)</li> <li>\\(3 \\times 4\\)</li> <li>\\(4 \\times 3\\)</li> <li>\\(3 \\times 2\\)</li> </ol> Show Answer <p>The correct answer is B. For matrix multiplication \\(AB\\) where \\(A\\) is \\(3 \\times 2\\) and \\(B\\) is \\(2 \\times 4\\), the inner dimensions (2) match, so multiplication is valid. The result has dimensions from the outer dimensions: \\(3 \\times 4\\).</p> <p>Concept Tested: Matrix Multiplication</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#7-a-symmetric-matrix-satisfies-which-condition","title":"7. A symmetric matrix satisfies which condition?","text":"<ol> <li>\\(A = -A\\)</li> <li>\\(A = A^T\\)</li> <li>\\(A = A^{-1}\\)</li> <li>\\(A = A^2\\)</li> </ol> Show Answer <p>The correct answer is B. A symmetric matrix equals its own transpose: \\(A = A^T\\). This means \\(A_{ij} = A_{ji}\\) for all \\(i, j\\). Symmetric matrices have real eigenvalues and orthogonal eigenvectors.</p> <p>Concept Tested: Symmetric Matrix</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#8-what-does-the-rank-of-a-matrix-represent","title":"8. What does the rank of a matrix represent?","text":"<ol> <li>The number of rows in the matrix</li> <li>The number of non-zero elements</li> <li>The dimension of the column space (number of linearly independent columns)</li> <li>The determinant value</li> </ol> Show Answer <p>The correct answer is C. The rank of a matrix is the dimension of its column space, which equals the number of linearly independent columns (or equivalently, rows). Rank indicates the effective dimensionality of the transformation the matrix represents.</p> <p>Concept Tested: Matrix Rank</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#9-if-a-1-exists-what-is-a-cdot-a-1","title":"9. If \\(A^{-1}\\) exists, what is \\(A \\cdot A^{-1}\\)?","text":"<ol> <li>The zero matrix</li> <li>The identity matrix</li> <li>The transpose of \\(A\\)</li> <li>\\(A\\) squared</li> </ol> Show Answer <p>The correct answer is B. By definition, \\(A \\cdot A^{-1} = A^{-1} \\cdot A = I\\), the identity matrix. The inverse \"undoes\" the transformation performed by the original matrix.</p> <p>Concept Tested: Matrix Inverse</p>"},{"location":"chapters/02-matrices-and-matrix-operations/quiz/#10-an-orthogonal-matrix-q-satisfies","title":"10. An orthogonal matrix \\(Q\\) satisfies:","text":"<ol> <li>\\(Q^T = Q\\)</li> <li>\\(Q^T = Q^{-1}\\)</li> <li>\\(Q^2 = I\\)</li> <li>\\(Q + Q^T = I\\)</li> </ol> Show Answer <p>The correct answer is B. An orthogonal matrix satisfies \\(Q^T = Q^{-1}\\), which means \\(Q^TQ = QQ^T = I\\). Orthogonal matrices preserve lengths and angles, representing rotations and reflections.</p> <p>Concept Tested: Orthogonal Matrix</p>"},{"location":"chapters/03-systems-of-linear-equations/","title":"Systems of Linear Equations","text":""},{"location":"chapters/03-systems-of-linear-equations/#summary","title":"Summary","text":"<p>This chapter teaches you to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields. You will master Gaussian elimination, understand row echelon forms, and learn to analyze when solutions exist, are unique, or are infinite. These computational techniques are essential for everything from optimization to neural network training.</p>"},{"location":"chapters/03-systems-of-linear-equations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"chapters/03-systems-of-linear-equations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#introduction","title":"Introduction","text":"<p>Systems of linear equations appear everywhere in science, engineering, and data analysis. Whether you're balancing chemical reactions, analyzing electrical circuits, fitting models to data, or training neural networks, you're solving linear systems. This chapter develops the systematic methods for solving these systems and\u2014equally important\u2014understanding when solutions exist and what form they take.</p> <p>The power of linear algebra lies in its ability to represent complex systems compactly and solve them efficiently. A problem that might involve dozens or thousands of equations becomes a single matrix equation, and the solution emerges through systematic elimination procedures that computers execute with remarkable speed.</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equations-and-systems","title":"Linear Equations and Systems","text":""},{"location":"chapters/03-systems-of-linear-equations/#what-is-a-linear-equation","title":"What is a Linear Equation?","text":"<p>A linear equation in variables \\(x_1, x_2, \\ldots, x_n\\) has the form:</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equation-standard-form","title":"Linear Equation Standard Form","text":"<p>\\(a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\\)</p> <p>where:</p> <ul> <li>\\(a_1, a_2, \\ldots, a_n\\) are the coefficients (constants)</li> <li>\\(x_1, x_2, \\ldots, x_n\\) are the variables (unknowns)</li> <li>\\(b\\) is the constant term (right-hand side)</li> </ul> <p>The equation is \"linear\" because each variable appears only to the first power and is not multiplied by other variables. The graph of a linear equation in two variables is a line; in three variables, it's a plane.</p> <p>Examples of linear equations:</p> <ul> <li>\\(3x + 2y = 7\\)</li> <li>\\(x_1 - 4x_2 + x_3 = 0\\)</li> <li>\\(w + 2x - y + 3z = 5\\)</li> </ul> <p>Non-linear equations (not covered by these methods):</p> <ul> <li>\\(x^2 + y = 5\\) (squared term)</li> <li>\\(xy = 3\\) (product of variables)</li> <li>\\(\\sin(x) + y = 1\\) (nonlinear function)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#systems-of-equations","title":"Systems of Equations","text":"<p>A system of equations is a collection of two or more equations involving the same variables. A solution to the system must satisfy all equations simultaneously.</p> <p>Consider a system of \\(m\\) linear equations in \\(n\\) unknowns:</p> <p>\\(a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1\\)</p> <p>\\(a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2\\)</p> <p>\\(\\vdots\\)</p> <p>\\(a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m\\)</p> <p>Geometrically, each equation defines a hyperplane, and solving the system means finding where all hyperplanes intersect.</p> Equations Variables Geometric Interpretation 2 equations 2 variables Intersection of two lines 3 equations 3 variables Intersection of three planes \\(m\\) equations \\(n\\) variables Intersection of \\(m\\) hyperplanes in \\(\\mathbb{R}^n\\)"},{"location":"chapters/03-systems-of-linear-equations/#diagram-system-of-equations-geometry","title":"Diagram: System of Equations Geometry","text":"System of Equations Geometric Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, explain</p> <p>Learning Objective: Help students visualize how the solution to a system of linear equations corresponds to the geometric intersection of lines (2D) or planes (3D).</p> <p>Canvas layout: - Main area: 2D/3D coordinate system with equations plotted - Right panel: Equation display and controls</p> <p>Visual elements: - 2D mode: Two or three lines with intersection point highlighted - 3D mode: Two or three planes with intersection shown - Solution point marked with a distinct marker - Coordinate axes with labels - Grid for reference</p> <p>Interactive controls: - Toggle: 2D / 3D mode - Sliders: Coefficients for each equation (a, b, c values) - Dropdown: Number of equations (2 or 3) - Checkbox: Show solution coordinates - Button: Generate random system - Dropdown: Solution type (unique, infinite, none)</p> <p>Default parameters: - Mode: 2D - Equations: 2 - Example: x + y = 3, x - y = 1 (solution at (2, 1))</p> <p>Behavior: - Lines/planes update in real-time as coefficients change - Intersection point updates dynamically - When lines are parallel (no solution), display message - When lines are coincident (infinite solutions), highlight entire line - 3D mode allows rotation and zoom</p> <p>Implementation: p5.js with WEBGL for 3D mode</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-representation","title":"Matrix Representation","text":""},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form","title":"Matrix Equation Form","text":"<p>Any system of linear equations can be written compactly as a single matrix equation:</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form_1","title":"Matrix Equation Form","text":"<p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is the \\(m \\times n\\) coefficient matrix</li> <li>\\(\\mathbf{x}\\) is the \\(n \\times 1\\) vector of unknowns</li> <li>\\(\\mathbf{b}\\) is the \\(m \\times 1\\) vector of constants</li> </ul> <p>For the system:</p> <p>\\(2x + 3y = 8\\)</p> <p>\\(x - y = 1\\)</p> <p>The matrix form is:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 1 \\end{bmatrix}\\)</p> <p>This representation reveals the structure: the coefficient matrix \\(\\mathbf{A}\\) encodes how variables combine, while \\(\\mathbf{b}\\) specifies the targets.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-augmented-matrix","title":"The Augmented Matrix","text":"<p>The augmented matrix combines the coefficient matrix and the right-hand side into a single matrix for manipulation:</p> <p>\\([\\mathbf{A} \\mid \\mathbf{b}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{bmatrix}\\)</p> <p>The vertical bar separates coefficients from constants, though it's sometimes omitted. All solution procedures work on the augmented matrix, treating coefficients and constants together.</p> <p>For our example:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; | &amp; 8 \\\\ 1 &amp; -1 &amp; | &amp; 1 \\end{bmatrix}\\)</p> <p>Why Augmented Matrices?</p> <p>The augmented matrix is the standard data structure for solving linear systems. It keeps all relevant information together and allows us to track how row operations affect both coefficients and constants simultaneously.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-operations","title":"Row Operations","text":"<p>The key to solving linear systems is transforming the augmented matrix into a simpler form while preserving the solution set. Three row operations accomplish this.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-three-elementary-row-operations","title":"The Three Elementary Row Operations","text":"<ol> <li>Row Swap (Interchange): Exchange two rows</li> <li>Notation: \\(R_i \\leftrightarrow R_j\\)</li> <li> <p>Swaps row \\(i\\) with row \\(j\\)</p> </li> <li> <p>Row Scaling (Multiplication): Multiply a row by a nonzero constant</p> </li> <li>Notation: \\(kR_i \\rightarrow R_i\\) (where \\(k \\neq 0\\))</li> <li> <p>Multiplies every entry in row \\(i\\) by \\(k\\)</p> </li> <li> <p>Row Addition (Replacement): Add a multiple of one row to another</p> </li> <li>Notation: \\(R_i + kR_j \\rightarrow R_i\\)</li> <li>Adds \\(k\\) times row \\(j\\) to row \\(i\\)</li> </ol> Operation Effect Preserves Solutions? Row Swap Reorders equations Yes Row Scaling Scales an equation Yes (if \\(k \\neq 0\\)) Row Addition Combines equations Yes <p>These operations correspond to valid algebraic manipulations of equations:</p> <ul> <li>Swapping two equations doesn't change solutions</li> <li>Multiplying an equation by a nonzero constant doesn't change its solutions</li> <li>Adding equations produces a valid new equation satisfied by the same solutions</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-row-operations-interactive","title":"Diagram: Row Operations Interactive","text":"Row Operations Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, demonstrate</p> <p>Learning Objective: Enable students to practice applying the three elementary row operations and observe how they transform the augmented matrix while preserving solutions.</p> <p>Canvas layout: - Top: Current augmented matrix (large, clear display) - Middle: Operation selector and parameters - Bottom: History of operations performed</p> <p>Visual elements: - Matrix displayed as a grid with clear row/column separation - Active rows highlighted during operation - Animation showing transformation - Vertical bar separating coefficients from constants - Operation history as a scrollable list</p> <p>Interactive controls: - Dropdown: Select operation (Swap, Scale, Add) - For Swap: Two row selectors - For Scale: Row selector + scalar input (prevent 0) - For Add: Target row + source row + multiplier - Button: Apply operation - Button: Undo last operation - Button: Reset to original - Toggle: Show intermediate steps with animation</p> <p>Default parameters: - Starting matrix: 3\u00d74 augmented matrix - Example system: 2x + y - z = 8, -3x - y + 2z = -11, -2x + y + 2z = -3</p> <p>Behavior: - Selected rows highlight before operation - Animation shows values changing - History updates with notation (e.g., \"R\u2081 \u2194 R\u2082\") - Undo restores previous state - Invalid operations (scaling by 0) show error</p> <p>Implementation: p5.js with animation states</p>"},{"location":"chapters/03-systems-of-linear-equations/#gaussian-elimination","title":"Gaussian Elimination","text":"<p>Gaussian elimination is the systematic procedure for reducing an augmented matrix to row echelon form using row operations. Named after Carl Friedrich Gauss, this algorithm is the workhorse of computational linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-algorithm","title":"The Algorithm","text":"<p>Gaussian elimination proceeds column by column, from left to right:</p> <ol> <li>Find a pivot: Locate the leftmost column with a nonzero entry</li> <li>Position the pivot: Use row swaps to move a nonzero entry to the top of the working submatrix</li> <li>Eliminate below: Use row addition to create zeros below the pivot</li> <li>Move down: Repeat for the next column with the remaining rows</li> </ol> <p>The goal is to create an \"upper triangular\" structure where all entries below the main diagonal are zero.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-echelon-form","title":"Row Echelon Form","text":"<p>A matrix is in row echelon form (REF) if:</p> <ul> <li>All zero rows are at the bottom</li> <li>The leading entry (first nonzero entry) of each row is to the right of the leading entry of the row above</li> <li>All entries below a leading entry are zero</li> </ul> <p>A pivot position is the location of a leading 1 (or leading nonzero entry). A pivot column is a column containing a pivot position.</p> <p>Example of row echelon form:</p> <p>\\(\\begin{bmatrix} \\boxed{2} &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; \\boxed{1} &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; \\boxed{3} &amp; | &amp; 6 \\end{bmatrix}\\)</p> <p>The boxed entries are pivots. Each pivot is to the right of the pivot above it.</p>"},{"location":"chapters/03-systems-of-linear-equations/#back-substitution","title":"Back Substitution","text":"<p>Once a matrix is in row echelon form, we solve for the variables using back substitution\u2014working from the bottom row upward:</p> <ol> <li>Solve the last equation for its variable</li> <li>Substitute into the equation above and solve</li> <li>Continue upward until all variables are found</li> </ol> <p>For the example above:</p> <ul> <li>Row 3: \\(3z = 6 \\Rightarrow z = 2\\)</li> <li>Row 2: \\(y + 4(2) = -2 \\Rightarrow y = -10\\)</li> <li>Row 1: \\(2x + 3(-10) - 2 = 5 \\Rightarrow x = 18.5\\)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-gaussian-elimination-visualizer","title":"Diagram: Gaussian Elimination Visualizer","text":"Gaussian Elimination Step-by-Step Animator <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, implement</p> <p>Learning Objective: Guide students through the complete Gaussian elimination algorithm, showing each row operation and explaining why it's performed.</p> <p>Canvas layout: - Top: Current matrix state with pivot highlighted - Middle: Current operation being performed with explanation - Bottom: Controls and solution display</p> <p>Visual elements: - Matrix with current pivot position highlighted in yellow - Rows being modified highlighted in blue - Zeros created by elimination shown in green (briefly) - Current column being processed indicated - Progress indicator showing algorithm phase - Final solution displayed when complete</p> <p>Interactive controls: - Button: Next Step (advances one row operation) - Button: Auto-solve (animates entire solution) - Slider: Animation speed - Button: Reset - Dropdown: Example system (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show explanatory text for each step - Checkbox: Enable back substitution phase</p> <p>Default parameters: - Matrix size: 3\u00d73 - Animation speed: 1 second per step - Show explanations: true</p> <p>Behavior: - Each step highlights the relevant rows and pivot - Explanation text describes the operation and purpose - Algorithm phases clearly labeled (Forward elimination, Back substitution) - Solution verified by substitution at end - Can step forward/backward through operations</p> <p>Implementation: p5.js with state machine for algorithm steps</p>"},{"location":"chapters/03-systems-of-linear-equations/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>While row echelon form allows back substitution, reduced row echelon form (RREF) goes further to directly reveal solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#definition-and-properties","title":"Definition and Properties","text":"<p>A matrix is in reduced row echelon form if:</p> <ul> <li>It is in row echelon form</li> <li>Each pivot is 1</li> <li>Each pivot is the only nonzero entry in its column</li> </ul> <p>The Gauss-Jordan elimination algorithm extends Gaussian elimination to produce RREF by:</p> <ol> <li>Scaling each pivot row to make the pivot equal to 1</li> <li>Eliminating entries above each pivot (not just below)</li> </ol> <p>Example transformation to RREF:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; 3 &amp; | &amp; 6 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; | &amp; 18.5 \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -10 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>In RREF, the solution is immediately visible: \\(x = 18.5\\), \\(y = -10\\), \\(z = 2\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#comparing-ref-and-rref","title":"Comparing REF and RREF","text":"Property Row Echelon Form Reduced Row Echelon Form Zero rows At bottom At bottom Leading entries Nonzero (any value) Exactly 1 Below pivots All zeros All zeros Above pivots Any value All zeros Solution method Back substitution Direct reading Uniqueness Not unique Unique <p>Computational Efficiency</p> <p>For solving a single system, stopping at REF and using back substitution is often more efficient. RREF requires additional operations. However, RREF is valuable for understanding solution structure and for systems requiring multiple right-hand sides.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-ref-vs-rref-comparison","title":"Diagram: REF vs RREF Comparison","text":"REF vs RREF Side-by-Side Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Help students understand the difference between row echelon form and reduced row echelon form, and when each is preferable.</p> <p>Layout: Side-by-side matrix displays with transformation arrows</p> <p>Visual elements: - Left panel: Original matrix - Center-left: Row echelon form with pivots highlighted - Center-right: Reduced row echelon form with pivots highlighted - Annotations showing key differences - Color coding: pivots (gold), zeros created (green), coefficients (blue)</p> <p>Interactive features: - Button: Generate new random system - Dropdown: Matrix size (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show step count for each form - Toggle: Show solution extraction method - Hover: Show definition of each form</p> <p>Information displayed: - Number of operations to reach each form - Solution method for each (back substitution vs direct) - Uniqueness property highlighted</p> <p>Color scheme: - Pivot positions: gold - Created zeros: light green - Original nonzero entries: blue - Structure indicators: gray outlines</p> <p>Implementation: HTML/CSS/JavaScript with matrix computation</p>"},{"location":"chapters/03-systems-of-linear-equations/#solution-analysis","title":"Solution Analysis","text":"<p>Not every system of linear equations has a solution, and some have infinitely many. Understanding solution existence and uniqueness is as important as computing solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#types-of-solution-sets","title":"Types of Solution Sets","text":"<p>The solution set of a system is the collection of all vectors \\(\\mathbf{x}\\) satisfying \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). Three possibilities exist:</p> <ol> <li>Unique Solution: Exactly one solution exists</li> <li>Infinite Solutions: Infinitely many solutions exist (forming a line, plane, or higher-dimensional subspace)</li> <li>No Solution: No solution exists (the system is inconsistent)</li> </ol> <p>The row echelon form reveals which case applies:</p> <ul> <li>No solution: A row of the form \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) where \\(c \\neq 0\\) (equation \\(0 = c\\))</li> <li>Unique solution: Every column is a pivot column (as many pivots as variables)</li> <li>Infinite solutions: Some columns are not pivot columns (fewer pivots than variables)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#basic-and-free-variables","title":"Basic and Free Variables","text":"<p>In systems with infinitely many solutions, variables split into two types:</p> <ul> <li>Basic variables correspond to pivot columns\u2014they can be expressed in terms of other variables</li> <li>Free variables correspond to non-pivot columns\u2014they can take any value</li> </ul> <p>The number of free variables determines the \"dimension\" of the solution set:</p> Free Variables Solution Set 0 Unique point 1 Line 2 Plane \\(k\\) \\(k\\)-dimensional subspace <p>Example with a free variable:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>Here \\(x_1\\) and \\(x_3\\) are basic variables (columns 1 and 3 are pivot columns), while \\(x_2\\) is free. The general solution is:</p> <p>\\(x_1 = 4 - 2x_2 - 3(2) = -2 - 2x_2\\)</p> <p>\\(x_2 = \\text{free}\\)</p> <p>\\(x_3 = 2\\)</p> <p>Or in vector form: \\(\\mathbf{x} = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\)</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-solution-set-visualizer","title":"Diagram: Solution Set Visualizer","text":"Solution Set Type Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: classify, examine</p> <p>Learning Objective: Enable students to explore how different systems produce unique solutions, infinite solutions (lines/planes), or no solution, and to identify the determining factors.</p> <p>Canvas layout: - Left: Augmented matrix (editable or preset) - Center: 2D/3D geometric visualization - Right: Solution analysis panel</p> <p>Visual elements: - Matrix with pivot positions marked - Geometric view showing lines/planes - Solution point, line, or empty set visualized - Pivot columns highlighted - Free variable columns indicated - Parametric solution displayed (for infinite solutions)</p> <p>Interactive controls: - Dropdown: Preset examples (unique, infinite, none) - Editable matrix cells (for custom exploration) - Toggle: 2D / 3D visualization - Checkbox: Show row echelon form - Checkbox: Show reduced row echelon form - Button: Random consistent system - Button: Random inconsistent system</p> <p>Default parameters: - Mode: 2D - Example: unique solution case</p> <p>Behavior: - Matrix edits update visualization in real-time - Inconsistent systems show parallel lines/planes (no intersection) - Infinite solutions show line or plane of solutions - Solution type automatically detected and labeled - Free variables identified and highlighted</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#homogeneous-systems","title":"Homogeneous Systems","text":"<p>A homogeneous system has all zero constants on the right-hand side:</p> <p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)</p> <p>Homogeneous systems have special properties that make them particularly important in linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-trivial-solution","title":"The Trivial Solution","text":"<p>Every homogeneous system has at least one solution: \\(\\mathbf{x} = \\mathbf{0}\\). This is called the trivial solution because it's obvious\u2014all zeros satisfy any homogeneous equation.</p> <p>The interesting question is whether nontrivial solutions exist.</p>"},{"location":"chapters/03-systems-of-linear-equations/#existence-of-nontrivial-solutions","title":"Existence of Nontrivial Solutions","text":"<p>A homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has nontrivial solutions if and only if it has free variables. This happens when:</p> <ul> <li>The number of variables exceeds the number of pivot columns</li> <li>Equivalently, the number of variables exceeds the rank of \\(\\mathbf{A}\\)</li> </ul> <p>Important consequence: If a homogeneous system has more variables than equations, it always has nontrivial solutions.</p> Variables vs Equations Nontrivial Solutions? Variables &gt; Equations Always Variables = Equations Maybe (depends on matrix) Variables &lt; Equations Maybe (depends on matrix)"},{"location":"chapters/03-systems-of-linear-equations/#solution-space-structure","title":"Solution Space Structure","text":"<p>The solution set of a homogeneous system forms a subspace called the null space of \\(\\mathbf{A}\\). This subspace:</p> <ul> <li>Contains the zero vector</li> <li>Is closed under addition (sum of solutions is a solution)</li> <li>Is closed under scalar multiplication (scalar times solution is a solution)</li> </ul> <p>This structure is fundamental to understanding linear transformations and will be explored further in later chapters.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-homogeneous-system-explorer","title":"Diagram: Homogeneous System Explorer","text":"Homogeneous System Solution Space Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that homogeneous systems always have the trivial solution and may have additional solutions forming a subspace through the origin.</p> <p>Canvas layout: - Left: Coefficient matrix A (editable) - Center: 2D/3D visualization of solution space - Right: Solution analysis</p> <p>Visual elements: - Matrix display with rank calculation - Coordinate axes through origin - Trivial solution (origin) always marked - Nontrivial solution space (line or plane through origin) when present - Basis vectors for solution space shown as arrows - Null space dimension displayed</p> <p>Interactive controls: - Matrix size selector (2\u00d72, 2\u00d73, 3\u00d73, 3\u00d74) - Editable matrix entries - Button: Generate random full-rank matrix (only trivial solution) - Button: Generate random rank-deficient matrix (nontrivial solutions) - Toggle: Show basis vectors for null space - Slider: Rotate 3D view</p> <p>Default parameters: - Size: 3\u00d73 - Initial matrix: rank-deficient (has null space)</p> <p>Behavior: - Real-time rank calculation - Solution space updates as matrix changes - Clear indication of trivial-only vs nontrivial solutions - Null space dimension shown (n - rank) - 3D rotation for spatial understanding</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#numerical-stability","title":"Numerical Stability","text":"<p>In practice, solving linear systems on computers introduces challenges beyond the pure mathematics.</p>"},{"location":"chapters/03-systems-of-linear-equations/#sources-of-numerical-error","title":"Sources of Numerical Error","text":"<p>Numerical stability refers to how errors propagate through a computation. In linear systems, instability can arise from:</p> <ul> <li>Floating-point representation: Real numbers are stored with limited precision</li> <li>Round-off errors: Each arithmetic operation may introduce small errors</li> <li>Ill-conditioned systems: Some systems amplify small input errors into large output errors</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#partial-pivoting","title":"Partial Pivoting","text":"<p>Standard Gaussian elimination can suffer from numerical instability when small pivots lead to division by near-zero values. Partial pivoting addresses this by selecting the largest available pivot:</p> <ol> <li>Before eliminating in a column, scan downward for the entry with largest absolute value</li> <li>Swap rows to bring this entry to the pivot position</li> <li>Proceed with elimination</li> </ol> <p>This strategy prevents division by small numbers and improves numerical stability.</p>"},{"location":"chapters/03-systems-of-linear-equations/#condition-number","title":"Condition Number","text":"<p>The condition number of a matrix quantifies its sensitivity to perturbations. A high condition number indicates an ill-conditioned system where small changes in input cause large changes in output.</p> <ul> <li>Condition number \u2248 1: Well-conditioned (stable)</li> <li>Condition number large (e.g., \\(10^6\\)): Ill-conditioned (unstable)</li> <li>Condition number = \u221e: Singular matrix (no unique solution)</li> </ul> <p>Practical Implications</p> <p>When working with real data, always consider numerical stability. Libraries like NumPy use sophisticated algorithms (LU decomposition with partial pivoting) that are more stable than naive Gaussian elimination. Never compute \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly\u2014use specialized solvers instead.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-numerical-stability-demonstration","title":"Diagram: Numerical Stability Demonstration","text":"Numerical Stability and Condition Number Explorer <p>Type: microsim</p> <p>Bloom Level: Evaluate (L5) Bloom Verb: assess, judge</p> <p>Learning Objective: Demonstrate how small changes in matrix entries can cause large changes in solutions for ill-conditioned systems, and how partial pivoting improves stability.</p> <p>Canvas layout: - Left: Original system and computed solution - Center: Perturbed system and new solution - Right: Analysis (condition number, error magnification)</p> <p>Visual elements: - Two matrices side-by-side (original and perturbed) - Solutions displayed with precision indicators - Error visualization (bar chart showing input vs output error) - Condition number prominently displayed - Color coding: green (well-conditioned) to red (ill-conditioned) - 2D geometric view showing how solution moves</p> <p>Interactive controls: - Dropdown: Example type (well-conditioned, moderately ill-conditioned, severely ill-conditioned) - Slider: Perturbation magnitude (0.0001 to 0.1) - Toggle: Use partial pivoting - Button: Apply random perturbation - Checkbox: Show geometric interpretation - Display: Precision (decimal places shown)</p> <p>Default parameters: - Example: Hilbert matrix 3\u00d73 (ill-conditioned) - Perturbation: 0.001 - Partial pivoting: off</p> <p>Behavior: - Perturbation applied to random entries - Solution error calculated and displayed - Error magnification factor shown (output error / input error) - Comparison with/without partial pivoting - Geometric view shows solution point movement</p> <p>Implementation: p5.js with high-precision arithmetic library</p>"},{"location":"chapters/03-systems-of-linear-equations/#applications","title":"Applications","text":""},{"location":"chapters/03-systems-of-linear-equations/#balancing-chemical-equations","title":"Balancing Chemical Equations","text":"<p>Chemical equations must balance atoms. For the reaction:</p> <p>\\(a \\text{CH}_4 + b \\text{O}_2 \\rightarrow c \\text{CO}_2 + d \\text{H}_2\\text{O}\\)</p> <p>Balancing each element gives a linear system:</p> <ul> <li>Carbon: \\(a = c\\)</li> <li>Hydrogen: \\(4a = 2d\\)</li> <li>Oxygen: \\(2b = 2c + d\\)</li> </ul> <p>This homogeneous system has a one-dimensional solution space. Setting \\(a = 1\\) gives the balanced equation: \\(\\text{CH}_4 + 2\\text{O}_2 \\rightarrow \\text{CO}_2 + 2\\text{H}_2\\text{O}\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#network-flow-analysis","title":"Network Flow Analysis","text":"<p>In electrical circuits or traffic networks, conservation laws produce linear systems. At each node, inflow equals outflow. The resulting system determines currents or traffic flows throughout the network.</p>"},{"location":"chapters/03-systems-of-linear-equations/#machine-learning-linear-regression","title":"Machine Learning: Linear Regression","text":"<p>Fitting a linear model \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}\\) to data leads to the normal equations:</p> <p>\\(\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\)</p> <p>Solving this system gives the least-squares coefficients \\(\\boldsymbol{\\beta}\\) that minimize prediction error.</p>"},{"location":"chapters/03-systems-of-linear-equations/#neural-network-training","title":"Neural Network Training","text":"<p>Training neural networks involves solving systems of equations (approximately) at each optimization step. The gradient computations that guide learning rely on the same matrix operations covered in this chapter.</p>"},{"location":"chapters/03-systems-of-linear-equations/#computational-implementation","title":"Computational Implementation","text":""},{"location":"chapters/03-systems-of-linear-equations/#numpy-example","title":"NumPy Example","text":"<pre><code>import numpy as np\n\n# Define the system Ax = b\nA = np.array([[2, 3, -1],\n              [4, 4, -3],\n              [1, -1, 2]])\nb = np.array([5, 3, 1])\n\n# Solve using NumPy (LU decomposition with pivoting)\nx = np.linalg.solve(A, b)\nprint(f\"Solution: {x}\")\n\n# Verify: check that Ax = b\nprint(f\"Verification (Ax): {A @ x}\")\nprint(f\"Condition number: {np.linalg.cond(A):.2f}\")\n</code></pre>"},{"location":"chapters/03-systems-of-linear-equations/#key-functions","title":"Key Functions","text":"Function Purpose <code>np.linalg.solve(A, b)</code> Solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) <code>np.linalg.matrix_rank(A)</code> Compute rank <code>np.linalg.cond(A)</code> Compute condition number <code>scipy.linalg.lu(A)</code> LU decomposition <code>np.linalg.lstsq(A, b)</code> Least squares solution"},{"location":"chapters/03-systems-of-linear-equations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter developed the theory and practice of solving systems of linear equations.</p> <p>Formulation:</p> <ul> <li>A linear equation has variables appearing to the first power only</li> <li>A system of equations requires simultaneous satisfaction of multiple equations</li> <li>Matrix equation form \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) compactly represents the system</li> <li>The augmented matrix \\([\\mathbf{A} | \\mathbf{b}]\\) combines coefficients and constants</li> </ul> <p>Solution Methods:</p> <ul> <li>Row operations (swap, scale, add) transform systems while preserving solutions</li> <li>Gaussian elimination reduces to row echelon form</li> <li>Back substitution solves from bottom to top</li> <li>Reduced row echelon form allows direct solution reading</li> </ul> <p>Solution Analysis:</p> <ul> <li>Pivot positions and pivot columns determine solution structure</li> <li>Unique solution: all columns are pivot columns</li> <li>Infinite solutions: some columns are free (non-pivot)</li> <li>No solution: inconsistent row \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) with \\(c \\neq 0\\)</li> <li>Free variables can take any value; basic variables are determined by them</li> </ul> <p>Special Systems:</p> <ul> <li>Homogeneous systems (\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)) always have the trivial solution</li> <li>Nontrivial solutions exist when there are free variables</li> <li>The solution set of a homogeneous system is a subspace</li> </ul> <p>Computational Considerations:</p> <ul> <li>Numerical stability matters for practical computation</li> <li>Partial pivoting improves stability</li> <li>Condition number measures sensitivity to perturbation</li> <li>Use library functions rather than explicit inverse computation</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#exercises","title":"Exercises","text":"Exercise 1: Row Echelon Form <p>Reduce the following augmented matrix to row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; | &amp; 3 \\\\ 2 &amp; 5 &amp; 1 &amp; | &amp; 8 \\\\ 3 &amp; 7 &amp; 0 &amp; | &amp; 11 \\end{bmatrix}\\)</p> <p>Then use back substitution to find the solution.</p> Exercise 2: Solution Type Identification <p>For each augmented matrix in row echelon form, determine whether the system has a unique solution, infinite solutions, or no solution:</p> <p>a) \\(\\begin{bmatrix} 1 &amp; 3 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>b) \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>c) \\(\\begin{bmatrix} 1 &amp; 2 &amp; | &amp; 3 \\\\ 0 &amp; 0 &amp; | &amp; 5 \\end{bmatrix}\\)</p> Exercise 3: Free and Basic Variables <p>Given the reduced row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; | &amp; 3 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; | &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; | &amp; 4 \\end{bmatrix}\\)</p> <p>Identify the pivot columns, free variables, and basic variables. Write the general solution in parametric form.</p> Exercise 4: Homogeneous System <p>Consider the homogeneous system with coefficient matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\)</p> <p>Determine whether nontrivial solutions exist. If so, find a basis for the solution space.</p> Exercise 5: Numerical Stability <p>The Hilbert matrix \\(H_n\\) has entries \\(h_{ij} = \\frac{1}{i+j-1}\\). For \\(n = 3\\):</p> <p>\\(H_3 = \\begin{bmatrix} 1 &amp; 1/2 &amp; 1/3 \\\\ 1/2 &amp; 1/3 &amp; 1/4 \\\\ 1/3 &amp; 1/4 &amp; 1/5 \\end{bmatrix}\\)</p> <p>Compute the condition number of \\(H_3\\). What does this tell you about solving systems with this coefficient matrix?</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/","title":"Quiz: Systems of Linear Equations","text":"<p>Test your understanding of solving linear systems and related concepts.</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#1-what-is-the-matrix-equation-form-of-a-system-of-linear-equations","title":"1. What is the matrix equation form of a system of linear equations?","text":"<ol> <li>\\(A + \\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(\\mathbf{x}A = \\mathbf{b}\\)</li> <li>\\(A\\mathbf{b} = \\mathbf{x}\\)</li> </ol> Show Answer <p>The correct answer is B. A system of linear equations can be written as \\(A\\mathbf{x} = \\mathbf{b}\\), where \\(A\\) is the coefficient matrix, \\(\\mathbf{x}\\) is the vector of unknowns, and \\(\\mathbf{b}\\) is the vector of constants.</p> <p>Concept Tested: Matrix Equation</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#2-what-is-the-purpose-of-gaussian-elimination","title":"2. What is the purpose of Gaussian elimination?","text":"<ol> <li>To compute the determinant of a matrix</li> <li>To transform a matrix to row echelon form for solving linear systems</li> <li>To find the eigenvalues of a matrix</li> <li>To compute the transpose of a matrix</li> </ol> Show Answer <p>The correct answer is B. Gaussian elimination transforms a matrix (or augmented matrix) to row echelon form using elementary row operations. This simplifies the system so that solutions can be found through back substitution.</p> <p>Concept Tested: Gaussian Elimination</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#3-a-system-of-linear-equations-has-infinitely-many-solutions-when","title":"3. A system of linear equations has infinitely many solutions when:","text":"<ol> <li>The coefficient matrix is square</li> <li>There are more unknowns than equations and the system is consistent</li> <li>The determinant of the coefficient matrix is non-zero</li> <li>All equations are identical</li> </ol> Show Answer <p>The correct answer is B. A system has infinitely many solutions when it is consistent (at least one solution exists) but has free variables\u2014typically when there are more unknowns than independent equations, or when rows become zero during elimination.</p> <p>Concept Tested: Solution Types</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#4-what-is-row-echelon-form","title":"4. What is row echelon form?","text":"<ol> <li>A matrix where all elements are in ascending order</li> <li>A matrix with a staircase pattern of leading ones and zeros below them</li> <li>A diagonal matrix</li> <li>A matrix equal to its transpose</li> </ol> Show Answer <p>The correct answer is B. Row echelon form has a staircase pattern: each row's leading entry (pivot) is to the right of the row above it, and all entries below each pivot are zero. This form enables back substitution.</p> <p>Concept Tested: Row Echelon Form</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#5-what-does-it-mean-for-a-system-to-be-inconsistent","title":"5. What does it mean for a system to be inconsistent?","text":"<ol> <li>The system has exactly one solution</li> <li>The system has infinitely many solutions</li> <li>The system has no solution</li> <li>The system has negative solutions</li> </ol> Show Answer <p>The correct answer is C. An inconsistent system has no solution\u2014the equations are contradictory. This occurs when row reduction produces a row like \\([0, 0, \\ldots, 0 | c]\\) where \\(c \\neq 0\\), representing \\(0 = c\\).</p> <p>Concept Tested: Consistent vs Inconsistent Systems</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#6-lu-decomposition-factors-a-matrix-a-as","title":"6. LU decomposition factors a matrix \\(A\\) as:","text":"<ol> <li>\\(A = L + U\\)</li> <li>\\(A = LU\\) where \\(L\\) is lower triangular and \\(U\\) is upper triangular</li> <li>\\(A = L^TU^T\\)</li> <li>\\(A = U^{-1}L^{-1}\\)</li> </ol> Show Answer <p>The correct answer is B. LU decomposition factors a matrix as \\(A = LU\\), where \\(L\\) is a lower triangular matrix (with ones on the diagonal in Doolittle form) and \\(U\\) is an upper triangular matrix. This is useful for efficiently solving multiple systems with the same coefficient matrix.</p> <p>Concept Tested: LU Decomposition</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#7-what-is-back-substitution-used-for","title":"7. What is back substitution used for?","text":"<ol> <li>Converting a matrix to row echelon form</li> <li>Solving a triangular system by working from the last equation upward</li> <li>Computing the inverse of a matrix</li> <li>Finding the determinant</li> </ol> Show Answer <p>The correct answer is B. Back substitution solves an upper triangular system by starting with the last equation (which has only one unknown), solving for that variable, then substituting back into previous equations to find remaining unknowns.</p> <p>Concept Tested: Back Substitution</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#8-the-least-squares-solution-minimizes","title":"8. The least squares solution minimizes:","text":"<ol> <li>The number of variables</li> <li>The sum of squared residuals \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\)</li> <li>The determinant of the coefficient matrix</li> <li>The condition number</li> </ol> Show Answer <p>The correct answer is B. The least squares solution finds the \\(\\mathbf{x}\\) that minimizes the squared error \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\). This is essential for overdetermined systems (more equations than unknowns) where an exact solution may not exist.</p> <p>Concept Tested: Least Squares</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#9-which-matrix-operation-is-not-an-elementary-row-operation","title":"9. Which matrix operation is NOT an elementary row operation?","text":"<ol> <li>Swapping two rows</li> <li>Multiplying a row by a non-zero scalar</li> <li>Adding a multiple of one row to another</li> <li>Taking the transpose of a row</li> </ol> Show Answer <p>The correct answer is D. The three elementary row operations are: (1) swapping two rows, (2) multiplying a row by a non-zero scalar, and (3) adding a multiple of one row to another. Transposing is not an elementary row operation.</p> <p>Concept Tested: Elementary Row Operations</p>"},{"location":"chapters/03-systems-of-linear-equations/quiz/#10-the-normal-equations-for-least-squares-are","title":"10. The normal equations for least squares are:","text":"<ol> <li>\\(A\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\)</li> <li>\\(AA^T\\mathbf{x} = \\mathbf{b}\\)</li> <li>\\(A^{-1}\\mathbf{b} = \\mathbf{x}\\)</li> </ol> Show Answer <p>The correct answer is B. The normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) are derived by setting the gradient of the squared error to zero. They provide the least squares solution when \\(A^TA\\) is invertible.</p> <p>Concept Tested: Normal Equations</p>"},{"location":"chapters/04-linear-transformations/","title":"Linear Transformations","text":""},{"location":"chapters/04-linear-transformations/#summary","title":"Summary","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition. You will learn about rotation, scaling, shearing, reflection, and projection transformations, and understand abstract concepts like kernel, range, and change of basis. These ideas are fundamental to computer graphics, robotics, and understanding how neural networks transform data.</p>"},{"location":"chapters/04-linear-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"chapters/04-linear-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> </ul>"},{"location":"chapters/04-linear-transformations/#introduction","title":"Introduction","text":"<p>Every matrix multiplication \\(\\mathbf{A}\\mathbf{x}\\) can be viewed as a transformation\u2014taking an input vector \\(\\mathbf{x}\\) and producing an output vector \\(\\mathbf{y}\\). This perspective transforms matrices from static arrays of numbers into dynamic operators that rotate, scale, shear, project, and transform geometric objects.</p> <p>Understanding transformations is essential for computer graphics, where every rotation, scaling, and perspective projection is a matrix multiplication. It's equally crucial for machine learning, where neural networks apply layer after layer of linear transformations (with nonlinearities between them) to map inputs to outputs. This chapter develops the mathematical framework for understanding these transformations and their properties.</p>"},{"location":"chapters/04-linear-transformations/#functions-and-transformations","title":"Functions and Transformations","text":""},{"location":"chapters/04-linear-transformations/#functions-between-vector-spaces","title":"Functions Between Vector Spaces","text":"<p>A function \\(T\\) from a set \\(V\\) to a set \\(W\\), written \\(T: V \\rightarrow W\\), is a rule that assigns to each element \\(\\mathbf{v}\\) in \\(V\\) exactly one element \\(T(\\mathbf{v})\\) in \\(W\\).</p> <p>Key terminology:</p> <ul> <li>The domain of \\(T\\) is the set \\(V\\) of all possible inputs</li> <li>The codomain of \\(T\\) is the set \\(W\\) of all possible outputs</li> <li>The image of a vector \\(\\mathbf{v}\\) is its output \\(T(\\mathbf{v})\\)</li> <li>The range (or image of \\(T\\)) is the set of all outputs: \\(\\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</li> </ul> <p>Range vs Codomain</p> <p>The codomain is the set where outputs could live. The range is where outputs actually live. For example, \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) has codomain \\(\\mathbb{R}^3\\), but its range might be a plane within \\(\\mathbb{R}^3\\).</p>"},{"location":"chapters/04-linear-transformations/#linear-transformations_1","title":"Linear Transformations","text":"<p>A linear transformation (or linear map) is a function \\(T: V \\rightarrow W\\) between vector spaces that preserves vector addition and scalar multiplication:</p>"},{"location":"chapters/04-linear-transformations/#linearity-conditions","title":"Linearity Conditions","text":"<ol> <li> <p>\\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in V\\)</p> </li> <li> <p>\\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) for all \\(\\mathbf{v} \\in V\\) and scalars \\(c\\)</p> </li> </ol> <p>These two conditions can be combined into one:</p> <p>\\(T(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2) = c_1 T(\\mathbf{v}_1) + c_2 T(\\mathbf{v}_2)\\)</p> <p>Linear transformations preserve linear combinations. They map lines to lines (or points), planes to planes (or lines or points), and the origin to the origin.</p> Property Linear Non-Linear Origin maps to origin Always Not necessarily Lines map to... Lines or points Curves possible Parallelism preserved Yes No Grid structure Preserved Distorted"},{"location":"chapters/04-linear-transformations/#the-transformation-matrix","title":"The Transformation Matrix","text":"<p>Every linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) can be represented by an \\(m \\times n\\) matrix \\(\\mathbf{A}\\). The transformation is then:</p> <p>\\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\)</p> <p>To find the transformation matrix, apply \\(T\\) to each standard basis vector:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} T(\\mathbf{e}_1) &amp; T(\\mathbf{e}_2) &amp; \\cdots &amp; T(\\mathbf{e}_n) \\end{bmatrix}\\)</p> <p>The columns of \\(\\mathbf{A}\\) are the images of the standard basis vectors. This fundamental observation connects abstract transformations to concrete matrix computations.</p>"},{"location":"chapters/04-linear-transformations/#diagram-linear-transformation-visualizer","title":"Diagram: Linear Transformation Visualizer","text":"Linear Transformation Fundamentals Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that linear transformations preserve the grid structure and that the transformation is completely determined by where basis vectors map.</p> <p>Canvas layout: - Left: Original coordinate plane with basis vectors and grid - Right: Transformed plane showing result of applying T - Bottom: Matrix display and controls</p> <p>Visual elements: - Standard basis vectors e\u2081 (red) and e\u2082 (blue) in original space - Transformed basis vectors T(e\u2081) and T(e\u2082) in target space - Grid of points in original space - Corresponding grid in transformed space - Sample vector (user-controlled) showing before/after - 2\u00d72 transformation matrix displayed</p> <p>Interactive controls: - Draggable endpoints for T(e\u2081) and T(e\u2082) to define transformation - Slider: Animation between original and transformed states - Button: Reset to identity - Dropdown: Preset transformations (rotation, scaling, shear, reflection) - Checkbox: Show grid lines - Checkbox: Show sample vector path</p> <p>Default parameters: - Initial transformation: identity - Grid: 5\u00d75 - Sample vector: (1, 1)</p> <p>Behavior: - Dragging basis vector endpoints updates matrix in real-time - Grid morphs smoothly during animation - Matrix entries update as endpoints move - Preset dropdown smoothly transitions to new transformation - Sample vector shows how arbitrary points transform</p> <p>Implementation: p5.js with smooth animation</p>"},{"location":"chapters/04-linear-transformations/#geometric-transformations-in-2d","title":"Geometric Transformations in 2D","text":"<p>The power of linear transformations becomes vivid when we visualize their geometric effects. Each type of transformation has a characteristic matrix structure.</p>"},{"location":"chapters/04-linear-transformations/#rotation-matrix","title":"Rotation Matrix","text":"<p>A rotation matrix rotates vectors around the origin by a fixed angle. For 2D rotation by angle \\(\\theta\\) counterclockwise:</p>"},{"location":"chapters/04-linear-transformations/#2d-rotation-matrix","title":"2D Rotation Matrix","text":"<p>\\(\\mathbf{R}(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the rotation angle (positive = counterclockwise)</li> <li>The columns are the rotated basis vectors</li> </ul> <p>Properties of 2D rotation matrices:</p> <ul> <li>\\(\\mathbf{R}(\\theta)^T = \\mathbf{R}(-\\theta) = \\mathbf{R}(\\theta)^{-1}\\) (orthogonal matrix)</li> <li>\\(\\det(\\mathbf{R}(\\theta)) = 1\\) (preserves area and orientation)</li> <li>\\(\\mathbf{R}(\\alpha)\\mathbf{R}(\\beta) = \\mathbf{R}(\\alpha + \\beta)\\) (rotations compose by adding angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-2d-rotation-interactive","title":"Diagram: 2D Rotation Interactive","text":"2D Rotation Matrix Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: demonstrate, calculate</p> <p>Learning Objective: Enable students to see how the rotation matrix transforms vectors and shapes, and verify the relationship between angle and matrix entries.</p> <p>Canvas layout: - Main area: Coordinate plane with original and rotated shapes - Right panel: Matrix display with cos/sin values - Bottom: Angle control</p> <p>Visual elements: - Unit circle for reference - Original shape (arrow, square, or F-shape) in blue - Rotated shape in red (semi-transparent) - Angle arc showing rotation amount - Basis vectors before and after rotation - Matrix entries updating with angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (-360\u00b0 to 360\u00b0) - Dropdown: Shape to rotate (arrow, square, triangle, F-shape) - Checkbox: Show unit circle - Checkbox: Show angle arc - Button: Animate full rotation - Input: Enter specific angle in degrees</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: F-shape (to show orientation) - Unit circle: visible</p> <p>Behavior: - Shape rotates smoothly as angle slider moves - Matrix entries display cos(\u03b8) and sin(\u03b8) values - Animation shows continuous rotation - F-shape clearly shows orientation preservation</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/04-linear-transformations/#3d-rotation","title":"3D Rotation","text":"<p>3D rotation is more complex because we must specify an axis of rotation. The three fundamental rotation matrices rotate around the coordinate axes:</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-x-axis","title":"Rotation Around X-Axis","text":"<p>\\(\\mathbf{R}_x(\\theta) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\theta &amp; -\\sin\\theta \\\\ 0 &amp; \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-y-axis","title":"Rotation Around Y-Axis","text":"<p>\\(\\mathbf{R}_y(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; 0 &amp; \\sin\\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\theta &amp; 0 &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-z-axis","title":"Rotation Around Z-Axis","text":"<p>\\(\\mathbf{R}_z(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>General 3D rotations can be composed from these basic rotations, though the order matters (rotations don't commute in 3D).</p> <p>Gimbal Lock</p> <p>When composing rotations using Euler angles, certain orientations cause \"gimbal lock\" where a degree of freedom is lost. Quaternions provide an alternative representation that avoids this problem, used extensively in robotics and game development.</p>"},{"location":"chapters/04-linear-transformations/#scaling-matrix","title":"Scaling Matrix","text":"<p>A scaling matrix stretches or compresses vectors along the coordinate axes.</p> <p>Uniform scaling scales equally in all directions:</p> <p>\\(\\mathbf{S}_{\\text{uniform}}(k) = \\begin{bmatrix} k &amp; 0 \\\\ 0 &amp; k \\end{bmatrix}\\)</p> <p>Non-uniform scaling scales differently along each axis:</p> <p>\\(\\mathbf{S}(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(s_x\\) is the scaling factor along the x-axis</li> <li>\\(s_y\\) is the scaling factor along the y-axis</li> <li>\\(|s| &gt; 1\\) stretches; \\(|s| &lt; 1\\) compresses</li> <li>\\(s &lt; 0\\) also reflects</li> </ul> Scaling Type Effect Determinant Uniform \\(k &gt; 1\\) Enlarges \\(k^2\\) Uniform \\(0 &lt; k &lt; 1\\) Shrinks \\(k^2\\) Non-uniform Stretches/compresses differently \\(s_x \\cdot s_y\\) Negative factor Also reflects Negative"},{"location":"chapters/04-linear-transformations/#shear-matrix","title":"Shear Matrix","text":"<p>A shear matrix skews shapes by shifting points parallel to an axis, proportional to their distance from that axis.</p> <p>Horizontal shear (shifts x based on y):</p> <p>\\(\\mathbf{H}_x(k) = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Vertical shear (shifts y based on x):</p> <p>\\(\\mathbf{H}_y(k) = \\begin{bmatrix} 1 &amp; 0 \\\\ k &amp; 1 \\end{bmatrix}\\)</p> <p>Shear transformations:</p> <ul> <li>Turn rectangles into parallelograms</li> <li>Preserve area (\\(\\det = 1\\))</li> <li>Are not orthogonal (don't preserve angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#reflection-matrix","title":"Reflection Matrix","text":"<p>A reflection matrix mirrors points across a line (in 2D) or plane (in 3D).</p> <p>Reflection across the x-axis:</p> <p>\\(\\mathbf{F}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</p> <p>Reflection across the y-axis:</p> <p>\\(\\mathbf{F}_y = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Reflection across a line through the origin at angle \\(\\theta\\):</p> <p>\\(\\mathbf{F}(\\theta) = \\begin{bmatrix} \\cos 2\\theta &amp; \\sin 2\\theta \\\\ \\sin 2\\theta &amp; -\\cos 2\\theta \\end{bmatrix}\\)</p> <p>Reflections have determinant \\(-1\\), indicating they reverse orientation (turning clockwise into counterclockwise).</p>"},{"location":"chapters/04-linear-transformations/#diagram-geometric-transformations-gallery","title":"Diagram: Geometric Transformations Gallery","text":"Geometric Transformations Interactive Gallery <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Enable students to compare and contrast different geometric transformations, understanding their visual effects and matrix structures.</p> <p>Canvas layout: - Main area: Split view showing original and transformed shapes - Right panel: Transformation type selector and parameters - Bottom: Matrix display</p> <p>Visual elements: - Original shape (configurable) in blue - Transformed shape in red - Grid showing deformation - Transformation type label - Matrix with current values - Key properties (determinant, orthogonality)</p> <p>Interactive controls: - Tabs: Rotation | Scaling | Shear | Reflection - For Rotation: Angle slider - For Scaling: sx and sy sliders (or single k for uniform) - For Shear: k slider, direction toggle (horizontal/vertical) - For Reflection: Angle slider for reflection line - Dropdown: Shape (square, circle of points, F-shape, arrow) - Toggle: Show grid deformation - Button: Animate transformation</p> <p>Default parameters: - Transformation: Rotation - Shape: F-shape - Show grid: true</p> <p>Behavior: - Smooth animation between original and transformed states - Matrix updates in real-time with parameter changes - Properties panel shows det, orthogonality, etc. - Grid clearly shows how space is warped - Comparisons possible by switching between tabs</p> <p>Implementation: p5.js with tabbed interface</p>"},{"location":"chapters/04-linear-transformations/#projection","title":"Projection","text":"<p>A projection maps vectors onto a subspace (line, plane, etc.). Unlike the transformations above, projections typically reduce dimension and are not invertible.</p>"},{"location":"chapters/04-linear-transformations/#orthogonal-projection","title":"Orthogonal Projection","text":"<p>An orthogonal projection projects vectors perpendicularly onto a subspace. For projection onto a line through the origin with direction \\(\\mathbf{u}\\):</p>"},{"location":"chapters/04-linear-transformations/#projection-onto-a-line","title":"Projection onto a Line","text":"<p>\\(\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}\\)</p> <p>The projection matrix onto the line spanned by unit vector \\(\\hat{\\mathbf{u}}\\) is:</p> <p>\\(\\mathbf{P} = \\hat{\\mathbf{u}} \\hat{\\mathbf{u}}^T\\)</p> <p>For projection onto the x-axis:</p> <p>\\(\\mathbf{P}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>Properties of orthogonal projection matrices:</p> <ul> <li>\\(\\mathbf{P}^2 = \\mathbf{P}\\) (applying twice gives same result)</li> <li>\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)</li> <li>Eigenvalues are 0 and 1</li> <li>Not invertible (determinant = 0)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-orthogonal-projection-visualizer","title":"Diagram: Orthogonal Projection Visualizer","text":"Orthogonal Projection Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students visualize how orthogonal projection maps vectors onto lines or planes, showing the perpendicular relationship between the original vector, its projection, and the error component.</p> <p>Canvas layout: - Main area: 2D/3D coordinate space - Right panel: Vector components and projection formula</p> <p>Visual elements: - Original vector v (blue arrow) - Projection line/plane (gray) - Projected vector proj(v) (red arrow on line) - Error vector (v - proj(v)) shown as dashed green arrow - Right angle indicator showing orthogonality - Unit direction vector u</p> <p>Interactive controls: - Draggable vector v endpoint - Slider: Direction of projection line (angle) - Toggle: 2D / 3D mode - Checkbox: Show error vector - Checkbox: Show right angle indicator - Checkbox: Show projection formula with values - Button: Animate projection process</p> <p>Default parameters: - Mode: 2D - Projection line: 30\u00b0 from x-axis - Vector v: (3, 2)</p> <p>Behavior: - Vector v can be dragged to any position - Projection updates in real-time - Error vector clearly perpendicular to projection line - Formula shows computed values - 3D mode allows projection onto plane</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/04-linear-transformations/#composition-of-transformations","title":"Composition of Transformations","text":"<p>Applying one transformation after another is called composition. If \\(S\\) and \\(T\\) are linear transformations, the composition \\(S \\circ T\\) applies \\(T\\) first, then \\(S\\):</p> <p>\\((S \\circ T)(\\mathbf{x}) = S(T(\\mathbf{x}))\\)</p> <p>For matrix representations, composition corresponds to matrix multiplication:</p> <p>\\(\\mathbf{A}_{S \\circ T} = \\mathbf{A}_S \\mathbf{A}_T\\)</p> <p>Order Matters</p> <p>Matrix multiplication is not commutative, so \\(\\mathbf{A}_S \\mathbf{A}_T \\neq \\mathbf{A}_T \\mathbf{A}_S\\) in general. Rotating then scaling gives different results than scaling then rotating.</p> <p>Common composition examples:</p> <ul> <li>Rotation around a point: Translate to origin, rotate, translate back</li> <li>Scaling about a point: Translate to origin, scale, translate back</li> <li>Euler angles: Compose three axis-aligned rotations</li> </ul> Composition Matrix Product Application Rotate then scale \\(\\mathbf{S}\\mathbf{R}\\) Graphics, robotics Scale then rotate \\(\\mathbf{R}\\mathbf{S}\\) Different result! Multiple rotations \\(\\mathbf{R}_3 \\mathbf{R}_2 \\mathbf{R}_1\\) 3D orientation"},{"location":"chapters/04-linear-transformations/#diagram-transformation-composition","title":"Diagram: Transformation Composition","text":"Transformation Composition Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare</p> <p>Learning Objective: Demonstrate that the order of transformations matters and show how composed transformations combine into a single matrix product.</p> <p>Canvas layout: - Top: Two transformation pipelines side by side (T then S vs S then T) - Bottom: Resulting shapes and combined matrices</p> <p>Visual elements: - Original shape in center - Pipeline 1: Original \u2192 T \u2192 S (with intermediate state shown) - Pipeline 2: Original \u2192 S \u2192 T (with intermediate state shown) - Final shapes for each pipeline (different unless transformations commute) - Matrices for T, S, and their products ST and TS</p> <p>Interactive controls: - Dropdown: First transformation type (rotation, scaling, shear) - Parameters for first transformation - Dropdown: Second transformation type - Parameters for second transformation - Checkbox: Show intermediate states - Button: Animate both pipelines - Toggle: Show matrix products</p> <p>Default parameters: - T: Rotation 45\u00b0 - S: Scaling (2, 1) - Shape: unit square</p> <p>Behavior: - Both pipelines animate simultaneously for comparison - Intermediate shapes visible between transformations - Matrix products computed and displayed - Clear visual demonstration that order matters - Commutative cases (e.g., two rotations) show same result</p> <p>Implementation: p5.js with parallel animation</p>"},{"location":"chapters/04-linear-transformations/#kernel-and-range","title":"Kernel and Range","text":"<p>Every linear transformation has two fundamental subspaces that reveal its structure.</p>"},{"location":"chapters/04-linear-transformations/#kernel-null-space","title":"Kernel (Null Space)","text":"<p>The kernel of a linear transformation \\(T: V \\rightarrow W\\) is the set of all vectors that map to zero:</p> <p>\\(\\ker(T) = \\{\\mathbf{v} \\in V : T(\\mathbf{v}) = \\mathbf{0}\\}\\)</p> <p>For a matrix transformation \\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the kernel equals the null space of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Null}(\\mathbf{A}) = \\{\\mathbf{x} : \\mathbf{A}\\mathbf{x} = \\mathbf{0}\\}\\)</p> <p>The kernel is always a subspace of the domain. Its dimension is called the nullity of \\(T\\).</p>"},{"location":"chapters/04-linear-transformations/#range-column-space","title":"Range (Column Space)","text":"<p>The range of \\(T\\) is the set of all possible outputs:</p> <p>\\(\\text{Range}(T) = \\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</p> <p>For a matrix \\(\\mathbf{A}\\), the range equals the column space\u2014the span of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Col}(\\mathbf{A}) = \\text{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\\)</p> <p>The range is always a subspace of the codomain. Its dimension is the rank of \\(T\\).</p> Subspace Definition Dimension Name Kernel / Null Space Vectors mapping to zero Nullity Range / Column Space All possible outputs Rank"},{"location":"chapters/04-linear-transformations/#diagram-kernel-and-range-visualizer","title":"Diagram: Kernel and Range Visualizer","text":"Kernel and Range Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: differentiate, examine</p> <p>Learning Objective: Help students visualize the kernel (what maps to zero) and range (what outputs are possible) of a linear transformation, understanding their relationship to the matrix structure.</p> <p>Canvas layout: - Left: Domain space showing kernel - Right: Codomain space showing range - Center: Transformation arrow - Bottom: Matrix and dimension information</p> <p>Visual elements: - Domain with kernel subspace highlighted (line or plane in gray) - Arrows from domain to codomain - Codomain with range subspace highlighted (line or plane in color) - Zero vector in codomain marked - Vectors in kernel shown collapsing to zero - Sample vectors outside kernel shown mapping to range</p> <p>Interactive controls: - Matrix editor (2\u00d72 or 3\u00d72 or 2\u00d73) - Button: Random full-rank matrix - Button: Random rank-deficient matrix - Checkbox: Show kernel vectors - Checkbox: Show how kernel maps to zero - Checkbox: Animate transformation - Display: Rank and nullity values</p> <p>Default parameters: - Matrix: 2\u00d73 with rank 2 (nontrivial kernel)</p> <p>Behavior: - Kernel automatically computed and displayed - Range computed as column space - Arrows show transformation action - Rank and nullity update with matrix changes - Animation shows vectors transforming</p> <p>Implementation: p5.js with linear algebra computations</p>"},{"location":"chapters/04-linear-transformations/#the-rank-nullity-theorem","title":"The Rank-Nullity Theorem","text":"<p>One of the most important theorems in linear algebra connects the dimensions of the kernel and range.</p>"},{"location":"chapters/04-linear-transformations/#statement-of-the-theorem","title":"Statement of the Theorem","text":"<p>For a linear transformation \\(T: V \\rightarrow W\\) where \\(V\\) is finite-dimensional:</p>"},{"location":"chapters/04-linear-transformations/#rank-nullity-theorem","title":"Rank-Nullity Theorem","text":"<p>\\(\\dim(V) = \\text{rank}(T) + \\text{nullity}(T)\\)</p> <p>where:</p> <ul> <li>\\(\\dim(V)\\) is the dimension of the domain (number of columns of \\(\\mathbf{A}\\))</li> <li>\\(\\text{rank}(T)\\) is the dimension of the range</li> <li>\\(\\text{nullity}(T)\\) is the dimension of the kernel</li> </ul> <p>For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\):</p> <p>\\(n = \\text{rank}(\\mathbf{A}) + \\text{nullity}(\\mathbf{A})\\)</p>"},{"location":"chapters/04-linear-transformations/#intuition","title":"Intuition","text":"<p>The theorem says that dimension is conserved: what doesn't go into the kernel must go somewhere (the range). If more vectors collapse to zero (higher nullity), fewer independent output directions remain (lower rank).</p>"},{"location":"chapters/04-linear-transformations/#consequences","title":"Consequences","text":"<p>The Rank-Nullity Theorem has powerful implications:</p> <ul> <li>If \\(\\text{rank}(\\mathbf{A}) = n\\) (full column rank), then \\(\\text{nullity} = 0\\), so \\(T\\) is injective (one-to-one)</li> <li>If \\(\\text{rank}(\\mathbf{A}) = m\\) (full row rank), then \\(T\\) is surjective (onto)</li> <li>If both, \\(T\\) is bijective (invertible) and \\(m = n\\)</li> </ul> Matrix Size Rank Nullity Properties \\(3 \\times 3\\), rank 3 3 0 Invertible \\(3 \\times 3\\), rank 2 2 1 Kernel is a line \\(3 \\times 4\\), rank 3 3 1 Kernel is a line \\(4 \\times 3\\), rank 3 3 0 One-to-one"},{"location":"chapters/04-linear-transformations/#invertible-transformations","title":"Invertible Transformations","text":"<p>An invertible transformation is a linear transformation with an inverse\u2014a transformation that \"undoes\" the original.</p>"},{"location":"chapters/04-linear-transformations/#conditions-for-invertibility","title":"Conditions for Invertibility","text":"<p>A linear transformation \\(T: V \\rightarrow W\\) is invertible if and only if:</p> <ul> <li>\\(T\\) is one-to-one (injective): different inputs give different outputs</li> <li>\\(T\\) is onto (surjective): every output is achieved</li> <li>Equivalently: \\(\\ker(T) = \\{\\mathbf{0}\\}\\) and \\(\\text{Range}(T) = W\\)</li> </ul> <p>For a matrix \\(\\mathbf{A}\\):</p> <ul> <li>Must be square (\\(m = n\\))</li> <li>Must have full rank (\\(\\text{rank}(\\mathbf{A}) = n\\))</li> <li>Must have nullity zero</li> <li>Must have nonzero determinant</li> </ul>"},{"location":"chapters/04-linear-transformations/#inverse-of-geometric-transformations","title":"Inverse of Geometric Transformations","text":"Transformation Matrix Inverse Rotation by \\(\\theta\\) \\(\\mathbf{R}(\\theta)\\) \\(\\mathbf{R}(-\\theta) = \\mathbf{R}^T\\) Scaling by \\((s_x, s_y)\\) \\(\\text{diag}(s_x, s_y)\\) \\(\\text{diag}(1/s_x, 1/s_y)\\) Shear by \\(k\\) \\(\\mathbf{H}(k)\\) \\(\\mathbf{H}(-k)\\) Reflection \\(\\mathbf{F}\\) \\(\\mathbf{F}\\) (self-inverse) Projection \\(\\mathbf{P}\\) Not invertible <p>Projections are never invertible because they collapse dimension\u2014once information is lost, it cannot be recovered.</p>"},{"location":"chapters/04-linear-transformations/#change-of-basis","title":"Change of Basis","text":"<p>Different bases provide different \"viewpoints\" on the same vector space. Change of basis allows us to translate between these viewpoints.</p>"},{"location":"chapters/04-linear-transformations/#basis-transition-matrix","title":"Basis Transition Matrix","text":"<p>Given two bases \\(\\mathcal{B} = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) and \\(\\mathcal{C} = \\{\\mathbf{c}_1, \\ldots, \\mathbf{c}_n\\}\\), the basis transition matrix \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) converts coordinates from \\(\\mathcal{C}\\) to \\(\\mathcal{B}\\).</p> <p>If \\([\\mathbf{v}]_{\\mathcal{C}}\\) are the coordinates of \\(\\mathbf{v}\\) in basis \\(\\mathcal{C}\\), then:</p> <p>\\([\\mathbf{v}]_{\\mathcal{B}} = \\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}} [\\mathbf{v}]_{\\mathcal{C}}\\)</p> <p>The columns of \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) are the \\(\\mathcal{C}\\) basis vectors expressed in \\(\\mathcal{B}\\) coordinates.</p>"},{"location":"chapters/04-linear-transformations/#similar-matrices","title":"Similar Matrices","text":"<p>If \\(\\mathbf{A}\\) represents a transformation in the standard basis and \\(\\mathbf{P}\\) is the change of basis matrix, then:</p> <p>\\(\\mathbf{A}' = \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\)</p> <p>represents the same transformation in the new basis. Matrices related this way are called similar matrices\u2014they represent the same transformation in different coordinate systems.</p> <p>Similar matrices have the same:</p> <ul> <li>Determinant</li> <li>Trace</li> <li>Eigenvalues</li> <li>Rank</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-change-of-basis-visualizer","title":"Diagram: Change of Basis Visualizer","text":"Change of Basis Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that the same vector has different coordinate representations in different bases, and how the transition matrix converts between them.</p> <p>Canvas layout: - Left: Standard basis view with vector - Right: Custom basis view with same vector - Bottom: Coordinate displays and transition matrix</p> <p>Visual elements: - Standard basis vectors (e\u2081, e\u2082) in black - Custom basis vectors (b\u2081, b\u2082) in purple - Same geometric vector shown in both views - Coordinates displayed in each basis - Transition matrix P - Grid lines for each basis</p> <p>Interactive controls: - Draggable endpoints for custom basis vectors - Draggable vector to transform - Button: Reset to standard basis - Checkbox: Show both bases overlaid - Checkbox: Show transition matrix calculation - Dropdown: Preset bases (standard, rotated, skewed)</p> <p>Default parameters: - Custom basis: rotated 30\u00b0 from standard - Vector: (2, 1) in standard coordinates</p> <p>Behavior: - Vector stays fixed geometrically as basis changes - Coordinates update to reflect new basis - Transition matrix updates with basis - Overlay mode shows both grids simultaneously - Clear visualization that vector is unchanged, only representation</p> <p>Implementation: p5.js with coordinate transformation</p>"},{"location":"chapters/04-linear-transformations/#applications","title":"Applications","text":""},{"location":"chapters/04-linear-transformations/#computer-graphics","title":"Computer Graphics","text":"<p>Every transformation in computer graphics\u2014modeling, viewing, projection\u2014is a linear (or affine) transformation represented by matrices. The graphics pipeline applies a sequence of transformations:</p> <ol> <li>Model matrix: Object space \u2192 World space</li> <li>View matrix: World space \u2192 Camera space</li> <li>Projection matrix: Camera space \u2192 Clip space</li> </ol> <p>GPUs are optimized for these matrix multiplications, processing millions of vertices per second.</p>"},{"location":"chapters/04-linear-transformations/#robotics","title":"Robotics","text":"<p>Robot arm kinematics uses transformation matrices to track how joints connect. Each joint applies a rotation or translation, and composing these gives the end-effector position:</p> <p>\\(\\mathbf{T}_{\\text{total}} = \\mathbf{T}_1 \\mathbf{T}_2 \\cdots \\mathbf{T}_n\\)</p>"},{"location":"chapters/04-linear-transformations/#neural-networks","title":"Neural Networks","text":"<p>Each layer of a neural network applies a linear transformation (weights) followed by a nonlinearity:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>Understanding transformations helps interpret:</p> <ul> <li>What the network \"sees\" at each layer</li> <li>How information flows and transforms</li> <li>Why deep networks can learn complex mappings</li> </ul>"},{"location":"chapters/04-linear-transformations/#principal-component-analysis","title":"Principal Component Analysis","text":"<p>PCA finds a change of basis that:</p> <ul> <li>Aligns axes with directions of maximum variance</li> <li>Decorrelates the data</li> <li>Enables dimensionality reduction by projecting onto top components</li> </ul> <p>This is a change of basis to the eigenvector basis of the covariance matrix.</p>"},{"location":"chapters/04-linear-transformations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter connected matrix algebra to geometric transformation.</p> <p>Foundations:</p> <ul> <li>A function maps inputs from a domain to outputs in a codomain</li> <li>A linear transformation preserves addition and scalar multiplication</li> <li>Every linear transformation has a transformation matrix whose columns are images of basis vectors</li> </ul> <p>Geometric Transformations:</p> <ul> <li>Rotation matrices rotate while preserving lengths and angles</li> <li>Scaling matrices stretch or compress along coordinate axes</li> <li>Shear matrices skew shapes by sliding parallel to an axis</li> <li>Reflection matrices mirror across a line or plane</li> <li>Projection matrices map onto lower-dimensional subspaces</li> </ul> <p>Composition and Structure:</p> <ul> <li>Composition of transformations corresponds to matrix multiplication</li> <li>Order matters: \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) in general</li> <li>The kernel contains vectors mapping to zero; its dimension is nullity</li> <li>The range is the set of all outputs; its dimension is rank</li> <li>Rank-Nullity Theorem: \\(n = \\text{rank} + \\text{nullity}\\)</li> </ul> <p>Invertibility and Basis:</p> <ul> <li>Invertible transformations have trivial kernel and full range</li> <li>Change of basis provides different coordinate views of the same transformation</li> <li>Similar matrices represent the same transformation in different bases</li> </ul> <p>Key Properties:</p> Transformation Preserves Lengths? Preserves Angles? Invertible? Determinant Rotation Yes Yes Yes 1 Uniform Scaling No Yes Yes (if \\(k \\neq 0\\)) \\(k^n\\) Shear No No Yes 1 Reflection Yes Yes Yes \\(-1\\) Projection No No No 0"},{"location":"chapters/04-linear-transformations/#exercises","title":"Exercises","text":"Exercise 1: Finding Transformation Matrices <p>Find the 2\u00d72 matrix for the linear transformation that:</p> <p>a) Reflects across the line \\(y = x\\)</p> <p>b) Rotates by 90\u00b0 counterclockwise, then scales by factor 2</p> <p>c) Projects onto the line \\(y = 2x\\)</p> Exercise 2: Composition Order <p>Let \\(R\\) be rotation by 45\u00b0 and \\(S\\) be scaling by \\((2, 1)\\). Compute both \\(RS\\) and \\(SR\\), and describe geometrically why they differ.</p> Exercise 3: Kernel and Range <p>For the matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix}\\):</p> <p>a) Find a basis for the kernel (null space)</p> <p>b) Find a basis for the range (column space)</p> <p>c) Verify the Rank-Nullity Theorem</p> Exercise 4: Invertibility <p>Determine which transformations are invertible and find the inverse if it exists:</p> <p>a) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{bmatrix}\\mathbf{x}\\)</p> <p>b) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 3 \\end{bmatrix}\\mathbf{x}\\)</p> <p>c) Projection onto the x-axis</p> Exercise 5: Change of Basis <p>Let \\(\\mathcal{B} = \\{(1, 1), (1, -1)\\}\\) be a basis for \\(\\mathbb{R}^2\\).</p> <p>a) Find the change of basis matrix from standard coordinates to \\(\\mathcal{B}\\)-coordinates</p> <p>b) Express the vector \\((3, 1)\\) in \\(\\mathcal{B}\\)-coordinates</p> <p>c) If a transformation is rotation by 90\u00b0 in standard coordinates, what matrix represents it in \\(\\mathcal{B}\\)-coordinates?</p>"},{"location":"chapters/04-linear-transformations/quiz/","title":"Quiz: Linear Transformations","text":"<p>Test your understanding of linear transformations and their properties.</p>"},{"location":"chapters/04-linear-transformations/quiz/#1-a-transformation-t-is-linear-if-it-satisfies","title":"1. A transformation \\(T\\) is linear if it satisfies:","text":"<ol> <li>\\(T(\\mathbf{x} + \\mathbf{y}) = T(\\mathbf{x}) + T(\\mathbf{y})\\) and \\(T(c\\mathbf{x}) = cT(\\mathbf{x})\\)</li> <li>\\(T(\\mathbf{x}) = \\mathbf{x}\\) for all \\(\\mathbf{x}\\)</li> <li>\\(T(\\mathbf{x}) \\cdot T(\\mathbf{y}) = T(\\mathbf{x} \\cdot \\mathbf{y})\\)</li> <li>\\(T(\\mathbf{0}) \\neq \\mathbf{0}\\)</li> </ol> Show Answer <p>The correct answer is A. A linear transformation satisfies two properties: additivity \\(T(\\mathbf{x} + \\mathbf{y}) = T(\\mathbf{x}) + T(\\mathbf{y})\\) and homogeneity \\(T(c\\mathbf{x}) = cT(\\mathbf{x})\\). These can be combined as \\(T(a\\mathbf{x} + b\\mathbf{y}) = aT(\\mathbf{x}) + bT(\\mathbf{y})\\).</p> <p>Concept Tested: Linear Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#2-every-linear-transformation-can-be-represented-by","title":"2. Every linear transformation can be represented by:","text":"<ol> <li>A scalar</li> <li>A vector</li> <li>A matrix</li> <li>A polynomial</li> </ol> Show Answer <p>The correct answer is C. Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. For \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\), there exists a unique \\(m \\times n\\) matrix \\(A\\) such that \\(T(\\mathbf{x}) = A\\mathbf{x}\\).</p> <p>Concept Tested: Matrix Representation</p>"},{"location":"chapters/04-linear-transformations/quiz/#3-what-is-the-kernel-null-space-of-a-linear-transformation","title":"3. What is the kernel (null space) of a linear transformation?","text":"<ol> <li>The set of all possible outputs</li> <li>The set of all inputs that map to the zero vector</li> <li>The identity transformation</li> <li>The inverse transformation</li> </ol> Show Answer <p>The correct answer is B. The kernel (or null space) of a linear transformation \\(T\\) is the set of all vectors \\(\\mathbf{x}\\) such that \\(T(\\mathbf{x}) = \\mathbf{0}\\). It measures the \"collapse\" in the transformation.</p> <p>Concept Tested: Kernel</p>"},{"location":"chapters/04-linear-transformations/quiz/#4-the-image-range-of-a-linear-transformation-is","title":"4. The image (range) of a linear transformation is:","text":"<ol> <li>The set of all inputs</li> <li>The set of all possible outputs</li> <li>The zero vector</li> <li>The inverse of the kernel</li> </ol> Show Answer <p>The correct answer is B. The image (or range) of a linear transformation is the set of all possible outputs\u2014all vectors that can be reached by applying the transformation to some input. It equals the column space of the matrix representation.</p> <p>Concept Tested: Image</p>"},{"location":"chapters/04-linear-transformations/quiz/#5-a-rotation-in-2d-by-angle-theta-counterclockwise-is-represented-by","title":"5. A rotation in 2D by angle \\(\\theta\\) counterclockwise is represented by:","text":"<ol> <li>\\(\\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\sin\\theta &amp; \\cos\\theta \\\\ -\\cos\\theta &amp; \\sin\\theta \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} \\theta &amp; 0 \\\\ 0 &amp; \\theta \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is B. The standard 2D rotation matrix for counterclockwise rotation by angle \\(\\theta\\) is \\(\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\). This preserves lengths and rotates vectors by the specified angle.</p> <p>Concept Tested: Rotation Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#6-a-linear-transformation-is-injective-one-to-one-if-and-only-if","title":"6. A linear transformation is injective (one-to-one) if and only if:","text":"<ol> <li>Its image is the entire codomain</li> <li>Its kernel contains only the zero vector</li> <li>It is represented by a square matrix</li> <li>It preserves the dot product</li> </ol> Show Answer <p>The correct answer is B. A linear transformation is injective if and only if its kernel is trivial (contains only \\(\\mathbf{0}\\)). This means different inputs always produce different outputs\u2014no two distinct vectors map to the same output.</p> <p>Concept Tested: Injective Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#7-which-transformation-scales-a-vector-by-2-in-the-x-direction-only","title":"7. Which transformation scales a vector by 2 in the x-direction only?","text":"<ol> <li>\\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 1 &amp; 2 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 0 &amp; 2 \\\\ 2 &amp; 0 \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is B. The matrix \\(\\begin{bmatrix} 2 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\) scales the x-component by 2 while leaving the y-component unchanged. This is a non-uniform scaling (stretching) transformation.</p> <p>Concept Tested: Scaling Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#8-the-composition-of-two-linear-transformations-t_1-and-t_2-corresponds-to","title":"8. The composition of two linear transformations \\(T_1\\) and \\(T_2\\) corresponds to:","text":"<ol> <li>Adding their matrices</li> <li>Multiplying their matrices</li> <li>Finding the inverse of their matrices</li> <li>Taking the transpose of their matrices</li> </ol> Show Answer <p>The correct answer is B. The composition of linear transformations corresponds to matrix multiplication. If \\(T_1\\) is represented by \\(A\\) and \\(T_2\\) by \\(B\\), then \\(T_2 \\circ T_1\\) (apply \\(T_1\\) first, then \\(T_2\\)) is represented by \\(BA\\).</p> <p>Concept Tested: Composition of Transformations</p>"},{"location":"chapters/04-linear-transformations/quiz/#9-a-reflection-across-the-x-axis-is-represented-by","title":"9. A reflection across the x-axis is represented by:","text":"<ol> <li>\\(\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix}\\)</li> <li>\\(\\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</li> </ol> Show Answer <p>The correct answer is A. The matrix \\(\\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\) reflects vectors across the x-axis by negating the y-component while preserving the x-component.</p> <p>Concept Tested: Reflection Transformation</p>"},{"location":"chapters/04-linear-transformations/quiz/#10-a-shear-transformation-in-the-x-direction","title":"10. A shear transformation in the x-direction:","text":"<ol> <li>Rotates vectors around the origin</li> <li>Scales all vectors uniformly</li> <li>Shifts x-coordinates proportionally to y-coordinates</li> <li>Projects vectors onto the x-axis</li> </ol> Show Answer <p>The correct answer is C. A shear in the x-direction shifts x-coordinates by an amount proportional to the y-coordinate, represented by \\(\\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\). This slants rectangles into parallelograms while preserving area.</p> <p>Concept Tested: Shear Transformation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/","title":"Determinants and Matrix Properties","text":""},{"location":"chapters/05-determinants-and-matrix-properties/#summary","title":"Summary","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes. This chapter covers determinant computation methods, their properties, and geometric interpretation as the volume scaling factor of a transformation. You will also learn Cramer's rule and understand the relationship between determinants and matrix invertibility.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 13 concepts from the learning graph:</p> <ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"chapters/05-determinants-and-matrix-properties/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#introduction","title":"Introduction","text":"<p>The determinant is one of the most important numbers associated with a square matrix. While its definition might seem algebraically arbitrary at first, the determinant has profound geometric meaning: it measures how a linear transformation scales area (in 2D) or volume (in 3D and higher dimensions). A determinant of 2 means the transformation doubles all areas; a determinant of 0 means the transformation collapses space onto a lower dimension.</p> <p>Understanding determinants connects algebraic matrix operations to geometric intuition about transformations. When you compute a determinant, you're answering the question: \"How much does this transformation stretch or compress space?\" This perspective makes determinants essential for computer graphics, physics simulations, and understanding when systems of equations have unique solutions.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#geometric-motivation-signed-area-and-volume","title":"Geometric Motivation: Signed Area and Volume","text":"<p>Before diving into formulas, let's build intuition for what determinants measure geometrically.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#signed-area-in-2d","title":"Signed Area in 2D","text":"<p>Consider two vectors \\(\\mathbf{u} = \\begin{bmatrix} a \\\\ c \\end{bmatrix}\\) and \\(\\mathbf{v} = \\begin{bmatrix} b \\\\ d \\end{bmatrix}\\) in 2D. These vectors form a parallelogram. The signed area of this parallelogram is:</p> \\[\\text{Signed Area} = ad - bc\\] <p>The \"signed\" aspect captures orientation:</p> <ul> <li>Positive area: \\(\\mathbf{v}\\) is counterclockwise from \\(\\mathbf{u}\\)</li> <li>Negative area: \\(\\mathbf{v}\\) is clockwise from \\(\\mathbf{u}\\)</li> <li>Zero area: vectors are parallel (parallelogram collapses to a line)</li> </ul> Configuration Signed Area Interpretation CCW orientation Positive Preserves orientation CW orientation Negative Reverses orientation Parallel vectors Zero Collapses to lower dimension"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-signed-area-visualizer","title":"Diagram: Signed Area Visualizer","text":"<p>Run the Signed Area Visualizer Fullscreen</p> Signed Area Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, explain</p> <p>Learning Objective: Help students visualize the signed area of the parallelogram formed by two vectors, understanding how orientation affects the sign.</p> <p>Canvas layout: - Main area: 2D coordinate plane with vectors and parallelogram - Right panel: Area calculation display - Bottom: Controls</p> <p>Visual elements: - Vector u (red arrow) from origin - Vector v (blue arrow) from origin - Parallelogram shaded (green if positive, red if negative area) - Grid background for reference - Area value displayed prominently - Sign indicator (+/-) with color coding</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Checkbox: Show parallelogram - Checkbox: Show area calculation formula - Button: Reset to default vectors - Display: Current signed area value</p> <p>Default parameters: - u = (2, 1) - v = (1, 2) - Show parallelogram: true</p> <p>Behavior: - Parallelogram updates in real-time as vectors are dragged - Area calculation shows ad - bc with current values - Color changes from green to red when area becomes negative - When area approaches zero, parallelogram visibly flattens</p> <p>Implementation: p5.js with interactive vector dragging</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#volume-scaling-factor","title":"Volume Scaling Factor","text":"<p>When a matrix \\(\\mathbf{A}\\) represents a linear transformation, the determinant \\(\\det(\\mathbf{A})\\) tells us how the transformation scales volumes:</p> <ul> <li>\\(|\\det(\\mathbf{A})| &gt; 1\\): Transformation expands volumes</li> <li>\\(|\\det(\\mathbf{A})| &lt; 1\\): Transformation compresses volumes</li> <li>\\(|\\det(\\mathbf{A})| = 1\\): Transformation preserves volumes</li> <li>\\(\\det(\\mathbf{A}) &lt; 0\\): Transformation also flips orientation</li> <li>\\(\\det(\\mathbf{A}) = 0\\): Transformation collapses dimension</li> </ul> <p>The absolute value \\(|\\det(\\mathbf{A})|\\) gives the volume scaling factor, while the sign indicates whether orientation is preserved or reversed.</p> <p>Volume Scaling Examples</p> <ul> <li>Rotation matrices have \\(\\det(\\mathbf{R}) = 1\\) (preserve area, preserve orientation)</li> <li>Reflection matrices have \\(\\det(\\mathbf{F}) = -1\\) (preserve area, reverse orientation)</li> <li>Scaling by factor \\(k\\) in all directions: \\(\\det = k^n\\) where \\(n\\) is dimension</li> <li>Projection matrices have \\(\\det = 0\\) (collapse to lower dimension)</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#the-22-determinant","title":"The 2\u00d72 Determinant","text":"<p>The determinant of a 2\u00d72 matrix is defined as:</p> \\[\\det\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} = ad - bc\\] <p>This formula directly gives the signed area of the parallelogram formed by the column vectors. We also write \\(\\det(\\mathbf{A})\\) as \\(|\\mathbf{A}|\\).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#computing-22-determinants","title":"Computing 2\u00d72 Determinants","text":"<p>The pattern is simple: multiply along the main diagonal, subtract the product along the anti-diagonal.</p> \\[\\det\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} = \\underbrace{a \\cdot d}_{\\text{main diagonal}} - \\underbrace{b \\cdot c}_{\\text{anti-diagonal}}\\] <p>Examples:</p> \\[\\det\\begin{bmatrix} 3 &amp; 2 \\\\ 1 &amp; 4 \\end{bmatrix} = (3)(4) - (2)(1) = 12 - 2 = 10\\] \\[\\det\\begin{bmatrix} 2 &amp; 6 \\\\ 1 &amp; 3 \\end{bmatrix} = (2)(3) - (6)(1) = 6 - 6 = 0\\] <p>The second example has determinant zero\u2014the columns \\((2, 1)\\) and \\((6, 3)\\) are parallel (one is a scalar multiple of the other).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-22-determinant-calculator","title":"Diagram: 2\u00d72 Determinant Calculator","text":"<p>Run the 2\u00d72 Determinant Calculator Fullscreen</p> 2\u00d72 Determinant Interactive Calculator <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to practice computing 2\u00d72 determinants and see the geometric interpretation simultaneously.</p> <p>Canvas layout: - Left: Matrix input area with editable cells - Center: Geometric visualization (parallelogram) - Right: Step-by-step calculation - Bottom: Controls</p> <p>Visual elements: - 2\u00d72 matrix with editable numerical entries - Parallelogram formed by column vectors - Main diagonal highlighted (green) - Anti-diagonal highlighted (red) - Area shading with sign indication - Step-by-step calculation: ad = ?, bc = ?, ad - bc = ?</p> <p>Interactive controls: - Editable matrix entries (click to edit, -10 to 10 range) - Button: Random matrix - Button: Identity matrix - Button: Singular matrix (det = 0) - Checkbox: Show calculation steps - Checkbox: Animate diagonal products</p> <p>Default parameters: - Matrix: [[3, 1], [2, 4]] - Show calculation: true</p> <p>Behavior: - Parallelogram updates as matrix entries change - Calculation steps update in real-time - Color changes for positive/negative/zero determinant - Animation highlights diagonal products when enabled</p> <p>Implementation: p5.js with text input handling</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#the-33-determinant","title":"The 3\u00d73 Determinant","text":"<p>For a 3\u00d73 matrix, the determinant represents the signed volume of the parallelepiped (3D parallelogram) formed by the three column vectors.</p> \\[\\det\\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{bmatrix} = a(ei - fh) - b(di - fg) + c(dh - eg)\\]"},{"location":"chapters/05-determinants-and-matrix-properties/#the-rule-of-sarrus","title":"The Rule of Sarrus","text":"<p>A mnemonic for 3\u00d73 determinants is the Rule of Sarrus. Copy the first two columns to the right, then:</p> <ul> <li>Add products along the three downward diagonals</li> <li>Subtract products along the three upward diagonals</li> </ul> \\[\\det(\\mathbf{A}) = aei + bfg + cdh - ceg - afh - bdi\\] <p>While Sarrus' rule only works for 3\u00d73 matrices (not larger ones), it provides a quick calculation method.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-33-determinant-sarrus-visualizer","title":"Diagram: 3\u00d73 Determinant Sarrus Visualizer","text":"<p>Run the Rule of Sarrus Visualizer Fullscreen</p> Rule of Sarrus Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, calculate</p> <p>Learning Objective: Help students master the Rule of Sarrus for computing 3\u00d73 determinants through visual step-by-step animation.</p> <p>Canvas layout: - Main area: 3\u00d73 matrix with extended columns - Diagonal lines shown during calculation - Running total display - Bottom: Controls</p> <p>Visual elements: - 3\u00d73 matrix grid with values - First two columns repeated to the right - Downward diagonal lines (green, positive terms) - Upward diagonal lines (red, negative terms) - Product values along each diagonal - Running total that builds up the determinant</p> <p>Interactive controls: - Editable matrix entries - Button: Step through calculation - Button: Play animation - Button: Reset - Slider: Animation speed - Dropdown: Example matrices (identity, rotation, random)</p> <p>Default parameters: - Matrix: [[1, 2, 3], [4, 5, 6], [7, 8, 9]] - Animation speed: medium</p> <p>Behavior: - Step mode highlights one diagonal at a time - Shows product calculation for each diagonal - Running total updates with each step - Final result highlighted when complete - Color coding for positive (green) and negative (red) terms</p> <p>Implementation: p5.js with step-based animation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#cofactor-expansion","title":"Cofactor Expansion","text":"<p>For matrices larger than 3\u00d73, we use cofactor expansion (also called Laplace expansion). This recursive method expresses a determinant in terms of smaller determinants.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#minors-and-cofactors","title":"Minors and Cofactors","text":"<p>For a matrix \\(\\mathbf{A}\\), the minor \\(M_{ij}\\) is the determinant of the submatrix obtained by deleting row \\(i\\) and column \\(j\\).</p> <p>The cofactor \\(C_{ij}\\) includes a sign factor:</p> \\[C_{ij} = (-1)^{i+j} M_{ij}\\] <p>The sign pattern alternates in a checkerboard fashion:</p> \\[\\begin{bmatrix} + &amp; - &amp; + &amp; - &amp; \\cdots \\\\ - &amp; + &amp; - &amp; + &amp; \\cdots \\\\ + &amp; - &amp; + &amp; - &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\]"},{"location":"chapters/05-determinants-and-matrix-properties/#cofactor-expansion-formula","title":"Cofactor Expansion Formula","text":"<p>The determinant can be computed by expanding along any row or column:</p> <p>Expansion along row \\(i\\): \\(\\(\\det(\\mathbf{A}) = \\sum_{j=1}^{n} a_{ij} C_{ij} = \\sum_{j=1}^{n} (-1)^{i+j} a_{ij} M_{ij}\\)\\)</p> <p>Expansion along column \\(j\\): \\(\\(\\det(\\mathbf{A}) = \\sum_{i=1}^{n} a_{ij} C_{ij} = \\sum_{i=1}^{n} (-1)^{i+j} a_{ij} M_{ij}\\)\\)</p> <p>The result is the same regardless of which row or column you choose. For efficiency, expand along a row or column with the most zeros.</p> <p>Example: Compute \\(\\det\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 4 &amp; 5 \\\\ 0 &amp; 0 &amp; 6 \\end{bmatrix}\\) by expanding along column 1:</p> \\[\\det = 1 \\cdot C_{11} + 0 \\cdot C_{21} + 0 \\cdot C_{31} = 1 \\cdot (+1) \\cdot \\det\\begin{bmatrix} 4 &amp; 5 \\\\ 0 &amp; 6 \\end{bmatrix} = 1 \\cdot (24 - 0) = 24\\] <p>Efficiency Tip</p> <p>Always expand along the row or column with the most zeros. This minimizes the number of cofactor calculations needed.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-cofactor-expansion-step-by-step","title":"Diagram: Cofactor Expansion Step-by-Step","text":"<p>Run the Cofactor Expansion Visualizer Fullscreen</p> Cofactor Expansion Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, implement</p> <p>Learning Objective: Guide students through the cofactor expansion process step-by-step, showing how minors and cofactors combine to compute the determinant.</p> <p>Canvas layout: - Top: Original matrix with selectable expansion row/column - Middle: Submatrices (minors) being calculated - Bottom: Cofactor combination and final result - Controls panel</p> <p>Visual elements: - Matrix with highlighted expansion row/column - Sign checkerboard pattern overlay - Submatrices shown when computing each minor - Term-by-term calculation: a_ij \u00d7 C_ij = ? - Summation showing all terms combining - Color coding: positive terms (green), negative terms (red)</p> <p>Interactive controls: - Matrix size selector (3\u00d73, 4\u00d74) - Editable matrix entries - Dropdown: Select row or column for expansion - Button: Step through expansion - Button: Auto-play animation - Button: Reset - Checkbox: Show sign pattern</p> <p>Default parameters: - Matrix size: 3\u00d73 - Matrix: [[2, 1, 3], [4, 5, 6], [7, 8, 9]] - Expand along: row 1</p> <p>Behavior: - Highlights selected row/column for expansion - Shows each minor being computed - Displays sign factor for each cofactor - Accumulates partial sums - Final determinant displayed with verification</p> <p>Implementation: p5.js with step-based animation and submatrix visualization</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#properties-of-determinants","title":"Properties of Determinants","text":"<p>Determinants satisfy several important properties that simplify calculations and reveal structural information about matrices.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#key-determinant-properties","title":"Key Determinant Properties","text":"Property Statement Implication Row swap Swapping two rows negates the determinant \\(\\det(\\text{swap}) = -\\det(\\mathbf{A})\\) Row scaling Multiplying a row by \\(k\\) multiplies det by \\(k\\) \\(\\det(k\\mathbf{A}) = k^n \\det(\\mathbf{A})\\) Row addition Adding a multiple of one row to another doesn't change det Useful for Gaussian elimination Triangular Determinant of triangular matrix = product of diagonal Fast computation Zero row/column If any row or column is all zeros, det = 0 Matrix is singular Proportional rows If two rows are proportional, det = 0 Linear dependence"},{"location":"chapters/05-determinants-and-matrix-properties/#multiplicative-property","title":"Multiplicative Property","text":"<p>One of the most important properties: the determinant of a product equals the product of determinants.</p> \\[\\det(\\mathbf{A}\\mathbf{B}) = \\det(\\mathbf{A}) \\cdot \\det(\\mathbf{B})\\] <p>Geometric interpretation: If transformation \\(\\mathbf{A}\\) scales volume by factor \\(|\\det(\\mathbf{A})|\\) and transformation \\(\\mathbf{B}\\) scales by \\(|\\det(\\mathbf{B})|\\), then the composition scales by \\(|\\det(\\mathbf{A})| \\cdot |\\det(\\mathbf{B})|\\).</p> <p>Consequences:</p> <ul> <li>\\(\\det(\\mathbf{A}^{-1}) = \\frac{1}{\\det(\\mathbf{A})}\\) (if \\(\\mathbf{A}\\) is invertible)</li> <li>\\(\\det(\\mathbf{A}^n) = (\\det(\\mathbf{A}))^n\\)</li> <li>\\(\\det(\\mathbf{I}) = 1\\)</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#transpose-determinant","title":"Transpose Determinant","text":"<p>The determinant is unchanged by transposition:</p> \\[\\det(\\mathbf{A}^T) = \\det(\\mathbf{A})\\] <p>This means every property that holds for rows also holds for columns. If two columns are proportional, the determinant is zero. If you scale a column by \\(k\\), the determinant scales by \\(k\\).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-determinant-properties-explorer","title":"Diagram: Determinant Properties Explorer","text":"<p>Run the Determinant Properties Explorer Fullscreen</p> Determinant Properties Interactive Explorer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, differentiate</p> <p>Learning Objective: Allow students to experiment with row/column operations and observe how each operation affects the determinant, building intuition for determinant properties.</p> <p>Canvas layout: - Left: Original matrix with determinant - Center: Operation selector and visualization - Right: Modified matrix with new determinant - Bottom: Property explanation panel</p> <p>Visual elements: - Two matrices side by side (before/after operation) - Determinant values displayed for both - Parallelogram visualizations (for 2\u00d72 mode) - Ratio of determinants shown - Operation description text</p> <p>Interactive controls: - Matrix size selector (2\u00d72, 3\u00d73) - Editable matrix entries - Operation buttons:   - Swap rows i and j   - Scale row i by k   - Add k \u00d7 row i to row j   - Transpose   - Multiply by another matrix - Slider: Scale factor k (-5 to 5) - Row/column selectors</p> <p>Default parameters: - Matrix size: 2\u00d72 - Matrix: [[2, 1], [3, 4]] - Operation: none selected</p> <p>Behavior: - Shows before/after matrices - Displays determinant change: det(A) \u2192 det(A') - Explains the property being demonstrated - For 2\u00d72, shows parallelogram area change - Highlights which property is being used</p> <p>Implementation: p5.js with operation buttons and real-time calculation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#singular-matrices","title":"Singular Matrices","text":"<p>A square matrix \\(\\mathbf{A}\\) is singular (non-invertible) if and only if \\(\\det(\\mathbf{A}) = 0\\).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#characterizations-of-singular-matrices","title":"Characterizations of Singular Matrices","text":"<p>The following are equivalent for an \\(n \\times n\\) matrix \\(\\mathbf{A}\\):</p> <ul> <li>\\(\\det(\\mathbf{A}) = 0\\)</li> <li>\\(\\mathbf{A}\\) is not invertible</li> <li>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has non-trivial solutions</li> <li>The columns of \\(\\mathbf{A}\\) are linearly dependent</li> <li>The rows of \\(\\mathbf{A}\\) are linearly dependent</li> <li>\\(\\text{rank}(\\mathbf{A}) &lt; n\\)</li> <li>The transformation collapses space to lower dimension</li> <li>Zero is an eigenvalue of \\(\\mathbf{A}\\)</li> </ul> <p>Singular Matrices in Applications</p> <p>In numerical computing, matrices with determinants close to zero (nearly singular) cause problems. Small errors get amplified, leading to unreliable results. The condition number measures how close a matrix is to being singular.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#geometric-interpretation-of-singularity","title":"Geometric Interpretation of Singularity","text":"<p>When \\(\\det(\\mathbf{A}) = 0\\), the linear transformation represented by \\(\\mathbf{A}\\) collapses at least one dimension:</p> <ul> <li>In 2D: A plane gets squashed onto a line or point</li> <li>In 3D: A volume gets flattened onto a plane, line, or point</li> </ul> <p>This explains why singular matrices aren't invertible\u2014once you collapse a dimension, you can't recover the original information.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-singular-vs-non-singular-transformation","title":"Diagram: Singular vs Non-Singular Transformation","text":"<p>Run the Singular Matrix Visualizer Fullscreen</p> Singular vs Non-Singular Matrix Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Visualize the geometric difference between singular (det=0) and non-singular matrices, showing how singular matrices collapse dimension.</p> <p>Canvas layout: - Left: Original unit square with grid - Right: Transformed shape - Bottom: Matrix display and determinant - Controls panel</p> <p>Visual elements: - Unit square (original) with grid points - Transformed parallelogram or collapsed line - Column vectors highlighted - Determinant value prominently displayed - \"Singular\" or \"Non-singular\" label - Animation showing transformation process</p> <p>Interactive controls: - Editable 2\u00d72 matrix entries - Slider: Interpolate between identity and current matrix - Button: Set to singular example (e.g., [[2,4],[1,2]]) - Button: Set to non-singular example - Button: Random matrix - Checkbox: Show grid transformation</p> <p>Default parameters: - Matrix: [[2, 1], [4, 2]] (singular) - Animation: at full transformation</p> <p>Behavior: - When det \u2248 0, shape collapses to line - Color changes to indicate singularity - Animation shows smooth collapse process - Explains why inverse doesn't exist geometrically - Shows rank deficiency visually</p> <p>Implementation: p5.js with smooth animation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#volume-scaling-in-higher-dimensions","title":"Volume Scaling in Higher Dimensions","text":"<p>The determinant generalizes the concept of signed area to higher dimensions:</p> Dimension Geometric Object Determinant Measures 2D Parallelogram Signed area 3D Parallelepiped Signed volume nD n-dimensional parallelotope Signed hypervolume"},{"location":"chapters/05-determinants-and-matrix-properties/#computing-volumes-with-determinants","title":"Computing Volumes with Determinants","text":"<p>Given vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\) in \\(\\mathbb{R}^n\\), the (signed) volume of the parallelepiped they span is:</p> \\[\\text{Volume} = \\det\\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ \\mathbf{v}_1 &amp; \\mathbf{v}_2 &amp; \\cdots &amp; \\mathbf{v}_n \\\\ | &amp; | &amp; &amp; | \\end{bmatrix}\\] <p>For the unsigned volume, take the absolute value: \\(|\\det(\\mathbf{A})|\\).</p> <p>Application: In computer graphics, determinants help determine:</p> <ul> <li>Whether a set of points is coplanar (det = 0)</li> <li>The orientation of a triangle (clockwise vs counterclockwise)</li> <li>The volume of a tetrahedron for collision detection</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-3d-volume-scaling-visualizer","title":"Diagram: 3D Volume Scaling Visualizer","text":"<p>Run the 3D Volume Scaling Visualizer Fullscreen</p> 3D Volume Scaling Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students visualize how 3\u00d73 matrix transformations scale 3D volumes, connecting the determinant to geometric volume change.</p> <p>Canvas layout: - Main area: 3D view with unit cube and transformed parallelepiped - Right panel: Matrix and determinant display - Bottom: Controls</p> <p>Visual elements: - Unit cube (wireframe, semi-transparent) - Transformed parallelepiped (solid, colored by det sign) - Three column vectors as arrows from origin - Axes with labels - Volume ratio display: V'/V = |det(A)| - Determinant value with sign</p> <p>Interactive controls: - 3\u00d73 matrix editor - Camera rotation (click and drag) - Slider: Animation morph (0 = identity, 1 = full transform) - Button: Rotation matrix example - Button: Scaling matrix example - Button: Singular matrix example - Checkbox: Show original cube - Checkbox: Show column vectors</p> <p>Default parameters: - Matrix: [[2,0,0],[0,1.5,0],[0,0,1]] (scaling) - Animation: at 1.0 - Camera angle: isometric view</p> <p>Behavior: - Real-time 3D rendering with rotation - Smooth morphing animation - Volume label updates with determinant - For singular matrices, parallelepiped collapses - Color indicates positive (green) or negative (red) determinant</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#cramers-rule","title":"Cramer's Rule","text":"<p>Cramer's Rule provides explicit formulas for solving systems of linear equations using determinants.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#the-formula","title":"The Formula","text":"<p>For a system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) where \\(\\mathbf{A}\\) is \\(n \\times n\\) and \\(\\det(\\mathbf{A}) \\neq 0\\):</p> \\[x_i = \\frac{\\det(\\mathbf{A}_i)}{\\det(\\mathbf{A})}\\] <p>where \\(\\mathbf{A}_i\\) is the matrix formed by replacing column \\(i\\) of \\(\\mathbf{A}\\) with \\(\\mathbf{b}\\).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#example-22-system","title":"Example: 2\u00d72 System","text":"<p>Solve: \\(\\begin{cases} 2x + 3y = 7 \\\\ x + 4y = 9 \\end{cases}\\)</p> <p>Matrix form: \\(\\mathbf{A} = \\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; 4 \\end{bmatrix}\\), \\(\\mathbf{b} = \\begin{bmatrix} 7 \\\\ 9 \\end{bmatrix}\\)</p> <p>Step 1: \\(\\det(\\mathbf{A}) = 2(4) - 3(1) = 5\\)</p> <p>Step 2: Replace column 1 with \\(\\mathbf{b}\\): \\(\\(\\det(\\mathbf{A}_1) = \\det\\begin{bmatrix} 7 &amp; 3 \\\\ 9 &amp; 4 \\end{bmatrix} = 28 - 27 = 1\\)\\)</p> <p>Step 3: Replace column 2 with \\(\\mathbf{b}\\): \\(\\(\\det(\\mathbf{A}_2) = \\det\\begin{bmatrix} 2 &amp; 7 \\\\ 1 &amp; 9 \\end{bmatrix} = 18 - 7 = 11\\)\\)</p> <p>Solution: \\(x = \\frac{1}{5}\\), \\(y = \\frac{11}{5}\\)</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#when-to-use-cramers-rule","title":"When to Use Cramer's Rule","text":"Situation Recommendation Small systems (2\u00d72, 3\u00d73) Cramer's rule is practical Solve for one variable only Efficient\u2014only compute relevant det Large systems Use Gaussian elimination (more efficient) Theoretical analysis Cramer's rule gives explicit formulas \\(\\det(\\mathbf{A}) = 0\\) Cramer's rule doesn't apply <p>Computational Efficiency</p> <p>For large systems, Cramer's rule requires computing \\(n+1\\) determinants, each of which is \\(O(n!)\\) by cofactor expansion or \\(O(n^3)\\) by LU decomposition. Gaussian elimination solves the system in \\(O(n^3)\\), making it much more efficient for large \\(n\\).</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#diagram-cramers-rule-step-by-step-solver","title":"Diagram: Cramer's Rule Step-by-Step Solver","text":"<p>Run the Cramer's Rule Solver Fullscreen</p> Cramer's Rule Interactive Solver <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: solve, calculate</p> <p>Learning Objective: Guide students through applying Cramer's rule to solve 2\u00d72 and 3\u00d73 systems, showing each replacement and determinant calculation.</p> <p>Canvas layout: - Top: Original system of equations - Middle: Step-by-step determinant calculations - Bottom: Solution display and geometric interpretation (for 2\u00d72) - Controls panel</p> <p>Visual elements: - System of equations in standard form - Matrix A and vector b displayed - Each modified matrix A_i highlighted - Determinant calculations shown - Solution vector - For 2\u00d72: geometric interpretation showing intersection of lines</p> <p>Interactive controls: - System size selector (2\u00d72, 3\u00d73) - Editable coefficient matrix A - Editable vector b - Button: Step through solution - Button: Auto-solve with animation - Button: Random system - Button: Singular system (to show failure case)</p> <p>Default parameters: - System size: 2\u00d72 - A = [[2, 3], [1, 4]] - b = [7, 9]</p> <p>Behavior: - Highlights which column is being replaced - Shows each determinant calculation - Builds up solution step by step - If det(A) = 0, shows \"No unique solution\" message - Geometric view shows lines and their intersection</p> <p>Implementation: p5.js with step-based animation</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#computing-determinants-efficiently","title":"Computing Determinants Efficiently","text":"<p>For practical computation, several methods are available:</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#method-comparison","title":"Method Comparison","text":"Method Time Complexity Best For Direct formula \\(O(1)\\) for 2\u00d72 2\u00d72 matrices Sarrus rule \\(O(1)\\) 3\u00d73 matrices Cofactor expansion \\(O(n!)\\) Small matrices, matrices with many zeros Row reduction \\(O(n^3)\\) Large matrices LU decomposition \\(O(n^3)\\) Multiple determinants, also need inverse"},{"location":"chapters/05-determinants-and-matrix-properties/#row-reduction-method","title":"Row Reduction Method","text":"<ol> <li>Reduce \\(\\mathbf{A}\\) to upper triangular form \\(\\mathbf{U}\\) using row operations</li> <li>Track row swaps (each negates the determinant)</li> <li>Track row scalings (each multiplies the determinant)</li> <li>\\(\\det(\\mathbf{A}) = (\\text{sign from swaps}) \\times (\\text{scaling factors}) \\times \\prod_{i} u_{ii}\\)</li> </ol> <p>For an upper triangular matrix, the determinant is simply the product of diagonal entries:</p> \\[\\det(\\mathbf{U}) = u_{11} \\cdot u_{22} \\cdot \\ldots \\cdot u_{nn}\\]"},{"location":"chapters/05-determinants-and-matrix-properties/#applications","title":"Applications","text":""},{"location":"chapters/05-determinants-and-matrix-properties/#computer-graphics","title":"Computer Graphics","text":"<p>Determinants are used to:</p> <ul> <li>Test if three points are collinear (2\u00d72 det = 0)</li> <li>Determine triangle orientation for rendering</li> <li>Calculate areas for texture mapping</li> <li>Check if a transformation preserves handedness</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#physics-and-engineering","title":"Physics and Engineering","text":"<ul> <li>Calculate moments of inertia (involving volume integrals)</li> <li>Solve systems in circuit analysis</li> <li>Determine stability of equilibrium points</li> <li>Compute Jacobians for coordinate transformations</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#machine-learning","title":"Machine Learning","text":"<ul> <li>Check if features are linearly independent</li> <li>Compute multivariate Gaussian distributions</li> <li>Calculate volume elements in probability spaces</li> <li>Understand transformation properties in neural networks</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter explored determinants as fundamental matrix quantities with both algebraic and geometric significance.</p> <p>Core Concepts:</p> <ul> <li>The determinant measures how a transformation scales volumes</li> <li>Signed area (2D) and volume scaling factor (3D+) connect determinants to geometry</li> <li>The 2\u00d72 determinant is \\(ad - bc\\) (main diagonal minus anti-diagonal)</li> <li>The 3\u00d73 determinant can be computed via Sarrus' rule or cofactor expansion</li> <li>Cofactor expansion generalizes to any size using minors and cofactors</li> </ul> <p>Key Properties:</p> <ul> <li>Multiplicative property: \\(\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\\)</li> <li>Transpose property: \\(\\det(\\mathbf{A}^T) = \\det(\\mathbf{A})\\)</li> <li>Row operations affect determinants predictably</li> <li>Triangular matrices have det = product of diagonal</li> </ul> <p>Singularity:</p> <ul> <li>\\(\\det(\\mathbf{A}) = 0\\) means \\(\\mathbf{A}\\) is singular (not invertible)</li> <li>Geometrically: transformation collapses dimension</li> <li>Algebraically: columns are linearly dependent</li> </ul> <p>Applications:</p> <ul> <li>Cramer's Rule solves systems using determinant ratios</li> <li>Efficient computation uses row reduction for large matrices</li> <li>Determinants appear throughout graphics, physics, and ML</li> </ul>"},{"location":"chapters/05-determinants-and-matrix-properties/#exercises","title":"Exercises","text":"Exercise 1: Computing Determinants <p>Calculate the determinant of each matrix:</p> <p>a) \\(\\begin{bmatrix} 5 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\)</p> <p>b) \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 4 &amp; 5 \\\\ 0 &amp; 0 &amp; 6 \\end{bmatrix}\\)</p> <p>c) \\(\\begin{bmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 3 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix}\\)</p> Exercise 2: Determinant Properties <p>If \\(\\det(\\mathbf{A}) = 3\\) and \\(\\det(\\mathbf{B}) = -2\\), find:</p> <p>a) \\(\\det(\\mathbf{AB})\\)</p> <p>b) \\(\\det(\\mathbf{A}^{-1})\\)</p> <p>c) \\(\\det(2\\mathbf{A})\\) for a 3\u00d73 matrix</p> <p>d) \\(\\det(\\mathbf{A}^T\\mathbf{B})\\)</p> Exercise 3: Cramer's Rule <p>Use Cramer's rule to solve: \\(\\begin{cases} 3x + 2y = 8 \\\\ x - y = 1 \\end{cases}\\)</p> Exercise 4: Geometric Interpretation <p>The transformation \\(T\\) has matrix \\(\\mathbf{A} = \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix}\\).</p> <p>a) What is \\(\\det(\\mathbf{A})\\)?</p> <p>b) If a triangle has area 5, what is the area of its image under \\(T\\)?</p> <p>c) Is the transformation orientation-preserving?</p> Exercise 5: Singularity <p>For what values of \\(k\\) is the matrix singular? \\(\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; k \\\\ 0 &amp; k-1 &amp; 3 \\\\ 0 &amp; 0 &amp; k+2 \\end{bmatrix}\\)\\)</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/","title":"Quiz: Determinants and Matrix Properties","text":"<p>Test your understanding of determinants and fundamental matrix properties.</p> <p>Note: This quiz covers key concepts from the chapter outline. Full chapter content is under development.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#1-what-does-the-determinant-of-a-matrix-represent-geometrically","title":"1. What does the determinant of a matrix represent geometrically?","text":"<ol> <li>The sum of diagonal elements</li> <li>The signed volume scaling factor of the transformation</li> <li>The number of pivots in row echelon form</li> <li>The trace of the matrix</li> </ol> Show Answer <p>The correct answer is B. The determinant represents the signed scaling factor for volumes (areas in 2D). A determinant of 2 means the transformation doubles volumes; a negative determinant indicates the transformation reverses orientation.</p> <p>Concept Tested: Determinant</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#2-if-deta-0-then-the-matrix-a-is","title":"2. If \\(\\det(A) = 0\\), then the matrix \\(A\\) is:","text":"<ol> <li>Orthogonal</li> <li>Symmetric</li> <li>Singular (non-invertible)</li> <li>Positive definite</li> </ol> Show Answer <p>The correct answer is C. A matrix with zero determinant is singular, meaning it has no inverse. The transformation collapses space in at least one dimension, making the operation irreversible.</p> <p>Concept Tested: Singular Matrix</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#3-for-any-square-matrices-a-and-b-which-property-holds","title":"3. For any square matrices \\(A\\) and \\(B\\), which property holds?","text":"<ol> <li>\\(\\det(A + B) = \\det(A) + \\det(B)\\)</li> <li>\\(\\det(AB) = \\det(A) \\cdot \\det(B)\\)</li> <li>\\(\\det(AB) = \\det(A) + \\det(B)\\)</li> <li>\\(\\det(A^{-1}) = \\det(A)\\)</li> </ol> Show Answer <p>The correct answer is B. The determinant is multiplicative: \\(\\det(AB) = \\det(A) \\cdot \\det(B)\\). This reflects that composing transformations multiplies their volume scaling factors.</p> <p>Concept Tested: Determinant Properties</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#4-the-trace-of-a-matrix-is","title":"4. The trace of a matrix is:","text":"<ol> <li>The product of diagonal elements</li> <li>The sum of diagonal elements</li> <li>The sum of all elements</li> <li>The determinant divided by the dimension</li> </ol> Show Answer <p>The correct answer is B. The trace of a square matrix is the sum of its diagonal elements: \\(\\text{tr}(A) = \\sum_{i=1}^n A_{ii}\\). The trace equals the sum of eigenvalues.</p> <p>Concept Tested: Trace</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#5-what-is-detat-in-terms-of-deta","title":"5. What is \\(\\det(A^T)\\) in terms of \\(\\det(A)\\)?","text":"<ol> <li>\\(-\\det(A)\\)</li> <li>\\(\\det(A)\\)</li> <li>\\(1/\\det(A)\\)</li> <li>\\(\\det(A)^2\\)</li> </ol> Show Answer <p>The correct answer is B. The determinant of a transpose equals the original determinant: \\(\\det(A^T) = \\det(A)\\). Transposing swaps rows and columns but doesn't change the volume scaling factor.</p> <p>Concept Tested: Determinant of Transpose</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#6-if-matrix-a-has-determinant-5-what-is-det2a-for-a-3-times-3-matrix","title":"6. If matrix \\(A\\) has determinant 5, what is \\(\\det(2A)\\) for a \\(3 \\times 3\\) matrix?","text":"<ol> <li>10</li> <li>25</li> <li>40</li> <li>80</li> </ol> Show Answer <p>The correct answer is C. For an \\(n \\times n\\) matrix, \\(\\det(cA) = c^n \\det(A)\\). For a \\(3 \\times 3\\) matrix with \\(\\det(A) = 5\\): \\(\\det(2A) = 2^3 \\cdot 5 = 8 \\cdot 5 = 40\\).</p> <p>Concept Tested: Scalar Multiplication and Determinants</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#7-a-positive-definite-matrix-has","title":"7. A positive definite matrix has:","text":"<ol> <li>All positive entries</li> <li>All positive eigenvalues</li> <li>Positive determinant only</li> <li>Positive trace only</li> </ol> Show Answer <p>The correct answer is B. A positive definite matrix has all positive eigenvalues. Equivalently, \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\) for all non-zero vectors \\(\\mathbf{x}\\). This implies the matrix is invertible.</p> <p>Concept Tested: Positive Definite</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#8-which-row-operation-does-not-change-the-determinant","title":"8. Which row operation does NOT change the determinant?","text":"<ol> <li>Adding a multiple of one row to another</li> <li>Multiplying a row by a non-zero scalar</li> <li>Swapping two rows</li> <li>All row operations change the determinant</li> </ol> Show Answer <p>The correct answer is A. Adding a multiple of one row to another does not change the determinant. Swapping rows multiplies the determinant by \\(-1\\), and multiplying a row by scalar \\(c\\) multiplies the determinant by \\(c\\).</p> <p>Concept Tested: Determinant and Row Operations</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#9-what-is-deta-1-in-terms-of-deta","title":"9. What is \\(\\det(A^{-1})\\) in terms of \\(\\det(A)\\)?","text":"<ol> <li>\\(\\det(A)\\)</li> <li>\\(-\\det(A)\\)</li> <li>\\(1/\\det(A)\\)</li> <li>\\(\\det(A)^2\\)</li> </ol> Show Answer <p>The correct answer is C. Since \\(AA^{-1} = I\\) and \\(\\det(I) = 1\\), we have \\(\\det(A) \\cdot \\det(A^{-1}) = 1\\), so \\(\\det(A^{-1}) = 1/\\det(A)\\).</p> <p>Concept Tested: Determinant of Inverse</p>"},{"location":"chapters/05-determinants-and-matrix-properties/quiz/#10-a-matrix-is-positive-semidefinite-if","title":"10. A matrix is positive semidefinite if:","text":"<ol> <li>All eigenvalues are strictly positive</li> <li>All eigenvalues are non-negative (zero or positive)</li> <li>The determinant is positive</li> <li>All entries are non-negative</li> </ol> Show Answer <p>The correct answer is B. A positive semidefinite matrix has all non-negative eigenvalues (zero or positive). Equivalently, \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) for all vectors \\(\\mathbf{x}\\).</p> <p>Concept Tested: Positive Semidefinite</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/","title":"Eigenvalues and Eigenvectors","text":""},{"location":"chapters/06-eigenvalues-and-eigenvectors/#summary","title":"Summary","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations. This chapter covers eigenvalues, eigenvectors, characteristic polynomials, and diagonalization. You will learn the spectral theorem for symmetric matrices and the power iteration method. These concepts are essential for PCA, stability analysis, and understanding how neural networks learn.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 5: Determinants and Matrix Properties</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#introduction","title":"Introduction","text":"<p>When a linear transformation acts on a vector, it typically changes both the direction and magnitude of that vector. However, certain special vectors maintain their direction under transformation\u2014they may stretch, shrink, or flip, but they remain on the same line through the origin. These exceptional vectors, called eigenvectors, and their associated scaling factors, called eigenvalues, reveal the fundamental structure of linear transformations.</p> <p>Understanding eigenanalysis is crucial for modern AI and machine learning applications. Principal Component Analysis (PCA) uses eigenvectors to find the directions of maximum variance in data. Google's PageRank algorithm models web importance as an eigenvector problem. Neural networks converge based on the eigenvalues of their weight matrices. Stability analysis of dynamical systems depends entirely on eigenvalue properties.</p> <p>This chapter develops eigenanalysis from first principles, building intuition through visualizations before presenting the computational techniques used in practice.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigenvalues-and-eigenvectors-the-core-concepts","title":"Eigenvalues and Eigenvectors: The Core Concepts","text":"<p>Consider a linear transformation represented by a square matrix \\(A\\). When we apply \\(A\\) to most vectors, both the direction and magnitude change. But for special vectors, the transformation only scales the vector without changing its direction.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-eigen-equation","title":"The Eigen Equation","text":"<p>The relationship between a matrix \\(A\\), an eigenvector \\(\\mathbf{v}\\), and its eigenvalue \\(\\lambda\\) is captured by the eigen equation:</p> <p>\\(A\\mathbf{v} = \\lambda\\mathbf{v}\\)</p> <p>where:</p> <ul> <li>\\(A\\) is an \\(n \\times n\\) square matrix</li> <li>\\(\\mathbf{v}\\) is a non-zero vector (the eigenvector)</li> <li>\\(\\lambda\\) is a scalar (the eigenvalue)</li> </ul> <p>This equation states that applying the transformation \\(A\\) to the eigenvector \\(\\mathbf{v}\\) produces the same result as simply scaling \\(\\mathbf{v}\\) by the factor \\(\\lambda\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenvector-transformation-visualization","title":"Diagram: Eigenvector Transformation Visualization","text":"<p>Run the Eigenvector Transformation Visualizer Fullscreen</p> Eigenvector Transformation Visualization <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Demonstrate visually how eigenvectors maintain their direction under linear transformation while other vectors change direction</p> <p>Visual elements: - 2D coordinate plane with grid lines - A unit circle showing sample vectors - Original vector (blue arrow) that user can drag to different positions - Transformed vector (red arrow) showing result of A*v - Eigenvector directions displayed as dashed lines through origin - Matrix A displayed in corner with editable values</p> <p>Interactive controls: - Drag handle on the blue vector to change its direction and magnitude - 2x2 matrix input fields for A (pre-populated with example: [[2, 1], [1, 2]]) - \"Show Eigenvectors\" toggle button - \"Animate Transformation\" button that smoothly morphs vector to its transformed position - Reset button</p> <p>Default parameters: - Matrix A = [[2, 1], [1, 2]] (eigenvalues 3 and 1) - Initial vector at (1, 0) - Canvas size: responsive, minimum 600x500px</p> <p>Behavior: - As user drags the vector, show both original and transformed positions - When vector aligns with an eigenvector direction, highlight this with a glow effect - Display \"Eigenvector detected!\" message when alignment occurs - Show eigenvalue as the ratio of transformed to original length - Color code: vectors along eigenvectors glow green, others remain blue/red</p> <p>Implementation: p5.js with responsive canvas design</p> <p>The key insight is that eigenvectors define the \"natural axes\" of a linear transformation. Along these axes, the transformation acts as pure scaling\u2014no rotation or shearing occurs.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>Different eigenvalue values produce different geometric behaviors:</p> Eigenvalue Geometric Effect \\(\\lambda &gt; 1\\) Stretches the eigenvector away from origin \\(0 &lt; \\lambda &lt; 1\\) Compresses the eigenvector toward origin \\(\\lambda = 1\\) Leaves the eigenvector unchanged \\(\\lambda = 0\\) Collapses the eigenvector to the origin \\(\\lambda &lt; 0\\) Flips and scales the eigenvector <p>Eigenvectors Are Not Unique in Scale</p> <p>If \\(\\mathbf{v}\\) is an eigenvector with eigenvalue \\(\\lambda\\), then any non-zero scalar multiple \\(c\\mathbf{v}\\) is also an eigenvector with the same eigenvalue. We often normalize eigenvectors to have unit length for convenience.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#finding-eigenvalues-the-characteristic-polynomial","title":"Finding Eigenvalues: The Characteristic Polynomial","text":"<p>To find eigenvalues, we need to solve the eigen equation systematically. Rearranging \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) gives us:</p> <p>\\(A\\mathbf{v} - \\lambda\\mathbf{v} = \\mathbf{0}\\)</p> <p>\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)</p> <p>where:</p> <ul> <li>\\(I\\) is the identity matrix of the same size as \\(A\\)</li> <li>\\(\\mathbf{0}\\) is the zero vector</li> </ul> <p>For a non-zero solution \\(\\mathbf{v}\\) to exist, the matrix \\((A - \\lambda I)\\) must be singular (non-invertible). This happens precisely when its determinant equals zero.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-characteristic-equation","title":"The Characteristic Equation","text":"<p>The characteristic equation determines all eigenvalues of a matrix:</p> <p>\\(\\det(A - \\lambda I) = 0\\)</p> <p>where:</p> <ul> <li>\\(\\det\\) denotes the determinant</li> <li>\\(A\\) is the square matrix</li> <li>\\(\\lambda\\) represents the unknown eigenvalue</li> <li>\\(I\\) is the identity matrix</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-characteristic-polynomial","title":"The Characteristic Polynomial","text":"<p>Expanding the determinant \\(\\det(A - \\lambda I)\\) produces a polynomial in \\(\\lambda\\) called the characteristic polynomial. For an \\(n \\times n\\) matrix, this polynomial has degree \\(n\\):</p> <p>\\(p(\\lambda) = \\det(A - \\lambda I) = (-1)^n \\lambda^n + c_{n-1}\\lambda^{n-1} + \\cdots + c_1\\lambda + c_0\\)</p> <p>The eigenvalues are the roots of this polynomial.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-finding-eigenvalues-of-a-22-matrix","title":"Example: Finding Eigenvalues of a 2\u00d72 Matrix","text":"<p>Consider the matrix:</p> <p>\\(A = \\begin{bmatrix} 4 &amp; 2 \\\\ 1 &amp; 3 \\end{bmatrix}\\)</p> <p>Step 1: Form \\((A - \\lambda I)\\):</p> <p>\\(A - \\lambda I = \\begin{bmatrix} 4-\\lambda &amp; 2 \\\\ 1 &amp; 3-\\lambda \\end{bmatrix}\\)</p> <p>Step 2: Compute the determinant:</p> <p>\\(\\det(A - \\lambda I) = (4-\\lambda)(3-\\lambda) - (2)(1)\\) \\(= 12 - 4\\lambda - 3\\lambda + \\lambda^2 - 2\\) \\(= \\lambda^2 - 7\\lambda + 10\\)</p> <p>Step 3: Solve the characteristic equation:</p> <p>\\(\\lambda^2 - 7\\lambda + 10 = 0\\) \\((\\lambda - 5)(\\lambda - 2) = 0\\)</p> <p>The eigenvalues are \\(\\lambda_1 = 5\\) and \\(\\lambda_2 = 2\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-characteristic-polynomial-explorer","title":"Diagram: Characteristic Polynomial Explorer","text":"<p>Run the Characteristic Polynomial Explorer Fullscreen</p> Characteristic Polynomial Explorer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Enable students to interactively compute characteristic polynomials and find eigenvalues for 2x2 and 3x3 matrices</p> <p>Visual elements: - Left panel: Matrix input grid (2x2 or 3x3) - Center panel: Step-by-step calculation display showing:   - The (A - \u03bbI) matrix with \u03bb as variable   - Determinant expansion   - Resulting polynomial in standard form - Right panel: Graph of the characteristic polynomial with x-axis as \u03bb - Eigenvalues marked as points where curve crosses x-axis - Vertical dashed lines from roots to x-axis</p> <p>Interactive controls: - Matrix size toggle (2x2 / 3x3) - Numeric input fields for matrix entries - \"Calculate\" button to compute polynomial - Slider to trace along the polynomial curve - Pre-set example matrices dropdown (identity, rotation, symmetric, defective)</p> <p>Default parameters: - 2x2 matrix mode - Matrix A = [[4, 2], [1, 3]] - Polynomial graph range: \u03bb from -2 to 8 - Canvas size: responsive, minimum 900x500px</p> <p>Behavior: - Real-time polynomial graph update as matrix values change - Highlight eigenvalues on graph with dots and labels - Show factored form when roots are nice numbers - Display \"Complex roots\" indicator when polynomial has no real zeros - Step-through animation of determinant calculation</p> <p>Implementation: p5.js with math expression rendering</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#finding-eigenvectors","title":"Finding Eigenvectors","text":"<p>Once we have an eigenvalue \\(\\lambda\\), we find its corresponding eigenvector(s) by solving:</p> <p>\\((A - \\lambda I)\\mathbf{v} = \\mathbf{0}\\)</p> <p>This is a homogeneous system of linear equations. We use row reduction to find the null space of \\((A - \\lambda I)\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-finding-eigenvectors","title":"Example: Finding Eigenvectors","text":"<p>Continuing with our matrix \\(A = \\begin{bmatrix} 4 &amp; 2 \\\\ 1 &amp; 3 \\end{bmatrix}\\):</p> <p>For \\(\\lambda_1 = 5\\):</p> <p>\\(A - 5I = \\begin{bmatrix} -1 &amp; 2 \\\\ 1 &amp; -2 \\end{bmatrix}\\)</p> <p>Row reduce to find the null space:</p> <p>\\(\\begin{bmatrix} -1 &amp; 2 \\\\ 1 &amp; -2 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; -2 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>From \\(x_1 - 2x_2 = 0\\), we get \\(x_1 = 2x_2\\). Setting \\(x_2 = 1\\):</p> <p>\\(\\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\)</p> <p>For \\(\\lambda_2 = 2\\):</p> <p>\\(A - 2I = \\begin{bmatrix} 2 &amp; 2 \\\\ 1 &amp; 1 \\end{bmatrix}\\)</p> <p>Row reduce:</p> <p>\\(\\begin{bmatrix} 2 &amp; 2 \\\\ 1 &amp; 1 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>From \\(x_1 + x_2 = 0\\), we get \\(x_1 = -x_2\\). Setting \\(x_2 = 1\\):</p> <p>\\(\\mathbf{v}_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigenspace","title":"Eigenspace","text":"<p>The eigenspace corresponding to an eigenvalue \\(\\lambda\\) is the set of all eigenvectors with that eigenvalue, together with the zero vector. Formally:</p> <p>\\(E_\\lambda = \\text{null}(A - \\lambda I) = \\{\\mathbf{v} \\in \\mathbb{R}^n : A\\mathbf{v} = \\lambda\\mathbf{v}\\}\\)</p> <p>The eigenspace is a vector subspace of \\(\\mathbb{R}^n\\). Its dimension is called the geometric multiplicity of the eigenvalue.</p> <p>Key properties of eigenspaces:</p> <ul> <li>Every eigenspace contains the zero vector</li> <li>Every eigenspace is closed under addition and scalar multiplication</li> <li>The dimension of an eigenspace is at least 1</li> <li>Eigenvectors from different eigenspaces are linearly independent</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenspace-visualization","title":"Diagram: Eigenspace Visualization","text":"<p>Run the Eigenspace Visualizer Fullscreen</p> Eigenspace Visualization <p>Type: diagram</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize eigenspaces as subspaces and understand how their dimension relates to geometric multiplicity</p> <p>Components to show: - 3D coordinate system with translucent planes/lines representing eigenspaces - For a 3x3 matrix with three distinct eigenvalues: three lines through origin - For a 3x3 matrix with a repeated eigenvalue (geometric multiplicity 2): one line and one plane - Original vectors and their transformed counterparts - Color coding by eigenvalue</p> <p>Layout: - Main 3D view showing eigenspaces - Rotation controls to view from different angles - Matrix display in corner - Legend showing eigenvalue-color correspondence</p> <p>Visual style: - Eigenspaces rendered as semi-transparent colored surfaces - Lines rendered as tubes for visibility - Sample vectors as arrows with different opacities</p> <p>Color scheme: - Eigenspace 1: Blue (line or plane) - Eigenspace 2: Orange (line or plane) - Eigenspace 3: Green (line) - Background grid: Light gray</p> <p>Implementation: Three.js or p5.js with WEBGL mode for 3D rendering</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#algebraic-and-geometric-multiplicity","title":"Algebraic and Geometric Multiplicity","text":"<p>The relationship between algebraic and geometric multiplicity is fundamental to understanding when matrices can be diagonalized.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#algebraic-multiplicity","title":"Algebraic Multiplicity","text":"<p>The algebraic multiplicity of an eigenvalue is the number of times it appears as a root of the characteristic polynomial. If the characteristic polynomial factors as:</p> <p>\\(p(\\lambda) = (\\lambda - \\lambda_1)^{m_1}(\\lambda - \\lambda_2)^{m_2} \\cdots (\\lambda - \\lambda_k)^{m_k}\\)</p> <p>then the algebraic multiplicity of \\(\\lambda_i\\) is \\(m_i\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-multiplicity","title":"Geometric Multiplicity","text":"<p>The geometric multiplicity of an eigenvalue is the dimension of its eigenspace:</p> <p>\\(g_i = \\dim(E_{\\lambda_i}) = \\dim(\\text{null}(A - \\lambda_i I))\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-multiplicity-inequality","title":"The Multiplicity Inequality","text":"<p>A fundamental theorem states that for any eigenvalue:</p> <p>\\(1 \\leq \\text{geometric multiplicity} \\leq \\text{algebraic multiplicity}\\)</p> <p>This inequality has profound implications for diagonalization.</p> Scenario Algebraic Geometric Diagonalizable? All eigenvalues distinct 1 each 1 each Yes Repeated eigenvalue, full eigenspace \\(m\\) \\(m\\) Yes Repeated eigenvalue, deficient eigenspace \\(m\\) \\(&lt; m\\) No"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-multiplicity-comparison-chart","title":"Diagram: Multiplicity Comparison Chart","text":"<p>Run the Multiplicity Comparison Chart Fullscreen</p> Multiplicity Comparison Chart <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare algebraic and geometric multiplicity across different matrix types and understand implications for diagonalization</p> <p>Layout: Three-column comparison with expandable examples</p> <p>Sections: 1. \"Distinct Eigenvalues\" column    - Example: A = [[2, 0], [0, 3]]    - Characteristic polynomial: (\u03bb-2)(\u03bb-3)    - Each eigenvalue has alg. mult. = geo. mult. = 1    - Status: Diagonalizable \u2713</p> <ol> <li>\"Repeated with Full Eigenspace\" column</li> <li>Example: A = [[2, 0], [0, 2]]</li> <li>Characteristic polynomial: (\u03bb-2)\u00b2</li> <li>Eigenvalue 2: alg. mult. = 2, geo. mult. = 2</li> <li> <p>Status: Diagonalizable \u2713</p> </li> <li> <p>\"Defective Matrix\" column</p> </li> <li>Example: A = [[2, 1], [0, 2]]</li> <li>Characteristic polynomial: (\u03bb-2)\u00b2</li> <li>Eigenvalue 2: alg. mult. = 2, geo. mult. = 1</li> <li>Status: Not Diagonalizable \u2717</li> </ol> <p>Interactive elements: - Hover over each example to see eigenspace visualization - Click to expand full calculation - Toggle between 2x2 and 3x3 examples</p> <p>Visual style: - Clean cards with matrix notation - Color indicators: green for diagonalizable, red for defective - Progress bars showing geometric/algebraic ratio</p> <p>Implementation: HTML/CSS/JavaScript with SVG visualizations</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#similar-matrices-and-diagonalization","title":"Similar Matrices and Diagonalization","text":"<p>Two matrices \\(A\\) and \\(B\\) are similar if there exists an invertible matrix \\(P\\) such that:</p> <p>\\(B = P^{-1}AP\\)</p> <p>Similar matrices share important properties:</p> <ul> <li>Same eigenvalues (with same algebraic multiplicities)</li> <li>Same determinant</li> <li>Same trace</li> <li>Same rank</li> <li>Same characteristic polynomial</li> </ul> <p>The geometric interpretation is that similar matrices represent the same linear transformation in different bases.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagonalization","title":"Diagonalization","text":"<p>A matrix \\(A\\) is diagonalizable if it is similar to a diagonal matrix. This means we can write:</p> <p>\\(A = PDP^{-1}\\)</p> <p>where:</p> <ul> <li>\\(D\\) is a diagonal matrix containing the eigenvalues</li> <li>\\(P\\) is a matrix whose columns are the corresponding eigenvectors</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-diagonal-form","title":"The Diagonal Form","text":"<p>The diagonal form \\(D\\) of a diagonalizable matrix contains eigenvalues on its main diagonal:</p> <p>\\(D = \\begin{bmatrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\lambda_n \\end{bmatrix}\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#conditions-for-diagonalizability","title":"Conditions for Diagonalizability","text":"<p>A matrix \\(A\\) is diagonalizable if and only if:</p> <ol> <li>The sum of geometric multiplicities equals \\(n\\) (the matrix dimension), OR</li> <li>For each eigenvalue, geometric multiplicity equals algebraic multiplicity, OR</li> <li>\\(A\\) has \\(n\\) linearly independent eigenvectors</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-diagonalization-process-workflow","title":"Diagram: Diagonalization Process Workflow","text":"<p>Run the Diagonalization Workflow Fullscreen</p> Diagonalization Process Workflow <p>Type: workflow</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Guide students through the step-by-step process of diagonalizing a matrix</p> <p>Visual style: Flowchart with decision diamonds and process rectangles</p> <p>Steps: 1. Start: \"Given matrix A\"    Hover text: \"Begin with an n\u00d7n matrix A\"</p> <ol> <li> <p>Process: \"Find characteristic polynomial det(A - \u03bbI)\"    Hover text: \"Expand determinant to get polynomial in \u03bb\"</p> </li> <li> <p>Process: \"Solve characteristic equation for eigenvalues\"    Hover text: \"Find all roots \u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2096\"</p> </li> <li> <p>Decision: \"All n eigenvalues found (counting multiplicity)?\"    Hover text: \"Complex eigenvalues count too\"</p> </li> <li> <p>No \u2192 End: \"Check for complex eigenvalues\"</p> </li> <li> <p>Process: \"For each eigenvalue, find eigenvectors\"    Hover text: \"Solve (A - \u03bbI)v = 0 for each \u03bb\"</p> </li> <li> <p>Process: \"Determine geometric multiplicity of each eigenvalue\"    Hover text: \"Count linearly independent eigenvectors\"</p> </li> <li> <p>Decision: \"Geometric mult. = Algebraic mult. for all eigenvalues?\"</p> </li> <li>No \u2192 End: \"Matrix is NOT diagonalizable\"</li> <li> <p>Yes \u2192 Continue</p> </li> <li> <p>Process: \"Form P from eigenvector columns\"    Hover text: \"P = [v\u2081 | v\u2082 | ... | v\u2099]\"</p> </li> <li> <p>Process: \"Form D from eigenvalues\"    Hover text: \"D = diag(\u03bb\u2081, \u03bb\u2082, ..., \u03bb\u2099)\"</p> </li> <li> <p>End: \"A = PDP\u207b\u00b9\"     Hover text: \"Diagonalization complete!\"</p> </li> </ol> <p>Color coding: - Blue: Computation steps - Yellow: Decision points - Green: Success outcomes - Red: Failure outcomes</p> <p>Implementation: Mermaid.js or custom SVG with hover interactions</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#why-diagonalization-matters","title":"Why Diagonalization Matters","text":"<p>Diagonalization simplifies many computations:</p> <p>Matrix Powers: Computing \\(A^k\\) becomes trivial:</p> <p>\\(A^k = PD^kP^{-1}\\)</p> <p>where \\(D^k = \\begin{bmatrix} \\lambda_1^k &amp; 0 &amp; \\cdots \\\\ 0 &amp; \\lambda_2^k &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\)</p> <p>Exponentials: The matrix exponential \\(e^A\\) is essential for solving differential equations:</p> <p>\\(e^A = Pe^DP^{-1}\\)</p> <p>where \\(e^D = \\begin{bmatrix} e^{\\lambda_1} &amp; 0 &amp; \\cdots \\\\ 0 &amp; e^{\\lambda_2} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\)</p> <p>Systems of Differential Equations: The system \\(\\frac{d\\mathbf{x}}{dt} = A\\mathbf{x}\\) has solution:</p> <p>\\(\\mathbf{x}(t) = c_1e^{\\lambda_1 t}\\mathbf{v}_1 + c_2e^{\\lambda_2 t}\\mathbf{v}_2 + \\cdots\\)</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-matrix-power-calculator","title":"Diagram: Matrix Power Calculator","text":"<p>Run the Matrix Power Calculator Fullscreen</p> Matrix Power Calculator MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Demonstrate how diagonalization simplifies computing matrix powers by comparing direct multiplication vs. the eigenvalue approach</p> <p>Visual elements: - Left panel: Input matrix A (2x2 or 3x3) - Center panel: Diagonalization display showing P, D, P\u207b\u00b9 - Right panel: Result matrix A\u1d4f - Bottom: Step-by-step calculation toggle</p> <p>Interactive controls: - Matrix entry fields for A - Power k slider (1 to 20) - \"Compute Direct\" button (shows A\u00d7A\u00d7...\u00d7A method) - \"Compute via Diagonalization\" button (shows PD^kP^{-1} method) - Speed comparison display - Animation speed slider</p> <p>Default parameters: - Matrix A = [[2, 1], [0, 3]] - Power k = 5 - Canvas: responsive layout</p> <p>Behavior: - Show step-by-step computation for both methods - Highlight the efficiency of eigenvalue method for large k - Display operation count for each method - Warning message if matrix is not diagonalizable - Show numerical error comparison for high powers</p> <p>Implementation: p5.js with matrix computation library</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#eigendecomposition","title":"Eigendecomposition","text":"<p>The eigendecomposition (also called spectral decomposition for symmetric matrices) expresses a diagonalizable matrix as a product of its eigenvectors and eigenvalues:</p> <p>\\(A = PDP^{-1} = \\sum_{i=1}^{n} \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) (for symmetric matrices with orthonormal eigenvectors)</p> <p>More generally, for any diagonalizable matrix:</p> <p>\\(A = \\sum_{i=1}^{n} \\lambda_i \\mathbf{v}_i \\mathbf{w}_i^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_i\\) are the right eigenvectors (columns of \\(P\\))</li> <li>\\(\\mathbf{w}_i\\) are the left eigenvectors (rows of \\(P^{-1}\\))</li> <li>\\(\\lambda_i\\) are the eigenvalues</li> </ul> <p>This representation reveals that a matrix can be decomposed into a sum of rank-1 matrices, each scaled by an eigenvalue.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#complex-eigenvalues","title":"Complex Eigenvalues","text":"<p>Real matrices can have complex eigenvalues. When they occur, complex eigenvalues always appear in conjugate pairs: if \\(\\lambda = a + bi\\) is an eigenvalue, so is \\(\\bar{\\lambda} = a - bi\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#geometric-interpretation_1","title":"Geometric Interpretation","text":"<p>Complex eigenvalues indicate rotation in the transformation. For a 2\u00d72 real matrix with eigenvalues \\(\\lambda = a \\pm bi\\):</p> <ul> <li>\\(|{\\lambda}| = \\sqrt{a^2 + b^2}\\) gives the scaling factor</li> <li>\\(\\theta = \\arctan(b/a)\\) gives the rotation angle</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#example-rotation-matrix","title":"Example: Rotation Matrix","text":"<p>The rotation matrix by angle \\(\\theta\\):</p> <p>\\(R_\\theta = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>has eigenvalues \\(\\lambda = \\cos\\theta \\pm i\\sin\\theta = e^{\\pm i\\theta}\\).</p> <p>For a 90\u00b0 rotation:</p> <p>\\(R_{90\u00b0} = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}\\)</p> <p>The eigenvalues are \\(\\lambda = \\pm i\\), which are purely imaginary\u2014reflecting pure rotation with no scaling.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-complex-eigenvalue-visualizer","title":"Diagram: Complex Eigenvalue Visualizer","text":"<p>Run the Complex Eigenvalue Visualizer Fullscreen</p> Complex Eigenvalue Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how complex eigenvalues correspond to rotation-scaling transformations in 2D</p> <p>Visual elements: - Left panel: 2D plane showing transformation of a unit square - Right panel: Complex plane showing eigenvalue locations - Unit circle on complex plane for reference - Spiral path showing repeated application of transformation - Angle arc showing rotation per step</p> <p>Interactive controls: - Slider for real part a of eigenvalue (range: -2 to 2) - Slider for imaginary part b of eigenvalue (range: -2 to 2) - \"Animate\" button to show repeated transformation - Step counter display - \"Show Conjugate Pair\" toggle - Reset button</p> <p>Default parameters: - a = 0.9 (slight contraction) - b = 0.4 (rotation component) - Canvas: 800x400px responsive</p> <p>Behavior: - As sliders adjust, show corresponding matrix A - Animate unit square through multiple transformation steps - Plot trajectory of corner point as spiral - Display eigenvalue magnitude and angle - Show connection between eigenvalue position and transformation behavior:   - |\u03bb| &gt; 1: spiral outward   - |\u03bb| &lt; 1: spiral inward   - |\u03bb| = 1: pure rotation (circle) - Highlight conjugate pair relationship</p> <p>Implementation: p5.js with complex number support</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#symmetric-matrices-and-the-spectral-theorem","title":"Symmetric Matrices and the Spectral Theorem","text":"<p>Symmetric matrices (\\(A = A^T\\)) have particularly nice eigenvalue properties that make them central to applications in machine learning, physics, and engineering.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#symmetric-eigenvalues","title":"Symmetric Eigenvalues","text":"<p>For a symmetric matrix \\(A\\):</p> <ol> <li>All eigenvalues are real (no complex eigenvalues)</li> <li>Eigenvectors corresponding to distinct eigenvalues are orthogonal</li> <li>The matrix is always diagonalizable</li> </ol> <p>These properties follow from the fact that symmetric matrices equal their own transposes, constraining the characteristic polynomial to have only real roots.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-spectral-theorem","title":"The Spectral Theorem","text":"<p>The Spectral Theorem provides a complete characterization of symmetric matrices:</p> <p>The Spectral Theorem for Real Symmetric Matrices</p> <p>A real matrix \\(A\\) is symmetric if and only if it can be orthogonally diagonalized:</p> <p>\\(A = Q\\Lambda Q^T\\)</p> <p>where \\(Q\\) is an orthogonal matrix (\\(Q^TQ = I\\)) whose columns are orthonormal eigenvectors, and \\(\\Lambda\\) is a diagonal matrix of real eigenvalues.</p> <p>The beauty of orthogonal diagonalization is that \\(Q^{-1} = Q^T\\), which is computationally simple to obtain.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#spectral-decomposition-form","title":"Spectral Decomposition Form","text":"<p>For a symmetric matrix, the eigendecomposition takes the elegant form:</p> <p>\\(A = \\sum_{i=1}^{n} \\lambda_i \\mathbf{q}_i \\mathbf{q}_i^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{q}_i\\) are orthonormal eigenvectors</li> <li>\\(\\lambda_i\\) are real eigenvalues</li> <li>Each \\(\\mathbf{q}_i \\mathbf{q}_i^T\\) is a projection matrix onto the eigenspace</li> </ul> <p>This decomposition is the foundation of Principal Component Analysis (PCA).</p> Property General Matrix Symmetric Matrix Eigenvalues May be complex Always real Eigenvectors Generally not orthogonal Orthogonal (for distinct \u03bb) Diagonalization Not guaranteed Always possible Inverse of P Must compute P\u207b\u00b9 Simply P^T Numerical stability May have issues Highly stable"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-spectral-theorem-for-symmetric-matrices","title":"Diagram: Spectral Theorem for Symmetric Matrices","text":"<p>Run the Spectral Theorem Demo Fullscreen</p> Spectral Theorem Interactive Demonstration <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Demonstrate the spectral theorem by showing how symmetric matrices decompose into orthogonal eigenvectors and real eigenvalues</p> <p>Visual elements: - Left panel: 2D/3D visualization of transformation - Center panel: Matrix equation display A = Q\u039bQ\u1d40 - Right panel: Individual rank-1 components \u03bb\u1d62q\u1d62q\u1d62\u1d40 - Orthogonality indicator showing q\u1d62\u00b7q\u2c7c = 0 for i\u2260j - Unit sphere showing eigenvector directions</p> <p>Interactive controls: - Symmetric matrix input (auto-enforced: entering a\u1d62\u2c7c sets a\u2c7c\u1d62) - Slider to blend between original matrix and diagonal form - Component selector to highlight individual \u03bb\u1d62q\u1d62q\u1d62\u1d40 terms - \"Verify Orthogonality\" button - 2D/3D toggle (for 2x2 and 3x3 matrices)</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 3]] (symmetric) - Canvas: responsive</p> <p>Behavior: - Real-time eigenvalue/eigenvector computation - Show eigenvectors as perpendicular on unit circle/sphere - Animate decomposition into sum of outer products - Verify QQ\u1d40 = I visually - Display reconstruction error when summing components</p> <p>Implementation: p5.js with linear algebra computations</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#power-iteration-computing-the-dominant-eigenvalue","title":"Power Iteration: Computing the Dominant Eigenvalue","text":"<p>For large matrices, computing eigenvalues through the characteristic polynomial is impractical. Power iteration is a simple iterative algorithm for finding the largest eigenvalue and its eigenvector.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#the-dominant-eigenvalue","title":"The Dominant Eigenvalue","text":"<p>The dominant eigenvalue is the eigenvalue with the largest absolute value:</p> <p>\\(|\\lambda_1| &gt; |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_n|\\)</p> <p>The corresponding eigenvector is called the dominant eigenvector.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#power-iteration-algorithm","title":"Power Iteration Algorithm","text":"<p>The algorithm works by repeatedly multiplying a random vector by the matrix:</p> <ol> <li>Start with a random non-zero vector \\(\\mathbf{x}_0\\)</li> <li>Compute \\(\\mathbf{y}_{k+1} = A\\mathbf{x}_k\\)</li> <li>Normalize: \\(\\mathbf{x}_{k+1} = \\mathbf{y}_{k+1} / \\|\\mathbf{y}_{k+1}\\|\\)</li> <li>Repeat until convergence</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#convergence-rate","title":"Convergence Rate","text":"<p>The convergence rate depends on the ratio of the two largest eigenvalues:</p> <p>\\(\\text{error after } k \\text{ iterations} \\approx O\\left(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k\\right)\\)</p> <p>If \\(|\\lambda_1| \\gg |\\lambda_2|\\), convergence is fast. If \\(|\\lambda_1| \\approx |\\lambda_2|\\), convergence is slow.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#rayleigh-quotient","title":"Rayleigh Quotient","text":"<p>After obtaining an approximate eigenvector \\(\\mathbf{x}\\), we can estimate the eigenvalue using the Rayleigh quotient:</p> <p>\\(\\lambda \\approx R(\\mathbf{x}) = \\frac{\\mathbf{x}^T A \\mathbf{x}}{\\mathbf{x}^T \\mathbf{x}}\\)</p> <p>The Rayleigh quotient gives a more accurate eigenvalue estimate than examining vector scaling alone.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-power-iteration-microsim","title":"Diagram: Power Iteration MicroSim","text":"<p>Run the Power Iteration Visualizer Fullscreen</p> Power Iteration Algorithm Visualization <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how power iteration converges to the dominant eigenvector and how convergence rate depends on the eigenvalue ratio</p> <p>Visual elements: - Left panel: 2D plane showing vector iterations   - Current vector (solid arrow)   - Previous vectors (faded arrows showing history)   - True dominant eigenvector direction (dashed line)   - Angle error indicator - Right panel: Convergence plot   - X-axis: iteration number   - Y-axis: log(error) where error = angle to true eigenvector   - Theoretical convergence rate line for comparison - Bottom panel: Matrix and current eigenvalue estimate</p> <p>Interactive controls: - 2x2 matrix input - \"Step\" button for single iteration - \"Run\" button for continuous animation - Speed slider for animation - \"Reset with Random Vector\" button - Convergence threshold input - Display of \u03bb\u2082/\u03bb\u2081 ratio</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 2]] (eigenvalues \u2248 3.62, 1.38) - Initial vector: random unit vector - Canvas: 800x600px responsive</p> <p>Behavior: - Show each iteration step clearly - Highlight when convergence criterion met - Display iteration count and current eigenvalue estimate - Show Rayleigh quotient computation - Compare to true eigenvalue (computed analytically for 2x2) - Demonstrate slow convergence when eigenvalue ratio near 1 - Warning if matrix has complex dominant eigenvalue</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#variants-of-power-iteration","title":"Variants of Power Iteration","text":"<p>Several important algorithms extend the basic power iteration:</p> <ul> <li>Inverse Power Iteration: Find the smallest eigenvalue by applying power iteration to \\(A^{-1}\\)</li> <li>Shifted Inverse Iteration: Find eigenvalue closest to a given shift \\(\\sigma\\) using \\((A - \\sigma I)^{-1}\\)</li> <li>QR Algorithm: Industry-standard method that finds all eigenvalues simultaneously</li> <li>Lanczos Algorithm: Efficient for large sparse symmetric matrices</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#applications-in-machine-learning-and-ai","title":"Applications in Machine Learning and AI","text":"<p>Eigenanalysis is fundamental to numerous AI and machine learning algorithms:</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA finds the directions of maximum variance in data by computing eigenvectors of the covariance matrix:</p> <ol> <li>Center the data: \\(\\bar{X} = X - \\mu\\)</li> <li>Compute covariance matrix: \\(C = \\frac{1}{n-1}\\bar{X}^T\\bar{X}\\)</li> <li>Find eigenvalues and eigenvectors of \\(C\\)</li> <li>Project data onto top \\(k\\) eigenvectors</li> </ol> <p>The eigenvectors are the principal components, and eigenvalues indicate variance explained.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#pagerank-algorithm","title":"PageRank Algorithm","text":"<p>Google's PageRank models web page importance as the dominant eigenvector of a modified adjacency matrix:</p> <p>\\(\\mathbf{r} = M\\mathbf{r}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{r}\\) is the rank vector (eigenvector)</li> <li>\\(M\\) is the transition probability matrix</li> <li>PageRank is the eigenvector with eigenvalue 1</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#stability-analysis","title":"Stability Analysis","text":"<p>In dynamical systems and neural network training, eigenvalues determine stability:</p> <ul> <li>\\(|\\lambda| &lt; 1\\) for all eigenvalues: system is stable (converges)</li> <li>\\(|\\lambda| = 1\\): system is marginally stable (oscillates)</li> <li>\\(|\\lambda| &gt; 1\\): system is unstable (explodes)</li> </ul> <p>Neural networks with weight matrices having eigenvalues far from 1 suffer from vanishing or exploding gradients.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#diagram-eigenvalue-applications-map","title":"Diagram: Eigenvalue Applications Map","text":"<p>Run the Eigenvalue Applications Map Fullscreen</p> Eigenvalue Applications in ML and AI <p>Type: infographic</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Connect eigenanalysis concepts to real-world machine learning applications and understand when to apply each technique</p> <p>Layout: Central hub-and-spoke diagram with clickable nodes</p> <p>Central concept: \"Eigenanalysis\"</p> <p>Spokes (applications): 1. PCA / Dimensionality Reduction    - Uses: Covariance matrix eigenvectors    - Key insight: Eigenvectors = directions of max variance    - Example: Face recognition (Eigenfaces)</p> <ol> <li>Spectral Clustering</li> <li>Uses: Graph Laplacian eigenvectors</li> <li>Key insight: Second eigenvector separates clusters</li> <li> <p>Example: Image segmentation</p> </li> <li> <p>Google PageRank</p> </li> <li>Uses: Dominant eigenvector</li> <li>Key insight: Power iteration at web scale</li> <li> <p>Example: Web page ranking</p> </li> <li> <p>Neural Network Stability</p> </li> <li>Uses: Weight matrix eigenvalues</li> <li>Key insight: |\u03bb| controls gradient flow</li> <li> <p>Example: RNN vanishing gradients</p> </li> <li> <p>Recommender Systems</p> </li> <li>Uses: Matrix factorization (related to eigendecomposition)</li> <li>Key insight: Low-rank approximation</li> <li> <p>Example: Netflix recommendations</p> </li> <li> <p>Quantum Computing</p> </li> <li>Uses: Eigenvalues as measurement outcomes</li> <li>Key insight: Observables are Hermitian operators</li> <li>Example: Quantum simulation</li> </ol> <p>Interactive elements: - Click each spoke to expand details - Hover for quick summary - Links to related concepts in other chapters</p> <p>Visual style: - Modern flat design with icons for each application - Color coding by application domain (ML, physics, web, etc.) - Connecting lines showing concept flow</p> <p>Implementation: D3.js or custom SVG with interaction handlers</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#computational-considerations","title":"Computational Considerations","text":"<p>When working with eigenproblems in practice, several computational issues arise:</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#numerical-stability","title":"Numerical Stability","text":"<ul> <li>Direct polynomial root-finding is numerically unstable for large matrices</li> <li>The QR algorithm is the standard stable method</li> <li>Symmetric matrices have more stable algorithms (divide-and-conquer, MRRR)</li> </ul>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#computational-complexity","title":"Computational Complexity","text":"Method Complexity Use Case Characteristic polynomial \\(O(n^3)\\) for determinant Theoretical, small matrices Power iteration \\(O(n^2)\\) per iteration Dominant eigenvalue only QR algorithm \\(O(n^3)\\) total All eigenvalues, dense matrices Lanczos/Arnoldi \\(O(kn^2)\\) Top \\(k\\) eigenvalues, sparse matrices"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#software-libraries","title":"Software Libraries","text":"<p>In practice, use optimized libraries:</p> <ul> <li>NumPy/SciPy: <code>np.linalg.eig()</code>, <code>np.linalg.eigh()</code> for symmetric</li> <li>PyTorch: <code>torch.linalg.eig()</code> for GPU acceleration</li> <li>LAPACK: Industry-standard Fortran library underlying most implementations</li> </ul> <pre><code>import numpy as np\n\n# General eigenvalue problem\nA = np.array([[4, 2], [1, 3]])\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Symmetric matrix (more stable)\nS = np.array([[3, 1], [1, 3]])\neigenvalues, eigenvectors = np.linalg.eigh(S)\n</code></pre>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#summary_1","title":"Summary","text":"<p>This chapter introduced eigenanalysis as a fundamental tool for understanding linear transformations:</p> <p>Core Concepts:</p> <ul> <li>Eigenvalues and eigenvectors satisfy \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), revealing directions preserved by transformations</li> <li>The characteristic polynomial \\(\\det(A - \\lambda I) = 0\\) yields eigenvalues as its roots</li> <li>Eigenspaces are subspaces containing all eigenvectors for a given eigenvalue</li> </ul> <p>Multiplicities and Diagonalization:</p> <ul> <li>Algebraic multiplicity counts eigenvalue repetition in the characteristic polynomial</li> <li>Geometric multiplicity measures eigenspace dimension</li> <li>A matrix is diagonalizable when geometric equals algebraic multiplicity for all eigenvalues</li> <li>Similar matrices share eigenvalues and represent the same transformation in different bases</li> </ul> <p>Special Cases:</p> <ul> <li>Complex eigenvalues indicate rotation and appear in conjugate pairs for real matrices</li> <li>Symmetric matrices have real eigenvalues and orthogonal eigenvectors (Spectral Theorem)</li> </ul> <p>Computation:</p> <ul> <li>Power iteration finds the dominant eigenvalue through repeated matrix-vector multiplication</li> <li>The Rayleigh quotient provides eigenvalue estimates from approximate eigenvectors</li> <li>Eigendecomposition \\(A = PDP^{-1}\\) enables efficient computation of matrix powers and exponentials</li> </ul> <p>Key Takeaways for AI/ML:</p> <ol> <li>PCA reduces dimensionality using covariance matrix eigenvectors</li> <li>PageRank is an eigenvector problem solved by power iteration</li> <li>Neural network stability depends on weight matrix eigenvalues</li> <li>The spectral theorem guarantees nice properties for symmetric matrices (common in ML)</li> </ol> Self-Check: Can you identify which eigenvalue property determines whether a neural network's gradients will vanish or explode? <p>The magnitude of the eigenvalues of the weight matrices determines gradient behavior. If \\(|\\lambda| &lt; 1\\) for all eigenvalues, gradients shrink exponentially (vanishing). If \\(|\\lambda| &gt; 1\\), gradients grow exponentially (exploding). Stable training requires eigenvalues near \\(|\\lambda| = 1\\).</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/","title":"Quiz: Eigenvalues and Eigenvectors","text":"<p>Test your understanding of eigenvalues, eigenvectors, and their applications.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#1-an-eigenvector-mathbfv-of-matrix-a-satisfies","title":"1. An eigenvector \\(\\mathbf{v}\\) of matrix \\(A\\) satisfies:","text":"<ol> <li>\\(A\\mathbf{v} = \\mathbf{0}\\)</li> <li>\\(A\\mathbf{v} = \\lambda\\mathbf{v}\\) for some scalar \\(\\lambda\\)</li> <li>\\(A + \\mathbf{v} = \\lambda\\)</li> <li>\\(\\mathbf{v}^T A = \\lambda\\)</li> </ol> Show Answer <p>The correct answer is B. An eigenvector \\(\\mathbf{v}\\) of matrix \\(A\\) satisfies \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), where \\(\\lambda\\) is the corresponding eigenvalue. The matrix simply scales the eigenvector rather than changing its direction.</p> <p>Concept Tested: Eigenvector</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#2-the-characteristic-polynomial-of-matrix-a-is","title":"2. The characteristic polynomial of matrix \\(A\\) is:","text":"<ol> <li>\\(\\det(A)\\)</li> <li>\\(\\det(A - \\lambda I)\\)</li> <li>\\(\\det(A + \\lambda I)\\)</li> <li>\\(\\text{tr}(A) - \\lambda\\)</li> </ol> Show Answer <p>The correct answer is B. The characteristic polynomial is \\(p(\\lambda) = \\det(A - \\lambda I)\\). Setting this equal to zero gives the characteristic equation, whose roots are the eigenvalues.</p> <p>Concept Tested: Characteristic Polynomial</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#3-the-eigenvalues-of-a-symmetric-matrix-are-always","title":"3. The eigenvalues of a symmetric matrix are always:","text":"<ol> <li>Complex numbers</li> <li>Real numbers</li> <li>Positive numbers</li> <li>Zero</li> </ol> Show Answer <p>The correct answer is B. A key property of symmetric matrices is that all eigenvalues are real numbers. Additionally, eigenvectors corresponding to distinct eigenvalues are orthogonal.</p> <p>Concept Tested: Eigenvalues of Symmetric Matrices</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#4-the-sum-of-all-eigenvalues-of-a-matrix-equals","title":"4. The sum of all eigenvalues of a matrix equals:","text":"<ol> <li>The determinant</li> <li>The trace</li> <li>The rank</li> <li>Zero</li> </ol> Show Answer <p>The correct answer is B. The sum of all eigenvalues (counting multiplicities) equals the trace of the matrix. Similarly, the product of all eigenvalues equals the determinant.</p> <p>Concept Tested: Eigenvalue Properties</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#5-if-lambda-is-an-eigenvalue-of-a-then-the-eigenvalue-of-a2-for-the-same-eigenvector-is","title":"5. If \\(\\lambda\\) is an eigenvalue of \\(A\\), then the eigenvalue of \\(A^2\\) for the same eigenvector is:","text":"<ol> <li>\\(\\lambda\\)</li> <li>\\(2\\lambda\\)</li> <li>\\(\\lambda^2\\)</li> <li>\\(\\sqrt{\\lambda}\\)</li> </ol> Show Answer <p>The correct answer is C. If \\(A\\mathbf{v} = \\lambda\\mathbf{v}\\), then \\(A^2\\mathbf{v} = A(A\\mathbf{v}) = A(\\lambda\\mathbf{v}) = \\lambda(A\\mathbf{v}) = \\lambda^2\\mathbf{v}\\). Raising a matrix to a power raises its eigenvalues to the same power.</p> <p>Concept Tested: Powers of Matrices</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#6-eigendecomposition-of-matrix-a-when-possible-is","title":"6. Eigendecomposition of matrix \\(A\\) (when possible) is:","text":"<ol> <li>\\(A = V + D\\)</li> <li>\\(A = VDV^{-1}\\)</li> <li>\\(A = V^T D V\\)</li> <li>\\(A = D - V\\)</li> </ol> Show Answer <p>The correct answer is B. Eigendecomposition expresses a matrix as \\(A = VDV^{-1}\\), where \\(V\\) is the matrix of eigenvectors and \\(D\\) is a diagonal matrix of eigenvalues. For symmetric matrices, \\(V\\) is orthogonal so \\(A = VDV^T\\).</p> <p>Concept Tested: Eigendecomposition</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#7-a-matrix-is-diagonalizable-if","title":"7. A matrix is diagonalizable if:","text":"<ol> <li>It is symmetric</li> <li>It has \\(n\\) linearly independent eigenvectors</li> <li>All eigenvalues are positive</li> <li>It is invertible</li> </ol> Show Answer <p>The correct answer is B. A matrix is diagonalizable if and only if it has \\(n\\) linearly independent eigenvectors, where \\(n\\) is the matrix dimension. Symmetric matrices always satisfy this condition.</p> <p>Concept Tested: Diagonalizability</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#8-the-spectral-theorem-states-that-for-a-real-symmetric-matrix","title":"8. The spectral theorem states that for a real symmetric matrix:","text":"<ol> <li>All eigenvalues are complex</li> <li>The matrix has an orthonormal basis of eigenvectors</li> <li>The matrix is not diagonalizable</li> <li>Eigenvalues form a spectrum of colors</li> </ol> Show Answer <p>The correct answer is B. The spectral theorem guarantees that real symmetric matrices have real eigenvalues and an orthonormal basis of eigenvectors. This enables the decomposition \\(A = Q\\Lambda Q^T\\) where \\(Q\\) is orthogonal.</p> <p>Concept Tested: Spectral Theorem</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#9-why-are-eigenvalues-important-in-stability-analysis","title":"9. Why are eigenvalues important in stability analysis?","text":"<ol> <li>They determine the color of the system</li> <li>Negative real parts indicate stable equilibria</li> <li>They measure the matrix size</li> <li>They count the number of solutions</li> </ol> Show Answer <p>The correct answer is B. In dynamical systems \\(\\dot{\\mathbf{x}} = A\\mathbf{x}\\), eigenvalues determine stability. If all eigenvalues have negative real parts, perturbations decay over time and the system is stable. Positive real parts indicate instability.</p> <p>Concept Tested: Stability Analysis</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/quiz/#10-in-principal-component-analysis-pca-eigenvectors-of-the-covariance-matrix-represent","title":"10. In Principal Component Analysis (PCA), eigenvectors of the covariance matrix represent:","text":"<ol> <li>Random noise directions</li> <li>Directions of maximum variance in the data</li> <li>The mean of the data</li> <li>Outlier locations</li> </ol> Show Answer <p>The correct answer is B. In PCA, eigenvectors of the covariance matrix (principal components) point in directions of maximum variance. The corresponding eigenvalues indicate how much variance is captured by each direction.</p> <p>Concept Tested: Principal Component Analysis</p>"},{"location":"chapters/07-matrix-decompositions/","title":"Matrix Decompositions","text":""},{"location":"chapters/07-matrix-decompositions/#summary","title":"Summary","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction. This chapter covers LU, QR, Cholesky, and Singular Value Decomposition (SVD). Each decomposition has specific use cases: LU for solving systems efficiently, QR for least squares problems, Cholesky for symmetric positive definite matrices, and SVD for low-rank approximations and recommender systems.</p>"},{"location":"chapters/07-matrix-decompositions/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"chapters/07-matrix-decompositions/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 8: Vector Spaces and Inner Products (for Gram-Schmidt)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#introduction","title":"Introduction","text":"<p>Just as integers can be factored into primes to reveal their structure, matrices can be decomposed into products of simpler matrices. These matrix factorizations expose the underlying structure of linear transformations, enable efficient computation, and provide geometric insight into how matrices act on vectors.</p> <p>Matrix decompositions are the workhorses of numerical linear algebra. When you solve a system of linear equations, compute a least-squares fit, reduce dimensionality with PCA, or build a recommender system, you are using matrix decompositions behind the scenes. Understanding these factorizations\u2014when to use each one and what makes them numerically stable\u2014is essential for any practitioner working with data.</p> <p>This chapter covers four major decompositions, each with distinct purposes:</p> Decomposition Form Primary Use Cases LU \\(A = LU\\) Solving systems, computing determinants QR \\(A = QR\\) Least squares, eigenvalue algorithms Cholesky \\(A = LL^T\\) Symmetric positive definite systems SVD \\(A = U\\Sigma V^T\\) Low-rank approximation, dimensionality reduction"},{"location":"chapters/07-matrix-decompositions/#matrix-rank-a-foundation-for-decompositions","title":"Matrix Rank: A Foundation for Decompositions","text":"<p>Before diving into specific decompositions, we need to understand matrix rank, which fundamentally determines what decompositions are possible and their properties.</p>"},{"location":"chapters/07-matrix-decompositions/#matrix-rank","title":"Matrix Rank","text":"<p>The rank of a matrix \\(A\\) is the dimension of its column space (equivalently, its row space):</p> <p>\\(\\text{rank}(A) = \\dim(\\text{col}(A)) = \\dim(\\text{row}(A))\\)</p> <p>For an \\(m \\times n\\) matrix:</p> <ul> <li>\\(\\text{rank}(A) \\leq \\min(m, n)\\)</li> <li>If \\(\\text{rank}(A) = \\min(m, n)\\), the matrix has full rank</li> <li>If \\(\\text{rank}(A) &lt; \\min(m, n)\\), the matrix is rank-deficient</li> </ul> <p>The rank tells us how many linearly independent columns (or rows) the matrix contains, which directly impacts the uniqueness of solutions to linear systems and the structure of decompositions.</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-matrix-rank-visualizer","title":"Diagram: Matrix Rank Visualizer","text":"<p>Run the Matrix Rank Visualizer Fullscreen</p> Matrix Rank Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how matrix rank relates to the column space and understand rank-deficient matrices geometrically</p> <p>Visual elements: - Left panel: 3x3 matrix input with editable values - Center panel: 3D visualization showing column vectors as arrows from origin - Column space displayed as a plane (rank 2) or line (rank 1) or point (rank 0) - Right panel: Row echelon form showing pivot positions</p> <p>Interactive controls: - Matrix entry fields (3x3) - \"Compute Rank\" button - Toggle to show/hide individual column vectors - Toggle to show column space as shaded region - Preset examples dropdown (full rank, rank 2, rank 1) - Animation to show column reduction</p> <p>Default parameters: - Matrix A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] (rank 2) - Canvas: responsive, minimum 900x500px</p> <p>Behavior: - Real-time rank computation as matrix values change - Highlight linearly dependent columns - Show column space dimension visually - Display row echelon form with pivot columns highlighted - Animate transition when rank changes</p> <p>Implementation: p5.js with WEBGL for 3D visualization</p>"},{"location":"chapters/07-matrix-decompositions/#lu-decomposition","title":"LU Decomposition","text":"<p>LU Decomposition factors a square matrix into the product of a lower triangular matrix \\(L\\) and an upper triangular matrix \\(U\\):</p> <p>\\(A = LU\\)</p> <p>where:</p> <ul> <li>\\(L\\) is lower triangular with 1s on the diagonal</li> <li>\\(U\\) is upper triangular</li> </ul> <p>This decomposition essentially records the steps of Gaussian elimination in matrix form.</p>"},{"location":"chapters/07-matrix-decompositions/#why-lu-decomposition-matters","title":"Why LU Decomposition Matters","text":"<p>LU decomposition transforms the problem of solving \\(A\\mathbf{x} = \\mathbf{b}\\) into two simpler triangular systems:</p> <ol> <li>Solve \\(L\\mathbf{y} = \\mathbf{b}\\) for \\(\\mathbf{y}\\) (forward substitution)</li> <li>Solve \\(U\\mathbf{x} = \\mathbf{y}\\) for \\(\\mathbf{x}\\) (back substitution)</li> </ol> <p>Each triangular solve takes only \\(O(n^2)\\) operations, compared to \\(O(n^3)\\) for the original system. The key advantage is that once we have computed the LU factorization (which costs \\(O(n^3)\\)), we can solve for multiple right-hand sides \\(\\mathbf{b}_1, \\mathbf{b}_2, \\ldots\\) with only \\(O(n^2)\\) additional work each.</p>"},{"location":"chapters/07-matrix-decompositions/#computing-lu-decomposition","title":"Computing LU Decomposition","text":"<p>The algorithm follows Gaussian elimination, but instead of modifying the right-hand side, we store the multipliers:</p> <ol> <li>For each column \\(k = 1, \\ldots, n-1\\):</li> <li> <p>For each row \\(i = k+1, \\ldots, n\\):</p> <ul> <li>Compute multiplier: \\(l_{ik} = a_{ik}/a_{kk}\\)</li> <li>Eliminate: \\(a_{ij} \\leftarrow a_{ij} - l_{ik} \\cdot a_{kj}\\) for \\(j = k, \\ldots, n\\)</li> </ul> </li> <li> <p>The multipliers form \\(L\\), and the reduced matrix becomes \\(U\\)</p> </li> </ol>"},{"location":"chapters/07-matrix-decompositions/#example-lu-decomposition","title":"Example: LU Decomposition","text":"<p>Consider the matrix:</p> <p>\\(A = \\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 4 &amp; 3 &amp; 3 \\\\ 8 &amp; 7 &amp; 9 \\end{bmatrix}\\)</p> <p>Step 1: Eliminate below the first pivot (2):</p> <ul> <li>Multiplier for row 2: \\(l_{21} = 4/2 = 2\\)</li> <li>Multiplier for row 3: \\(l_{31} = 8/2 = 4\\)</li> </ul> <p>After elimination:</p> <p>\\(\\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 3 &amp; 5 \\end{bmatrix}\\)</p> <p>Step 2: Eliminate below the second pivot (1):</p> <ul> <li>Multiplier for row 3: \\(l_{32} = 3/1 = 3\\)</li> </ul> <p>After elimination:</p> <p>\\(U = \\begin{bmatrix} 2 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 2 \\end{bmatrix}\\)</p> <p>The multipliers form \\(L\\):</p> <p>\\(L = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 4 &amp; 3 &amp; 1 \\end{bmatrix}\\)</p> <p>You can verify: \\(LU = A\\).</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-lu-decomposition-step-by-step","title":"Diagram: LU Decomposition Step-by-Step","text":"<p>Run the LU Decomposition Visualizer Fullscreen</p> LU Decomposition Algorithm Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand the LU decomposition algorithm by watching elimination steps and multiplier storage</p> <p>Visual elements: - Left panel: Original matrix A with current state - Center panel: L matrix being built (showing multipliers) - Right panel: U matrix being formed (showing elimination) - Highlight current pivot element - Arrows showing elimination operations - Step counter and description</p> <p>Interactive controls: - Matrix size selector (2x2, 3x3, 4x4) - Matrix entry fields - \"Next Step\" button for manual stepping - \"Auto Run\" button with speed slider - \"Reset\" button - \"Verify LU = A\" button</p> <p>Default parameters: - 3x3 matrix mode - Matrix A = [[2, 1, 1], [4, 3, 3], [8, 7, 9]] - Canvas: responsive</p> <p>Behavior: - Highlight current pivot in yellow - Highlight elements being eliminated in red - Show multiplier calculation - Animate row operations - Display running product L\u00d7U for verification - Warning if zero pivot encountered (needs pivoting)</p> <p>Implementation: p5.js with matrix animation</p>"},{"location":"chapters/07-matrix-decompositions/#partial-pivoting","title":"Partial Pivoting","text":"<p>The basic LU decomposition fails when a pivot element is zero, and becomes numerically unstable when pivots are small. Partial pivoting addresses this by swapping rows to bring the largest element in the column to the pivot position.</p> <p>With partial pivoting, we compute:</p> <p>\\(PA = LU\\)</p> <p>where:</p> <ul> <li>\\(P\\) is a permutation matrix recording the row swaps</li> <li>\\(L\\) is lower triangular with entries \\(|l_{ij}| \\leq 1\\)</li> <li>\\(U\\) is upper triangular</li> </ul> <p>The permutation matrix \\(P\\) is orthogonal (\\(P^{-1} = P^T\\)), so solving \\(A\\mathbf{x} = \\mathbf{b}\\) becomes:</p> <ol> <li>Compute \\(P\\mathbf{b}\\) (apply row permutations)</li> <li>Solve \\(L\\mathbf{y} = P\\mathbf{b}\\)</li> <li>Solve \\(U\\mathbf{x} = \\mathbf{y}\\)</li> </ol> <p>Numerical Stability</p> <p>Always use partial pivoting in practice. Without it, even small rounding errors can be amplified catastrophically. Most numerical libraries (LAPACK, NumPy) use partial pivoting by default.</p>"},{"location":"chapters/07-matrix-decompositions/#computational-cost","title":"Computational Cost","text":"Operation Cost LU factorization \\(\\frac{2}{3}n^3\\) flops Forward substitution \\(n^2\\) flops Back substitution \\(n^2\\) flops Total for one solve \\(\\frac{2}{3}n^3 + 2n^2\\) flops Additional solves \\(2n^2\\) flops each"},{"location":"chapters/07-matrix-decompositions/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<p>For symmetric positive definite matrices, we can use a more efficient and stable decomposition called Cholesky decomposition:</p> <p>\\(A = LL^T\\)</p> <p>where:</p> <ul> <li>\\(A\\) is symmetric (\\(A = A^T\\)) and positive definite</li> <li>\\(L\\) is lower triangular with positive diagonal entries</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#positive-definite-matrix","title":"Positive Definite Matrix","text":"<p>A symmetric matrix \\(A\\) is positive definite if:</p> <p>\\(\\mathbf{x}^T A \\mathbf{x} &gt; 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}\\)</p> <p>Equivalent characterizations:</p> <ul> <li>All eigenvalues of \\(A\\) are positive</li> <li>All leading principal minors are positive</li> <li>\\(A\\) can be written as \\(A = B^TB\\) for some matrix \\(B\\) with full column rank</li> <li>The Cholesky decomposition exists</li> </ul> <p>Positive definite matrices arise frequently in applications:</p> <ul> <li>Covariance matrices in statistics</li> <li>Gram matrices \\(X^TX\\) in machine learning</li> <li>Hessians of convex functions at minima</li> <li>Stiffness matrices in finite element analysis</li> </ul> Property Positive Definite Positive Semi-Definite Eigenvalues All \\(&gt; 0\\) All \\(\\geq 0\\) Quadratic form \\(\\mathbf{x}^TA\\mathbf{x} &gt; 0\\) for \\(\\mathbf{x} \\neq 0\\) \\(\\mathbf{x}^TA\\mathbf{x} \\geq 0\\) for all \\(\\mathbf{x}\\) Invertibility Always invertible May be singular Cholesky Unique \\(LL^T\\) exists \\(LL^T\\) exists but \\(L\\) may have zeros"},{"location":"chapters/07-matrix-decompositions/#diagram-positive-definiteness-visualizer","title":"Diagram: Positive Definiteness Visualizer","text":"<p>Run the Positive Definiteness Visualizer Fullscreen</p> Positive Definiteness Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize the quadratic form \\(\\mathbf{x}^T A \\mathbf{x}\\) and understand how positive definiteness relates to the shape of the surface</p> <p>Visual elements: - Left panel: 2x2 symmetric matrix input - Center panel: 3D surface plot of \\(f(x,y) = [x, y] A [x, y]^T\\) - Eigenvalue display with color coding (positive=green, negative=red) - Level curves (contour plot) below 3D surface - Classification label: \"Positive Definite\", \"Negative Definite\", \"Indefinite\", \"Positive Semi-Definite\"</p> <p>Interactive controls: - Matrix entry fields (symmetric: entering a\u2081\u2082 sets a\u2082\u2081) - Rotation controls for 3D view - Toggle contour lines - Preset examples: positive definite, negative definite, indefinite, semi-definite</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 2]] (positive definite) - Surface plot range: x, y \u2208 [-2, 2] - Canvas: responsive</p> <p>Behavior: - Real-time surface update as matrix changes - Color surface by height (red for negative, green for positive) - Show eigenvalues and eigenvector directions on contour plot - Highlight minimum point for positive definite matrices - Saddle point visualization for indefinite matrices</p> <p>Implementation: p5.js with WEBGL for 3D surface rendering</p>"},{"location":"chapters/07-matrix-decompositions/#computing-cholesky-decomposition","title":"Computing Cholesky Decomposition","text":"<p>The Cholesky algorithm computes \\(L\\) column by column:</p> <p>For \\(j = 1, \\ldots, n\\):</p> <p>\\(l_{jj} = \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} l_{jk}^2}\\)</p> <p>For \\(i = j+1, \\ldots, n\\):</p> <p>\\(l_{ij} = \\frac{1}{l_{jj}}\\left(a_{ij} - \\sum_{k=1}^{j-1} l_{ik}l_{jk}\\right)\\)</p>"},{"location":"chapters/07-matrix-decompositions/#example-cholesky-decomposition","title":"Example: Cholesky Decomposition","text":"<p>Consider the positive definite matrix:</p> <p>\\(A = \\begin{bmatrix} 4 &amp; 2 &amp; 2 \\\\ 2 &amp; 5 &amp; 1 \\\\ 2 &amp; 1 &amp; 6 \\end{bmatrix}\\)</p> <p>Column 1:</p> <ul> <li>\\(l_{11} = \\sqrt{4} = 2\\)</li> <li>\\(l_{21} = 2/2 = 1\\)</li> <li>\\(l_{31} = 2/2 = 1\\)</li> </ul> <p>Column 2:</p> <ul> <li>\\(l_{22} = \\sqrt{5 - 1^2} = \\sqrt{4} = 2\\)</li> <li>\\(l_{32} = (1 - 1 \\cdot 1)/2 = 0\\)</li> </ul> <p>Column 3:</p> <ul> <li>\\(l_{33} = \\sqrt{6 - 1^2 - 0^2} = \\sqrt{5}\\)</li> </ul> <p>Result:</p> <p>\\(L = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; 0 &amp; \\sqrt{5} \\end{bmatrix}\\)</p>"},{"location":"chapters/07-matrix-decompositions/#advantages-of-cholesky","title":"Advantages of Cholesky","text":"<ul> <li>Half the work: Cholesky requires \\(\\frac{1}{3}n^3\\) flops vs. \\(\\frac{2}{3}n^3\\) for LU</li> <li>No pivoting needed: Positive definiteness guarantees numerical stability</li> <li>Natural for applications: Covariance matrices and Gram matrices are positive semi-definite</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#qr-decomposition","title":"QR Decomposition","text":"<p>QR Decomposition factors a matrix into an orthogonal matrix \\(Q\\) and an upper triangular matrix \\(R\\):</p> <p>\\(A = QR\\)</p> <p>where:</p> <ul> <li>\\(Q\\) is orthogonal (\\(Q^TQ = I\\), columns form orthonormal basis)</li> <li>\\(R\\) is upper triangular</li> </ul> <p>For an \\(m \\times n\\) matrix with \\(m \\geq n\\): - Full QR: \\(Q\\) is \\(m \\times m\\), \\(R\\) is \\(m \\times n\\) - Reduced QR: \\(Q\\) is \\(m \\times n\\), \\(R\\) is \\(n \\times n\\)</p>"},{"location":"chapters/07-matrix-decompositions/#why-qr-decomposition-matters","title":"Why QR Decomposition Matters","text":"<p>QR decomposition is the foundation of:</p> <ol> <li>Least squares problems: Solving \\(A\\mathbf{x} \\approx \\mathbf{b}\\) in the overdetermined case</li> <li>QR algorithm: The standard method for computing eigenvalues</li> <li>Orthogonalization: Creating orthonormal bases from arbitrary vectors</li> </ol> <p>For least squares, the normal equations \\(A^TA\\mathbf{x} = A^T\\mathbf{b}\\) can have numerical issues. Using QR, we instead solve:</p> <p>\\(R\\mathbf{x} = Q^T\\mathbf{b}\\)</p> <p>which is numerically stable because \\(Q\\) preserves norms.</p>"},{"location":"chapters/07-matrix-decompositions/#gram-schmidt-qr","title":"Gram-Schmidt QR","text":"<p>The Gram-Schmidt process constructs \\(Q\\) by orthonormalizing the columns of \\(A\\) one at a time:</p> <p>For \\(j = 1, \\ldots, n\\):</p> <ol> <li>Start with column \\(\\mathbf{a}_j\\)</li> <li>Subtract projections onto previous \\(\\mathbf{q}\\) vectors:    \\(\\mathbf{v}_j = \\mathbf{a}_j - \\sum_{i=1}^{j-1} (\\mathbf{q}_i^T \\mathbf{a}_j) \\mathbf{q}_i\\)</li> <li>Normalize: \\(\\mathbf{q}_j = \\mathbf{v}_j / \\|\\mathbf{v}_j\\|\\)</li> </ol> <p>The coefficients form the upper triangular matrix \\(R\\):</p> <ul> <li>\\(r_{ij} = \\mathbf{q}_i^T \\mathbf{a}_j\\) for \\(i &lt; j\\)</li> <li>\\(r_{jj} = \\|\\mathbf{v}_j\\|\\)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#diagram-gram-schmidt-orthogonalization","title":"Diagram: Gram-Schmidt Orthogonalization","text":"<p>Run the Gram-Schmidt Visualizer Fullscreen</p> Gram-Schmidt Process Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how Gram-Schmidt creates orthonormal vectors by visualizing the projection and subtraction steps</p> <p>Visual elements: - 3D coordinate system - Original vectors a\u2081, a\u2082, a\u2083 as colored arrows - Orthonormal vectors q\u2081, q\u2082, q\u2083 as they are computed - Projection vectors showing what is subtracted - Right-angle indicators between orthogonal vectors</p> <p>Interactive controls: - Input matrix A (3 columns, 3 rows for 3D visualization) - \"Step\" button to advance one orthonormalization step - \"Run All\" button for complete animation - \"Reset\" button - Speed slider - Toggle to show/hide projection components</p> <p>Default parameters: - Matrix A with 3 linearly independent columns - Canvas: responsive 3D view</p> <p>Behavior: - Show each projection step clearly - Animate subtraction of projection - Show normalization step - Display current q vector and r values - Highlight orthogonality with right-angle symbols - Warning if vectors are nearly linearly dependent</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p> <p>Classical vs. Modified Gram-Schmidt</p> <p>The classical Gram-Schmidt algorithm described above can lose orthogonality due to rounding errors. The modified Gram-Schmidt algorithm recomputes projections against the updated vectors rather than original vectors, providing better numerical stability.</p>"},{"location":"chapters/07-matrix-decompositions/#householder-qr","title":"Householder QR","text":"<p>Householder QR uses orthogonal reflections (Householder transformations) to zero out elements below the diagonal. Each Householder matrix has the form:</p> <p>\\(H = I - 2\\mathbf{v}\\mathbf{v}^T\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}\\) is a unit vector defining the reflection plane</li> <li>\\(H\\) is orthogonal and symmetric (\\(H = H^T = H^{-1}\\))</li> </ul> <p>The algorithm applies successive Householder reflections:</p> <p>\\(H_n \\cdots H_2 H_1 A = R\\)</p> <p>Therefore:</p> <p>\\(Q = H_1 H_2 \\cdots H_n\\)</p>"},{"location":"chapters/07-matrix-decompositions/#advantages-of-householder-qr","title":"Advantages of Householder QR","text":"<ul> <li>Numerically stable: Each transformation is exactly orthogonal</li> <li>Efficient storage: Only need to store reflection vectors \\(\\mathbf{v}\\)</li> <li>Standard choice: Used by LAPACK and all major numerical libraries</li> </ul> Method Stability Flops Storage Classical Gram-Schmidt Poor \\(2mn^2\\) \\(mn + n^2\\) Modified Gram-Schmidt Good \\(2mn^2\\) \\(mn + n^2\\) Householder Excellent \\(2mn^2 - \\frac{2}{3}n^3\\) \\(mn\\) (compact)"},{"location":"chapters/07-matrix-decompositions/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>The Singular Value Decomposition is perhaps the most important and versatile matrix decomposition. It applies to any \\(m \\times n\\) matrix (not just square matrices):</p> <p>\\(A = U\\Sigma V^T\\)</p> <p>where:</p> <ul> <li>\\(U\\) is an \\(m \\times m\\) orthogonal matrix (left singular vectors)</li> <li>\\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix (singular values)</li> <li>\\(V\\) is an \\(n \\times n\\) orthogonal matrix (right singular vectors)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#understanding-svd-components","title":"Understanding SVD Components","text":""},{"location":"chapters/07-matrix-decompositions/#singular-values","title":"Singular Values","text":"<p>The singular values \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\) are the diagonal entries of \\(\\Sigma\\), where \\(r = \\text{rank}(A)\\). They measure how much the matrix stretches vectors in each principal direction.</p> <p>Key relationships:</p> <ul> <li>\\(\\sigma_i = \\sqrt{\\lambda_i(A^TA)} = \\sqrt{\\lambda_i(AA^T)}\\) (singular values are square roots of eigenvalues)</li> <li>\\(\\|A\\|_2 = \\sigma_1\\) (spectral norm is largest singular value)</li> <li>\\(\\|A\\|_F = \\sqrt{\\sum_i \\sigma_i^2}\\) (Frobenius norm)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#left-singular-vectors","title":"Left Singular Vectors","text":"<p>The columns of \\(U\\) are left singular vectors. They form an orthonormal basis for the column space (first \\(r\\) vectors) and left null space (remaining vectors) of \\(A\\).</p> <p>\\(A\\mathbf{v}_i = \\sigma_i \\mathbf{u}_i\\)</p>"},{"location":"chapters/07-matrix-decompositions/#right-singular-vectors","title":"Right Singular Vectors","text":"<p>The columns of \\(V\\) are right singular vectors. They form an orthonormal basis for the row space (first \\(r\\) vectors) and null space (remaining vectors) of \\(A\\).</p> <p>\\(A^T\\mathbf{u}_i = \\sigma_i \\mathbf{v}_i\\)</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-svd-geometry","title":"Diagram: SVD Geometry","text":"<p>Run the SVD Geometry Visualizer Fullscreen</p> SVD Geometric Interpretation <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize SVD as a sequence of rotation, scaling, and rotation operations on the unit sphere</p> <p>Visual elements: - Four panels showing transformation stages:   1. Original unit circle/sphere   2. After V^T rotation (aligns with principal axes)   3. After \u03a3 scaling (ellipse with \u03c3\u2081, \u03c3\u2082 semi-axes)   4. After U rotation (final orientation) - Matrix display showing A = U\u03a3V^T - Singular values displayed as axis lengths</p> <p>Interactive controls: - 2x2 or 2x3 matrix input for A - \"Animate Transformation\" button showing step-by-step - Slider to interpolate between stages - Toggle to show/hide singular vectors - Toggle 2D (circle\u2192ellipse) vs 3D (sphere\u2192ellipsoid)</p> <p>Default parameters: - Matrix A = [[3, 1], [1, 3]] - Animation duration: 2 seconds per stage - Canvas: responsive</p> <p>Behavior: - Show unit circle transforming to ellipse - Label semi-axes with singular values - Show right singular vectors as directions on original circle - Show left singular vectors as directions on transformed ellipse - Display matrix factorization alongside visualization</p> <p>Implementation: p5.js with smooth animation transitions</p>"},{"location":"chapters/07-matrix-decompositions/#the-fundamental-picture","title":"The Fundamental Picture","text":"<p>The SVD reveals the geometry of any linear transformation:</p> <ol> <li>\\(V^T\\) rotates the input space to align with the principal axes</li> <li>\\(\\Sigma\\) stretches/compresses along each axis by the singular values</li> <li>\\(U\\) rotates the output space to the final orientation</li> </ol> <p>This decomposition exposes the four fundamental subspaces:</p> Subspace Dimension Basis from SVD Column space \\(r\\) First \\(r\\) columns of \\(U\\) Left null space \\(m - r\\) Last \\(m - r\\) columns of \\(U\\) Row space \\(r\\) First \\(r\\) columns of \\(V\\) Null space \\(n - r\\) Last \\(n - r\\) columns of \\(V\\)"},{"location":"chapters/07-matrix-decompositions/#full-svd-vs-compact-svd-vs-truncated-svd","title":"Full SVD vs. Compact SVD vs. Truncated SVD","text":"<p>There are three forms of SVD depending on how we handle zero singular values:</p>"},{"location":"chapters/07-matrix-decompositions/#full-svd","title":"Full SVD","text":"<p>The full SVD includes all \\(m\\) left singular vectors and all \\(n\\) right singular vectors:</p> <p>\\(A = U_{m \\times m} \\Sigma_{m \\times n} V^T_{n \\times n}\\)</p>"},{"location":"chapters/07-matrix-decompositions/#compact-svd","title":"Compact SVD","text":"<p>The compact (reduced) SVD keeps only the \\(r\\) non-zero singular values and their vectors:</p> <p>\\(A = U_{m \\times r} \\Sigma_{r \\times r} V^T_{r \\times n}\\)</p> <p>This is more memory-efficient and is what NumPy's <code>np.linalg.svd(full_matrices=False)</code> returns.</p>"},{"location":"chapters/07-matrix-decompositions/#truncated-svd","title":"Truncated SVD","text":"<p>The truncated SVD keeps only the \\(k\\) largest singular values (where \\(k &lt; r\\)):</p> <p>\\(A_k = U_{m \\times k} \\Sigma_{k \\times k} V^T_{k \\times n}\\)</p> <p>This gives the best rank-\\(k\\) approximation to \\(A\\) (Eckart-Young theorem).</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-svd-forms-comparison","title":"Diagram: SVD Forms Comparison","text":"<p>Run the SVD Forms Comparison Fullscreen</p> SVD Forms Comparison <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare full, compact, and truncated SVD visually and understand when to use each form</p> <p>Layout: Three columns showing matrix dimensions for each SVD form</p> <p>Sections: 1. \"Full SVD\" column    - Matrix dimensions: U(m\u00d7m), \u03a3(m\u00d7n), V^T(n\u00d7n)    - Visual: Full-size matrices with zero padding shown    - Use case: Complete basis for all four fundamental subspaces    - Memory: O(m\u00b2 + mn + n\u00b2)</p> <ol> <li>\"Compact SVD\" column</li> <li>Matrix dimensions: U(m\u00d7r), \u03a3(r\u00d7r), V^T(r\u00d7n)</li> <li>Visual: Trimmed matrices, only rank-r portion</li> <li>Use case: Efficient storage, exact reconstruction</li> <li> <p>Memory: O(mr + r\u00b2 + rn)</p> </li> <li> <p>\"Truncated SVD\" column</p> </li> <li>Matrix dimensions: U(m\u00d7k), \u03a3(k\u00d7k), V^T(k\u00d7n)</li> <li>Visual: Further trimmed to k components</li> <li>Use case: Low-rank approximation, denoising</li> <li>Memory: O(mk + k\u00b2 + kn)</li> <li>Note: \"k &lt; r, approximate reconstruction\"</li> </ol> <p>Interactive elements: - Hover to see example with specific dimensions - Click to see Python code for each form - Toggle to show reconstruction error for truncated form</p> <p>Visual style: - Matrix blocks with dimension labels - Color coding: kept components in blue, discarded in gray - Singular values shown as bar chart below</p> <p>Implementation: HTML/CSS/JavaScript with SVG matrices</p>"},{"location":"chapters/07-matrix-decompositions/#computing-svd","title":"Computing SVD","text":"<p>SVD is typically computed in two stages:</p> <ol> <li>Bidiagonalization: Transform \\(A\\) to bidiagonal form using Householder reflections</li> <li>Diagonalization: Apply the QR algorithm (or divide-and-conquer) to find singular values</li> </ol> <p>For an \\(m \\times n\\) matrix with \\(m \\geq n\\):</p> Operation Flops Full SVD \\(2mn^2 + 11n^3\\) Compact SVD \\(2mn^2 + 11n^3\\) Truncated SVD (randomized) \\(O(mnk + (m+n)k^2)\\) <p>Randomized SVD for Large Matrices</p> <p>For very large matrices where only the top \\(k\\) singular values are needed, randomized algorithms provide significant speedups. Libraries like <code>sklearn.decomposition.TruncatedSVD</code> implement these efficient methods.</p>"},{"location":"chapters/07-matrix-decompositions/#low-rank-approximation","title":"Low-Rank Approximation","text":"<p>One of the most powerful applications of SVD is low-rank approximation. Given a matrix \\(A\\) with rank \\(r\\), the best rank-\\(k\\) approximation (for \\(k &lt; r\\)) is:</p> <p>\\(A_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\)</p>"},{"location":"chapters/07-matrix-decompositions/#the-eckart-young-theorem","title":"The Eckart-Young Theorem","text":"<p>The truncated SVD provides the optimal low-rank approximation:</p> <p>\\(A_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|_F\\)</p> <p>The approximation error is:</p> <p>\\(\\|A - A_k\\|_F = \\sqrt{\\sigma_{k+1}^2 + \\sigma_{k+2}^2 + \\cdots + \\sigma_r^2}\\)</p> <p>\\(\\|A - A_k\\|_2 = \\sigma_{k+1}\\)</p> <p>This theorem justifies using truncated SVD for:</p> <ul> <li>Image compression: Store only top \\(k\\) singular components</li> <li>Noise reduction: Small singular values often correspond to noise</li> <li>Dimensionality reduction: Project data onto top \\(k\\) principal directions</li> <li>Recommender systems: Approximate user-item matrices</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#diagram-image-compression-with-svd","title":"Diagram: Image Compression with SVD","text":"<p>Run the SVD Image Compression MicroSim Fullscreen</p> Image Compression MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how truncated SVD compresses images by observing the quality-storage tradeoff as k varies</p> <p>Visual elements: - Left panel: Original grayscale image (as matrix) - Center panel: Reconstructed image using rank-k approximation - Right panel: Difference image (error visualization) - Bottom: Singular value spectrum (bar chart or line plot) - Statistics display: compression ratio, PSNR, % variance captured</p> <p>Interactive controls: - Image selector (sample images or upload) - Slider for rank k (1 to full rank) - \"Show Singular Values\" toggle - \"Show Error Image\" toggle - \"Compare Side-by-Side\" view option</p> <p>Default parameters: - Sample grayscale image (256\u00d7256) - Initial k = 50 - Canvas: responsive</p> <p>Behavior: - Real-time reconstruction as k changes - Show compression ratio: k(m+n+1) / (m\u00d7n) - Display percentage of Frobenius norm captured - Highlight \"elbow\" in singular value plot - Show time/memory comparison</p> <p>Implementation: p5.js with image processing</p>"},{"location":"chapters/07-matrix-decompositions/#applications-of-low-rank-approximation","title":"Applications of Low-Rank Approximation","text":"<ol> <li>Recommender Systems (Netflix Problem)</li> <li>User-movie rating matrix is approximately low-rank</li> <li>Missing entries predicted from low-rank factors</li> <li> <p>\\(R \\approx UV^T\\) where \\(U\\) = user factors, \\(V\\) = item factors</p> </li> <li> <p>Latent Semantic Analysis (LSA)</p> </li> <li>Term-document matrix decomposed via SVD</li> <li>Captures semantic relationships between words</li> <li> <p>Precursor to modern word embeddings</p> </li> <li> <p>Principal Component Analysis (PCA)</p> </li> <li>Centered data matrix \\(X\\) decomposed as \\(X = U\\Sigma V^T\\)</li> <li>Principal components are columns of \\(V\\)</li> <li>Variance along each PC is \\(\\sigma_i^2/(n-1)\\)</li> </ol>"},{"location":"chapters/07-matrix-decompositions/#numerical-rank-and-condition-number","title":"Numerical Rank and Condition Number","text":"<p>In exact arithmetic, rank is well-defined. In floating-point computation, small singular values may arise from noise rather than true rank deficiency.</p>"},{"location":"chapters/07-matrix-decompositions/#numerical-rank","title":"Numerical Rank","text":"<p>The numerical rank is the number of singular values larger than a threshold \\(\\epsilon\\):</p> <p>\\(\\text{rank}_\\epsilon(A) = |\\{i : \\sigma_i &gt; \\epsilon\\}|\\)</p> <p>Common choices for \\(\\epsilon\\):</p> <ul> <li>\\(\\epsilon = \\max(m,n) \\cdot \\epsilon_{\\text{machine}} \\cdot \\sigma_1\\)</li> <li>\\(\\epsilon = \\sqrt{\\epsilon_{\\text{machine}}} \\cdot \\sigma_1\\)</li> </ul> <p>where \\(\\epsilon_{\\text{machine}} \\approx 2.2 \\times 10^{-16}\\) for double precision.</p>"},{"location":"chapters/07-matrix-decompositions/#condition-number","title":"Condition Number","text":"<p>The condition number measures how sensitive a matrix computation is to perturbations:</p> <p>\\(\\kappa(A) = \\frac{\\sigma_1}{\\sigma_r} = \\|A\\| \\cdot \\|A^{-1}\\|\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_1\\) is the largest singular value</li> <li>\\(\\sigma_r\\) is the smallest non-zero singular value</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#interpretation","title":"Interpretation","text":"Condition Number Interpretation \\(\\kappa \\approx 1\\) Well-conditioned (stable computation) \\(\\kappa \\approx 10^k\\) Lose ~\\(k\\) digits of accuracy \\(\\kappa = \\infty\\) Singular matrix (not invertible) <p>For solving \\(A\\mathbf{x} = \\mathbf{b}\\), the relative error in the solution satisfies:</p> <p>\\(\\frac{\\|\\delta \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leq \\kappa(A) \\cdot \\frac{\\|\\delta \\mathbf{b}\\|}{\\|\\mathbf{b}\\|}\\)</p> <p>A small perturbation \\(\\delta \\mathbf{b}\\) in the right-hand side can be amplified by \\(\\kappa(A)\\) in the solution.</p>"},{"location":"chapters/07-matrix-decompositions/#diagram-condition-number-visualizer","title":"Diagram: Condition Number Visualizer","text":"<p>Run the Condition Number Visualizer Fullscreen</p> Condition Number and Sensitivity Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how condition number affects solution sensitivity by perturbing linear systems</p> <p>Visual elements: - Left panel: 2D visualization of linear system Ax = b   - Two lines representing equations   - Intersection point (solution)   - Perturbation region around b   - Resulting uncertainty region around x - Right panel: Singular value bar chart   - \u03c3\u2081 and \u03c3\u2082 (or more) as bars   - Condition number \u03ba = \u03c3\u2081/\u03c3\u2082 displayed - Matrix A and vectors b, x displayed</p> <p>Interactive controls: - 2x2 matrix input - Slider to control perturbation magnitude \u03b5 - \"Add Random Perturbation\" button - Toggle between well-conditioned and ill-conditioned examples - Show/hide uncertainty ellipse</p> <p>Default parameters: - Well-conditioned example: A = [[2, 0], [0, 2]] (\u03ba = 1) - Ill-conditioned example: A = [[1, 1], [1, 1.0001]] (\u03ba \u2248 10000) - Canvas: responsive</p> <p>Behavior: - Show how nearly parallel lines (ill-conditioned) create large uncertainty - Animate perturbations and show solution movement - Display digits of accuracy lost - Compare \u03ba calculation methods</p> <p>Implementation: p5.js with geometric visualization</p>"},{"location":"chapters/07-matrix-decompositions/#improving-conditioning","title":"Improving Conditioning","text":"<p>Several techniques can improve numerical conditioning:</p> <ul> <li>Preconditioning: Transform \\(A\\mathbf{x} = \\mathbf{b}\\) to \\(M^{-1}A\\mathbf{x} = M^{-1}\\mathbf{b}\\) where \\(M \\approx A\\)</li> <li>Regularization: Replace \\(A^TA\\) with \\(A^TA + \\lambda I\\) (ridge regression)</li> <li>Scaling: Equilibrate row and column norms of \\(A\\)</li> </ul>"},{"location":"chapters/07-matrix-decompositions/#choosing-the-right-decomposition","title":"Choosing the Right Decomposition","text":"<p>The choice of decomposition depends on the matrix structure and application:</p> <pre><code>Is A square?\n\u251c\u2500\u2500 Yes: Is A symmetric positive definite?\n\u2502   \u251c\u2500\u2500 Yes: Use Cholesky (fastest, most stable)\n\u2502   \u2514\u2500\u2500 No: Use LU with partial pivoting\n\u2502\n\u2514\u2500\u2500 No (rectangular): What's the goal?\n    \u251c\u2500\u2500 Least squares: Use QR\n    \u251c\u2500\u2500 Low-rank approximation: Use SVD\n    \u2514\u2500\u2500 Eigenvalue-like analysis: Use SVD\n</code></pre> Application Recommended Decomposition Why Solve \\(A\\mathbf{x} = \\mathbf{b}\\) (multiple \\(\\mathbf{b}\\)) LU Factor once, solve many Solve \\(A\\mathbf{x} = \\mathbf{b}\\) (\\(A\\) SPD) Cholesky Half the work, more stable Least squares QR Numerically stable Eigenvalues (symmetric) QR algorithm on tridiagonal form Standard method Singular values SVD via bidiagonalization Definitive answer Low-rank approximation Truncated SVD Optimal by Eckart-Young Matrix rank SVD Count significant \\(\\sigma_i\\)"},{"location":"chapters/07-matrix-decompositions/#diagram-decomposition-decision-tree","title":"Diagram: Decomposition Decision Tree","text":"<p>Run the Decomposition Selection Guide Fullscreen</p> Matrix Decomposition Selection Guide <p>Type: workflow</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Guide students to select the appropriate matrix decomposition based on matrix properties and application needs</p> <p>Visual style: Interactive decision tree flowchart</p> <p>Steps: 1. Start: \"Given matrix A, what do you need?\"    Hover text: \"First consider your goal and matrix properties\"</p> <ol> <li>Decision: \"Goal?\"</li> <li>\"Solve linear system\" \u2192 Branch A</li> <li>\"Least squares / overdetermined\" \u2192 Branch B</li> <li>\"Low-rank approximation\" \u2192 Branch C</li> <li>\"Eigenvalues\" \u2192 Branch D</li> </ol> <p>3a. Decision: \"Is A symmetric positive definite?\"     - Yes \u2192 \"Use Cholesky LL^T\"       Hover text: \"Fastest, half the flops of LU\"     - No \u2192 \"Use LU with pivoting\"       Hover text: \"Works for any invertible matrix\"</p> <p>3b. Process: \"Use QR Decomposition\"     Hover text: \"More stable than normal equations\"</p> <p>3c. Process: \"Use Truncated SVD\"     Hover text: \"Optimal rank-k approximation by Eckart-Young\"</p> <p>3d. Decision: \"Is A symmetric?\"     - Yes \u2192 \"Eigendecomposition via QR algorithm\"     - No \u2192 \"Consider SVD for singular values\"</p> <p>Color coding: - Blue: Decision nodes - Green: Recommended decompositions - Orange: Computation nodes - Gray: Information nodes</p> <p>Interactive: - Click nodes to see code examples - Hover for complexity information - Links to relevant sections</p> <p>Implementation: D3.js or Mermaid.js with interaction handlers</p>"},{"location":"chapters/07-matrix-decompositions/#practical-implementation","title":"Practical Implementation","text":"<p>Here is how to use these decompositions in Python:</p> <pre><code>import numpy as np\nfrom scipy import linalg\n\n# Sample matrices\nA = np.array([[4, 2, 1], [2, 5, 3], [1, 3, 6]], dtype=float)\nb = np.array([1, 2, 3], dtype=float)\n\n# LU Decomposition (with pivoting)\nP, L, U = linalg.lu(A)\nx_lu = linalg.solve_triangular(U,\n       linalg.solve_triangular(L, P @ b, lower=True))\n\n# Cholesky (if A is positive definite)\nL_chol = linalg.cholesky(A, lower=True)\nx_chol = linalg.cho_solve((L_chol, True), b)\n\n# QR Decomposition\nQ, R = linalg.qr(A)\nx_qr = linalg.solve_triangular(R, Q.T @ b)\n\n# SVD\nU, s, Vh = linalg.svd(A)\n# Solve via pseudoinverse\nx_svd = Vh.T @ np.diag(1/s) @ U.T @ b\n\n# Condition number\ncond = np.linalg.cond(A)\nprint(f\"Condition number: {cond:.2f}\")\n\n# Low-rank approximation\nk = 2  # rank of approximation\nA_k = U[:, :k] @ np.diag(s[:k]) @ Vh[:k, :]\nerror = np.linalg.norm(A - A_k, 'fro')\nprint(f\"Rank-{k} approximation error: {error:.4f}\")\n</code></pre>"},{"location":"chapters/07-matrix-decompositions/#summary_1","title":"Summary","text":"<p>This chapter covered the four essential matrix decompositions:</p> <p>LU Decomposition:</p> <ul> <li>Factors \\(A = LU\\) (with pivoting: \\(PA = LU\\))</li> <li>Used for solving linear systems efficiently</li> <li>Cost: \\(\\frac{2}{3}n^3\\) flops to factor, \\(2n^2\\) per solve</li> </ul> <p>Cholesky Decomposition:</p> <ul> <li>Factors \\(A = LL^T\\) for symmetric positive definite matrices</li> <li>Half the cost of LU, no pivoting needed</li> <li>Positive definite means all eigenvalues positive, \\(\\mathbf{x}^TA\\mathbf{x} &gt; 0\\)</li> </ul> <p>QR Decomposition:</p> <ul> <li>Factors \\(A = QR\\) with orthogonal \\(Q\\)</li> <li>Foundation for least squares and eigenvalue algorithms</li> <li>Gram-Schmidt (intuitive) vs. Householder (stable)</li> </ul> <p>Singular Value Decomposition:</p> <ul> <li>Factors \\(A = U\\Sigma V^T\\) for any matrix</li> <li>Singular values reveal matrix structure and rank</li> <li>Truncated SVD gives optimal low-rank approximation</li> </ul> <p>Key Concepts:</p> <ul> <li>Matrix rank determines decomposition structure</li> <li>Numerical rank accounts for floating-point limitations</li> <li>Condition number \\(\\kappa = \\sigma_1/\\sigma_r\\) measures sensitivity</li> </ul> <p>Practical Guidelines:</p> <ol> <li>Use Cholesky for symmetric positive definite systems</li> <li>Use LU for general square systems (with pivoting!)</li> <li>Use QR for least squares problems</li> <li>Use SVD for low-rank approximation and dimensionality reduction</li> <li>Always check condition number before trusting numerical results</li> </ol> Self-Check: When would you choose SVD over QR for a least squares problem? <p>SVD is preferred when the matrix is rank-deficient or nearly rank-deficient (ill-conditioned). QR can fail or give unstable results when columns are nearly linearly dependent, while SVD explicitly reveals the rank through singular values and handles rank deficiency gracefully via the pseudoinverse. SVD also provides the minimum-norm solution when multiple solutions exist.</p>"},{"location":"chapters/07-matrix-decompositions/quiz/","title":"Quiz: Matrix Decompositions","text":"<p>Test your understanding of SVD, QR, and other matrix decompositions.</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#1-the-singular-value-decomposition-svd-factors-a-matrix-a-as","title":"1. The Singular Value Decomposition (SVD) factors a matrix \\(A\\) as:","text":"<ol> <li>\\(A = U\\Sigma V\\)</li> <li>\\(A = U\\Sigma V^T\\)</li> <li>\\(A = \\Sigma UV\\)</li> <li>\\(A = U + \\Sigma + V\\)</li> </ol> Show Answer <p>The correct answer is B. The SVD decomposes any \\(m \\times n\\) matrix as \\(A = U\\Sigma V^T\\), where \\(U\\) and \\(V\\) are orthogonal matrices and \\(\\Sigma\\) is a diagonal matrix of singular values.</p> <p>Concept Tested: Singular Value Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#2-singular-values-of-a-matrix-are-always","title":"2. Singular values of a matrix are always:","text":"<ol> <li>Complex numbers</li> <li>Negative or zero</li> <li>Non-negative real numbers</li> <li>Equal to the eigenvalues</li> </ol> Show Answer <p>The correct answer is C. Singular values are always non-negative real numbers, typically arranged in decreasing order: \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0\\). They are the square roots of eigenvalues of \\(A^TA\\).</p> <p>Concept Tested: Singular Values</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#3-the-rank-of-a-matrix-equals","title":"3. The rank of a matrix equals:","text":"<ol> <li>The number of rows</li> <li>The number of non-zero singular values</li> <li>The largest singular value</li> <li>The trace</li> </ol> Show Answer <p>The correct answer is B. The rank of a matrix equals the number of non-zero singular values. This provides a robust numerical way to determine rank, especially when using a tolerance for \"effectively zero\" values.</p> <p>Concept Tested: Rank and SVD</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#4-in-the-low-rank-approximation-via-svd-keeping-only-the-top-k-singular-values-minimizes","title":"4. In the low-rank approximation via SVD, keeping only the top \\(k\\) singular values minimizes:","text":"<ol> <li>The Frobenius norm of the error</li> <li>The rank of the matrix</li> <li>The number of computations</li> <li>The trace of the matrix</li> </ol> Show Answer <p>The correct answer is A. The truncated SVD gives the best rank-\\(k\\) approximation in both the Frobenius norm and the spectral norm. This is the Eckart-Young-Mirsky theorem.</p> <p>Concept Tested: Low-Rank Approximation</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#5-qr-decomposition-factors-a-matrix-as","title":"5. QR decomposition factors a matrix as:","text":"<ol> <li>\\(A = QR\\) where \\(Q\\) is orthogonal and \\(R\\) is upper triangular</li> <li>\\(A = QR\\) where \\(Q\\) is diagonal and \\(R\\) is symmetric</li> <li>\\(A = R^TQ\\)</li> <li>\\(A = Q + R\\)</li> </ol> Show Answer <p>The correct answer is A. QR decomposition expresses a matrix as \\(A = QR\\), where \\(Q\\) is an orthogonal matrix (orthonormal columns) and \\(R\\) is upper triangular. It is used in solving least squares and computing eigenvalues.</p> <p>Concept Tested: QR Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#6-the-gram-schmidt-process-produces","title":"6. The Gram-Schmidt process produces:","text":"<ol> <li>A diagonal matrix</li> <li>An orthonormal basis from a set of vectors</li> <li>The inverse of a matrix</li> <li>The determinant</li> </ol> Show Answer <p>The correct answer is B. The Gram-Schmidt process takes a set of linearly independent vectors and produces an orthonormal set spanning the same space. It is the foundation of QR decomposition.</p> <p>Concept Tested: Gram-Schmidt Process</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#7-cholesky-decomposition-applies-to","title":"7. Cholesky decomposition applies to:","text":"<ol> <li>Any square matrix</li> <li>Positive definite symmetric matrices</li> <li>Singular matrices only</li> <li>Rectangular matrices</li> </ol> Show Answer <p>The correct answer is B. Cholesky decomposition factors a positive definite symmetric matrix as \\(A = LL^T\\), where \\(L\\) is lower triangular. It is twice as efficient as LU decomposition for applicable matrices.</p> <p>Concept Tested: Cholesky Decomposition</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#8-the-left-singular-vectors-columns-of-u-in-svd-are-eigenvectors-of","title":"8. The left singular vectors (columns of \\(U\\) in SVD) are eigenvectors of:","text":"<ol> <li>\\(A\\)</li> <li>\\(A^TA\\)</li> <li>\\(AA^T\\)</li> <li>\\(A + A^T\\)</li> </ol> Show Answer <p>The correct answer is C. The left singular vectors (columns of \\(U\\)) are eigenvectors of \\(AA^T\\), while the right singular vectors (columns of \\(V\\)) are eigenvectors of \\(A^TA\\). The squared singular values are the eigenvalues.</p> <p>Concept Tested: SVD and Eigenvalues</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#9-the-pseudoinverse-a-computed-via-svd-satisfies","title":"9. The pseudoinverse \\(A^+\\) computed via SVD satisfies:","text":"<ol> <li>\\(AA^+ = I\\) always</li> <li>\\(AA^+A = A\\)</li> <li>\\(A^+ = A^T\\)</li> <li>\\(A^+ = A^{-1}\\) always</li> </ol> Show Answer <p>The correct answer is B. The Moore-Penrose pseudoinverse satisfies \\(AA^+A = A\\) (among other conditions). It generalizes the inverse to non-square and singular matrices, providing least-squares solutions.</p> <p>Concept Tested: Pseudoinverse</p>"},{"location":"chapters/07-matrix-decompositions/quiz/#10-which-decomposition-is-most-useful-for-image-compression","title":"10. Which decomposition is most useful for image compression?","text":"<ol> <li>LU decomposition</li> <li>QR decomposition</li> <li>SVD with truncation</li> <li>Cholesky decomposition</li> </ol> Show Answer <p>The correct answer is C. Truncated SVD is ideal for image compression because it provides the optimal low-rank approximation. Keeping only the largest singular values captures the most important image features while reducing storage.</p> <p>Concept Tested: SVD Applications</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/","title":"Vector Spaces and Inner Products","text":""},{"location":"chapters/08-vector-spaces-and-inner-products/#summary","title":"Summary","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications. You will learn about abstract vector spaces, inner products, orthogonality, the Gram-Schmidt orthogonalization process, and projections. This chapter also covers the four fundamental subspaces of a matrix and the pseudoinverse, which are essential for least squares and machine learning.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"chapters/08-vector-spaces-and-inner-products/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#introduction","title":"Introduction","text":"<p>So far, we have worked with vectors as arrows in \\(\\mathbb{R}^n\\)\u2014ordered lists of real numbers that we can add and scale. But the power of linear algebra extends far beyond number arrays. Functions, matrices, polynomials, and even quantum states can all be treated as \"vectors\" in appropriately defined spaces.</p> <p>This chapter develops the abstract framework that unifies these diverse applications. By identifying the essential properties that make \\(\\mathbb{R}^n\\) useful\u2014the ability to add vectors, scale them, measure lengths, and find angles\u2014we can extend linear algebra to any mathematical structure satisfying these properties.</p> <p>The payoff is enormous. The same techniques that solve systems of equations in \\(\\mathbb{R}^n\\) can approximate functions with polynomials, denoise signals, and find optimal solutions in infinite-dimensional spaces. The abstract perspective reveals that linear algebra is not just about numbers\u2014it's about structure.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#abstract-vector-spaces","title":"Abstract Vector Spaces","text":"<p>An abstract vector space is a set \\(V\\) equipped with two operations\u2014vector addition and scalar multiplication\u2014that satisfy certain axioms. The elements of \\(V\\) are called vectors, though they need not be column vectors in the traditional sense.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#vector-space-axioms","title":"Vector Space Axioms","text":"<p>A vector space over the real numbers must satisfy the following vector space axioms:</p> <p>Addition axioms:</p> <ol> <li>Closure under addition: For all \\(\\mathbf{u}, \\mathbf{v} \\in V\\), we have \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</li> <li>Commutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associativity: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Zero vector: There exists \\(\\mathbf{0} \\in V\\) such that \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\) for all \\(\\mathbf{v}\\)</li> <li>Additive inverse: For each \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ol> <p>Scalar multiplication axioms:</p> <ol> <li>Closure under scalar multiplication: For all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{v} \\in V\\), we have \\(c\\mathbf{v} \\in V\\)</li> <li>Distributivity over vector addition: \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> <li>Distributivity over scalar addition: \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</li> <li>Associativity of scalar multiplication: \\(c(d\\mathbf{v}) = (cd)\\mathbf{v}\\)</li> <li>Identity element: \\(1 \\cdot \\mathbf{v} = \\mathbf{v}\\)</li> </ol> <p>These axioms capture the essential algebraic properties needed for linear algebra to work.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#examples-of-vector-spaces","title":"Examples of Vector Spaces","text":"Vector Space Elements Addition Scalar Multiplication \\(\\mathbb{R}^n\\) Column vectors Component-wise Component-wise \\(\\mathcal{P}_n\\) Polynomials of degree \\(\\leq n\\) Add coefficients Multiply coefficients \\(\\mathcal{C}[a,b]\\) Continuous functions on \\([a,b]\\) \\((f+g)(x) = f(x)+g(x)\\) \\((cf)(x) = c \\cdot f(x)\\) \\(\\mathbb{R}^{m \\times n}\\) \\(m \\times n\\) matrices Entry-wise Entry-wise Solutions to \\(A\\mathbf{x} = \\mathbf{0}\\) Null space vectors Inherited from \\(\\mathbb{R}^n\\) Inherited from \\(\\mathbb{R}^n\\) <p>The Zero Vector</p> <p>Every vector space must contain a zero vector \\(\\mathbf{0}\\). In \\(\\mathbb{R}^n\\), it's the origin. In function spaces, it's the function \\(f(x) = 0\\). In matrix spaces, it's the zero matrix.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-vector-space-examples-gallery","title":"Diagram: Vector Space Examples Gallery","text":"<p>Run the Vector Space Gallery Fullscreen</p> Vector Space Examples Gallery <p>Type: infographic</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Recognize diverse examples of vector spaces and identify the zero vector and operations in each</p> <p>Layout: Grid of 6 cards, each representing a different vector space</p> <p>Cards: 1. \"\\(\\mathbb{R}^2\\): The Plane\"    - Visual: 2D coordinate system with vectors    - Zero: Origin (0, 0)    - Example: \\(\\mathbf{v} = (3, 4)\\)</p> <ol> <li>\"\\(\\mathbb{R}^3\\): 3D Space\"</li> <li>Visual: 3D coordinate system</li> <li>Zero: Origin (0, 0, 0)</li> <li> <p>Example: \\(\\mathbf{v} = (1, 2, 3)\\)</p> </li> <li> <p>\"\\(\\mathcal{P}_2\\): Quadratic Polynomials\"</p> </li> <li>Visual: Parabola graphs</li> <li>Zero: \\(p(x) = 0\\)</li> <li> <p>Example: \\(p(x) = 2x^2 - 3x + 1\\)</p> </li> <li> <p>\"Continuous Functions\"</p> </li> <li>Visual: Function curve plot</li> <li>Zero: \\(f(x) = 0\\) (horizontal axis)</li> <li> <p>Example: \\(f(x) = \\sin(x)\\)</p> </li> <li> <p>\"\\(\\mathbb{R}^{2 \\times 2}\\): 2\u00d72 Matrices\"</p> </li> <li>Visual: Matrix grid representation</li> <li>Zero: Zero matrix</li> <li> <p>Example: \\(A = [[1,2],[3,4]]\\)</p> </li> <li> <p>\"Null Space of A\"</p> </li> <li>Visual: Plane through origin in 3D</li> <li>Zero: Origin</li> <li>Example: All \\(\\mathbf{x}\\) where \\(A\\mathbf{x} = \\mathbf{0}\\)</li> </ol> <p>Interactive elements: - Hover to see addition and scalar multiplication examples - Click to verify axioms interactively</p> <p>Visual style: - Consistent card format with icons - Color-coded by dimension (finite vs. infinite)</p> <p>Implementation: HTML/CSS grid with SVG visualizations</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#subspaces","title":"Subspaces","text":"<p>A subspace of a vector space \\(V\\) is a non-empty subset \\(W \\subseteq V\\) that is itself a vector space under the same operations. Rather than checking all ten axioms, we can use a simpler test.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#subspace-test","title":"Subspace Test","text":"<p>A non-empty subset \\(W\\) of \\(V\\) is a subspace if and only if:</p> <ol> <li>Closed under addition: For all \\(\\mathbf{u}, \\mathbf{w} \\in W\\), we have \\(\\mathbf{u} + \\mathbf{w} \\in W\\)</li> <li>Closed under scalar multiplication: For all \\(c \\in \\mathbb{R}\\) and \\(\\mathbf{w} \\in W\\), we have \\(c\\mathbf{w} \\in W\\)</li> </ol> <p>Equivalently (single condition): For all \\(\\mathbf{u}, \\mathbf{w} \\in W\\) and scalars \\(c, d\\): \\(c\\mathbf{u} + d\\mathbf{w} \\in W\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#important-subspaces","title":"Important Subspaces","text":"<p>Every matrix \\(A\\) has associated subspaces:</p> <ul> <li>Column space \\(\\text{col}(A)\\): All linear combinations of columns of \\(A\\)</li> <li>Null space \\(\\text{null}(A)\\): All solutions to \\(A\\mathbf{x} = \\mathbf{0}\\)</li> <li>Row space \\(\\text{row}(A)\\): All linear combinations of rows of \\(A\\)</li> <li>Left null space \\(\\text{null}(A^T)\\): All solutions to \\(A^T\\mathbf{y} = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#non-examples","title":"Non-Examples","text":"<p>Not every subset is a subspace:</p> <ul> <li>The unit circle in \\(\\mathbb{R}^2\\) is not a subspace (not closed under addition)</li> <li>The first quadrant in \\(\\mathbb{R}^2\\) is not a subspace (not closed under scalar multiplication by negatives)</li> <li>A line not through the origin is not a subspace (no zero vector)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-subspace-tester","title":"Diagram: Subspace Tester","text":"<p>Run the Subspace Tester Fullscreen</p> Subspace Tester MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Test whether sets are subspaces by checking closure under linear combinations</p> <p>Visual elements: - 2D coordinate plane - Set definition displayed (equation or description) - Test vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) as draggable arrows - Linear combination \\(c\\mathbf{u} + d\\mathbf{v}\\) shown - Set boundary highlighted</p> <p>Interactive controls: - Preset sets dropdown: \"Line through origin\", \"Line not through origin\", \"First quadrant\", \"Circle\", \"Plane in 3D\" - Sliders for scalars c and d - Draggable points for u and v (constrained to set) - \"Check if Subspace\" button with explanation</p> <p>Default parameters: - Set: Line through origin (y = 2x) - Scalars c = 1, d = 1 - Canvas: responsive</p> <p>Behavior: - Highlight when linear combination leaves the set (subspace test fails) - Green indicator when combination stays in set - Explain which property fails for non-subspaces - Show counter-example automatically</p> <p>Implementation: p5.js with interactive geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#inner-products-and-inner-product-spaces","title":"Inner Products and Inner Product Spaces","text":"<p>An inner product generalizes the dot product to abstract vector spaces, enabling us to measure lengths and angles.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#inner-product-definition","title":"Inner Product Definition","text":"<p>An inner product on a vector space \\(V\\) is a function \\(\\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{R}\\) satisfying:</p> <ol> <li>Symmetry: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{v}, \\mathbf{u} \\rangle\\)</li> <li>Linearity in first argument: \\(\\langle c\\mathbf{u} + d\\mathbf{w}, \\mathbf{v} \\rangle = c\\langle \\mathbf{u}, \\mathbf{v} \\rangle + d\\langle \\mathbf{w}, \\mathbf{v} \\rangle\\)</li> <li>Positive definiteness: \\(\\langle \\mathbf{v}, \\mathbf{v} \\rangle \\geq 0\\), with equality iff \\(\\mathbf{v} = \\mathbf{0}\\)</li> </ol> <p>A vector space equipped with an inner product is called an inner product space.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#standard-inner-products","title":"Standard Inner Products","text":"Space Inner Product Formula \\(\\mathbb{R}^n\\) Dot product \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u}^T\\mathbf{v} = \\sum_{i=1}^n u_i v_i\\) \\(\\mathcal{C}[a,b]\\) Integral \\(\\langle f, g \\rangle = \\int_a^b f(x)g(x)\\,dx\\) \\(\\mathbb{R}^{m \\times n}\\) Frobenius \\(\\langle A, B \\rangle = \\text{tr}(A^TB) = \\sum_{i,j} a_{ij}b_{ij}\\) Weighted \\(\\mathbb{R}^n\\) Weighted dot \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle_W = \\mathbf{u}^TW\\mathbf{v}\\) (W positive definite)"},{"location":"chapters/08-vector-spaces-and-inner-products/#norm-from-inner-product","title":"Norm from Inner Product","text":"<p>Every inner product induces a norm (length function):</p> <p>\\(\\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\\)</p> <p>For the standard dot product on \\(\\mathbb{R}^n\\), this gives the Euclidean norm:</p> <p>\\(\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\\)</p> <p>The norm satisfies:</p> <ul> <li>Positivity: \\(\\|\\mathbf{v}\\| \\geq 0\\), with equality iff \\(\\mathbf{v} = \\mathbf{0}\\)</li> <li>Homogeneity: \\(\\|c\\mathbf{v}\\| = |c| \\cdot \\|\\mathbf{v}\\|\\)</li> <li>Triangle inequality: \\(\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-inner-product-visualizer","title":"Diagram: Inner Product Visualizer","text":"<p>Run the Inner Product Visualizer Fullscreen</p> Inner Product Space Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how different inner products define different notions of length and angle</p> <p>Visual elements: - 2D plane with two adjustable vectors u and v - Unit circle for standard inner product - Transformed unit \"circle\" (ellipse) for weighted inner product - Angle arc between vectors - Length labels for each vector</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Inner product selector: \"Standard dot product\", \"Weighted (diagonal W)\", \"Weighted (general W)\" - Weight matrix input (for weighted inner products) - Display: inner product value, norms, angle</p> <p>Default parameters: - Standard dot product - u = (3, 1), v = (1, 2) - Canvas: responsive</p> <p>Behavior: - Real-time update of inner product, norms, angle - Show how unit ball changes with different inner products - Demonstrate that angle definition depends on inner product - Verify Cauchy-Schwarz inequality visually</p> <p>Implementation: p5.js with dynamic geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-cauchy-schwarz-inequality","title":"The Cauchy-Schwarz Inequality","text":"<p>The Cauchy-Schwarz inequality is one of the most important results in linear algebra:</p> <p>\\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|\\)</p> <p>where:</p> <ul> <li>Equality holds if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are linearly dependent</li> <li>This inequality holds in every inner product space</li> </ul> <p>For the standard dot product in \\(\\mathbb{R}^n\\):</p> <p>\\(|\\mathbf{u} \\cdot \\mathbf{v}| \\leq \\|\\mathbf{u}\\|_2 \\cdot \\|\\mathbf{v}\\|_2\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#angle-between-vectors","title":"Angle Between Vectors","text":"<p>The Cauchy-Schwarz inequality guarantees that:</p> <p>\\(-1 \\leq \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|} \\leq 1\\)</p> <p>This allows us to define the angle \\(\\theta\\) between vectors:</p> <p>\\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\cdot \\|\\mathbf{v}\\|}\\)</p> <p>Cauchy-Schwarz in Applications</p> <p>Cauchy-Schwarz appears throughout machine learning:</p> <ul> <li>Cosine similarity: \\(\\cos\\theta = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\) measures document/word similarity</li> <li>Correlation coefficient: Normalized covariance uses Cauchy-Schwarz to bound \\(|\\rho| \\leq 1\\)</li> <li>Attention mechanisms: Softmax of dot products for similarity scoring</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthogonality","title":"Orthogonality","text":"<p>Orthogonality is the generalization of perpendicularity to abstract vector spaces.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthogonal-vectors","title":"Orthogonal Vectors","text":"<p>Two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal (written \\(\\mathbf{u} \\perp \\mathbf{v}\\)) if:</p> <p>\\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\)</p> <p>In \\(\\mathbb{R}^n\\) with the standard dot product, this means \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\).</p> <p>Key properties:</p> <ul> <li>The zero vector is orthogonal to every vector</li> <li>Orthogonal non-zero vectors are linearly independent</li> <li>The Pythagorean theorem generalizes: if \\(\\mathbf{u} \\perp \\mathbf{v}\\), then \\(\\|\\mathbf{u} + \\mathbf{v}\\|^2 = \\|\\mathbf{u}\\|^2 + \\|\\mathbf{v}\\|^2\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#orthonormal-sets-and-bases","title":"Orthonormal Sets and Bases","text":"<p>An orthonormal set is a set of vectors that are:</p> <ol> <li>Pairwise orthogonal: \\(\\langle \\mathbf{u}_i, \\mathbf{u}_j \\rangle = 0\\) for \\(i \\neq j\\)</li> <li>Unit length: \\(\\|\\mathbf{u}_i\\| = 1\\) for all \\(i\\)</li> </ol> <p>An orthonormal basis is an orthonormal set that spans the entire space.</p> Property Orthogonal Set Orthonormal Set Orthonormal Basis Pairwise orthogonal \u2713 \u2713 \u2713 Unit vectors \u2717 \u2713 \u2713 Spans space \u2717 \u2717 \u2713 Linearly independent \u2713 (if non-zero) \u2713 \u2713"},{"location":"chapters/08-vector-spaces-and-inner-products/#why-orthonormal-bases-matter","title":"Why Orthonormal Bases Matter","text":"<p>Orthonormal bases dramatically simplify computations:</p> <p>Coordinate computation: If \\(\\{\\mathbf{q}_1, \\ldots, \\mathbf{q}_n\\}\\) is orthonormal:</p> <p>\\(\\mathbf{v} = \\sum_{i=1}^n \\langle \\mathbf{v}, \\mathbf{q}_i \\rangle \\mathbf{q}_i\\)</p> <p>The coefficients are just inner products\u2014no matrix inversion needed!</p> <p>Parseval's identity:</p> <p>\\(\\|\\mathbf{v}\\|^2 = \\sum_{i=1}^n |\\langle \\mathbf{v}, \\mathbf{q}_i \\rangle|^2\\)</p> <p>Orthogonal matrices: A matrix \\(Q\\) is orthogonal if its columns form an orthonormal basis:</p> <p>\\(Q^TQ = I \\quad \\Rightarrow \\quad Q^{-1} = Q^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-orthonormal-basis-coordinate-finder","title":"Diagram: Orthonormal Basis Coordinate Finder","text":"<p>Run the Orthonormal Basis Finder Fullscreen</p> Orthonormal Basis Coordinate Finder <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Demonstrate how orthonormal bases simplify finding coordinates via inner products</p> <p>Visual elements: - 2D or 3D coordinate system - Standard basis vectors (gray, dashed) - Orthonormal basis vectors q\u2081, q\u2082 (colored arrows) - Target vector v (black arrow) - Projection lines from v to each q\u1d62 - Coordinate display in both bases</p> <p>Interactive controls: - Draggable orthonormal basis vectors (constrained to stay orthonormal) - Draggable target vector v - Toggle between 2D and 3D - \"Show Projections\" toggle - \"Compare to Standard Basis\" toggle</p> <p>Default parameters: - 2D mode - Orthonormal basis at 45\u00b0 rotation - v = (3, 2) - Canvas: responsive</p> <p>Behavior: - Show coefficients as inner products: c\u1d62 = \u27e8v, q\u1d62\u27e9 - Demonstrate reconstruction: v = c\u2081q\u2081 + c\u2082q\u2082 - Verify Parseval's identity visually - Compare computation effort with non-orthonormal basis</p> <p>Implementation: p5.js with vector geometry</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-gram-schmidt-process","title":"The Gram-Schmidt Process","text":"<p>The Gram-Schmidt process converts any linearly independent set of vectors into an orthonormal set spanning the same subspace.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#algorithm","title":"Algorithm","text":"<p>Given linearly independent vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\):</p> <p>Step 1: First vector</p> <p>\\(\\mathbf{u}_1 = \\mathbf{v}_1, \\quad \\mathbf{q}_1 = \\frac{\\mathbf{u}_1}{\\|\\mathbf{u}_1\\|}\\)</p> <p>Step 2: Subtract projection, normalize</p> <p>For \\(k = 2, \\ldots, n\\):</p> <p>\\(\\mathbf{u}_k = \\mathbf{v}_k - \\sum_{j=1}^{k-1} \\langle \\mathbf{v}_k, \\mathbf{q}_j \\rangle \\mathbf{q}_j\\)</p> <p>\\(\\mathbf{q}_k = \\frac{\\mathbf{u}_k}{\\|\\mathbf{u}_k\\|}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>Each step:</p> <ol> <li>Takes the next input vector \\(\\mathbf{v}_k\\)</li> <li>Subtracts its projections onto all previously computed \\(\\mathbf{q}_j\\)</li> <li>Normalizes the result to unit length</li> </ol> <p>The projection \\(\\langle \\mathbf{v}_k, \\mathbf{q}_j \\rangle \\mathbf{q}_j\\) removes the component of \\(\\mathbf{v}_k\\) in the direction of \\(\\mathbf{q}_j\\), leaving only the component orthogonal to all previous vectors.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-gram-schmidt-process-visualizer","title":"Diagram: Gram-Schmidt Process Visualizer","text":"<p>Run the Gram-Schmidt Visualizer Fullscreen</p> Gram-Schmidt Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand the Gram-Schmidt process by watching projection and orthogonalization steps</p> <p>Visual elements: - 3D coordinate system - Input vectors v\u2081, v\u2082, v\u2083 (original, semi-transparent after processing) - Current vector being processed (highlighted) - Projection vectors being subtracted (dashed arrows) - Output orthonormal vectors q\u2081, q\u2082, q\u2083 (solid, colored) - Right-angle indicators</p> <p>Interactive controls: - Input matrix (3 column vectors) - \"Next Step\" button - \"Auto Run\" with speed slider - \"Reset\" button - \"Show All Projections\" toggle - \"Show Residual\" toggle</p> <p>Default parameters: - Three linearly independent vectors in 3D - Step-by-step mode - Canvas: responsive 3D view</p> <p>Behavior: - Animate each projection subtraction - Show normalization as length scaling - Highlight orthogonality between output vectors - Display intermediate u vectors before normalization - Warning if vectors become nearly dependent</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#connection-to-qr-decomposition","title":"Connection to QR Decomposition","text":"<p>Gram-Schmidt applied to the columns of matrix \\(A\\) produces:</p> <p>\\(A = QR\\)</p> <p>where:</p> <ul> <li>\\(Q\\) contains the orthonormal vectors \\(\\mathbf{q}_1, \\ldots, \\mathbf{q}_n\\)</li> <li>\\(R\\) is upper triangular with \\(r_{ij} = \\langle \\mathbf{v}_j, \\mathbf{q}_i \\rangle\\) for \\(i &lt; j\\) and \\(r_{ii} = \\|\\mathbf{u}_i\\|\\)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-subspaces","title":"Projection onto Subspaces","text":"<p>Projection finds the closest point in a subspace to a given vector\u2014the foundation of least squares.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-a-line","title":"Projection onto a Line","text":"<p>The projection of \\(\\mathbf{v}\\) onto the line spanned by \\(\\mathbf{u}\\) is:</p> <p>\\(\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u} = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\mathbf{u}^T\\mathbf{u}} \\mathbf{u}\\)</p> <p>The projection matrix onto the line is:</p> <p>\\(P = \\frac{\\mathbf{u}\\mathbf{u}^T}{\\mathbf{u}^T\\mathbf{u}}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#projection-onto-a-subspace","title":"Projection onto a Subspace","text":"<p>For a subspace \\(W\\) with orthonormal basis \\(\\{\\mathbf{q}_1, \\ldots, \\mathbf{q}_k\\}\\):</p> <p>\\(\\text{proj}_W(\\mathbf{v}) = \\sum_{i=1}^k \\langle \\mathbf{v}, \\mathbf{q}_i \\rangle \\mathbf{q}_i\\)</p> <p>The projection matrix is:</p> <p>\\(P = QQ^T\\)</p> <p>where \\(Q = [\\mathbf{q}_1 | \\cdots | \\mathbf{q}_k]\\).</p> <p>For a general subspace with basis columns of \\(A\\):</p> <p>\\(P = A(A^TA)^{-1}A^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#properties-of-projection-matrices","title":"Properties of Projection Matrices","text":"<p>Projection matrices satisfy:</p> <ul> <li>Symmetric: \\(P = P^T\\)</li> <li>Idempotent: \\(P^2 = P\\) (projecting twice gives the same result)</li> <li>Eigenvalues: Only 0 and 1 (0 for orthogonal complement, 1 for subspace)</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-projection-visualizer","title":"Diagram: Projection Visualizer","text":"<p>Run the Projection Visualizer Fullscreen</p> Projection onto Subspace Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize projection as finding the closest point in a subspace and understand the orthogonal error</p> <p>Visual elements: - 3D coordinate system - Subspace W shown as a plane through origin (or line) - Vector v (starting point) - Projection p = proj_W(v) (on subspace) - Error vector e = v - p (perpendicular to subspace) - Right angle indicator between e and subspace</p> <p>Interactive controls: - Draggable vector v - Subspace definition: basis vectors or normal vector - Toggle between 1D subspace (line) and 2D subspace (plane) - \"Show Projection Matrix\" toggle - \"Show Error Vector\" toggle</p> <p>Default parameters: - 2D subspace (plane) in 3D - v outside the subspace - Canvas: responsive 3D view</p> <p>Behavior: - Real-time projection update as v moves - Show that e is orthogonal to subspace - Display projection formula and computation - Highlight that p is closest point in subspace to v - Show distance ||e|| as length of error</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-least-squares-problem","title":"The Least Squares Problem","text":"<p>When a system \\(A\\mathbf{x} = \\mathbf{b}\\) has no exact solution (overdetermined), we seek the least squares solution\u2014the \\(\\mathbf{x}\\) that minimizes the error \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#geometric-view","title":"Geometric View","text":"<p>The least squares problem asks: find the point \\(A\\mathbf{x}\\) in the column space of \\(A\\) closest to \\(\\mathbf{b}\\).</p> <p>The answer is the projection of \\(\\mathbf{b}\\) onto the column space:</p> <p>\\(A\\hat{\\mathbf{x}} = \\text{proj}_{\\text{col}(A)}(\\mathbf{b})\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-normal-equations","title":"The Normal Equations","text":"<p>The least squares solution satisfies the normal equations:</p> <p>\\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(A^TA\\) is an \\(n \\times n\\) symmetric positive semi-definite matrix</li> <li>\\(A^T\\mathbf{b}\\) is an \\(n \\times 1\\) vector</li> <li>If \\(A\\) has full column rank, \\(A^TA\\) is positive definite and invertible</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#derivation","title":"Derivation","text":"<p>The error vector \\(\\mathbf{e} = \\mathbf{b} - A\\hat{\\mathbf{x}}\\) must be orthogonal to the column space of \\(A\\):</p> <p>\\(A^T\\mathbf{e} = \\mathbf{0}\\)</p> <p>\\(A^T(\\mathbf{b} - A\\hat{\\mathbf{x}}) = \\mathbf{0}\\)</p> <p>\\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#solving-least-squares","title":"Solving Least Squares","text":"Method Formula When to Use Normal equations \\(\\hat{\\mathbf{x}} = (A^TA)^{-1}A^T\\mathbf{b}\\) Small, well-conditioned problems QR decomposition \\(R\\hat{\\mathbf{x}} = Q^T\\mathbf{b}\\) General, numerically stable SVD \\(\\hat{\\mathbf{x}} = V\\Sigma^{-1}U^T\\mathbf{b}\\) Rank-deficient or ill-conditioned <p>Numerical Stability</p> <p>Avoid explicitly forming \\(A^TA\\) when possible. It squares the condition number, amplifying numerical errors. Use QR or SVD instead.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-least-squares-visualizer","title":"Diagram: Least Squares Visualizer","text":"<p>Run the Least Squares Visualizer Fullscreen</p> Least Squares Problem Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand least squares as projection and visualize the geometric relationship between b, Ax\u0302, and the error</p> <p>Visual elements: - 3D space showing column space of A as a plane - Vector b (outside the plane) - Projection Ax\u0302 (on the plane) - Error vector e = b - Ax\u0302 (perpendicular to plane) - Data points and fitted line (for 2D regression example)</p> <p>Interactive controls: - Switch between \"Geometric View\" and \"Regression View\" - In geometric view: adjust b position - In regression view: drag data points - \"Show Normal Equations\" toggle - \"Compare Methods\" (normal eq vs QR vs SVD)</p> <p>Default parameters: - Simple 2D linear regression example - 5 data points - Canvas: responsive</p> <p>Behavior: - Real-time update of least squares solution - Show residuals as vertical lines to fitted line - Display sum of squared residuals - Highlight that solution minimizes total squared error - Show condition number warning if ill-conditioned</p> <p>Implementation: p5.js with dual view modes</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#linear-regression-as-least-squares","title":"Linear Regression as Least Squares","text":"<p>Fitting a line \\(y = mx + c\\) to data points \\((x_1, y_1), \\ldots, (x_n, y_n)\\):</p> <p>\\(\\begin{bmatrix} x_1 &amp; 1 \\\\ x_2 &amp; 1 \\\\ \\vdots &amp; \\vdots \\\\ x_n &amp; 1 \\end{bmatrix} \\begin{bmatrix} m \\\\ c \\end{bmatrix} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\\)</p> <p>This is \\(A\\mathbf{x} = \\mathbf{b}\\) with \\(n &gt; 2\\) equations and 2 unknowns\u2014overdetermined!</p> <p>The least squares solution minimizes \\(\\sum_{i=1}^n (y_i - mx_i - c)^2\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-four-fundamental-subspaces","title":"The Four Fundamental Subspaces","text":"<p>Every \\(m \\times n\\) matrix \\(A\\) defines four fundamental subspaces that partition \\(\\mathbb{R}^n\\) and \\(\\mathbb{R}^m\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#column-space-and-row-space","title":"Column Space and Row Space","text":"<p>The column space \\(\\text{col}(A)\\) is the span of the columns of \\(A\\):</p> <p>\\(\\text{col}(A) = \\{A\\mathbf{x} : \\mathbf{x} \\in \\mathbb{R}^n\\} \\subseteq \\mathbb{R}^m\\)</p> <p>The row space \\(\\text{row}(A)\\) is the span of the rows of \\(A\\) (equivalently, column space of \\(A^T\\)):</p> <p>\\(\\text{row}(A) = \\text{col}(A^T) \\subseteq \\mathbb{R}^n\\)</p> <p>Both have dimension equal to the rank of \\(A\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#null-space-and-left-null-space","title":"Null Space and Left Null Space","text":"<p>The null space (kernel) \\(\\text{null}(A)\\) contains all solutions to \\(A\\mathbf{x} = \\mathbf{0}\\):</p> <p>\\(\\text{null}(A) = \\{\\mathbf{x} \\in \\mathbb{R}^n : A\\mathbf{x} = \\mathbf{0}\\}\\)</p> <p>The left null space \\(\\text{null}(A^T)\\) contains all solutions to \\(A^T\\mathbf{y} = \\mathbf{0}\\):</p> <p>\\(\\text{null}(A^T) = \\{\\mathbf{y} \\in \\mathbb{R}^m : A^T\\mathbf{y} = \\mathbf{0}\\}\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-fundamental-theorem","title":"The Fundamental Theorem","text":"<p>The Four Subspaces Theorem reveals beautiful orthogonal relationships:</p> Subspace Dimension Orthogonal Complement Column space \\(\\text{col}(A)\\) \\(r\\) Left null space \\(\\text{null}(A^T)\\) Row space \\(\\text{row}(A)\\) \\(r\\) Null space \\(\\text{null}(A)\\) Null space \\(\\text{null}(A)\\) \\(n - r\\) Row space \\(\\text{row}(A)\\) Left null space \\(\\text{null}(A^T)\\) \\(m - r\\) Column space \\(\\text{col}(A)\\) <p>where \\(r = \\text{rank}(A)\\).</p> <p>Key insights:</p> <ul> <li>\\(\\mathbb{R}^n = \\text{row}(A) \\oplus \\text{null}(A)\\) (direct sum)</li> <li>\\(\\mathbb{R}^m = \\text{col}(A) \\oplus \\text{null}(A^T)\\) (direct sum)</li> <li>The matrix \\(A\\) maps row space to column space (bijectively if full rank)</li> <li>The matrix \\(A\\) maps null space to zero</li> </ul>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-four-fundamental-subspaces","title":"Diagram: Four Fundamental Subspaces","text":"<p>Run the Four Subspaces Visualizer Fullscreen</p> Four Fundamental Subspaces Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Visualize the four fundamental subspaces and their orthogonal relationships</p> <p>Visual elements: - Two side-by-side panels: Domain (R^n) and Codomain (R^m) - In R^n: Row space and null space as orthogonal subspaces - In R^m: Column space and left null space as orthogonal subspaces - Arrow showing A maps row space \u2192 column space - Arrow showing A maps null space \u2192 {0} - Dimension labels on each subspace</p> <p>Interactive controls: - Matrix A input (up to 4\u00d74) - \"Compute Subspaces\" button - Toggle to show basis vectors for each subspace - Toggle to show orthogonality verification - Slider to highlight one subspace at a time</p> <p>Default parameters: - 3\u00d74 matrix with rank 2 - Show all four subspaces - Canvas: responsive dual-panel layout</p> <p>Behavior: - Compute and display bases for each subspace - Verify orthogonality numerically - Show rank and dimension formulas - Animate vector mapping from domain to codomain - Highlight which vectors map to zero</p> <p>Implementation: p5.js with SVG diagrams</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#visualization","title":"Visualization","text":"<pre><code>             A\n    R^n  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  R^m\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502      \u2502             \u2502\n\u2502  row(A)     \u2502 \u2500\u2500\u2500\u2192 \u2502   col(A)    \u2502\n\u2502  dim = r    \u2502      \u2502   dim = r   \u2502\n\u2502             \u2502      \u2502             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502             \u2502      \u2502             \u2502\n\u2502  null(A)    \u2502 \u2500\u2500\u2500\u2192 \u2502   {0}       \u2502\n\u2502  dim = n-r  \u2502      \u2502             \u2502\n\u2502             \u2502      \u2502  null(A^T)  \u2502\n\u2502             \u2502      \u2502  dim = m-r  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        \u22a5                  \u22a5\n</code></pre>"},{"location":"chapters/08-vector-spaces-and-inner-products/#the-pseudoinverse","title":"The Pseudoinverse","text":"<p>The pseudoinverse (Moore-Penrose inverse) \\(A^+\\) generalizes the matrix inverse to rectangular and singular matrices.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#definition-via-svd","title":"Definition via SVD","text":"<p>If \\(A = U\\Sigma V^T\\) is the SVD with non-zero singular values \\(\\sigma_1, \\ldots, \\sigma_r\\):</p> <p>\\(A^+ = V\\Sigma^+ U^T\\)</p> <p>where:</p> <p>\\(\\Sigma^+ = \\begin{bmatrix} \\sigma_1^{-1} &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_r^{-1} \\\\ &amp; \\mathbf{0} &amp; \\end{bmatrix}^T\\)</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#properties","title":"Properties","text":"<p>The pseudoinverse satisfies the Moore-Penrose conditions:</p> <ol> <li>\\(AA^+A = A\\)</li> <li>\\(A^+AA^+ = A^+\\)</li> <li>\\((AA^+)^T = AA^+\\) (symmetric)</li> <li>\\((A^+A)^T = A^+A\\) (symmetric)</li> </ol>"},{"location":"chapters/08-vector-spaces-and-inner-products/#special-cases","title":"Special Cases","text":"Matrix Type Pseudoinverse Invertible \\(A^+ = A^{-1}\\) Full column rank (\\(m &gt; n\\)) \\(A^+ = (A^TA)^{-1}A^T\\) (left inverse) Full row rank (\\(m &lt; n\\)) \\(A^+ = A^T(AA^T)^{-1}\\) (right inverse) Rank-deficient Use SVD formula"},{"location":"chapters/08-vector-spaces-and-inner-products/#least-squares-via-pseudoinverse","title":"Least Squares via Pseudoinverse","text":"<p>The least squares solution is:</p> <p>\\(\\hat{\\mathbf{x}} = A^+\\mathbf{b}\\)</p> <p>When \\(A\\) has full column rank, this equals \\((A^TA)^{-1}A^T\\mathbf{b}\\).</p> <p>When \\(A\\) is rank-deficient, the pseudoinverse gives the minimum-norm least squares solution\u2014the smallest \\(\\hat{\\mathbf{x}}\\) that minimizes \\(\\|A\\mathbf{x} - \\mathbf{b}\\|\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#diagram-pseudoinverse-application","title":"Diagram: Pseudoinverse Application","text":"<p>Run the Pseudoinverse Solver Fullscreen</p> Pseudoinverse Solver MicroSim <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how the pseudoinverse solves least squares problems, especially for rank-deficient systems</p> <p>Visual elements: - Matrix A display with rank indicator - Vector b input - Solution x = A\u207ab display - Residual ||Ax - b|| display - SVD components visualization - Comparison: exact solution (if exists) vs least squares</p> <p>Interactive controls: - Matrix A entry fields (up to 4\u00d74) - Vector b entry fields - \"Compute Pseudoinverse\" button - \"Show SVD\" toggle - Preset examples: full rank, rank deficient, underdetermined</p> <p>Default parameters: - 3\u00d72 matrix (overdetermined) - Show solution and residual - Canvas: responsive</p> <p>Behavior: - Compute and display A\u207a - Solve x = A\u207ab - Show residual and verify it's minimal - For underdetermined: show minimum-norm property - Compare with direct (A^TA)^{-1}A^T when applicable</p> <p>Implementation: p5.js with numerical computation</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#practical-implementation","title":"Practical Implementation","text":"<pre><code>import numpy as np\nfrom scipy import linalg\n\n# Vector space operations\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\n\n# Inner product (dot product)\ninner = np.dot(v1, v2)\n\n# Norm from inner product\nnorm_v1 = np.sqrt(np.dot(v1, v1))  # or np.linalg.norm(v1)\n\n# Angle between vectors (Cauchy-Schwarz)\ncos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\ntheta = np.arccos(cos_theta)\n\n# Gram-Schmidt (via QR)\nA = np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]], dtype=float)\nQ, R = np.linalg.qr(A)\nprint(\"Orthonormal basis Q:\\n\", Q)\n\n# Projection onto column space\ndef project_onto_colspace(A, b):\n    \"\"\"Project b onto column space of A.\"\"\"\n    Q, _ = np.linalg.qr(A)\n    return Q @ Q.T @ b\n\n# Least squares\nA = np.array([[1, 1], [1, 2], [1, 3]], dtype=float)\nb = np.array([1, 2, 2], dtype=float)\n\n# Method 1: Normal equations (less stable)\nx_normal = np.linalg.solve(A.T @ A, A.T @ b)\n\n# Method 2: QR decomposition (stable)\nx_qr, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n\n# Method 3: Pseudoinverse\nA_pinv = np.linalg.pinv(A)\nx_pinv = A_pinv @ b\n\n# Four fundamental subspaces via SVD\nU, s, Vh = np.linalg.svd(A)\nrank = np.sum(s &gt; 1e-10)\n\ncol_space_basis = U[:, :rank]      # Column space\nleft_null_basis = U[:, rank:]       # Left null space\nrow_space_basis = Vh[:rank, :].T    # Row space\nnull_space_basis = Vh[rank:, :].T   # Null space\n\nprint(f\"Rank: {rank}\")\nprint(f\"Column space dim: {rank}, Left null space dim: {A.shape[0] - rank}\")\nprint(f\"Row space dim: {rank}, Null space dim: {A.shape[1] - rank}\")\n</code></pre>"},{"location":"chapters/08-vector-spaces-and-inner-products/#summary_1","title":"Summary","text":"<p>This chapter developed the abstract framework for linear algebra:</p> <p>Vector Spaces:</p> <ul> <li>Abstract vector spaces satisfy ten axioms enabling addition and scalar multiplication</li> <li>Subspaces are closed under linear combinations</li> <li>The framework applies to functions, matrices, and beyond \\(\\mathbb{R}^n\\)</li> </ul> <p>Inner Products:</p> <ul> <li>Inner products generalize dot products, enabling length and angle measurement</li> <li>Norms derive from inner products: \\(\\|\\mathbf{v}\\| = \\sqrt{\\langle \\mathbf{v}, \\mathbf{v} \\rangle}\\)</li> <li>Cauchy-Schwarz inequality: \\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> </ul> <p>Orthogonality:</p> <ul> <li>Orthogonal vectors satisfy \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = 0\\)</li> <li>Orthonormal bases simplify coordinate computation to inner products</li> <li>Gram-Schmidt converts any basis to orthonormal</li> </ul> <p>Projections and Least Squares:</p> <ul> <li>Projection finds the closest point in a subspace</li> <li>Least squares minimizes \\(\\|A\\mathbf{x} - \\mathbf{b}\\|^2\\)</li> <li>Normal equations: \\(A^TA\\hat{\\mathbf{x}} = A^T\\mathbf{b}\\)</li> </ul> <p>Fundamental Subspaces:</p> <ul> <li>Every matrix has four fundamental subspaces: column, row, null, left null</li> <li>Row space \\(\\perp\\) null space; column space \\(\\perp\\) left null space</li> <li>Dimensions sum correctly: \\(r + (n-r) = n\\) and \\(r + (m-r) = m\\)</li> </ul> <p>Pseudoinverse:</p> <ul> <li>\\(A^+\\) generalizes inversion to any matrix</li> <li>Provides minimum-norm least squares solutions</li> <li>Computed via SVD: \\(A^+ = V\\Sigma^+ U^T\\)</li> </ul> Self-Check: Why must the error vector in least squares be orthogonal to the column space of A? <p>The error \\(\\mathbf{e} = \\mathbf{b} - A\\hat{\\mathbf{x}}\\) must be orthogonal to \\(\\text{col}(A)\\) because projection gives the closest point. If \\(\\mathbf{e}\\) had any component in \\(\\text{col}(A)\\), we could subtract that component from \\(\\mathbf{e}\\) to get closer to \\(\\mathbf{b}\\), contradicting minimality. Mathematically, the first-order optimality condition \\(\\nabla_\\mathbf{x}\\|A\\mathbf{x} - \\mathbf{b}\\|^2 = 0\\) yields \\(A^T(A\\mathbf{x} - \\mathbf{b}) = 0\\), meaning \\(A^T\\mathbf{e} = 0\\)\u2014the error is orthogonal to every column of \\(A\\).</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/","title":"Quiz: Vector Spaces and Inner Products","text":"<p>Test your understanding of abstract vector spaces and inner product concepts.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#1-a-set-v-is-a-vector-space-if-it-satisfies","title":"1. A set \\(V\\) is a vector space if it satisfies:","text":"<ol> <li>Contains only numerical vectors</li> <li>Is closed under vector addition and scalar multiplication with 8 axioms satisfied</li> <li>Contains exactly three vectors</li> <li>Has a unique basis</li> </ol> Show Answer <p>The correct answer is B. A vector space must be closed under vector addition and scalar multiplication, and satisfy eight axioms including associativity, commutativity of addition, existence of zero vector, and distributive laws.</p> <p>Concept Tested: Vector Space Axioms</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#2-a-subspace-of-a-vector-space-must","title":"2. A subspace of a vector space must:","text":"<ol> <li>Have the same dimension as the parent space</li> <li>Contain the zero vector and be closed under addition and scalar multiplication</li> <li>Be finite-dimensional</li> <li>Contain at least two vectors</li> </ol> Show Answer <p>The correct answer is B. A subspace must contain the zero vector and be closed under vector addition and scalar multiplication. These conditions ensure the subspace is itself a vector space under the inherited operations.</p> <p>Concept Tested: Subspace</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#3-an-inner-product-langle-mathbfu-mathbfv-rangle-must-satisfy-all-except","title":"3. An inner product \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle\\) must satisfy all EXCEPT:","text":"<ol> <li>Linearity in the first argument</li> <li>Conjugate symmetry</li> <li>Positive definiteness</li> <li>Multiplicativity: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{u} \\rangle \\cdot \\langle \\mathbf{v} \\rangle\\)</li> </ol> Show Answer <p>The correct answer is D. Inner products require linearity (in first argument for real, conjugate-linear in second for complex), conjugate symmetry \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\overline{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}\\), and positive definiteness. There is no multiplicativity requirement.</p> <p>Concept Tested: Inner Product Properties</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#4-the-standard-inner-product-in-mathbbrn-is","title":"4. The standard inner product in \\(\\mathbb{R}^n\\) is:","text":"<ol> <li>The sum of vector components</li> <li>The dot product \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum u_i v_i\\)</li> <li>The cross product</li> <li>The Euclidean distance</li> </ol> Show Answer <p>The correct answer is B. The standard inner product in \\(\\mathbb{R}^n\\) is the dot product: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^n u_i v_i\\). It induces the Euclidean norm.</p> <p>Concept Tested: Standard Inner Product</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#5-an-orthonormal-basis-has-vectors-that-are","title":"5. An orthonormal basis has vectors that are:","text":"<ol> <li>Parallel and of any length</li> <li>Mutually orthogonal and each with unit length</li> <li>Linearly dependent</li> <li>All equal to each other</li> </ol> Show Answer <p>The correct answer is B. An orthonormal basis consists of vectors that are mutually orthogonal (pairwise dot product is zero) and each normalized to unit length. This makes coordinate representation and computation especially convenient.</p> <p>Concept Tested: Orthonormal Basis</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#6-the-projection-of-vector-mathbfu-onto-vector-mathbfv-is","title":"6. The projection of vector \\(\\mathbf{u}\\) onto vector \\(\\mathbf{v}\\) is:","text":"<ol> <li>\\(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}\\)</li> <li>\\(\\mathbf{u} + \\mathbf{v}\\)</li> <li>\\(\\mathbf{u} \\times \\mathbf{v}\\)</li> <li>\\(\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> </ol> Show Answer <p>The correct answer is A. The orthogonal projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is \\(\\text{proj}_{\\mathbf{v}}\\mathbf{u} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\mathbf{v}\\). This gives the component of \\(\\mathbf{u}\\) in the direction of \\(\\mathbf{v}\\).</p> <p>Concept Tested: Orthogonal Projection</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#7-the-orthogonal-complement-of-a-subspace-w-contains","title":"7. The orthogonal complement of a subspace \\(W\\) contains:","text":"<ol> <li>All vectors parallel to \\(W\\)</li> <li>All vectors orthogonal to every vector in \\(W\\)</li> <li>Only the zero vector</li> <li>The basis vectors of \\(W\\)</li> </ol> Show Answer <p>The correct answer is B. The orthogonal complement \\(W^\\perp\\) consists of all vectors that are orthogonal to every vector in \\(W\\). For any \\(\\mathbf{u} \\in W^\\perp\\) and \\(\\mathbf{w} \\in W\\), we have \\(\\langle \\mathbf{u}, \\mathbf{w} \\rangle = 0\\).</p> <p>Concept Tested: Orthogonal Complement</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#8-the-cauchy-schwarz-inequality-states","title":"8. The Cauchy-Schwarz inequality states:","text":"<ol> <li>\\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\)</li> <li>\\(\\|\\mathbf{u} + \\mathbf{v}\\| = \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> <li>\\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\|\\mathbf{u}\\|^2\\)</li> <li>\\(\\|\\mathbf{u} - \\mathbf{v}\\| &gt; \\|\\mathbf{u}\\|\\)</li> </ol> Show Answer <p>The correct answer is A. The Cauchy-Schwarz inequality bounds the inner product: \\(|\\langle \\mathbf{u}, \\mathbf{v} \\rangle| \\leq \\|\\mathbf{u}\\| \\|\\mathbf{v}\\|\\). Equality holds if and only if the vectors are parallel.</p> <p>Concept Tested: Cauchy-Schwarz Inequality</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#9-an-inner-product-space-is","title":"9. An inner product space is:","text":"<ol> <li>A vector space with a defined inner product</li> <li>A space containing only unit vectors</li> <li>A subspace of \\(\\mathbb{R}^3\\)</li> <li>A space with no zero vector</li> </ol> Show Answer <p>The correct answer is A. An inner product space is a vector space equipped with an inner product function that satisfies the required axioms. This structure enables defining lengths, angles, and orthogonality.</p> <p>Concept Tested: Inner Product Space</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/quiz/#10-the-angle-theta-between-two-non-zero-vectors-is-determined-by","title":"10. The angle \\(\\theta\\) between two non-zero vectors is determined by:","text":"<ol> <li>\\(\\cos\\theta = \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\)</li> <li>\\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\)</li> <li>\\(\\theta = \\mathbf{u} \\cdot \\mathbf{v}\\)</li> <li>\\(\\sin\\theta = \\frac{\\|\\mathbf{u}\\|}{\\|\\mathbf{v}\\|}\\)</li> </ol> Show Answer <p>The correct answer is B. The angle between vectors is given by \\(\\cos\\theta = \\frac{\\langle \\mathbf{u}, \\mathbf{v} \\rangle}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\). This generalizes the geometric definition of angle to any inner product space.</p> <p>Concept Tested: Angle Between Vectors</p>"},{"location":"chapters/09-machine-learning-foundations/","title":"Machine Learning Foundations","text":""},{"location":"chapters/09-machine-learning-foundations/#summary","title":"Summary","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques. You will learn how data is represented as matrices, understand covariance and correlation, master Principal Component Analysis (PCA) for dimensionality reduction, and implement linear regression with regularization. Gradient descent, the workhorse of machine learning optimization, is covered in detail.</p>"},{"location":"chapters/09-machine-learning-foundations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"chapters/09-machine-learning-foundations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 7: Matrix Decompositions</li> <li>Chapter 8: Vector Spaces and Inner Products</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#introduction","title":"Introduction","text":"<p>Machine learning is, at its core, applied linear algebra. When you train a model, you perform matrix operations. When you reduce dimensions, you compute eigendecompositions. When you optimize, you follow gradients through high-dimensional spaces. Understanding the linear algebra behind these operations transforms you from a user of black-box algorithms into a practitioner who can debug, optimize, and innovate.</p> <p>This chapter bridges abstract linear algebra and practical machine learning. We start with how data becomes matrices, develop statistical tools like covariance, build up to PCA for dimensionality reduction, implement regression with regularization, and master gradient descent for optimization. Each section reinforces that machine learning \"magic\" is really linear algebra in action.</p>"},{"location":"chapters/09-machine-learning-foundations/#data-as-matrices","title":"Data as Matrices","text":"<p>In machine learning, data is organized into matrices where each row represents an observation and each column represents a feature.</p>"},{"location":"chapters/09-machine-learning-foundations/#feature-vectors","title":"Feature Vectors","text":"<p>A feature vector represents a single data point as a vector of measurements or attributes:</p> <p>\\(\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\end{bmatrix} \\in \\mathbb{R}^d\\)</p> <p>where:</p> <ul> <li>\\(d\\) is the number of features (dimensionality)</li> <li>Each \\(x_i\\) is a measurement (e.g., height, weight, pixel intensity)</li> </ul> <p>Examples of feature vectors:</p> Domain Features Dimensionality Housing bedrooms, sqft, age, location 4+ Images pixel intensities 784 (28\u00d728) to millions Text word counts or embeddings 100 to 768+ Tabular mixed numerical/categorical varies"},{"location":"chapters/09-machine-learning-foundations/#feature-matrix-and-data-matrix","title":"Feature Matrix and Data Matrix","text":"<p>A feature matrix (also called data matrix) stacks \\(n\\) feature vectors as rows:</p> <p>\\(X = \\begin{bmatrix} \u2014 \\mathbf{x}_1^T \u2014 \\\\ \u2014 \\mathbf{x}_2^T \u2014 \\\\ \\vdots \\\\ \u2014 \\mathbf{x}_n^T \u2014 \\end{bmatrix} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(X\\) is \\(n \\times d\\) (n samples, d features)</li> <li>Row \\(i\\) is sample \\(\\mathbf{x}_i^T\\)</li> <li>Column \\(j\\) contains all values of feature \\(j\\)</li> </ul> <p>Convention Alert</p> <p>Some texts use columns for samples (X is d\u00d7n). We follow the more common machine learning convention where rows are samples, matching NumPy/Pandas defaults. Always check the convention when reading papers or documentation.</p>"},{"location":"chapters/09-machine-learning-foundations/#diagram-data-matrix-structure","title":"Diagram: Data Matrix Structure","text":"<p>Run the Data Matrix Structure Visualizer Fullscreen</p> Data Matrix Structure Visualizer <p>Type: infographic</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize the structure of data matrices and understand the relationship between rows (samples) and columns (features)</p> <p>Layout: Interactive matrix representation with labeled dimensions</p> <p>Visual elements: - Main panel: Color-coded matrix grid - Row labels: \"Sample 1\", \"Sample 2\", ..., \"Sample n\" - Column labels: \"Feature 1\", \"Feature 2\", ..., \"Feature d\" - Highlighted single row showing feature vector - Highlighted single column showing all values of one feature - Dimension annotations: n (rows) and d (columns)</p> <p>Interactive elements: - Click a row to highlight as feature vector - Click a column to highlight as feature across all samples - Hover to see individual cell value - Toggle to show actual example data (iris, housing, etc.)</p> <p>Example datasets: 1. Iris: 150 samples, 4 features (petal/sepal dimensions) 2. MNIST digit: 1 sample, 784 features (pixel values) 3. Housing: 506 samples, 13 features</p> <p>Visual style: - Heat map coloring for numerical values - Clean grid lines - Responsive sizing</p> <p>Implementation: HTML/CSS/JavaScript with interactive highlighting</p>"},{"location":"chapters/09-machine-learning-foundations/#statistical-foundations","title":"Statistical Foundations","text":"<p>Before applying machine learning algorithms, we must understand the statistical structure of our data.</p>"},{"location":"chapters/09-machine-learning-foundations/#standardization","title":"Standardization","text":"<p>Standardization transforms features to have zero mean and unit variance:</p> <p>\\(z_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}\\)</p> <p>where:</p> <ul> <li>\\(\\mu_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}\\) is the mean of feature \\(j\\)</li> <li>\\(\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\\) is the standard deviation</li> </ul> <p>In matrix form, if \\(\\boldsymbol{\\mu}\\) is the row vector of means:</p> <p>\\(Z = (X - \\mathbf{1}\\boldsymbol{\\mu}) \\text{diag}(\\boldsymbol{\\sigma})^{-1}\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#why-standardize","title":"Why Standardize?","text":"<ul> <li>Scale invariance: Features measured in different units become comparable</li> <li>Numerical stability: Prevents features with large values from dominating</li> <li>Algorithm requirements: Many algorithms (PCA, gradient descent, regularization) assume or benefit from standardized data</li> </ul> Algorithm Standardization PCA Required for meaningful results k-Means Recommended SVM Required (especially with RBF kernel) Neural Networks Strongly recommended Decision Trees Not necessary Linear Regression Recommended for regularization"},{"location":"chapters/09-machine-learning-foundations/#covariance-matrix","title":"Covariance Matrix","text":"<p>The covariance matrix captures how features vary together:</p> <p>\\(\\Sigma = \\frac{1}{n-1}(X - \\mathbf{1}\\boldsymbol{\\mu})^T(X - \\mathbf{1}\\boldsymbol{\\mu}) = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</p> <p>where:</p> <ul> <li>\\(\\tilde{X}\\) is the centered data matrix (mean subtracted)</li> <li>\\(\\Sigma\\) is a \\(d \\times d\\) symmetric positive semi-definite matrix</li> <li>\\(\\Sigma_{jk} = \\text{Cov}(X_j, X_k)\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#covariance-formula","title":"Covariance Formula","text":"<p>The covariance between features \\(j\\) and \\(k\\):</p> <p>\\(\\text{Cov}(X_j, X_k) = \\frac{1}{n-1}\\sum_{i=1}^n (x_{ij} - \\mu_j)(x_{ik} - \\mu_k)\\)</p> <p>Properties:</p> <ul> <li>Diagonal entries \\(\\Sigma_{jj} = \\text{Var}(X_j)\\) are variances</li> <li>Off-diagonal entries measure linear relationships</li> <li>\\(\\Sigma_{jk} &gt; 0\\): features increase together</li> <li>\\(\\Sigma_{jk} &lt; 0\\): one increases as other decreases</li> <li>\\(\\Sigma_{jk} = 0\\): no linear relationship (not necessarily independent)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#correlation-matrix","title":"Correlation Matrix","text":"<p>The correlation matrix is the standardized covariance:</p> <p>\\(R = D^{-1}\\Sigma D^{-1}\\)</p> <p>where:</p> <ul> <li>\\(D = \\text{diag}(\\sigma_1, \\ldots, \\sigma_d)\\) contains standard deviations</li> <li>\\(R_{jk} = \\frac{\\Sigma_{jk}}{\\sigma_j \\sigma_k} = \\frac{\\text{Cov}(X_j, X_k)}{\\sqrt{\\text{Var}(X_j)\\text{Var}(X_k)}}\\)</li> </ul> <p>Properties:</p> <ul> <li>Diagonal entries are 1 (features perfectly correlate with themselves)</li> <li>Off-diagonal entries satisfy \\(-1 \\leq R_{jk} \\leq 1\\)</li> <li>\\(R_{jk} = \\pm 1\\): perfect linear relationship</li> <li>The correlation matrix is the covariance matrix of standardized data</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-covariance-and-correlation-visualizer","title":"Diagram: Covariance and Correlation Visualizer","text":"<p>Run the Covariance and Correlation Visualizer Fullscreen</p> Covariance and Correlation Matrix Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how covariance and correlation capture relationships between features through interactive exploration</p> <p>Visual elements: - Left panel: Scatter plot matrix (pairs of features) - Center panel: Covariance matrix as heatmap - Right panel: Correlation matrix as heatmap - Color scale: Blue (negative) to White (zero) to Red (positive) - Eigenvalue display for covariance matrix</p> <p>Interactive controls: - Dataset selector (generated bivariate, iris, custom) - Draggable data points to modify dataset - \"Standardize\" toggle to see effect on covariance - Highlight cell to see corresponding scatter plot - Slider to add/remove correlation between features</p> <p>Default parameters: - 2D generated data with moderate positive correlation - 100 sample points - Canvas: responsive three-panel layout</p> <p>Behavior: - Real-time update of matrices as data changes - Show how correlation normalizes for scale - Highlight relationship between scatter plot shape and correlation value - Display eigenvalues/eigenvectors of covariance matrix - Demonstrate that standardized data has correlation = covariance</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Principal Component Analysis is a technique for dimensionality reduction that finds the directions of maximum variance in data.</p>"},{"location":"chapters/09-machine-learning-foundations/#the-goal-of-dimensionality-reduction","title":"The Goal of Dimensionality Reduction","text":"<p>High-dimensional data presents challenges:</p> <ul> <li>Visualization: Cannot plot more than 3 dimensions</li> <li>Computation: Many algorithms scale poorly with dimensions</li> <li>Curse of dimensionality: Data becomes sparse in high dimensions</li> <li>Noise: Some dimensions may be noise rather than signal</li> </ul> <p>Dimensionality reduction projects data from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}^k\\) where \\(k &lt; d\\), preserving as much information as possible.</p>"},{"location":"chapters/09-machine-learning-foundations/#principal-components","title":"Principal Components","text":"<p>Principal components are the eigenvectors of the covariance matrix, ordered by their eigenvalues:</p> <p>\\(\\Sigma \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_1\\) is the first principal component (direction of maximum variance)</li> <li>\\(\\mathbf{v}_2\\) is orthogonal to \\(\\mathbf{v}_1\\) and captures maximum remaining variance</li> <li>\\(\\lambda_i\\) is the variance explained by the \\(i\\)-th component</li> </ul> <p>The principal components form an orthonormal basis aligned with the data's natural axes of variation.</p>"},{"location":"chapters/09-machine-learning-foundations/#pca-algorithm","title":"PCA Algorithm","text":"<p>Step 1: Center the data</p> <p>\\(\\tilde{X} = X - \\mathbf{1}\\boldsymbol{\\mu}\\)</p> <p>Step 2: Compute covariance matrix</p> <p>\\(\\Sigma = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</p> <p>Step 3: Eigendecomposition</p> <p>\\(\\Sigma = V\\Lambda V^T\\)</p> <p>where \\(V = [\\mathbf{v}_1 | \\cdots | \\mathbf{v}_d]\\) and \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_d)\\) with \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_d\\).</p> <p>Step 4: Project onto top \\(k\\) components</p> <p>\\(Z = \\tilde{X}V_k\\)</p> <p>where \\(V_k = [\\mathbf{v}_1 | \\cdots | \\mathbf{v}_k]\\) contains the first \\(k\\) principal components.</p>"},{"location":"chapters/09-machine-learning-foundations/#variance-explained","title":"Variance Explained","text":"<p>The variance explained by each principal component is its eigenvalue:</p> <ul> <li>Total variance: \\(\\sum_{i=1}^d \\lambda_i = \\text{trace}(\\Sigma)\\)</li> <li>Proportion of variance explained by component \\(i\\): \\(\\frac{\\lambda_i}{\\sum_{j=1}^d \\lambda_j}\\)</li> <li>Cumulative variance explained by first \\(k\\) components: \\(\\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{j=1}^d \\lambda_j}\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#scree-plot","title":"Scree Plot","text":"<p>A scree plot visualizes eigenvalues to help choose the number of components:</p> <ul> <li>X-axis: Component number (1, 2, 3, ...)</li> <li>Y-axis: Eigenvalue (variance explained) or proportion of variance</li> <li>Look for an \"elbow\" where eigenvalues drop sharply</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-pca-interactive-explorer","title":"Diagram: PCA Interactive Explorer","text":"<p>Run the PCA Explorer Fullscreen</p> PCA Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand PCA by visualizing each step from raw data to projected low-dimensional representation</p> <p>Visual elements: - Panel 1: Original 2D/3D data with mean point - Panel 2: Centered data (translated to origin) - Panel 3: Principal component vectors overlaid on data - Panel 4: Projected 1D data along first PC - Scree plot showing eigenvalues - Variance explained percentage display</p> <p>Interactive controls: - Data generator: cluster shape, spread, rotation - Number of points slider (20-200) - Dimension selector (2D or 3D) - Step-through buttons: \"Center\", \"Find PCs\", \"Project\" - Number of components to keep (k) - \"Show Reconstruction\" toggle</p> <p>Default parameters: - Elongated 2D Gaussian cluster - 100 points - Canvas: responsive multi-panel</p> <p>Behavior: - Animate centering transformation - Show eigenvectors with length proportional to eigenvalue - Demonstrate projection onto first PC - Show reconstruction error when reducing dimensions - Display scree plot updating with data changes</p> <p>Implementation: p5.js with eigenvalue computation</p>"},{"location":"chapters/09-machine-learning-foundations/#pca-via-svd","title":"PCA via SVD","text":"<p>In practice, PCA is computed using SVD for numerical stability:</p> <p>\\(\\tilde{X} = U\\Sigma V^T\\)</p> <p>The relationship to eigendecomposition:</p> <ul> <li>Right singular vectors \\(V\\) are the principal components</li> <li>Singular values relate to eigenvalues: \\(\\lambda_i = \\frac{\\sigma_i^2}{n-1}\\)</li> <li>This avoids forming \\(\\tilde{X}^T\\tilde{X}\\) explicitly</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-scree-plot-interactive","title":"Diagram: Scree Plot Interactive","text":"<p>Run the Scree Plot Visualizer Fullscreen</p> Scree Plot and Component Selection <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Use scree plots and cumulative variance to select the optimal number of principal components</p> <p>Visual elements: - Left panel: Scree plot (bar chart of eigenvalues) - Right panel: Cumulative variance explained (line plot) - Threshold line for desired variance (e.g., 95%) - Elbow point detection and highlight - Reconstruction comparison at different k values</p> <p>Interactive controls: - Dataset selector (synthetic, iris, digits subset) - Draggable threshold line for variance target - Number of components slider - \"Show Reconstructed Data\" toggle - \"Compare Original vs Reconstructed\" toggle</p> <p>Default parameters: - Synthetic dataset with clear elbow at k=3 - 95% variance threshold line - Canvas: responsive dual-panel</p> <p>Behavior: - Highlight suggested k based on elbow detection - Show which k achieves target variance - Display reconstruction error as k changes - For image data: show visual reconstruction quality - Kaiser criterion line (eigenvalue = 1 for standardized data)</p> <p>Implementation: p5.js with statistical visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#linear-regression","title":"Linear Regression","text":"<p>Linear regression fits a linear model to predict a target variable from features.</p>"},{"location":"chapters/09-machine-learning-foundations/#the-model","title":"The Model","text":"<p>Given features \\(\\mathbf{x} \\in \\mathbb{R}^d\\) and target \\(y \\in \\mathbb{R}\\):</p> <p>\\(y = \\mathbf{w}^T\\mathbf{x} + b = w_1x_1 + w_2x_2 + \\cdots + w_dx_d + b\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector</li> <li>\\(b \\in \\mathbb{R}\\) is the bias (intercept)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#design-matrix","title":"Design Matrix","text":"<p>The design matrix augments features with a column of ones to absorb the bias:</p> <p>\\(X = \\begin{bmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\end{bmatrix}\\)</p> <p>Now the model becomes:</p> <p>\\(\\mathbf{y} = X\\boldsymbol{\\theta}\\)</p> <p>where \\(\\boldsymbol{\\theta} = [b, w_1, \\ldots, w_d]^T\\) combines bias and weights.</p>"},{"location":"chapters/09-machine-learning-foundations/#ordinary-least-squares","title":"Ordinary Least Squares","text":"<p>The least squares solution minimizes:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 = \\sum_{i=1}^n (x_i^T\\boldsymbol{\\theta} - y_i)^2\\)</p> <p>The closed-form solution (normal equations):</p> <p>\\(\\hat{\\boldsymbol{\\theta}} = (X^TX)^{-1}X^T\\mathbf{y}\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#diagram-linear-regression-visualizer","title":"Diagram: Linear Regression Visualizer","text":"<p>Run the Linear Regression Visualizer Fullscreen</p> Linear Regression Interactive Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand linear regression as finding the best-fit line/plane by minimizing squared errors</p> <p>Visual elements: - Main panel: Scatter plot of data points - Fitted line/plane (2D or 3D) - Residual lines from points to fitted line - Loss function surface (for 2D: 3D surface of loss vs w, b) - Current parameter values display</p> <p>Interactive controls: - Drag data points to modify dataset - Manual sliders for w and b to see effect on fit and loss - \"Fit OLS\" button to compute optimal parameters - Toggle residual visualization - Switch between 1D (line fit) and 2D (plane fit) examples</p> <p>Default parameters: - 2D scatter with linear relationship plus noise - 20 data points - Canvas: responsive</p> <p>Behavior: - Real-time residual and loss computation - Show that OLS solution is at minimum of loss surface - Display R\u00b2 score for goodness of fit - Highlight vertical (y) residuals vs perpendicular distance - Show normal equations computation</p> <p>Implementation: p5.js with regression computation</p>"},{"location":"chapters/09-machine-learning-foundations/#regularization","title":"Regularization","text":"<p>Regularization adds a penalty term to prevent overfitting by constraining model complexity.</p>"},{"location":"chapters/09-machine-learning-foundations/#why-regularize","title":"Why Regularize?","text":"<p>Without regularization, models can:</p> <ul> <li>Overfit to noise in training data</li> <li>Have large, unstable weights</li> <li>Perform poorly on new data</li> <li>Fail when features are correlated (multicollinearity)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#ridge-regression-l2","title":"Ridge Regression (L2)","text":"<p>Ridge regression adds an L2 penalty on weights:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 + \\alpha\\|\\boldsymbol{\\theta}\\|^2\\)</p> <p>where:</p> <ul> <li>\\(\\alpha \\geq 0\\) is the regularization strength</li> <li>\\(\\|\\boldsymbol{\\theta}\\|^2 = \\sum_j \\theta_j^2\\) (typically excluding bias)</li> </ul> <p>The closed-form solution:</p> <p>\\(\\hat{\\boldsymbol{\\theta}} = (X^TX + \\alpha I)^{-1}X^T\\mathbf{y}\\)</p> <p>Key properties:</p> <ul> <li>Always invertible (even if \\(X^TX\\) is singular)</li> <li>Shrinks weights toward zero</li> <li>Keeps all features (no feature selection)</li> <li>Equivalent to adding \"fake\" data points</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#lasso-regression-l1","title":"Lasso Regression (L1)","text":"<p>Lasso regression uses an L1 penalty:</p> <p>\\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2 + \\alpha\\|\\boldsymbol{\\theta}\\|_1\\)</p> <p>where:</p> <ul> <li>\\(\\|\\boldsymbol{\\theta}\\|_1 = \\sum_j |\\theta_j|\\) is the L1 norm</li> </ul> <p>Key properties:</p> <ul> <li>Produces sparse solutions (some weights exactly zero)</li> <li>Performs automatic feature selection</li> <li>No closed-form solution (requires iterative optimization)</li> <li>Useful when only few features are truly relevant</li> </ul> Property Ridge (L2) Lasso (L1) Penalty \\(\\sum \\theta_j^2\\) $\\sum Sparsity No Yes (feature selection) Closed-form Yes No Multicollinearity Handles well Picks one of correlated features Geometry Circular constraint Diamond constraint"},{"location":"chapters/09-machine-learning-foundations/#diagram-regularization-geometry","title":"Diagram: Regularization Geometry","text":"<p>Run the Regularization Geometry Visualizer Fullscreen</p> Regularization Geometry Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how L1 and L2 regularization constrain weights geometrically and why L1 produces sparsity</p> <p>Visual elements: - 2D parameter space (\u03b8\u2081 vs \u03b8\u2082) - Contour lines of unregularized loss function - L2 constraint region (circle) - L1 constraint region (diamond) - OLS solution point - Regularized solution point - Regularization path as \u03b1 varies</p> <p>Interactive controls: - Slider for regularization strength \u03b1 - Toggle between L1 and L2 - Drag ellipse center (changing OLS solution location) - \"Show Regularization Path\" toggle - Animation of solution as \u03b1 increases</p> <p>Default parameters: - OLS solution at (3, 2) - Moderate \u03b1 - Canvas: responsive</p> <p>Behavior: - Show how constraint region intersects loss contours - Demonstrate L1 hitting corners (sparse solution) - Animate solution moving toward origin as \u03b1 increases - Show weight values and their evolution - Display sparsity count for L1</p> <p>Implementation: p5.js with geometric visualization</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-based-optimization","title":"Gradient-Based Optimization","text":"<p>When closed-form solutions don't exist or are too expensive, we use iterative gradient descent.</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-vector","title":"Gradient Vector","text":"<p>The gradient vector of a scalar function \\(f : \\mathbb{R}^d \\to \\mathbb{R}\\) collects all partial derivatives:</p> <p>\\(\\nabla f(\\boldsymbol{\\theta}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial \\theta_1} \\\\ \\frac{\\partial f}{\\partial \\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial \\theta_d} \\end{bmatrix}\\)</p> <p>Key properties:</p> <ul> <li>Points in the direction of steepest increase</li> <li>Magnitude indicates rate of change</li> <li>At a minimum, \\(\\nabla f = \\mathbf{0}\\)</li> </ul> <p>For linear regression loss \\(J(\\boldsymbol{\\theta}) = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2\\):</p> <p>\\(\\nabla J(\\boldsymbol{\\theta}) = 2X^T(X\\boldsymbol{\\theta} - \\mathbf{y})\\)</p>"},{"location":"chapters/09-machine-learning-foundations/#gradient-descent-algorithm","title":"Gradient Descent Algorithm","text":"<p>Gradient descent iteratively moves in the negative gradient direction:</p> <p>\\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta \\nabla J(\\boldsymbol{\\theta}^{(t)})\\)</p> <p>where:</p> <ul> <li>\\(\\eta &gt; 0\\) is the learning rate (step size)</li> <li>\\(t\\) is the iteration number</li> <li>We move against the gradient to decrease the function</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#batch-gradient-descent","title":"Batch Gradient Descent","text":"<p>Batch gradient descent uses all training samples to compute the gradient at each step:</p> <pre><code>Initialize \u03b8 randomly\nFor t = 1, 2, ..., max_iterations:\n    gradient = (2/n) * X^T @ (X @ \u03b8 - y)  # Full batch gradient\n    \u03b8 = \u03b8 - \u03b7 * gradient\n    If ||gradient|| &lt; tolerance:\n        break\nReturn \u03b8\n</code></pre> <p>Characteristics:</p> <ul> <li>Deterministic updates (same path from same initialization)</li> <li>Smooth convergence</li> <li>Expensive per iteration for large datasets</li> <li>May be slow for large \\(n\\)</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#learning-rate","title":"Learning Rate","text":"<p>The learning rate \\(\\eta\\) controls step size and critically affects convergence:</p> Learning Rate Behavior Too small Very slow convergence, may take forever Just right Smooth, efficient convergence Too large Oscillation, overshooting Way too large Divergence (loss increases) <p>Choosing the learning rate:</p> <ul> <li>Start with \\(\\eta = 0.01\\) or \\(0.001\\)</li> <li>Use learning rate schedules (decay over time)</li> <li>Adaptive methods (Adam, RMSprop) adjust per-parameter</li> </ul>"},{"location":"chapters/09-machine-learning-foundations/#diagram-gradient-descent-visualizer","title":"Diagram: Gradient Descent Visualizer","text":"<p>Run the Gradient Descent Visualizer Fullscreen</p> Gradient Descent Interactive Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how gradient descent navigates the loss surface and how learning rate affects convergence</p> <p>Visual elements: - Main panel: 3D surface plot of loss function J(\u03b8\u2081, \u03b8\u2082) - Contour plot view (top-down) - Current position marker - Gradient arrow at current position - Path traced by optimization - Loss vs iteration plot</p> <p>Interactive controls: - Learning rate slider (0.001 to 1.0, log scale) - \"Step\" button for single iteration - \"Run\" button for continuous optimization - \"Reset\" button to reinitialize - Starting point selector (click on surface) - Loss function selector (quadratic, Rosenbrock, etc.)</p> <p>Default parameters: - Simple quadratic loss with single minimum - Learning rate = 0.1 - Starting point away from minimum - Canvas: responsive multi-view</p> <p>Behavior: - Show gradient vector at each step - Trace optimization path on contour plot - Display convergence (or divergence) in loss plot - Demonstrate oscillation with high learning rate - Show slow progress with low learning rate - Count iterations to convergence</p> <p>Implementation: p5.js with 3D surface rendering (WEBGL)</p>"},{"location":"chapters/09-machine-learning-foundations/#variants-of-gradient-descent","title":"Variants of Gradient Descent","text":"<p>Beyond batch gradient descent, several variants improve efficiency:</p> <p>Stochastic Gradient Descent (SGD):</p> <ul> <li>Uses single sample per update: \\(\\nabla J_i(\\boldsymbol{\\theta})\\)</li> <li>Fast iterations but noisy updates</li> <li>Can escape local minima due to noise</li> </ul> <p>Mini-batch Gradient Descent:</p> <ul> <li>Uses subset of samples (batch size \\(b\\)): \\(\\frac{1}{b}\\sum_{i \\in B}\\nabla J_i(\\boldsymbol{\\theta})\\)</li> <li>Balances noise and efficiency</li> <li>Standard in deep learning (batch size 32-256)</li> </ul> <p>Momentum:</p> <ul> <li>Accumulates velocity: \\(\\mathbf{v}^{(t+1)} = \\beta\\mathbf{v}^{(t)} + \\nabla J(\\boldsymbol{\\theta}^{(t)})\\)</li> <li>Update: \\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta\\mathbf{v}^{(t+1)}\\)</li> <li>Accelerates through flat regions, dampens oscillations</li> </ul> Method Per-Iteration Cost Convergence Noise Batch GD \\(O(nd)\\) Smooth None SGD \\(O(d)\\) Noisy High Mini-batch \\(O(bd)\\) Moderate Moderate"},{"location":"chapters/09-machine-learning-foundations/#diagram-learning-rate-effect-visualizer","title":"Diagram: Learning Rate Effect Visualizer","text":"<p>Run the Learning Rate Effect Visualizer Fullscreen</p> Learning Rate Effect on Convergence <p>Type: microsim</p> <p>Bloom Taxonomy Level: Evaluate</p> <p>Learning Objective: Understand how learning rate choice affects optimization behavior through side-by-side comparison</p> <p>Visual elements: - Three parallel contour plots with different learning rates - Path traces showing optimization trajectories - Loss curves for each learning rate - Status indicators: \"Converging\", \"Oscillating\", \"Diverging\" - Step count to convergence</p> <p>Interactive controls: - Individual learning rate sliders for each panel - Preset buttons: \"Too Small\", \"Just Right\", \"Too Large\" - Shared \"Run All\" button - \"Reset All\" button - Speed slider for animation</p> <p>Default parameters: - Left: \u03b7 = 0.01 (too small) - Center: \u03b7 = 0.1 (good) - Right: \u03b7 = 0.5 (too large) - Same starting point for all</p> <p>Behavior: - Simultaneous animation of all three optimizations - Real-time loss comparison plot - Show oscillation in too-large case - Show slow progress in too-small case - Identify optimal learning rate region - Display final loss values</p> <p>Implementation: p5.js with synchronized animations</p>"},{"location":"chapters/09-machine-learning-foundations/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete machine learning pipeline using these concepts:</p> <pre><code>import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Ridge, Lasso\n\n# Load data\nX_train, y_train = load_data()  # n\u00d7d feature matrix, n\u00d71 target\n\n# Step 1: Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# Step 2: PCA for dimensionality reduction\npca = PCA(n_components=0.95)  # Keep 95% variance\nX_reduced = pca.fit_transform(X_scaled)\nprint(f\"Reduced from {X_train.shape[1]} to {X_reduced.shape[1]} dimensions\")\n\n# Step 3: Examine variance explained\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.bar(range(len(pca.explained_variance_ratio_)),\n        pca.explained_variance_ratio_)\nplt.title('Scree Plot')\nplt.subplot(1, 2, 2)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.axhline(0.95, color='r', linestyle='--')\nplt.title('Cumulative Variance Explained')\nplt.show()\n\n# Step 4: Ridge regression\nridge = Ridge(alpha=1.0)\nridge.fit(X_reduced, y_train)\n\n# Step 5: Lasso for feature selection (on original features)\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_scaled, y_train)\nselected_features = np.where(lasso.coef_ != 0)[0]\nprint(f\"Lasso selected {len(selected_features)} features\")\n\n# Step 6: Manual gradient descent implementation\ndef gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n    n, d = X.shape\n    theta = np.zeros(d)\n    losses = []\n\n    for i in range(n_iterations):\n        # Compute predictions\n        y_pred = X @ theta\n\n        # Compute gradient\n        gradient = (2/n) * X.T @ (y_pred - y)\n\n        # Update parameters\n        theta = theta - learning_rate * gradient\n\n        # Track loss\n        loss = np.mean((y_pred - y)**2)\n        losses.append(loss)\n\n    return theta, losses\n\n# Add bias column and run gradient descent\nX_with_bias = np.c_[np.ones(len(X_reduced)), X_reduced]\ntheta_gd, losses = gradient_descent(X_with_bias, y_train,\n                                     learning_rate=0.1,\n                                     n_iterations=500)\n</code></pre>"},{"location":"chapters/09-machine-learning-foundations/#diagram-ml-pipeline-workflow","title":"Diagram: ML Pipeline Workflow","text":"<p>Run the ML Pipeline Workflow Fullscreen</p> Machine Learning Pipeline Workflow <p>Type: workflow</p> <p>Bloom Taxonomy Level: Create</p> <p>Learning Objective: Understand the complete ML pipeline from raw data to trained model</p> <p>Visual style: Flowchart with processing stages</p> <p>Steps: 1. Start: \"Raw Data\"    Hover text: \"Original features, possibly different scales and units\"</p> <ol> <li>Process: \"Standardization\"    Hover text: \"Transform to zero mean, unit variance\"</li> <li>Input: Raw features X</li> <li> <p>Output: Standardized Z</p> </li> <li> <p>Process: \"PCA (optional)\"    Hover text: \"Reduce dimensionality while preserving variance\"</p> </li> <li>Input: Standardized data Z</li> <li>Output: Reduced data (k dimensions)</li> <li> <p>Decision: Scree plot analysis</p> </li> <li> <p>Process: \"Train/Test Split\"    Hover text: \"Hold out data for evaluation\"</p> </li> <li> <p>Process: \"Model Selection\"    Hover text: \"Choose algorithm and hyperparameters\"</p> </li> <li>Branch A: OLS (no regularization)</li> <li>Branch B: Ridge (L2)</li> <li> <p>Branch C: Lasso (L1)</p> </li> <li> <p>Process: \"Optimization\"    Hover text: \"Find optimal parameters\"</p> </li> <li>Closed-form (Ridge) or</li> <li> <p>Gradient descent (Lasso, Neural Networks)</p> </li> <li> <p>Process: \"Evaluation\"    Hover text: \"Assess on test set\"</p> </li> <li> <p>Metrics: MSE, R\u00b2, etc.</p> </li> <li> <p>End: \"Trained Model\"</p> </li> </ol> <p>Color coding: - Blue: Data processing - Green: Modeling - Orange: Optimization - Purple: Evaluation</p> <p>Interactive: - Click nodes to see code examples - Hover for detailed explanations</p> <p>Implementation: D3.js or Mermaid.js</p>"},{"location":"chapters/09-machine-learning-foundations/#summary_1","title":"Summary","text":"<p>This chapter connected linear algebra to machine learning:</p> <p>Data Representation:</p> <ul> <li>Feature vectors represent samples as \\(d\\)-dimensional vectors</li> <li>Data matrices organize \\(n\\) samples as rows, \\(d\\) features as columns</li> <li>Consistent conventions are crucial for correct matrix operations</li> </ul> <p>Statistical Foundations:</p> <ul> <li>Standardization ensures comparable scales and improves algorithm performance</li> <li>Covariance matrices capture feature relationships: \\(\\Sigma = \\frac{1}{n-1}\\tilde{X}^T\\tilde{X}\\)</li> <li>Correlation matrices are standardized covariances with values in \\([-1, 1]\\)</li> </ul> <p>Dimensionality Reduction:</p> <ul> <li>PCA finds directions of maximum variance via eigendecomposition</li> <li>Principal components are eigenvectors of the covariance matrix</li> <li>Scree plots help choose the number of components to retain</li> <li>Use SVD for numerical stability in practice</li> </ul> <p>Regression:</p> <ul> <li>Linear regression minimizes squared error: \\(J = \\|X\\boldsymbol{\\theta} - \\mathbf{y}\\|^2\\)</li> <li>Design matrices incorporate the bias term</li> <li>Ridge (L2) shrinks weights, handles multicollinearity</li> <li>Lasso (L1) produces sparse solutions for feature selection</li> </ul> <p>Optimization:</p> <ul> <li>Gradient vectors point in the direction of steepest increase</li> <li>Gradient descent iteratively minimizes: \\(\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta\\nabla J\\)</li> <li>Learning rate is critical: too small = slow, too large = diverge</li> <li>Batch, mini-batch, and stochastic variants trade off noise vs. efficiency</li> </ul> Self-Check: Why does PCA use the covariance matrix of centered data rather than the original data? <p>Centering (subtracting the mean) is essential because PCA seeks directions of maximum variance from the data's center of mass. Without centering, the first principal component would largely capture the offset from the origin rather than the true variation structure. The covariance matrix of centered data measures how features vary around their means, which is exactly what PCA needs to find the principal directions of spread.</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/","title":"Quiz: Machine Learning Foundations","text":"<p>Test your understanding of linear algebra concepts in machine learning.</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#1-in-machine-learning-a-feature-vector-represents","title":"1. In machine learning, a feature vector represents:","text":"<ol> <li>The output label of a data point</li> <li>The numerical attributes of a single data sample</li> <li>The weights of a neural network</li> <li>The training algorithm</li> </ol> Show Answer <p>The correct answer is B. A feature vector contains the numerical attributes (features) that describe a single data sample. For example, an image might be represented as a vector of pixel values.</p> <p>Concept Tested: Feature Vector</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#2-the-weight-matrix-in-a-linear-model-maps","title":"2. The weight matrix in a linear model maps:","text":"<ol> <li>Labels to features</li> <li>Inputs to outputs through linear transformation</li> <li>Features to labels through division</li> <li>Errors to gradients</li> </ol> Show Answer <p>The correct answer is B. The weight matrix performs a linear transformation from input features to outputs. In \\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\), the matrix \\(W\\) determines how inputs combine to produce outputs.</p> <p>Concept Tested: Weight Matrix</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#3-the-gradient-of-a-loss-function-with-respect-to-parameters-indicates","title":"3. The gradient of a loss function with respect to parameters indicates:","text":"<ol> <li>The minimum value of the loss</li> <li>The direction of steepest increase</li> <li>The optimal parameters</li> <li>The training data size</li> </ol> Show Answer <p>The correct answer is B. The gradient points in the direction of steepest increase of the loss function. To minimize loss, gradient descent moves in the opposite direction (negative gradient).</p> <p>Concept Tested: Gradient</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#4-a-cost-function-in-machine-learning-measures","title":"4. A cost function in machine learning measures:","text":"<ol> <li>The monetary expense of training</li> <li>The discrepancy between predictions and true values</li> <li>The number of features</li> <li>The model complexity</li> </ol> Show Answer <p>The correct answer is B. The cost (or loss) function measures how far the model's predictions deviate from the true values. Training aims to minimize this function by adjusting parameters.</p> <p>Concept Tested: Cost Function</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#5-in-gradient-descent-the-update-rule-is","title":"5. In gradient descent, the update rule is:","text":"<ol> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} + \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\alpha \\nabla L\\)</li> <li>\\(\\mathbf{w} \\leftarrow \\mathbf{w} / \\nabla L\\)</li> </ol> Show Answer <p>The correct answer is B. Gradient descent subtracts the gradient scaled by learning rate \\(\\alpha\\): \\(\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla L\\). This moves parameters in the direction that decreases the loss.</p> <p>Concept Tested: Gradient Descent</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#6-the-learning-rate-alpha-controls","title":"6. The learning rate \\(\\alpha\\) controls:","text":"<ol> <li>The size of the dataset</li> <li>The step size in parameter updates</li> <li>The number of iterations</li> <li>The model architecture</li> </ol> Show Answer <p>The correct answer is B. The learning rate determines how large a step to take in the direction of the negative gradient. Too large can cause divergence; too small leads to slow convergence.</p> <p>Concept Tested: Learning Rate</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#7-linear-regression-finds-parameters-that-minimize","title":"7. Linear regression finds parameters that minimize:","text":"<ol> <li>The number of features</li> <li>The sum of squared residuals</li> <li>The number of training samples</li> <li>The weight magnitudes</li> </ol> Show Answer <p>The correct answer is B. Linear regression minimizes the sum of squared residuals: \\(\\sum_i (y_i - \\hat{y}_i)^2\\), where \\(\\hat{y}_i\\) are predictions. This has a closed-form solution via the normal equations.</p> <p>Concept Tested: Linear Regression</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#8-regularization-adds-to-the-loss-function","title":"8. Regularization adds to the loss function:","text":"<ol> <li>More training data</li> <li>A penalty term based on weight magnitudes</li> <li>Additional features</li> <li>More layers</li> </ol> Show Answer <p>The correct answer is B. Regularization adds a penalty term (like \\(\\lambda\\|\\mathbf{w}\\|^2\\) for L2) to prevent overfitting. This encourages smaller weights and simpler models.</p> <p>Concept Tested: Regularization</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#9-a-hyperplane-in-classification","title":"9. A hyperplane in classification:","text":"<ol> <li>Maximizes the loss function</li> <li>Separates data points of different classes</li> <li>Is always vertical</li> <li>Contains all training points</li> </ol> Show Answer <p>The correct answer is B. A hyperplane is a linear decision boundary that separates data points belonging to different classes. In \\(n\\) dimensions, a hyperplane has \\(n-1\\) dimensions.</p> <p>Concept Tested: Hyperplane</p>"},{"location":"chapters/09-machine-learning-foundations/quiz/#10-the-bias-term-in-a-linear-model","title":"10. The bias term in a linear model:","text":"<ol> <li>Must always be zero</li> <li>Allows the model to fit data not passing through the origin</li> <li>Increases overfitting</li> <li>Is the same as the weight</li> </ol> Show Answer <p>The correct answer is B. The bias term \\(\\mathbf{b}\\) in \\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\) allows the model to represent functions that don't pass through the origin, providing a translation or offset.</p> <p>Concept Tested: Bias Term</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/","title":"Neural Networks and Deep Learning","text":""},{"location":"chapters/10-neural-networks-and-deep-learning/#summary","title":"Summary","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning, covering neurons, activation functions, weight matrices, forward propagation, and backpropagation. You will also learn about specialized architectures including convolutional layers, batch normalization, and tensor operations.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 26 concepts from the learning graph:</p> <ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"chapters/10-neural-networks-and-deep-learning/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 13: Image Processing (for convolution concepts)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#introduction","title":"Introduction","text":"<p>Deep learning has revolutionized artificial intelligence, powering breakthroughs in image recognition, natural language processing, and game playing. Behind the impressive applications lies elegant mathematics: neural networks are compositions of linear transformations (matrix multiplications) and elementwise nonlinearities.</p> <p>Understanding neural networks through the lens of linear algebra provides crucial insights:</p> <ul> <li>Why networks need nonlinear activation functions</li> <li>How gradients flow backward through matrix operations</li> <li>Why certain architectures are more effective than others</li> <li>How to debug and optimize network training</li> </ul> <p>This chapter develops neural networks from first principles, revealing the matrix operations at every step. You'll see that deep learning is not magic\u2014it's linear algebra with a few clever twists.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-perceptron-where-it-began","title":"The Perceptron: Where It Began","text":"<p>The perceptron, invented by Frank Rosenblatt in 1958, is the simplest neural network\u2014a single artificial neuron that performs binary classification.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#perceptron-model","title":"Perceptron Model","text":"<p>Given input vector \\(\\mathbf{x} \\in \\mathbb{R}^d\\), the perceptron computes:</p> <p>\\(y = \\text{sign}(\\mathbf{w}^T\\mathbf{x} + b)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{w} \\in \\mathbb{R}^d\\) is the weight vector</li> <li>\\(b \\in \\mathbb{R}\\) is the bias</li> <li>\\(\\text{sign}(z) = +1\\) if \\(z \\geq 0\\), else \\(-1\\)</li> </ul> <p>The perceptron defines a hyperplane \\(\\mathbf{w}^T\\mathbf{x} + b = 0\\) that separates the input space into two half-spaces.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>The weight vector \\(\\mathbf{w}\\) is perpendicular to the decision boundary, and the bias \\(b\\) controls the boundary's offset from the origin. Points on one side of the hyperplane are classified as \\(+1\\), points on the other side as \\(-1\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-perceptron-decision-boundary","title":"Diagram: Perceptron Decision Boundary","text":"<p>Run Fullscreen</p> Perceptron Decision Boundary Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how perceptron weights and bias define a linear decision boundary</p> <p>Visual elements: - 2D input space with data points (two classes, different colors) - Decision boundary line - Weight vector shown perpendicular to boundary - Shaded regions for each class - Misclassified points highlighted</p> <p>Interactive controls: - Draggable weight vector (changes boundary orientation) - Bias slider (shifts boundary) - \"Add Point\" mode to create custom datasets - \"Run Perceptron Learning\" button - Preset datasets: linearly separable, XOR (not separable)</p> <p>Default parameters: - Linearly separable 2D dataset - Initial random weights - Canvas: responsive</p> <p>Behavior: - Real-time boundary update as weights change - Show classification accuracy - Highlight that XOR cannot be solved - Animate perceptron learning algorithm steps - Display weight update rule</p> <p>Implementation: p5.js with interactive geometry</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#limitations","title":"Limitations","text":"<p>The perceptron can only learn linearly separable functions. The famous XOR problem demonstrated that a single perceptron cannot compute XOR\u2014motivating multilayer networks.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-neuron-model","title":"The Neuron Model","text":"<p>A neuron (or unit) generalizes the perceptron with a continuous activation function:</p> <p>\\(a = \\sigma(\\mathbf{w}^T\\mathbf{x} + b) = \\sigma(z)\\)</p> <p>where:</p> <ul> <li>\\(z = \\mathbf{w}^T\\mathbf{x} + b\\) is the pre-activation (weighted sum)</li> <li>\\(\\sigma\\) is the activation function</li> <li>\\(a\\) is the activation (output)</li> </ul> <p>This two-step process\u2014linear combination followed by nonlinearity\u2014is the fundamental building block of all neural networks.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#activation-functions","title":"Activation Functions","text":"<p>Activation functions introduce nonlinearity, enabling neural networks to learn complex patterns. Without them, any depth of linear layers would collapse to a single linear transformation.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<p>The ReLU activation is the most widely used in modern deep learning:</p> <p>\\(\\text{ReLU}(z) = \\max(0, z) = \\begin{cases} z &amp; \\text{if } z &gt; 0 \\\\ 0 &amp; \\text{if } z \\leq 0 \\end{cases}\\)</p> <p>Properties:</p> <ul> <li>Computationally efficient (just a threshold)</li> <li>Sparse activations (many zeros)</li> <li>Avoids vanishing gradients for positive inputs</li> <li>\"Dead ReLU\" problem: neurons can get stuck at zero</li> </ul> <p>Derivative: \\(\\frac{d}{dz}\\text{ReLU}(z) = \\begin{cases} 1 &amp; \\text{if } z &gt; 0 \\\\ 0 &amp; \\text{if } z &lt; 0 \\end{cases}\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#sigmoid","title":"Sigmoid","text":"<p>The sigmoid function squashes inputs to the range \\((0, 1)\\):</p> <p>\\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)</p> <p>Properties:</p> <ul> <li>Output interpretable as probability</li> <li>Smooth and differentiable everywhere</li> <li>Saturates for large \\(|z|\\) (vanishing gradients)</li> <li>Outputs not zero-centered</li> </ul> <p>Derivative: \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tanh","title":"Tanh","text":"<p>The tanh function maps inputs to \\((-1, 1)\\):</p> <p>\\(\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\\sigma(2z) - 1\\)</p> <p>Properties:</p> <ul> <li>Zero-centered outputs (often better than sigmoid)</li> <li>Still suffers from vanishing gradients at extremes</li> <li>Commonly used in RNNs and LSTMs</li> </ul> <p>Derivative: \\(\\tanh'(z) = 1 - \\tanh^2(z)\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#softmax","title":"Softmax","text":"<p>The softmax function converts a vector of scores to a probability distribution:</p> <p>\\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\\)</p> <p>where:</p> <ul> <li>Input: \\(\\mathbf{z} \\in \\mathbb{R}^K\\) (logits for \\(K\\) classes)</li> <li>Output: probability vector with \\(\\sum_i \\text{softmax}(\\mathbf{z})_i = 1\\)</li> </ul> <p>Properties:</p> <ul> <li>Used for multi-class classification output layer</li> <li>Exponential amplifies differences between scores</li> <li>Numerically stabilized by subtracting \\(\\max(\\mathbf{z})\\) before exponentiating</li> </ul> Activation Range Use Case Gradient Issue ReLU \\([0, \\infty)\\) Hidden layers Dead neurons Sigmoid \\((0, 1)\\) Binary output Vanishing Tanh \\((-1, 1)\\) Hidden layers, RNNs Vanishing Softmax \\((0, 1)^K\\), sums to 1 Multi-class output \u2014"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-activation-function-comparison","title":"Diagram: Activation Function Comparison","text":"<p>Run Fullscreen</p> Activation Functions Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare activation functions by their shape, range, and gradient behavior</p> <p>Visual elements: - Main panel: Graph showing selected activation function - Derivative overlay (toggleable) - Gradient magnitude heatmap for different input values - Side panel: Function properties summary</p> <p>Interactive controls: - Activation selector: ReLU, Sigmoid, Tanh, Leaky ReLU, Softplus - Input range slider - \"Show Derivative\" toggle - \"Compare All\" mode showing functions overlaid - Input value slider to trace along curve</p> <p>Default parameters: - ReLU selected - Input range: [-5, 5] - Canvas: responsive</p> <p>Behavior: - Real-time function and derivative plotting - Highlight saturation regions (near-zero gradient) - Show numerical values at traced point - For softmax: show 3-class probability bar chart - Display gradient flow implications</p> <p>Implementation: p5.js with function plotting</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#network-architecture-layers-and-matrices","title":"Network Architecture: Layers and Matrices","text":"<p>A neural network organizes neurons into layers, with each layer performing a matrix operation.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#weight-matrix-and-bias-vector","title":"Weight Matrix and Bias Vector","text":"<p>For a layer with \\(n_{in}\\) inputs and \\(n_{out}\\) outputs:</p> <ul> <li>Weight matrix \\(W \\in \\mathbb{R}^{n_{out} \\times n_{in}}\\) contains connection strengths</li> <li>Bias vector \\(\\mathbf{b} \\in \\mathbb{R}^{n_{out}}\\) provides learnable offsets</li> </ul> <p>The layer computes:</p> <p>\\(\\mathbf{z} = W\\mathbf{x} + \\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x} \\in \\mathbb{R}^{n_{in}}\\) is the input</li> <li>\\(\\mathbf{z} \\in \\mathbb{R}^{n_{out}}\\) is the pre-activation</li> </ul> <p>Each row of \\(W\\) contains the weights for one output neuron.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#neural-network-layer","title":"Neural Network Layer","text":"<p>A complete neural network layer combines the linear transformation with an activation:</p> <p>\\(\\mathbf{a} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\)</p> <p>where \\(\\sigma\\) is applied elementwise.</p> <p>For a batch of \\(m\\) inputs (stored as columns of \\(X \\in \\mathbb{R}^{n_{in} \\times m}\\)):</p> <p>\\(A = \\sigma(WX + \\mathbf{b}\\mathbf{1}^T)\\)</p> <p>where \\(\\mathbf{1}^T = [1, 1, \\ldots, 1]\\) broadcasts the bias to all samples.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#hidden-layers-and-deep-networks","title":"Hidden Layers and Deep Networks","text":"<ul> <li>Hidden layers are layers between input and output\u2014their activations are not directly observed</li> <li>A deep network has multiple hidden layers, enabling hierarchical feature learning</li> </ul> <p>A network with \\(L\\) layers computes:</p> <p>\\(\\mathbf{a}^{[0]} = \\mathbf{x}\\) (input)</p> <p>\\(\\mathbf{z}^{[l]} = W^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\\) for \\(l = 1, \\ldots, L\\)</p> <p>\\(\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})\\) for \\(l = 1, \\ldots, L\\)</p> <p>The output is \\(\\mathbf{a}^{[L]}\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-network-architecture-visualizer","title":"Diagram: Network Architecture Visualizer","text":"<p>Run Fullscreen</p> Neural Network Architecture Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize neural network architecture and understand the dimensions of weight matrices at each layer</p> <p>Visual elements: - Node-and-edge diagram of network - Layer labels with dimensions - Weight matrix dimensions displayed on connections - Activation function icons at each layer - Parameter count summary</p> <p>Interactive controls: - Layer count slider (1-5 hidden layers) - Neurons per layer sliders - Activation function selector per layer - \"Show Dimensions\" toggle - \"Show Weight Matrices\" toggle (expands to show matrix shapes) - Input/output dimension selectors</p> <p>Default parameters: - 2 hidden layers - Architecture: 4 \u2192 8 \u2192 8 \u2192 2 - ReLU hidden, Softmax output - Canvas: responsive</p> <p>Behavior: - Real-time architecture update - Calculate and display total parameters - Show data flow animation - Highlight one layer at a time with matrix equation - Demonstrate dimension matching requirements</p> <p>Implementation: p5.js with network diagram rendering</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#forward-propagation","title":"Forward Propagation","text":"<p>Forward propagation computes the network output by passing input through all layers sequentially.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#algorithm","title":"Algorithm","text":"<pre><code>Input: x, weights {W[l], b[l]} for l = 1,...,L\n\na[0] = x\nFor l = 1 to L:\n    z[l] = W[l] @ a[l-1] + b[l]\n    a[l] = activation[l](z[l])\n\nOutput: a[L]\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#matrix-form-for-batches","title":"Matrix Form for Batches","text":"<p>For a batch of \\(m\\) samples (columns of \\(X\\)):</p> <p>\\(A^{[0]} = X\\)</p> <p>\\(Z^{[l]} = W^{[l]}A^{[l-1]} + \\mathbf{b}^{[l]}\\mathbf{1}^T\\)</p> <p>\\(A^{[l]} = \\sigma^{[l]}(Z^{[l]})\\)</p> <p>The output \\(A^{[L]}\\) has shape \\(n_L \\times m\\).</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#why-nonlinearity-is-essential","title":"Why Nonlinearity Is Essential","text":"<p>Consider a 2-layer network without activations:</p> <p>\\(\\mathbf{y} = W^{[2]}(W^{[1]}\\mathbf{x}) = (W^{[2]}W^{[1]})\\mathbf{x} = W'\\mathbf{x}\\)</p> <p>The composition of linear functions is linear! Without nonlinear activations, deep networks would have no more expressive power than a single layer.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-forward-propagation-visualizer","title":"Diagram: Forward Propagation Visualizer","text":"<p>Run Fullscreen</p> Forward Propagation Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Trace data flow through a neural network and see the matrix operations at each layer</p> <p>Visual elements: - Network diagram with numerical values - Current layer highlighted - Matrix multiplication visualization - Activation function application shown - Values flowing through connections</p> <p>Interactive controls: - \"Next Step\" button - \"Auto Run\" with speed slider - Input vector editor - Weight matrix display (expandable) - \"Show Matrix Multiplication\" detail toggle</p> <p>Default parameters: - 3-layer network: 2 \u2192 3 \u2192 2 - Random initialized weights - Input: [1.0, 0.5] - Canvas: responsive</p> <p>Behavior: - Step through z = Wa + b computation - Show activation function application - Display intermediate values at each neuron - Animate data flow - Verify dimensions at each step</p> <p>Implementation: p5.js with matrix computation display</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#loss-functions","title":"Loss Functions","text":"<p>Loss functions measure how well the network's predictions match the targets, providing the signal for learning.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#common-loss-functions","title":"Common Loss Functions","text":"<p>Mean Squared Error (MSE) for regression:</p> <p>\\(\\mathcal{L}_{MSE} = \\frac{1}{m}\\sum_{i=1}^m \\|\\mathbf{y}_i - \\hat{\\mathbf{y}}_i\\|^2\\)</p> <p>Cross-Entropy Loss for classification:</p> <p>For binary classification with sigmoid output \\(\\hat{y} \\in (0, 1)\\):</p> <p>\\(\\mathcal{L}_{BCE} = -\\frac{1}{m}\\sum_{i=1}^m [y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]\\)</p> <p>For multi-class with softmax output \\(\\hat{\\mathbf{y}} \\in \\mathbb{R}^K\\):</p> <p>\\(\\mathcal{L}_{CE} = -\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K y_{ik} \\log(\\hat{y}_{ik})\\)</p> <p>where \\(y_{ik}\\) is the one-hot encoded true label.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#why-cross-entropy","title":"Why Cross-Entropy?","text":"<p>Cross-entropy loss has favorable gradient properties:</p> <ul> <li>Combined with softmax, the gradient simplifies to \\(\\hat{\\mathbf{y}} - \\mathbf{y}\\)</li> <li>Penalizes confident wrong predictions heavily</li> <li>Derived from maximum likelihood estimation</li> </ul> Loss Function Output Layer Use Case MSE Linear Regression Binary Cross-Entropy Sigmoid Binary classification Categorical Cross-Entropy Softmax Multi-class classification"},{"location":"chapters/10-neural-networks-and-deep-learning/#backpropagation","title":"Backpropagation","text":"<p>Backpropagation efficiently computes gradients of the loss with respect to all parameters using the chain rule.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#the-chain-rule-for-matrices","title":"The Chain Rule for Matrices","text":"<p>For composed functions \\(\\mathcal{L} = f(g(h(\\theta)))\\), the chain rule gives:</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{L}}{\\partial f} \\cdot \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial h} \\cdot \\frac{\\partial h}{\\partial \\theta}\\)</p> <p>In neural networks, this chain extends through all layers.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#chain-rule-matrices-jacobians","title":"Chain Rule Matrices (Jacobians)","text":"<p>For vector-valued functions, derivatives become Jacobian matrices. If \\(\\mathbf{z} = f(\\mathbf{a})\\) where \\(\\mathbf{z} \\in \\mathbb{R}^n\\) and \\(\\mathbf{a} \\in \\mathbb{R}^m\\):</p> <p>\\(\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{a}} = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial a_1} &amp; \\cdots &amp; \\frac{\\partial z_1}{\\partial a_m} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial z_n}{\\partial a_1} &amp; \\cdots &amp; \\frac{\\partial z_n}{\\partial a_m} \\end{bmatrix}\\)</p> <p>For scalar loss \\(\\mathcal{L}\\), we work with gradient vectors and propagate them backward.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#backpropagation-algorithm","title":"Backpropagation Algorithm","text":"<p>Starting from the output layer and moving backward:</p> <p>Output layer gradient:</p> <p>\\(\\delta^{[L]} = \\nabla_{\\mathbf{a}^{[L]}} \\mathcal{L} \\odot \\sigma'^{[L]}(\\mathbf{z}^{[L]})\\)</p> <p>Propagate backward: For \\(l = L-1, \\ldots, 1\\):</p> <p>\\(\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot \\sigma'^{[l]}(\\mathbf{z}^{[l]})\\)</p> <p>Parameter gradients:</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\delta^{[l]} (\\mathbf{a}^{[l-1]})^T\\)</p> <p>\\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{[l]}} = \\delta^{[l]}\\)</p> <p>where:</p> <ul> <li>\\(\\delta^{[l]}\\) is the error signal at layer \\(l\\)</li> <li>\\(\\odot\\) is elementwise multiplication</li> <li>\\(\\sigma'\\) is the derivative of the activation function</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-backpropagation-visualizer","title":"Diagram: Backpropagation Visualizer","text":"<p>Run Fullscreen</p> Backpropagation Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how gradients flow backward through a network via the chain rule</p> <p>Visual elements: - Network diagram with forward values - Backward arrows showing gradient flow - Current layer highlighted - Gradient values displayed at each node - Matrix transpose visualization for weight gradients</p> <p>Interactive controls: - \"Forward Pass\" button (must run first) - \"Backward Step\" button - \"Auto Backprop\" with speed slider - Target value input - Loss function selector (MSE, Cross-Entropy) - \"Show Chain Rule\" detail toggle</p> <p>Default parameters: - 3-layer network: 2 \u2192 3 \u2192 1 - MSE loss - Single training example - Canvas: responsive</p> <p>Behavior: - Compute and display output error - Show gradient flowing backward through each layer - Display \u03b4 values at each neuron - Show weight gradient computation - Verify gradient dimensions match weight dimensions</p> <p>Implementation: p5.js with gradient computation display</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#gradient-flow-and-vanishingexploding-gradients","title":"Gradient Flow and Vanishing/Exploding Gradients","text":"<p>Gradients are multiplied by weight matrices at each layer during backpropagation. If weights are:</p> <ul> <li>Too small: gradients shrink exponentially \u2192 vanishing gradients</li> <li>Too large: gradients grow exponentially \u2192 exploding gradients</li> </ul> <p>This motivates careful weight initialization and architectural choices like residual connections.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>Convolutional layers exploit spatial structure in images and sequences through weight sharing.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolution-kernel","title":"Convolution Kernel","text":"<p>A convolution kernel (or filter) is a small matrix of learnable weights that slides across the input:</p> <p>\\((\\mathbf{I} * \\mathbf{K})_{i,j} = \\sum_{m}\\sum_{n} I_{i+m, j+n} \\cdot K_{m,n}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{I}\\) is the input (e.g., image)</li> <li>\\(\\mathbf{K}\\) is the kernel (e.g., \\(3 \\times 3\\))</li> <li>The sum is over all kernel positions</li> </ul> <p>A \\(3 \\times 3\\) kernel has only 9 parameters but processes arbitrarily large inputs\u2014this is weight sharing.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#convolutional-layer","title":"Convolutional Layer","text":"<p>A convolutional layer applies multiple kernels to produce multiple output channels:</p> <p>\\(\\text{Output}_{c_{out}, i, j} = \\sum_{c_{in}} (\\mathbf{I}_{c_{in}} * \\mathbf{K}_{c_{out}, c_{in}})_{i,j} + b_{c_{out}}\\)</p> <p>where:</p> <ul> <li>\\(c_{in}\\) input channels, \\(c_{out}\\) output channels</li> <li>Each output channel has its own set of kernels (one per input channel)</li> <li>Total parameters: \\(c_{out} \\times c_{in} \\times k_h \\times k_w + c_{out}\\)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#stride-and-padding","title":"Stride and Padding","text":"<p>Stride controls how many pixels the kernel moves between applications:</p> <ul> <li>Stride 1: kernel moves one pixel at a time (full resolution)</li> <li>Stride 2: kernel moves two pixels (halves spatial dimensions)</li> </ul> <p>Padding adds zeros around the input border:</p> <ul> <li>\"Valid\" padding: no padding, output shrinks</li> <li>\"Same\" padding: pad to keep output size equal to input</li> </ul> <p>Output dimension formula:</p> <p>\\(\\text{out\\_size} = \\frac{\\text{in\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\)</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#pooling-layer","title":"Pooling Layer","text":"<p>Pooling layers downsample spatial dimensions:</p> <ul> <li>Max pooling: takes maximum value in each window</li> <li>Average pooling: takes average value in each window</li> </ul> <p>Common configuration: \\(2 \\times 2\\) pool with stride 2 halves each spatial dimension.</p> Operation Effect Parameters Convolution Feature extraction Kernel weights Stride &gt; 1 Downsampling None (hyperparameter) Padding Preserve dimensions None (hyperparameter) Pooling Downsample, invariance None"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-convolution-visualizer","title":"Diagram: Convolution Visualizer","text":"<p>Run Fullscreen</p> Convolution Operation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how convolution kernels slide across images and the effect of stride and padding</p> <p>Visual elements: - Input image (grayscale, small e.g., 7\u00d77) - Kernel overlay showing current position - Output feature map being built - Kernel weights displayed - Highlighted multiplication-addition operation</p> <p>Interactive controls: - Step through kernel positions manually - Kernel size selector (3\u00d73, 5\u00d75) - Stride selector (1, 2) - Padding selector (valid, same) - \"Animate Convolution\" button - Preset kernels: edge detection, blur, sharpen</p> <p>Default parameters: - 7\u00d77 grayscale input - 3\u00d73 kernel - Stride 1, valid padding - Edge detection kernel</p> <p>Behavior: - Show kernel sliding across input - Display element-wise multiplication - Show sum being placed in output - Output dimensions update with settings - Visualize different kernel effects on sample image</p> <p>Implementation: p5.js with image processing</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#normalization-techniques","title":"Normalization Techniques","text":"<p>Normalization stabilizes training by controlling activation distributions.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#batch-normalization","title":"Batch Normalization","text":"<p>Batch normalization normalizes activations across the batch dimension:</p> <p>For a mini-batch \\(\\{x_i\\}_{i=1}^m\\):</p> <p>\\(\\mu_B = \\frac{1}{m}\\sum_{i=1}^m x_i\\)</p> <p>\\(\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_B)^2\\)</p> <p>\\(\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\)</p> <p>\\(y_i = \\gamma \\hat{x}_i + \\beta\\)</p> <p>where:</p> <ul> <li>\\(\\gamma\\) and \\(\\beta\\) are learnable scale and shift parameters</li> <li>\\(\\epsilon\\) is a small constant for numerical stability</li> <li>During inference, running averages of \\(\\mu\\) and \\(\\sigma\\) are used</li> </ul> <p>Benefits:</p> <ul> <li>Enables higher learning rates</li> <li>Reduces sensitivity to initialization</li> <li>Acts as regularization (due to batch statistics noise)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#layer-normalization","title":"Layer Normalization","text":"<p>Layer normalization normalizes across the feature dimension instead of batch:</p> <p>\\(\\mu_l = \\frac{1}{d}\\sum_{j=1}^d x_j\\)</p> <p>\\(\\sigma_l^2 = \\frac{1}{d}\\sum_{j=1}^d (x_j - \\mu_l)^2\\)</p> <p>Benefits:</p> <ul> <li>Works with batch size 1</li> <li>Preferred in transformers and RNNs</li> <li>No difference between training and inference</li> </ul> Normalization Normalize Over Best For Batch Norm Batch dimension CNNs, large batches Layer Norm Feature dimension Transformers, RNNs Instance Norm Spatial dimensions Style transfer Group Norm Channel groups Small batch CNNs"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-normalization-comparison","title":"Diagram: Normalization Comparison","text":"<p>Run Fullscreen</p> Batch vs Layer Normalization <p>Type: infographic</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand the difference between normalization techniques by visualizing which dimensions they operate over</p> <p>Layout: Side-by-side comparison with 3D tensor diagrams</p> <p>Visual elements: - 3D tensor representation (batch \u00d7 channels \u00d7 spatial) - Highlighted region showing normalization scope - Before/after distribution plots - Learnable parameter display</p> <p>Sections: 1. Batch Normalization    - Normalize across batch (vertical slice)    - Show statistics computed per channel    - Training vs inference difference noted</p> <ol> <li>Layer Normalization</li> <li>Normalize across features (horizontal slice)</li> <li>Show statistics computed per sample</li> <li> <p>Same computation in training and inference</p> </li> <li> <p>Comparison table</p> </li> <li>Batch dependency</li> <li>Use cases</li> <li>Computational considerations</li> </ol> <p>Interactive elements: - Toggle to highlight normalization region - Slider to show effect on activation distribution - Animation of statistics computation</p> <p>Implementation: HTML/CSS/JavaScript with 3D tensor SVG</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensors-and-tensor-operations","title":"Tensors and Tensor Operations","text":"<p>Modern deep learning operates on tensors\u2014multi-dimensional arrays that generalize vectors and matrices.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensor-definition","title":"Tensor Definition","text":"<p>A tensor is a multi-dimensional array:</p> <ul> <li>0D tensor: scalar (e.g., \\(3.14\\))</li> <li>1D tensor: vector (e.g., \\(\\mathbf{x} \\in \\mathbb{R}^n\\))</li> <li>2D tensor: matrix (e.g., \\(A \\in \\mathbb{R}^{m \\times n}\\))</li> <li>3D tensor: e.g., RGB image \\(\\in \\mathbb{R}^{H \\times W \\times 3}\\)</li> <li>4D tensor: batch of images \\(\\in \\mathbb{R}^{B \\times H \\times W \\times C}\\)</li> </ul>"},{"location":"chapters/10-neural-networks-and-deep-learning/#common-tensor-operations","title":"Common Tensor Operations","text":"Operation Description Example Reshape Change shape, preserve elements \\((6,) \\to (2, 3)\\) Transpose Permute axes \\((B, H, W, C) \\to (B, C, H, W)\\) Broadcasting Expand dimensions for elementwise ops \\((3,) + (2, 3) \\to (2, 3)\\) Concatenate Join along axis Two \\((B, 10) \\to (B, 20)\\) Stack Create new axis Two \\((H, W) \\to (2, H, W)\\) Squeeze/Unsqueeze Remove/add size-1 dimensions \\((1, 3, 1) \\to (3,)\\)"},{"location":"chapters/10-neural-networks-and-deep-learning/#tensor-operations-in-neural-networks","title":"Tensor Operations in Neural Networks","text":"<p>Batched matrix multiplication:</p> <p>For batched inputs \\(A \\in \\mathbb{R}^{B \\times M \\times K}\\) and \\(B \\in \\mathbb{R}^{B \\times K \\times N}\\):</p> <p>\\((A @ B)_{b,i,j} = \\sum_k A_{b,i,k} \\cdot B_{b,k,j}\\)</p> <p>Result has shape \\(B \\times M \\times N\\).</p> <p>Einsum notation provides a powerful way to express tensor operations:</p> <pre><code># Batched matrix multiply\ntorch.einsum('bik,bkj-&gt;bij', A, B)\n\n# Attention: Q @ K^T\ntorch.einsum('bqd,bkd-&gt;bqk', Q, K)\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#diagram-tensor-shape-visualizer","title":"Diagram: Tensor Shape Visualizer","text":"<p>Run Fullscreen</p> Tensor Operations Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand tensor shapes and how common operations transform them</p> <p>Visual elements: - 3D/4D tensor visualization as nested boxes - Shape annotation on each dimension - Operation selector showing input \u2192 output shapes - Animated transformation between shapes</p> <p>Interactive controls: - Input tensor shape editor - Operation selector: reshape, transpose, squeeze, concatenate, broadcast - Target shape input (for reshape) - Axis selector (for operations that need it) - \"Apply Operation\" button</p> <p>Default parameters: - Input tensor shape: (2, 3, 4) - Operation: reshape to (6, 4) - Canvas: responsive</p> <p>Behavior: - Visualize tensor as nested rectangles - Show valid reshape targets - Animate element rearrangement - Error message for invalid operations - Display resulting shape</p> <p>Implementation: p5.js with 3D tensor rendering</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete neural network implementation using these concepts:</p> <pre><code>import numpy as np\n\nclass NeuralNetwork:\n    def __init__(self, layer_sizes):\n        \"\"\"Initialize network with given layer sizes.\"\"\"\n        self.L = len(layer_sizes) - 1  # number of layers\n        self.weights = []\n        self.biases = []\n\n        # Xavier initialization\n        for i in range(self.L):\n            n_in, n_out = layer_sizes[i], layer_sizes[i+1]\n            W = np.random.randn(n_out, n_in) * np.sqrt(2.0 / n_in)\n            b = np.zeros((n_out, 1))\n            self.weights.append(W)\n            self.biases.append(b)\n\n    def relu(self, z):\n        return np.maximum(0, z)\n\n    def relu_derivative(self, z):\n        return (z &gt; 0).astype(float)\n\n    def softmax(self, z):\n        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\n    def forward(self, X):\n        \"\"\"Forward propagation.\"\"\"\n        self.activations = [X]\n        self.pre_activations = []\n\n        A = X\n        for l in range(self.L):\n            Z = self.weights[l] @ A + self.biases[l]\n            self.pre_activations.append(Z)\n\n            if l == self.L - 1:  # output layer\n                A = self.softmax(Z)\n            else:  # hidden layers\n                A = self.relu(Z)\n            self.activations.append(A)\n\n        return A\n\n    def backward(self, Y):\n        \"\"\"Backpropagation with cross-entropy loss.\"\"\"\n        m = Y.shape[1]\n        self.weight_grads = []\n        self.bias_grads = []\n\n        # Output layer gradient (softmax + cross-entropy)\n        dA = self.activations[-1] - Y  # simplified gradient\n\n        for l in range(self.L - 1, -1, -1):\n            if l == self.L - 1:\n                dZ = dA  # softmax-CE gradient\n            else:\n                dZ = dA * self.relu_derivative(self.pre_activations[l])\n\n            dW = (1/m) * dZ @ self.activations[l].T\n            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n            self.weight_grads.insert(0, dW)\n            self.bias_grads.insert(0, db)\n\n            if l &gt; 0:\n                dA = self.weights[l].T @ dZ\n\n    def update(self, learning_rate):\n        \"\"\"Gradient descent update.\"\"\"\n        for l in range(self.L):\n            self.weights[l] -= learning_rate * self.weight_grads[l]\n            self.biases[l] -= learning_rate * self.bias_grads[l]\n\n    def train(self, X, Y, epochs, learning_rate):\n        \"\"\"Training loop.\"\"\"\n        for epoch in range(epochs):\n            # Forward pass\n            predictions = self.forward(X)\n\n            # Compute loss\n            loss = -np.mean(Y * np.log(predictions + 1e-8))\n\n            # Backward pass\n            self.backward(Y)\n\n            # Update weights\n            self.update(learning_rate)\n\n            if epoch % 100 == 0:\n                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Example usage\nnet = NeuralNetwork([784, 128, 64, 10])  # MNIST-like\n</code></pre>"},{"location":"chapters/10-neural-networks-and-deep-learning/#summary_1","title":"Summary","text":"<p>This chapter revealed the linear algebra powering neural networks:</p> <p>Foundations:</p> <ul> <li>Perceptrons compute linear decision boundaries: \\(y = \\text{sign}(\\mathbf{w}^T\\mathbf{x} + b)\\)</li> <li>Neurons add nonlinear activations: \\(a = \\sigma(\\mathbf{w}^T\\mathbf{x} + b)\\)</li> </ul> <p>Activation Functions:</p> <ul> <li>ReLU: \\(\\max(0, z)\\) \u2014 efficient, sparse, avoids vanishing gradients</li> <li>Sigmoid: \\((0, 1)\\) output for probabilities</li> <li>Softmax: probability distribution over \\(K\\) classes</li> </ul> <p>Network Architecture:</p> <ul> <li>Weight matrices \\(W^{[l]}\\) transform between layers</li> <li>Hidden layers enable hierarchical feature learning</li> <li>Deep networks compose multiple transformations</li> </ul> <p>Training:</p> <ul> <li>Forward propagation: sequential matrix operations through layers</li> <li>Loss functions: MSE for regression, cross-entropy for classification</li> <li>Backpropagation: chain rule computes gradients via matrix transposes</li> </ul> <p>Convolutional Networks:</p> <ul> <li>Kernels slide across spatial dimensions with weight sharing</li> <li>Stride and padding control output dimensions</li> <li>Pooling provides downsampling and translation invariance</li> </ul> <p>Normalization and Tensors:</p> <ul> <li>Batch normalization: normalize across batch dimension</li> <li>Layer normalization: normalize across feature dimension</li> <li>Tensors: multi-dimensional arrays with reshape, transpose, broadcast operations</li> </ul> Self-Check: Why does backpropagation use the transpose of the weight matrix when propagating gradients? <p>When propagating gradients backward from layer \\(l+1\\) to layer \\(l\\), we need to compute how changes in \\(\\mathbf{a}^{[l]}\\) affect the loss. During forward propagation, we computed \\(\\mathbf{z}^{[l+1]} = W^{[l+1]}\\mathbf{a}^{[l]}\\). The Jacobian \\(\\frac{\\partial \\mathbf{z}^{[l+1]}}{\\partial \\mathbf{a}^{[l]}} = W^{[l+1]}\\). By the chain rule, the gradient with respect to \\(\\mathbf{a}^{[l]}\\) is \\((W^{[l+1]})^T \\delta^{[l+1]}\\), where the transpose distributes the error from each output neuron back to the input neurons that contributed to it.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/","title":"Quiz: Neural Networks and Deep Learning","text":"<p>Test your understanding of neural network architecture and deep learning concepts.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#1-a-fully-connected-dense-layer-computes","title":"1. A fully connected (dense) layer computes:","text":"<ol> <li>Element-wise multiplication only</li> <li>\\(\\mathbf{y} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\) with linear transform and activation</li> <li>Convolution of input with a kernel</li> <li>Max pooling of input values</li> </ol> Show Answer <p>The correct answer is B. A dense layer performs a linear transformation followed by a nonlinear activation: \\(\\mathbf{y} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\), where \\(W\\) is the weight matrix, \\(\\mathbf{b}\\) is the bias, and \\(\\sigma\\) is the activation function.</p> <p>Concept Tested: Dense Layer</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#2-the-purpose-of-an-activation-function-is-to","title":"2. The purpose of an activation function is to:","text":"<ol> <li>Initialize weights randomly</li> <li>Introduce nonlinearity into the network</li> <li>Reduce the number of parameters</li> <li>Normalize input data</li> </ol> Show Answer <p>The correct answer is B. Activation functions introduce nonlinearity, enabling neural networks to learn complex patterns. Without them, stacking layers would just produce another linear transformation.</p> <p>Concept Tested: Activation Function</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#3-the-relu-activation-function-is-defined-as","title":"3. The ReLU activation function is defined as:","text":"<ol> <li>\\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\)</li> <li>\\(\\sigma(x) = \\max(0, x)\\)</li> <li>\\(\\sigma(x) = \\tanh(x)\\)</li> <li>\\(\\sigma(x) = x^2\\)</li> </ol> Show Answer <p>The correct answer is B. ReLU (Rectified Linear Unit) is defined as \\(\\max(0, x)\\)\u2014it outputs \\(x\\) for positive inputs and 0 for negative inputs. It's computationally efficient and helps mitigate vanishing gradients.</p> <p>Concept Tested: ReLU Activation</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#4-backpropagation-computes","title":"4. Backpropagation computes:","text":"<ol> <li>The forward pass predictions</li> <li>Gradients of the loss with respect to all parameters</li> <li>The optimal learning rate</li> <li>The activation function values</li> </ol> Show Answer <p>The correct answer is B. Backpropagation efficiently computes gradients of the loss with respect to all network parameters using the chain rule, enabling gradient-based optimization.</p> <p>Concept Tested: Backpropagation</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#5-the-softmax-function","title":"5. The softmax function:","text":"<ol> <li>Sets all outputs to zero</li> <li>Converts scores to probabilities that sum to 1</li> <li>Applies ReLU to each element</li> <li>Computes the maximum value</li> </ol> Show Answer <p>The correct answer is B. Softmax converts a vector of real-valued scores to a probability distribution: \\(\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\\). All outputs are positive and sum to 1.</p> <p>Concept Tested: Softmax Function</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#6-a-tensor-in-deep-learning-is","title":"6. A tensor in deep learning is:","text":"<ol> <li>Always a 2D matrix</li> <li>A multidimensional array of numbers</li> <li>A type of activation function</li> <li>A loss function</li> </ol> Show Answer <p>The correct answer is B. A tensor is a multidimensional array\u2014a generalization of scalars (0D), vectors (1D), and matrices (2D) to higher dimensions. Tensors represent data and parameters in neural networks.</p> <p>Concept Tested: Tensor</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#7-cross-entropy-loss-is-commonly-used-for","title":"7. Cross-entropy loss is commonly used for:","text":"<ol> <li>Regression problems</li> <li>Classification problems with probability outputs</li> <li>Dimensionality reduction</li> <li>Feature extraction</li> </ol> Show Answer <p>The correct answer is B. Cross-entropy loss measures the difference between predicted probability distributions and true labels. It's the standard loss for classification, especially with softmax outputs.</p> <p>Concept Tested: Cross-Entropy Loss</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#8-batch-normalization","title":"8. Batch normalization:","text":"<ol> <li>Increases batch size automatically</li> <li>Normalizes layer inputs to have zero mean and unit variance</li> <li>Removes all biases from the network</li> <li>Converts images to grayscale</li> </ol> Show Answer <p>The correct answer is B. Batch normalization normalizes activations across a mini-batch to zero mean and unit variance, then applies learned scale and shift. This stabilizes training and can act as regularization.</p> <p>Concept Tested: Batch Normalization</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#9-the-chain-rule-in-backpropagation-enables","title":"9. The chain rule in backpropagation enables:","text":"<ol> <li>Computing gradients through composed functions</li> <li>Increasing network depth automatically</li> <li>Selecting the best activation function</li> <li>Determining batch size</li> </ol> Show Answer <p>The correct answer is A. The chain rule allows computing gradients through composed functions: \\(\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}\\). This is essential for propagating gradients through all network layers.</p> <p>Concept Tested: Chain Rule</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/quiz/#10-dropout-during-training","title":"10. Dropout during training:","text":"<ol> <li>Removes entire layers permanently</li> <li>Randomly sets some neuron outputs to zero</li> <li>Doubles the learning rate</li> <li>Freezes all weights</li> </ol> Show Answer <p>The correct answer is B. Dropout randomly sets a fraction of neuron outputs to zero during training, preventing co-adaptation of neurons. This serves as regularization and improves generalization.</p> <p>Concept Tested: Dropout</p>"},{"location":"chapters/11-generative-ai-and-llms/","title":"Generative AI and Large Language Models","text":""},{"location":"chapters/11-generative-ai-and-llms/#summary","title":"Summary","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of transformers and large language models, including embedding spaces, attention mechanisms, query-key-value matrices, and multi-head attention. You will also learn about LoRA for efficient fine-tuning and latent space interpolation in generative models.</p>"},{"location":"chapters/11-generative-ai-and-llms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 7: Matrix Decompositions (for low-rank approximation)</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#introduction","title":"Introduction","text":"<p>Large language models like GPT and Claude have transformed artificial intelligence, demonstrating remarkable capabilities in understanding and generating human language. At their core, these models are sophisticated linear algebra engines\u2014they represent words as vectors, compute relationships through matrix operations, and generate output by manipulating high-dimensional spaces.</p> <p>This chapter reveals the mathematical machinery behind modern generative AI:</p> <ul> <li>Embeddings map discrete tokens to continuous vector spaces</li> <li>Attention mechanisms compute dynamic, context-dependent relationships</li> <li>Transformers stack these operations to build powerful models</li> <li>Low-rank adaptation enables efficient fine-tuning</li> </ul> <p>Understanding these foundations demystifies AI systems and enables you to reason about their capabilities and limitations.</p>"},{"location":"chapters/11-generative-ai-and-llms/#embeddings-from-symbols-to-vectors","title":"Embeddings: From Symbols to Vectors","text":"<p>Natural language consists of discrete symbols\u2014words, characters, or subword tokens. Neural networks require continuous numerical input. Embeddings bridge this gap.</p>"},{"location":"chapters/11-generative-ai-and-llms/#what-is-an-embedding","title":"What Is an Embedding?","text":"<p>An embedding is a learned mapping from discrete items to continuous vectors:</p> <p>\\(e: \\{1, 2, \\ldots, V\\} \\to \\mathbb{R}^d\\)</p> <p>where:</p> <ul> <li>\\(V\\) is the vocabulary size (e.g., 50,000 tokens)</li> <li>\\(d\\) is the embedding dimension (e.g., 768, 1024, or 4096)</li> </ul> <p>In practice, embeddings are stored as an embedding matrix \\(E \\in \\mathbb{R}^{V \\times d}\\):</p> <p>\\(\\mathbf{e}_i = E[i, :] \\quad \\text{(row } i \\text{ of } E \\text{)}\\)</p>"},{"location":"chapters/11-generative-ai-and-llms/#embedding-space","title":"Embedding Space","text":"<p>The embedding space is the \\(d\\)-dimensional vector space where embeddings live. This space has remarkable properties:</p> <ul> <li>Similar items are mapped to nearby points</li> <li>Relationships are encoded as directions</li> <li>Arithmetic operations have semantic meaning</li> </ul> <p>The famous Word2Vec example: \\(\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\\)</p> <p>This works because the embedding space encodes the \"royalty\" and \"gender\" concepts as roughly orthogonal directions.</p>"},{"location":"chapters/11-generative-ai-and-llms/#word-embeddings","title":"Word Embeddings","text":"<p>Word embeddings specifically represent words (or subword tokens) as vectors. They are learned from large text corpora by predicting context:</p> <ul> <li>Word2Vec: Predict surrounding words from center word (or vice versa)</li> <li>GloVe: Factorize word co-occurrence matrix</li> <li>Transformer embeddings: Learned end-to-end with the model</li> </ul> Method Approach Context Word2Vec (Skip-gram) Predict context from word Fixed window Word2Vec (CBOW) Predict word from context Fixed window GloVe Matrix factorization Global co-occurrence BERT/GPT End-to-end transformer Full sequence"},{"location":"chapters/11-generative-ai-and-llms/#diagram-embedding-space-visualizer","title":"Diagram: Embedding Space Visualizer","text":"<p>Run the Embedding Space Visualizer Fullscreen</p> Embedding Space Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize how words are positioned in embedding space and explore semantic relationships</p> <p>Visual elements: - 2D projection (t-SNE or PCA) of word embeddings - Clusters of related words (colors, animals, countries) - Vector arithmetic visualization - Distance/similarity measurements</p> <p>Interactive controls: - Word search to highlight specific embeddings - \"Find Similar\" to show nearest neighbors - Vector arithmetic input: word1 - word2 + word3 = ? - Projection method selector (PCA, t-SNE) - Zoom and pan controls</p> <p>Default parameters: - Pre-computed embeddings for 1000 common words - 2D PCA projection - Sample clusters highlighted - Canvas: responsive</p> <p>Behavior: - Hover to see word labels - Click word to show nearest neighbors - Enter vector arithmetic to see result - Animate projection computation - Show similarity scores</p> <p>Implementation: p5.js with pre-computed embedding data</p>"},{"location":"chapters/11-generative-ai-and-llms/#measuring-similarity","title":"Measuring Similarity","text":"<p>How do we determine if two embeddings represent similar concepts?</p>"},{"location":"chapters/11-generative-ai-and-llms/#semantic-similarity","title":"Semantic Similarity","text":"<p>Semantic similarity measures how related two concepts are in meaning. In embedding space, semantic similarity corresponds to geometric proximity.</p> <p>Two approaches:</p> <ol> <li>Euclidean distance: \\(d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2\\)</li> <li>Cosine similarity: \\(\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\)</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#cosine-similarity","title":"Cosine Similarity","text":"<p>Cosine similarity measures the angle between vectors, ignoring magnitude:</p> <p>\\(\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\|\\mathbf{u}\\|_2 \\|\\mathbf{v}\\|_2} = \\frac{\\sum_{i=1}^d u_i v_i}{\\sqrt{\\sum_{i=1}^d u_i^2} \\sqrt{\\sum_{i=1}^d v_i^2}}\\)</p> <p>Properties:</p> <ul> <li>Range: \\([-1, 1]\\)</li> <li>\\(+1\\): identical direction (most similar)</li> <li>\\(0\\): orthogonal (unrelated)</li> <li>\\(-1\\): opposite direction (most dissimilar)</li> </ul> <p>Why cosine over Euclidean?</p> <ul> <li>Invariant to vector magnitude (important for variable-length documents)</li> <li>Efficient computation in high dimensions</li> <li>More robust to the \"curse of dimensionality\"</li> </ul> <p>For normalized embeddings (\\(\\|\\mathbf{u}\\| = \\|\\mathbf{v}\\| = 1\\)):</p> <p>\\(\\text{cosine}(\\mathbf{u}, \\mathbf{v}) = \\mathbf{u}^T\\mathbf{v}\\)</p> <p>This is just the dot product\u2014extremely fast to compute.</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-similarity-comparison","title":"Diagram: Similarity Comparison","text":"<p>Run the Cosine vs Euclidean Similarity Fullscreen</p> Cosine vs Euclidean Similarity <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Compare cosine and Euclidean similarity measures and understand when each is appropriate</p> <p>Visual elements: - 2D plane with origin - Two adjustable vectors (draggable endpoints) - Angle arc between vectors - Distance line between endpoints - Similarity scores displayed</p> <p>Interactive controls: - Drag vector endpoints - \"Normalize Vectors\" toggle - Show cosine similarity value - Show Euclidean distance value - Preset examples: same direction different magnitude, orthogonal, similar angle</p> <p>Default parameters: - Two vectors in 2D - Both similarity measures shown - Canvas: responsive</p> <p>Behavior: - Real-time similarity updates - Show how cosine ignores magnitude - Demonstrate normalization effect - Highlight angle vs distance - Show formulas with current values</p> <p>Implementation: p5.js with vector geometry</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-attention-mechanism","title":"The Attention Mechanism","text":"<p>Attention is the key innovation enabling transformers to model long-range dependencies. Rather than processing sequences left-to-right, attention allows every position to directly interact with every other position.</p>"},{"location":"chapters/11-generative-ai-and-llms/#core-idea","title":"Core Idea","text":"<p>Given a sequence of vectors, attention computes a weighted combination where the weights depend on the content of the vectors themselves. This is \"content-based\" addressing\u2014the model decides what to focus on based on what it's looking at.</p>"},{"location":"chapters/11-generative-ai-and-llms/#self-attention","title":"Self-Attention","text":"<p>Self-attention computes attention within a single sequence. Each position attends to all positions in the same sequence (including itself):</p> <p>For input sequence \\(X = [\\mathbf{x}_1, \\ldots, \\mathbf{x}_n]^T \\in \\mathbb{R}^{n \\times d}\\):</p> <ol> <li>Each position queries: \"What should I pay attention to?\"</li> <li>Each position offers keys: \"Here's what I contain\"</li> <li>Each position provides values: \"Here's my information\"</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#cross-attention","title":"Cross-Attention","text":"<p>Cross-attention computes attention between two different sequences:</p> <ul> <li>Queries come from one sequence (e.g., decoder)</li> <li>Keys and values come from another sequence (e.g., encoder)</li> </ul> <p>This enables:</p> <ul> <li>Machine translation (attend to source while generating target)</li> <li>Image captioning (attend to image regions while generating text)</li> <li>Question answering (attend to context while generating answer)</li> </ul> Attention Type Queries From Keys/Values From Use Case Self-attention Sequence X Sequence X Language modeling Cross-attention Sequence Y Sequence X Translation, QA Masked self-attention Sequence X Past positions of X Autoregressive generation"},{"location":"chapters/11-generative-ai-and-llms/#query-key-value-matrices","title":"Query, Key, Value Matrices","text":"<p>The attention mechanism is implemented using three learned linear projections.</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-qkv-framework","title":"The QKV Framework","text":"<p>For input \\(X \\in \\mathbb{R}^{n \\times d_{model}}\\):</p> <p>Query Matrix:</p> <p>\\(Q = XW^Q\\)</p> <p>where \\(W^Q \\in \\mathbb{R}^{d_{model} \\times d_k}\\)</p> <p>Key Matrix:</p> <p>\\(K = XW^K\\)</p> <p>where \\(W^K \\in \\mathbb{R}^{d_{model} \\times d_k}\\)</p> <p>Value Matrix:</p> <p>\\(V = XW^V\\)</p> <p>where \\(W^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\)</p> <p>Resulting shapes:</p> <ul> <li>\\(Q \\in \\mathbb{R}^{n \\times d_k}\\) \u2014 one query per position</li> <li>\\(K \\in \\mathbb{R}^{n \\times d_k}\\) \u2014 one key per position</li> <li>\\(V \\in \\mathbb{R}^{n \\times d_v}\\) \u2014 one value per position</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#intuition","title":"Intuition","text":"<p>Think of attention as a soft dictionary lookup:</p> <ul> <li>Query: \"I'm looking for information about X\"</li> <li>Keys: \"I contain information about Y\"</li> <li>Values: \"Here's my actual content\"</li> </ul> <p>The query-key dot product measures compatibility. High compatibility means \"this key matches my query,\" so that value should contribute more to my output.</p>"},{"location":"chapters/11-generative-ai-and-llms/#attention-scores","title":"Attention Scores","text":"<p>Attention scores measure query-key compatibility:</p> <p>\\(S = QK^T \\in \\mathbb{R}^{n \\times n}\\)</p> <p>where:</p> <ul> <li>\\(S_{ij} = \\mathbf{q}_i^T \\mathbf{k}_j\\) is the score between position \\(i\\)'s query and position \\(j\\)'s key</li> <li>Higher score means stronger relevance</li> </ul> <p>The scores are scaled to prevent softmax saturation:</p> <p>\\(S_{scaled} = \\frac{QK^T}{\\sqrt{d_k}}\\)</p> <p>The \\(\\sqrt{d_k}\\) factor keeps the variance of dot products manageable as dimension increases.</p>"},{"location":"chapters/11-generative-ai-and-llms/#attention-weights","title":"Attention Weights","text":"<p>Attention weights are normalized scores (probabilities):</p> <p>\\(A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\\)</p> <p>where softmax is applied row-wise:</p> <p>\\(A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^n \\exp(S_{ik})}\\)</p> <p>Properties:</p> <ul> <li>Each row sums to 1: \\(\\sum_j A_{ij} = 1\\)</li> <li>All entries non-negative: \\(A_{ij} \\geq 0\\)</li> <li>Represents a probability distribution over positions</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#complete-attention-formula","title":"Complete Attention Formula","text":"<p>The attention output combines values weighted by attention:</p> <p>\\(\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\)</p> <p>Output shape: \\(n \\times d_v\\) \u2014 one output vector per position.</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-attention-mechanism-visualizer","title":"Diagram: Attention Mechanism Visualizer","text":"<p>Run the Attention Mechanism Visualizer Fullscreen</p> Attention Mechanism Step-by-Step <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how attention computes weighted combinations through QKV projections</p> <p>Visual elements: - Input sequence (tokens with embeddings) - Q, K, V projection matrices - Attention score matrix (heatmap) - Softmax attention weights - Output vectors as weighted sums</p> <p>Interactive controls: - Input sequence editor (3-5 tokens) - Step-through: \"Project Q\", \"Project K\", \"Project V\", \"Compute Scores\", \"Softmax\", \"Weighted Sum\" - Query position selector (which position to visualize) - \"Show All Attention Heads\" toggle - Matrix dimension display</p> <p>Default parameters: - 4-token sequence - d_model = 8, d_k = d_v = 4 - Single attention head - Canvas: responsive</p> <p>Behavior: - Animate each projection step - Show attention score computation - Visualize softmax normalization - Demonstrate weighted sum of values - Highlight which positions attend to which</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#multi-head-attention","title":"Multi-Head Attention","text":"<p>Multi-head attention runs multiple attention operations in parallel, each learning different relationship patterns.</p>"},{"location":"chapters/11-generative-ai-and-llms/#why-multiple-heads","title":"Why Multiple Heads?","text":"<p>A single attention head might focus on one type of relationship (e.g., syntactic). Multiple heads can capture diverse patterns:</p> <ul> <li>Head 1: Subject-verb agreement</li> <li>Head 2: Coreference (pronouns to antecedents)</li> <li>Head 3: Positional patterns</li> <li>Head 4: Semantic similarity</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#multi-head-computation","title":"Multi-Head Computation","text":"<p>For \\(h\\) attention heads:</p> <p>\\(\\text{head}_i = \\text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\\)</p> <p>where each head has its own projection matrices.</p> <p>Heads are concatenated and projected:</p> <p>\\(\\text{MultiHead}(X) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\\)</p> <p>where \\(W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}\\) is the output projection.</p>"},{"location":"chapters/11-generative-ai-and-llms/#dimension-management","title":"Dimension Management","text":"<p>Typical configuration (e.g., \\(d_{model} = 512\\), \\(h = 8\\)):</p> <ul> <li>\\(d_k = d_v = d_{model}/h = 64\\)</li> <li>Each head operates on lower dimension</li> <li>Total computation similar to single full-dimension head</li> <li>But captures richer patterns</li> </ul> Component Shape Input \\(X\\) \\(n \\times d_{model}\\) Per-head \\(W^Q, W^K\\) \\(d_{model} \\times d_k\\) Per-head \\(W^V\\) \\(d_{model} \\times d_v\\) Per-head output \\(n \\times d_v\\) Concatenated \\(n \\times hd_v\\) After \\(W^O\\) \\(n \\times d_{model}\\)"},{"location":"chapters/11-generative-ai-and-llms/#diagram-multi-head-attention-visualizer","title":"Diagram: Multi-Head Attention Visualizer","text":"<p>Run the Multi-Head Attention Visualizer Fullscreen</p> Multi-Head Attention Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Analyze</p> <p>Learning Objective: Understand how multiple attention heads capture different patterns and combine their outputs</p> <p>Visual elements: - Parallel attention head diagrams - Per-head attention weight heatmaps - Concatenation visualization - Output projection - Comparison of what each head attends to</p> <p>Interactive controls: - Number of heads slider (1-8) - Input sequence editor - Head selector to highlight individual heads - \"Compare Heads\" mode - \"Show Learned Patterns\" toggle</p> <p>Default parameters: - 4 attention heads - 5-token sequence - Pre-trained attention patterns - Canvas: responsive</p> <p>Behavior: - Show each head's attention independently - Visualize concatenation step - Demonstrate different heads learning different patterns - Compare head outputs before/after combination - Display dimension flow through computation</p> <p>Implementation: p5.js with parallel visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-transformer-architecture","title":"The Transformer Architecture","text":"<p>The transformer stacks attention layers with feedforward networks to build powerful sequence models.</p>"},{"location":"chapters/11-generative-ai-and-llms/#transformer-block","title":"Transformer Block","text":"<p>A single transformer block contains:</p> <ol> <li>Multi-head self-attention (with residual connection and layer norm)</li> <li>Feedforward network (with residual connection and layer norm)</li> </ol> <pre><code>Input x\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nMultiHeadAttn          \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nFeedForward            \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\nOutput\n</code></pre>"},{"location":"chapters/11-generative-ai-and-llms/#residual-connections","title":"Residual Connections","text":"<p>Residual connections add the input to the output:</p> <p>\\(\\text{output} = x + \\text{SubLayer}(x)\\)</p> <p>Benefits:</p> <ul> <li>Enable gradient flow through deep networks</li> <li>Allow layers to learn \"refinements\" rather than full transformations</li> <li>Stabilize training</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#layer-normalization","title":"Layer Normalization","text":"<p>Layer norm normalizes across features (not batch):</p> <p>\\(\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta\\)</p> <p>Applied before or after each sublayer (pre-norm vs post-norm).</p>"},{"location":"chapters/11-generative-ai-and-llms/#position-encoding","title":"Position Encoding","text":"<p>Self-attention is permutation-invariant\u2014it treats input as a set, not a sequence. Position encodings inject positional information.</p> <p>Sinusoidal encoding (original transformer):</p> <p>\\(PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})\\)</p> <p>\\(PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})\\)</p> <p>where:</p> <ul> <li>\\(pos\\) is the position in the sequence</li> <li>\\(i\\) is the dimension index</li> </ul> <p>Properties:</p> <ul> <li>Unique encoding for each position</li> <li>Relative positions can be computed from absolute encodings</li> <li>Generalizes to longer sequences than seen during training</li> </ul> <p>Learned position embeddings (GPT, BERT):</p> <ul> <li>Add learnable position embedding matrix \\(P \\in \\mathbb{R}^{L_{max} \\times d_{model}}\\)</li> <li>\\(\\text{input} = \\text{token\\_embedding} + \\text{position\\_embedding}\\)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-transformer-architecture","title":"Diagram: Transformer Architecture","text":"<p>Run the Transformer Block Visualizer Fullscreen</p> Transformer Block Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Understand</p> <p>Learning Objective: Visualize data flow through a transformer block including attention, FFN, residuals, and layer norms</p> <p>Visual elements: - Block diagram of transformer layer - Data tensors at each stage (shape annotations) - Residual connection paths - Layer norm visualization - Stacking multiple blocks</p> <p>Interactive controls: - Number of blocks slider (1-6) - \"Step Through\" one operation at a time - \"Show Dimensions\" toggle - \"Highlight Residuals\" toggle - Input sequence length slider</p> <p>Default parameters: - 2 transformer blocks - Sequence length 4 - d_model = 512, d_ff = 2048 - Canvas: responsive</p> <p>Behavior: - Animate data flow through block - Show dimension changes at each stage - Visualize residual addition - Display parameter count per component - Compare pre-norm vs post-norm</p> <p>Implementation: p5.js with architecture diagram</p>"},{"location":"chapters/11-generative-ai-and-llms/#efficient-fine-tuning-with-lora","title":"Efficient Fine-Tuning with LoRA","text":"<p>Training all parameters of large models is expensive. LoRA (Low-Rank Adaptation) enables efficient fine-tuning.</p>"},{"location":"chapters/11-generative-ai-and-llms/#the-lora-idea","title":"The LoRA Idea","text":"<p>Instead of updating full weight matrices, LoRA adds trainable low-rank decomposition:</p> <p>\\(W' = W + \\Delta W = W + BA\\)</p> <p>where:</p> <ul> <li>\\(W \\in \\mathbb{R}^{d \\times k}\\) is the frozen pre-trained weight</li> <li>\\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) are trainable</li> <li>\\(r \\ll \\min(d, k)\\) is the rank (e.g., 4, 8, 16)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>For a weight matrix of size \\(d \\times k\\):</p> <ul> <li>Full fine-tuning: \\(dk\\) parameters</li> <li>LoRA: \\(r(d + k)\\) parameters</li> </ul> <p>Example: \\(d = k = 4096\\), \\(r = 8\\):</p> <ul> <li>Full: \\(16.7M\\) parameters per matrix</li> <li>LoRA: \\(65K\\) parameters per matrix (0.4%)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#why-low-rank-works","title":"Why Low-Rank Works","text":"<p>The update \\(\\Delta W\\) has rank at most \\(r\\). Research suggests that:</p> <ul> <li>Model adaptation often lies in a low-dimensional subspace</li> <li>The \"intrinsic dimension\" of fine-tuning is much smaller than parameter count</li> <li>Low-rank updates capture task-specific modifications</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#implementation","title":"Implementation","text":"<p>During fine-tuning:</p> <ol> <li>Freeze all original parameters \\(W\\)</li> <li>Initialize \\(A\\) with small random values, \\(B\\) with zeros</li> <li>Train only \\(A\\) and \\(B\\)</li> <li>Forward pass: \\(h = (W + BA)x = Wx + BAx\\)</li> </ol> <p>For inference, can merge: \\(W' = W + BA\\) (no added latency).</p>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-lora-visualization","title":"Diagram: LoRA Visualization","text":"<p>Run the LoRA Visualizer Fullscreen</p> LoRA Low-Rank Adaptation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand how LoRA approximates weight updates with low-rank matrices</p> <p>Visual elements: - Original weight matrix W (large, frozen) - Low-rank factors A and B - Product BA visualization - Updated matrix W' = W + BA - Parameter count comparison</p> <p>Interactive controls: - Matrix dimensions d and k sliders - Rank r slider (1-32) - \"Show Decomposition\" toggle - \"Compare to Full Fine-tune\" toggle - Animation of forward pass</p> <p>Default parameters: - d = k = 64 (for visualization) - r = 4 - Canvas: responsive</p> <p>Behavior: - Show A and B shapes change with r - Display parameter savings percentage - Visualize low-rank approximation quality - Compare BA to random full-rank update - Show merged weight option</p> <p>Implementation: p5.js with matrix visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#latent-spaces-in-generative-models","title":"Latent Spaces in Generative Models","text":"<p>Generative models learn to map between data and abstract latent spaces.</p>"},{"location":"chapters/11-generative-ai-and-llms/#what-is-a-latent-space","title":"What Is a Latent Space?","text":"<p>A latent space is a compressed, continuous representation where:</p> <ul> <li>Each point corresponds to a potential data sample</li> <li>Similar points produce similar outputs</li> <li>The space is typically lower-dimensional than data space</li> </ul> <p>For images:</p> <ul> <li>Data space: \\(\\mathbb{R}^{H \\times W \\times 3}\\) (millions of pixels)</li> <li>Latent space: \\(\\mathbb{R}^{512}\\) (compressed representation)</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#structure-in-latent-space","title":"Structure in Latent Space","text":"<p>Well-trained generative models learn latent spaces with meaningful structure:</p> <ul> <li>Clusters: Similar items group together</li> <li>Directions: Moving along certain directions changes specific attributes</li> <li>Arithmetic: Vector operations have semantic meaning</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#interpolation","title":"Interpolation","text":"<p>Interpolation generates intermediate points between two latent vectors:</p> <p>Linear interpolation:</p> <p>\\(\\mathbf{z}(t) = (1-t)\\mathbf{z}_1 + t\\mathbf{z}_2, \\quad t \\in [0, 1]\\)</p> <p>Spherical interpolation (slerp): Better for normalized latent spaces:</p> <p>\\(\\mathbf{z}(t) = \\frac{\\sin((1-t)\\theta)}{\\sin\\theta}\\mathbf{z}_1 + \\frac{\\sin(t\\theta)}{\\sin\\theta}\\mathbf{z}_2\\)</p> <p>where \\(\\theta = \\arccos(\\mathbf{z}_1 \\cdot \\mathbf{z}_2)\\) for normalized vectors.</p>"},{"location":"chapters/11-generative-ai-and-llms/#applications","title":"Applications","text":"<ul> <li>Image morphing: Smooth transition between faces</li> <li>Style mixing: Combine attributes of different samples</li> <li>Attribute editing: Move along discovered directions</li> <li>Data augmentation: Generate novel training samples</li> </ul>"},{"location":"chapters/11-generative-ai-and-llms/#diagram-latent-space-interpolation","title":"Diagram: Latent Space Interpolation","text":"<p>Run the Latent Space Interpolation Visualizer Fullscreen</p> Latent Space Interpolation Visualizer <p>Type: microsim</p> <p>Bloom Taxonomy Level: Apply</p> <p>Learning Objective: Understand latent space structure through interpolation and vector arithmetic</p> <p>Visual elements: - 2D latent space projection - Two endpoint vectors (selectable) - Interpolation path (linear vs spherical) - Generated samples along path - Vector arithmetic demonstration</p> <p>Interactive controls: - Select two points in latent space - Interpolation method: linear vs slerp - Number of intermediate steps slider - \"Show Path\" toggle - \"Try Vector Arithmetic\" mode</p> <p>Default parameters: - Pre-computed latent points for simple shapes - Linear interpolation - 5 intermediate steps - Canvas: responsive</p> <p>Behavior: - Visualize interpolation path - Show how generated output changes along path - Compare linear vs spherical interpolation - Demonstrate attribute manipulation via vector arithmetic - Show distance metrics along path</p> <p>Implementation: p5.js with latent space visualization</p>"},{"location":"chapters/11-generative-ai-and-llms/#practical-implementation","title":"Practical Implementation","text":"<p>Here's a simplified attention implementation:</p> <pre><code>import numpy as np\n\ndef softmax(x, axis=-1):\n    \"\"\"Numerically stable softmax.\"\"\"\n    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\ndef scaled_dot_product_attention(Q, K, V, mask=None):\n    \"\"\"\n    Compute scaled dot-product attention.\n\n    Args:\n        Q: Queries, shape (n, d_k)\n        K: Keys, shape (m, d_k)\n        V: Values, shape (m, d_v)\n        mask: Optional attention mask\n\n    Returns:\n        Attention output, shape (n, d_v)\n    \"\"\"\n    d_k = Q.shape[-1]\n\n    # Compute attention scores\n    scores = Q @ K.T / np.sqrt(d_k)  # (n, m)\n\n    # Apply mask if provided (for causal attention)\n    if mask is not None:\n        scores = np.where(mask, scores, -1e9)\n\n    # Softmax to get attention weights\n    attention_weights = softmax(scores, axis=-1)  # (n, m)\n\n    # Weighted sum of values\n    output = attention_weights @ V  # (n, d_v)\n\n    return output, attention_weights\n\ndef multi_head_attention(X, W_Q, W_K, W_V, W_O, num_heads):\n    \"\"\"\n    Multi-head attention.\n\n    Args:\n        X: Input, shape (n, d_model)\n        W_Q, W_K, W_V: Per-head projection weights\n        W_O: Output projection weight\n        num_heads: Number of attention heads\n    \"\"\"\n    n, d_model = X.shape\n    d_k = d_model // num_heads\n\n    heads = []\n    for h in range(num_heads):\n        Q = X @ W_Q[h]\n        K = X @ W_K[h]\n        V = X @ W_V[h]\n        head_output, _ = scaled_dot_product_attention(Q, K, V)\n        heads.append(head_output)\n\n    # Concatenate heads\n    concat = np.concatenate(heads, axis=-1)\n\n    # Output projection\n    output = concat @ W_O\n\n    return output\n\ndef cosine_similarity(u, v):\n    \"\"\"Compute cosine similarity between vectors.\"\"\"\n    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n\ndef lora_forward(x, W, A, B):\n    \"\"\"Forward pass with LoRA adaptation.\"\"\"\n    # Original path + low-rank update\n    return x @ W.T + x @ A.T @ B.T\n\n# Example: Embedding lookup\ndef embed(token_ids, embedding_matrix):\n    \"\"\"Look up embeddings for token IDs.\"\"\"\n    return embedding_matrix[token_ids]\n\n# Example: Position encoding\ndef positional_encoding(seq_len, d_model):\n    \"\"\"Generate sinusoidal position encodings.\"\"\"\n    positions = np.arange(seq_len)[:, np.newaxis]\n    dims = np.arange(d_model)[np.newaxis, :]\n\n    angles = positions / np.power(10000, (2 * (dims // 2)) / d_model)\n\n    # Apply sin to even, cos to odd\n    pe = np.zeros((seq_len, d_model))\n    pe[:, 0::2] = np.sin(angles[:, 0::2])\n    pe[:, 1::2] = np.cos(angles[:, 1::2])\n\n    return pe\n\n# Example usage\nseq_len, d_model = 10, 64\nX = np.random.randn(seq_len, d_model)\n\n# Add position encoding\npos_enc = positional_encoding(seq_len, d_model)\nX_with_pos = X + pos_enc\n\nprint(f\"Input shape: {X.shape}\")\nprint(f\"Position encoding shape: {pos_enc.shape}\")\n</code></pre>"},{"location":"chapters/11-generative-ai-and-llms/#summary_1","title":"Summary","text":"<p>This chapter explored the linear algebra foundations of modern generative AI:</p> <p>Embeddings:</p> <ul> <li>Embeddings map discrete tokens to continuous vectors</li> <li>Embedding spaces encode semantic relationships geometrically</li> <li>Cosine similarity measures semantic relatedness: \\(\\frac{\\mathbf{u}^T\\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\\)</li> </ul> <p>Attention Mechanism:</p> <ul> <li>Self-attention allows each position to attend to all others</li> <li>Query, Key, Value matrices create content-based addressing</li> <li>Attention scores \\(QK^T/\\sqrt{d_k}\\) measure query-key compatibility</li> <li>Attention weights are softmax-normalized scores</li> </ul> <p>Multi-Head and Transformers:</p> <ul> <li>Multi-head attention captures diverse relationship patterns</li> <li>Transformer blocks stack attention with feedforward layers</li> <li>Position encodings inject sequential order information</li> <li>Residual connections enable deep network training</li> </ul> <p>Efficient Adaptation:</p> <ul> <li>LoRA adds low-rank trainable matrices: \\(W' = W + BA\\)</li> <li>Reduces trainable parameters by 100-1000x</li> </ul> <p>Latent Spaces:</p> <ul> <li>Latent spaces provide compressed, continuous representations</li> <li>Interpolation generates smooth transitions between points</li> <li>Vector arithmetic enables semantic manipulation</li> </ul> Self-Check: Why does attention use \\(\\sqrt{d_k}\\) scaling in the score computation? <p>The dot product \\(QK^T\\) produces values with variance proportional to \\(d_k\\) (the key/query dimension). Without scaling, as \\(d_k\\) grows large, the dot products grow large, pushing softmax into regions of extreme gradients (near-zero for most values, near-one for the maximum). Dividing by \\(\\sqrt{d_k}\\) normalizes the variance to approximately 1, keeping softmax in a region where gradients flow well and the attention distribution isn't too peaked.</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/","title":"Quiz: Generative AI and Large Language Models","text":"<p>Test your understanding of embeddings, attention mechanisms, and transformer architecture.</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#1-an-embedding-maps","title":"1. An embedding maps:","text":"<ol> <li>Continuous values to discrete tokens</li> <li>Discrete tokens to continuous vectors</li> <li>Images to text</li> <li>Gradients to weights</li> </ol> Show Answer <p>The correct answer is B. An embedding maps discrete items (like words or tokens) to continuous vector representations. This enables neural networks to process symbolic data like text.</p> <p>Concept Tested: Embedding</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#2-cosine-similarity-between-two-vectors-measures","title":"2. Cosine similarity between two vectors measures:","text":"<ol> <li>Their Euclidean distance</li> <li>The angle between them (ignoring magnitude)</li> <li>Their element-wise product sum</li> <li>The difference in their norms</li> </ol> Show Answer <p>The correct answer is B. Cosine similarity measures the cosine of the angle between vectors: \\(\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\|\\|\\mathbf{v}\\|}\\). It ranges from -1 to 1 and ignores vector magnitudes.</p> <p>Concept Tested: Cosine Similarity</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#3-in-attention-mechanisms-the-query-key-dot-product-determines","title":"3. In attention mechanisms, the query-key dot product determines:","text":"<ol> <li>The size of the output</li> <li>How much each position attends to other positions</li> <li>The number of attention heads</li> <li>The embedding dimension</li> </ol> Show Answer <p>The correct answer is B. The query-key dot product computes attention scores that determine how much each position should focus on other positions. Higher scores mean stronger attention.</p> <p>Concept Tested: Attention Score</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#4-the-scaling-factor-sqrtd_k-in-attention-is-used-to","title":"4. The scaling factor \\(\\sqrt{d_k}\\) in attention is used to:","text":"<ol> <li>Increase the attention weights</li> <li>Prevent softmax saturation from large dot products</li> <li>Reduce the number of parameters</li> <li>Speed up training</li> </ol> Show Answer <p>The correct answer is B. Dividing by \\(\\sqrt{d_k}\\) prevents the dot products from becoming too large as dimension increases, which would push softmax into regions with very small gradients.</p> <p>Concept Tested: Scaled Dot-Product Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#5-multi-head-attention","title":"5. Multi-head attention:","text":"<ol> <li>Uses only one attention computation</li> <li>Runs multiple attention operations in parallel to capture different patterns</li> <li>Eliminates the need for queries and keys</li> <li>Replaces the transformer architecture</li> </ol> Show Answer <p>The correct answer is B. Multi-head attention runs several attention computations in parallel, each potentially learning different relationship patterns (syntactic, semantic, positional, etc.), then combines their outputs.</p> <p>Concept Tested: Multi-Head Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#6-self-attention-allows-each-position-to","title":"6. Self-attention allows each position to:","text":"<ol> <li>Only attend to itself</li> <li>Attend to all positions in the same sequence</li> <li>Only attend to previous positions</li> <li>Ignore all other positions</li> </ol> Show Answer <p>The correct answer is B. Self-attention allows each position in a sequence to attend to all other positions (including itself), enabling direct modeling of long-range dependencies.</p> <p>Concept Tested: Self-Attention</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#7-position-encoding-in-transformers-provides","title":"7. Position encoding in transformers provides:","text":"<ol> <li>Word meaning information</li> <li>Sequence order information since attention is permutation-invariant</li> <li>Grammar rules</li> <li>Vocabulary size</li> </ol> Show Answer <p>The correct answer is B. Since self-attention treats input as a set (permutation-invariant), position encodings inject information about the order of tokens in the sequence.</p> <p>Concept Tested: Position Encoding</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#8-lora-low-rank-adaptation-reduces-fine-tuning-cost-by","title":"8. LoRA (Low-Rank Adaptation) reduces fine-tuning cost by:","text":"<ol> <li>Removing all attention layers</li> <li>Adding trainable low-rank matrices instead of updating full weights</li> <li>Using smaller vocabulary</li> <li>Eliminating position encodings</li> </ol> Show Answer <p>The correct answer is B. LoRA keeps original weights frozen and adds small, trainable low-rank matrices (\\(A\\) and \\(B\\)) such that \\(W' = W + BA\\). This dramatically reduces the number of trainable parameters.</p> <p>Concept Tested: LoRA</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#9-in-a-latent-space-similar-items-typically","title":"9. In a latent space, similar items typically:","text":"<ol> <li>Have very different vector representations</li> <li>Are mapped to nearby points</li> <li>Have zero cosine similarity</li> <li>Require different embedding dimensions</li> </ol> Show Answer <p>The correct answer is B. A well-trained latent space maps semantically similar items to nearby points. This structure enables meaningful interpolation and arithmetic operations.</p> <p>Concept Tested: Latent Space</p>"},{"location":"chapters/11-generative-ai-and-llms/quiz/#10-the-value-matrix-in-attention-provides","title":"10. The Value matrix in attention provides:","text":"<ol> <li>The content to be aggregated based on attention weights</li> <li>The query for matching</li> <li>The key for compatibility</li> <li>The position information</li> </ol> Show Answer <p>The correct answer is A. The Value matrix contains the actual information to be retrieved. Attention weights (from Query-Key matching) determine how to combine Values to produce the output.</p> <p>Concept Tested: Value Matrix</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/","title":"Optimization and Learning Algorithms","text":""},{"location":"chapters/12-optimization-and-learning-algorithms/#summary","title":"Summary","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms. This chapter covers the Hessian matrix, convexity, Newton's method, and modern adaptive optimizers like Adam and RMSprop. You will also learn constrained optimization with Lagrange multipliers and KKT conditions.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 14 concepts from the learning graph:</p> <ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"chapters/12-optimization-and-learning-algorithms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 9: Machine Learning Foundations</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#introduction","title":"Introduction","text":"<p>Training machine learning models fundamentally requires solving optimization problems\u2014finding the parameters that minimize a loss function. From simple linear regression to complex deep neural networks, optimization algorithms determine both the quality and efficiency of learning. Linear algebra provides essential tools for understanding these algorithms:</p> <ul> <li>Gradients indicate the direction of steepest ascent</li> <li>Hessians capture curvature information</li> <li>Matrix operations enable efficient computation</li> <li>Eigenvalue analysis reveals optimization landscapes</li> </ul> <p>This chapter builds from foundational concepts like convexity through classical algorithms like Newton's method to modern adaptive optimizers used in deep learning.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#convexity-and-convex-functions","title":"Convexity and Convex Functions","text":"<p>Before diving into optimization algorithms, we need to understand the landscape we're optimizing over. Convexity is the most important property that makes optimization tractable.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#what-makes-a-set-convex","title":"What Makes a Set Convex?","text":"<p>A set \\(S\\) is convex if for any two points \\(\\mathbf{x}, \\mathbf{y} \\in S\\), the line segment connecting them lies entirely within \\(S\\):</p> <p>\\(\\lambda \\mathbf{x} + (1 - \\lambda) \\mathbf{y} \\in S \\quad \\text{for all } \\lambda \\in [0, 1]\\)</p> <p>Intuitively, a convex set has no \"dents\" or \"holes.\"</p> Set Type Examples Convex? Ball/Sphere interior \\(\\{\\mathbf{x} : \\|\\mathbf{x}\\| \\leq r\\}\\) Yes Hyperplane \\(\\{\\mathbf{x} : \\mathbf{a}^\\top \\mathbf{x} = b\\}\\) Yes Half-space \\(\\{\\mathbf{x} : \\mathbf{a}^\\top \\mathbf{x} \\leq b\\}\\) Yes Donut/Annulus \\(\\{\\mathbf{x} : r_1 \\leq \\|\\mathbf{x}\\| \\leq r_2\\}\\) No"},{"location":"chapters/12-optimization-and-learning-algorithms/#convex-functions","title":"Convex Functions","text":"<p>A function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\) is convex if its domain is a convex set and for all points in its domain:</p> <p>\\(f(\\lambda \\mathbf{x} + (1-\\lambda) \\mathbf{y}) \\leq \\lambda f(\\mathbf{x}) + (1-\\lambda) f(\\mathbf{y})\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x}, \\mathbf{y}\\) are any two points in the domain</li> <li>\\(\\lambda \\in [0, 1]\\) is a mixing coefficient</li> </ul> <p>Geometrically, the function lies below the chord connecting any two points on its graph\u2014it \"curves upward.\"</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-convex-function-visualizer","title":"Diagram: Convex Function Visualizer","text":"Convex Function Visualizer <p>Type: microsim</p> <p>Learning objective: Understand the geometric definition of convexity by visualizing the chord condition (Bloom: Understand)</p> <p>Visual elements: - 2D plot showing function curve f(x) - Two draggable points on the curve (x1, f(x1)) and (x2, f(x2)) - Line segment (chord) connecting the two points - Shaded region between chord and curve - Color indicator: green if convex condition satisfied, red otherwise</p> <p>Interactive controls: - Dropdown to select function: x\u00b2, |x|, x\u2074, x\u00b2 + sin(x), -x\u00b2 (non-convex) - Draggable points to adjust x1 and x2 positions - Slider for lambda (0 to 1) to show interpolation point - Display showing f(\u03bbx+(1-\u03bb)y) vs \u03bbf(x)+(1-\u03bb)f(y)</p> <p>Canvas layout: 700x500px with function plot area and value comparison</p> <p>Behavior: - As user drags points, chord updates in real-time - Lambda slider moves a point along chord and on curve to compare heights - Text displays the convexity inequality with current values - Red highlight appears when a non-convex function violates the condition</p> <p>Implementation: p5.js with interactive draggable elements</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#first-order-condition-for-convexity","title":"First-Order Condition for Convexity","text":"<p>For differentiable functions, convexity has an equivalent characterization using the gradient:</p> <p>\\(f(\\mathbf{y}) \\geq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top (\\mathbf{y} - \\mathbf{x})\\)</p> <p>This says the function always lies above its tangent hyperplane\u2014the linearization underestimates the function everywhere.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#second-order-condition-for-convexity","title":"Second-Order Condition for Convexity","text":"<p>For twice-differentiable functions, convexity is equivalent to the Hessian being positive semidefinite everywhere:</p> <p>\\(\\nabla^2 f(\\mathbf{x}) \\succeq 0 \\quad \\text{for all } \\mathbf{x}\\)</p> <p>This connects convexity to the eigenvalues of the Hessian matrix.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-hessian-matrix","title":"The Hessian Matrix","text":"<p>The Hessian matrix captures all second-order partial derivative information of a function. For a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}\\):</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#hessian-matrix-definition","title":"Hessian Matrix Definition","text":"<p>\\(\\mathbf{H} = \\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{H}\\) is an \\(n \\times n\\) symmetric matrix (if \\(f\\) is twice continuously differentiable)</li> <li>\\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\) is the \\((i,j)\\) entry</li> <li>Diagonal entries are second derivatives with respect to single variables</li> <li>Off-diagonal entries capture how the gradient changes in one direction as we move in another</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#eigenvalue-interpretation","title":"Eigenvalue Interpretation","text":"<p>The eigenvalues of the Hessian reveal the curvature of the function:</p> Eigenvalue Pattern Curvature Type Optimization Implication All positive Bowl (minimum) Local minimum found All negative Dome (maximum) Local maximum Mixed signs Saddle point Neither min nor max Zero eigenvalue Flat direction Infinitely many solutions"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-hessian-and-curvature-visualizer","title":"Diagram: Hessian and Curvature Visualizer","text":"Hessian and Curvature Visualizer <p>Type: microsim</p> <p>Learning objective: Connect Hessian eigenvalues to geometric curvature of 2D functions (Bloom: Analyze)</p> <p>Visual elements: - 3D surface plot of f(x,y) - Contour plot below the surface - Current point indicator (draggable) - Principal curvature directions shown as arrows at current point - Eigenvalue display with color coding</p> <p>Interactive controls: - Function selector: x\u00b2+y\u00b2, x\u00b2-y\u00b2 (saddle), x\u00b2+0.1y\u00b2, 2x\u00b2+y\u00b2 - Draggable point on contour plot to change evaluation location - Toggle to show/hide principal directions - Toggle to show local quadratic approximation</p> <p>Canvas layout: 800x600px with 3D plot and eigenvector overlay</p> <p>Behavior: - As point moves, Hessian is recomputed and displayed as matrix - Eigenvalues update with color (green=positive, red=negative) - Principal direction arrows scale with eigenvalue magnitude - Quadratic approximation surface overlays at current point</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-hessian-in-taylor-expansion","title":"The Hessian in Taylor Expansion","text":"<p>The Hessian appears in the second-order Taylor expansion:</p> <p>\\(f(\\mathbf{x} + \\mathbf{d}) \\approx f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^\\top \\mathbf{d} + \\frac{1}{2} \\mathbf{d}^\\top \\mathbf{H} \\mathbf{d}\\)</p> <p>This quadratic approximation is the foundation of Newton's method.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#newtons-method-for-optimization","title":"Newton's Method for Optimization","text":"<p>Newton's method uses second-order information to find where the gradient equals zero. By setting the gradient of the quadratic approximation to zero:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#newton-update-rule","title":"Newton Update Rule","text":"<p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{H}^{-1} \\nabla f(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{x}_k\\) is the current iterate</li> <li>\\(\\mathbf{H} = \\nabla^2 f(\\mathbf{x}_k)\\) is the Hessian at the current point</li> <li>\\(\\nabla f(\\mathbf{x}_k)\\) is the gradient at the current point</li> <li>The term \\(\\mathbf{H}^{-1} \\nabla f\\) is called the Newton direction</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#comparison-with-gradient-descent","title":"Comparison with Gradient Descent","text":"Property Gradient Descent Newton's Method Update direction \\(-\\nabla f\\) (steepest descent) \\(-\\mathbf{H}^{-1} \\nabla f\\) (Newton direction) Step size Requires tuning \\(\\alpha\\) Natural step size of 1 Convergence rate Linear Quadratic (near solution) Cost per iteration \\(O(n)\\) gradient \\(O(n^3)\\) Hessian solve Condition number sensitivity High Low <p>Newton's method effectively rescales the problem to have uniform curvature in all directions, making it condition-number independent.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-newton-vs-gradient-descent","title":"Diagram: Newton vs Gradient Descent","text":"Newton vs Gradient Descent Comparison <p>Type: microsim</p> <p>Learning objective: Compare convergence behavior of gradient descent and Newton's method on ill-conditioned problems (Bloom: Analyze)</p> <p>Visual elements: - Contour plot of 2D quadratic function with adjustable condition number - Two optimization paths: gradient descent (blue) and Newton (orange) - Current iterate markers for both methods - Iteration counter and function value display</p> <p>Interactive controls: - Slider: Condition number (1 to 100) - Slider: Learning rate for gradient descent (0.001 to 0.1) - Button: \"Step\" (advance one iteration) - Button: \"Run to Convergence\" - Button: \"Reset\" - Starting point selector (click on contour plot)</p> <p>Canvas layout: 700x600px with contour plot and convergence comparison</p> <p>Behavior: - Newton's method converges in 1 step for quadratics - Gradient descent shows zig-zag pattern for high condition numbers - Display shows iteration count to reach tolerance - Side panel shows current gradient norm for both methods</p> <p>Implementation: p5.js with eigenvalue-based ellipse generation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#challenges-with-newtons-method","title":"Challenges with Newton's Method","text":"<p>Despite quadratic convergence, Newton's method has practical limitations:</p> <ul> <li>Hessian computation: \\(O(n^2)\\) storage and \\(O(n^3)\\) inversion</li> <li>Non-convexity: May converge to saddle points or maxima</li> <li>Numerical stability: Requires Hessian to be positive definite</li> <li>Scalability: Impractical for deep learning with millions of parameters</li> </ul> <p>These limitations motivate quasi-Newton methods.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#quasi-newton-methods","title":"Quasi-Newton Methods","text":"<p>Quasi-Newton methods approximate the Hessian (or its inverse) using only gradient information, avoiding explicit Hessian computation.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-secant-equation","title":"The Secant Equation","text":"<p>Quasi-Newton methods build approximations \\(\\mathbf{B}_k \\approx \\mathbf{H}_k\\) satisfying the secant equation:</p> <p>\\(\\mathbf{B}_{k+1} \\mathbf{s}_k = \\mathbf{y}_k\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k\\) is the step taken</li> <li>\\(\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)\\) is the gradient difference</li> <li>This equation states that \\(\\mathbf{B}_{k+1}\\) correctly maps the step to the gradient change</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-bfgs-algorithm","title":"The BFGS Algorithm","text":"<p>The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is the most successful quasi-Newton method. It directly maintains an approximation \\(\\mathbf{M}_k \\approx \\mathbf{H}_k^{-1}\\):</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#bfgs-update-formula","title":"BFGS Update Formula","text":"<p>\\(\\mathbf{M}_{k+1} = \\left(\\mathbf{I} - \\rho_k \\mathbf{s}_k \\mathbf{y}_k^\\top\\right) \\mathbf{M}_k \\left(\\mathbf{I} - \\rho_k \\mathbf{y}_k \\mathbf{s}_k^\\top\\right) + \\rho_k \\mathbf{s}_k \\mathbf{s}_k^\\top\\)</p> <p>where:</p> <ul> <li>\\(\\rho_k = \\frac{1}{\\mathbf{y}_k^\\top \\mathbf{s}_k}\\)</li> <li>\\(\\mathbf{M}_k\\) is the current inverse Hessian approximation</li> <li>\\(\\mathbf{s}_k\\) and \\(\\mathbf{y}_k\\) are the step and gradient difference</li> </ul> <p>Key properties of BFGS:</p> <ul> <li>Maintains positive definiteness if initialized positive definite</li> <li>Converges superlinearly (faster than linear, slower than quadratic)</li> <li>Only requires gradient evaluations, not Hessian</li> <li>\\(O(n^2)\\) storage and update cost</li> </ul> Method Hessian Cost Storage Convergence Newton \\(O(n^3)\\) exact \\(O(n^2)\\) Quadratic BFGS \\(O(n^2)\\) approximate \\(O(n^2)\\) Superlinear L-BFGS \\(O(mn)\\) limited memory \\(O(mn)\\) Superlinear Gradient Descent None \\(O(n)\\) Linear <p>L-BFGS (Limited-memory BFGS) stores only the \\(m\\) most recent \\((\\mathbf{s}_k, \\mathbf{y}_k)\\) pairs, making it suitable for large-scale problems.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<p>While Newton and quasi-Newton methods work well for moderate-sized problems, deep learning requires optimizing millions of parameters using billions of data points. Stochastic Gradient Descent (SGD) trades accuracy for efficiency.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-sgd-algorithm","title":"The SGD Algorithm","text":"<p>Instead of computing the full gradient over all training examples:</p> <p>\\(\\nabla f(\\mathbf{x}) = \\frac{1}{N} \\sum_{i=1}^{N} \\nabla f_i(\\mathbf{x})\\)</p> <p>SGD uses a single randomly selected example:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#sgd-update-rule","title":"SGD Update Rule","text":"<p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f_{i_k}(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\alpha\\) is the learning rate</li> <li>\\(i_k\\) is a randomly selected index from \\(\\{1, \\ldots, N\\}\\)</li> <li>\\(\\nabla f_{i_k}\\) is the gradient for example \\(i_k\\)</li> </ul> <p>The stochastic gradient is an unbiased estimator of the true gradient:</p> <p>\\(\\mathbb{E}[\\nabla f_{i_k}(\\mathbf{x})] = \\nabla f(\\mathbf{x})\\)</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#mini-batch-sgd","title":"Mini-Batch SGD","text":"<p>Mini-batch SGD balances the efficiency of single-sample SGD with the stability of full-batch gradient descent:</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\cdot \\frac{1}{|B_k|} \\sum_{i \\in B_k} \\nabla f_i(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(B_k\\) is a randomly sampled mini-batch of size \\(|B_k|\\) (typically 32-512)</li> <li>The average reduces gradient variance by factor \\(|B_k|\\)</li> </ul> Batch Size Gradient Variance GPU Utilization Generalization 1 (pure SGD) High Poor Often good 32-64 Moderate Good Good 256-512 Low Excellent May need tuning Full batch Zero Memory limited May overfit"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-sgd-trajectory-visualizer","title":"Diagram: SGD Trajectory Visualizer","text":"SGD Trajectory Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how batch size affects SGD convergence behavior (Bloom: Apply)</p> <p>Visual elements: - 2D contour plot of loss landscape - Optimization trajectory showing recent steps - \"True gradient\" arrow and \"stochastic gradient\" arrow at current point - Noise cloud visualization around true gradient</p> <p>Interactive controls: - Slider: Batch size (1, 8, 32, 128, full) - Slider: Learning rate (0.001 to 0.5) - Button: \"Step\" (one update) - Button: \"Run 100 steps\" - Button: \"Reset\" - Toggle: Show gradient noise distribution</p> <p>Canvas layout: 700x600px with contour plot and gradient visualization</p> <p>Behavior: - Small batch sizes show noisy, erratic paths - Large batch sizes show smoother convergence - Display running average of gradient norm - Show variance estimate of stochastic gradient</p> <p>Implementation: p5.js with random gradient noise simulation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#momentum-based-optimization","title":"Momentum-Based Optimization","text":"<p>Pure SGD can oscillate in ravines\u2014directions where the curvature differs significantly. Momentum addresses this by accumulating gradient information across iterations.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#classical-momentum","title":"Classical Momentum","text":"<p>The momentum update maintains a velocity vector:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#momentum-update","title":"Momentum Update","text":"<p>\\(\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k)\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{v}_k\\) is the velocity (accumulated gradient)</li> <li>\\(\\beta \\in [0, 1)\\) is the momentum coefficient (typically 0.9)</li> <li>\\(\\alpha\\) is the learning rate</li> </ul> <p>Momentum has a physical interpretation: the parameters move like a ball rolling downhill with friction. The ball builds up speed in consistent gradient directions while damping oscillations.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#nesterov-accelerated-gradient","title":"Nesterov Accelerated Gradient","text":"<p>Nesterov momentum looks ahead before computing the gradient:</p> <p>\\(\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k - \\alpha \\beta \\mathbf{v}_k)\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}\\)</p> <p>This \"lookahead\" provides a correction that improves convergence, especially near the optimum.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-momentum-dynamics","title":"Diagram: Momentum Dynamics","text":"Momentum Dynamics Visualizer <p>Type: microsim</p> <p>Learning objective: Visualize how momentum accumulates and dampens oscillations (Bloom: Understand)</p> <p>Visual elements: - 2D contour plot with elongated elliptical contours (ill-conditioned) - Three simultaneous optimization paths: SGD (blue), Momentum (green), Nesterov (orange) - Velocity vectors shown as arrows at current positions - Trace of recent positions for each method</p> <p>Interactive controls: - Slider: Momentum coefficient beta (0 to 0.99) - Slider: Learning rate (0.001 to 0.1) - Button: \"Step\" - Button: \"Run to convergence\" - Button: \"Reset\" - Checkbox: Show velocity vectors</p> <p>Canvas layout: 750x600px with contour visualization</p> <p>Behavior: - SGD shows characteristic zig-zag oscillation - Momentum shows smooth acceleration toward minimum - Nesterov shows slightly faster convergence - Velocity vectors grow in consistent directions, shrink during oscillation</p> <p>Implementation: p5.js with physics-based animation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#adaptive-learning-rate-methods","title":"Adaptive Learning Rate Methods","text":"<p>Different parameters often require different learning rates. Adaptive methods automatically adjust per-parameter learning rates based on historical gradient information.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#rmsprop","title":"RMSprop","text":"<p>RMSprop (Root Mean Square Propagation) maintains a running average of squared gradients:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#rmsprop-update","title":"RMSprop Update","text":"<p>\\(\\mathbf{s}_{k+1} = \\gamma \\mathbf{s}_k + (1 - \\gamma) \\nabla f(\\mathbf{x}_k)^2\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\frac{\\alpha}{\\sqrt{\\mathbf{s}_{k+1} + \\epsilon}} \\odot \\nabla f(\\mathbf{x}_k)\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{s}_k\\) is the running average of squared gradients (element-wise)</li> <li>\\(\\gamma \\approx 0.9\\) is the decay rate</li> <li>\\(\\epsilon \\approx 10^{-8}\\) prevents division by zero</li> <li>\\(\\odot\\) denotes element-wise multiplication</li> <li>The division is element-wise</li> </ul> <p>RMSprop divides the learning rate by the root mean square of recent gradients, effectively:</p> <ul> <li>Reducing step size for parameters with large gradients</li> <li>Increasing step size for parameters with small gradients</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#the-adam-optimizer","title":"The Adam Optimizer","text":"<p>Adam (Adaptive Moment Estimation) combines momentum with RMSprop:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#adam-update","title":"Adam Update","text":"<p>\\(\\mathbf{m}_{k+1} = \\beta_1 \\mathbf{m}_k + (1 - \\beta_1) \\nabla f(\\mathbf{x}_k)\\)</p> <p>\\(\\mathbf{v}_{k+1} = \\beta_2 \\mathbf{v}_k + (1 - \\beta_2) \\nabla f(\\mathbf{x}_k)^2\\)</p> <p>\\(\\hat{\\mathbf{m}} = \\frac{\\mathbf{m}_{k+1}}{1 - \\beta_1^{k+1}}, \\quad \\hat{\\mathbf{v}} = \\frac{\\mathbf{v}_{k+1}}{1 - \\beta_2^{k+1}}\\)</p> <p>\\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\frac{\\alpha}{\\sqrt{\\hat{\\mathbf{v}}} + \\epsilon} \\odot \\hat{\\mathbf{m}}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{m}_k\\) is the first moment (mean) estimate</li> <li>\\(\\mathbf{v}_k\\) is the second moment (variance) estimate</li> <li>\\(\\beta_1 \\approx 0.9\\) is the first moment decay</li> <li>\\(\\beta_2 \\approx 0.999\\) is the second moment decay</li> <li>\\(\\hat{\\mathbf{m}}, \\hat{\\mathbf{v}}\\) are bias-corrected estimates</li> </ul> <p>The bias correction compensates for initialization at zero, which otherwise underestimates moments early in training.</p> Optimizer First Moment Second Moment Bias Correction SGD No No N/A Momentum Yes No No RMSprop No Yes No Adam Yes Yes Yes AdaGrad No Yes (cumulative) No"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-optimizer-comparison-arena","title":"Diagram: Optimizer Comparison Arena","text":"Optimizer Comparison Arena <p>Type: microsim</p> <p>Learning objective: Compare convergence behavior of different optimizers on various loss landscapes (Bloom: Evaluate)</p> <p>Visual elements: - 2D loss landscape (selectable) - Multiple optimization paths: SGD, Momentum, RMSprop, Adam - Color-coded trajectories with position markers - Loss vs iteration plot below main visualization</p> <p>Interactive controls: - Dropdown: Loss landscape (Quadratic, Rosenbrock, Beale, Saddle) - Sliders for each optimizer's hyperparameters - Button: \"Race!\" (run all optimizers simultaneously) - Button: \"Reset\" - Checkboxes to enable/disable each optimizer</p> <p>Canvas layout: 800x700px with landscape and convergence plot</p> <p>Behavior: - All optimizers start from same initial point - Animate simultaneous optimization - Display current loss value for each - Declare \"winner\" when first reaches tolerance - Show iteration count for each optimizer</p> <p>Implementation: p5.js with multiple optimizer state tracking</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\nclass Adam:\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.m = None  # First moment\n        self.v = None  # Second moment\n        self.t = 0     # Time step\n\n    def step(self, params, grads):\n        if self.m is None:\n            self.m = np.zeros_like(params)\n            self.v = np.zeros_like(params)\n\n        self.t += 1\n\n        # Update biased first and second moment estimates\n        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n\n        # Bias correction\n        m_hat = self.m / (1 - self.beta1**self.t)\n        v_hat = self.v / (1 - self.beta2**self.t)\n\n        # Update parameters\n        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n        return params\n</code></pre>"},{"location":"chapters/12-optimization-and-learning-algorithms/#constrained-optimization","title":"Constrained Optimization","text":"<p>Many optimization problems have constraints\u2014parameter bounds, equality requirements, or inequality restrictions. Constrained optimization finds optima while satisfying these constraints.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#problem-formulation","title":"Problem Formulation","text":"<p>A general constrained optimization problem:</p> <p>\\(\\min_{\\mathbf{x}} f(\\mathbf{x})\\)</p> <p>subject to:</p> <ul> <li>\\(g_i(\\mathbf{x}) \\leq 0\\) for \\(i = 1, \\ldots, m\\) (inequality constraints)</li> <li>\\(h_j(\\mathbf{x}) = 0\\) for \\(j = 1, \\ldots, p\\) (equality constraints)</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#lagrange-multipliers","title":"Lagrange Multipliers","text":"<p>For equality-constrained problems, Lagrange multipliers convert the constrained problem to an unconstrained one.</p> <p>Consider minimizing \\(f(\\mathbf{x})\\) subject to \\(h(\\mathbf{x}) = 0\\). We form the Lagrangian:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#lagrangian-function","title":"Lagrangian Function","text":"<p>\\(\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) + \\lambda h(\\mathbf{x})\\)</p> <p>where:</p> <ul> <li>\\(\\lambda\\) is the Lagrange multiplier</li> <li>The optimal \\((\\mathbf{x}^*, \\lambda^*)\\) satisfies \\(\\nabla_{\\mathbf{x}} \\mathcal{L} = 0\\) and \\(\\nabla_\\lambda \\mathcal{L} = 0\\)</li> </ul> <p>The condition \\(\\nabla_{\\mathbf{x}} \\mathcal{L} = 0\\) gives:</p> <p>\\(\\nabla f(\\mathbf{x}^*) = -\\lambda \\nabla h(\\mathbf{x}^*)\\)</p> <p>At the optimum, the gradient of \\(f\\) is parallel to the constraint gradient\u2014we can't improve \\(f\\) while staying on the constraint surface.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-lagrange-multiplier-geometry","title":"Diagram: Lagrange Multiplier Geometry","text":"Lagrange Multiplier Geometry <p>Type: microsim</p> <p>Learning objective: Visualize the geometric interpretation of Lagrange multipliers (Bloom: Understand)</p> <p>Visual elements: - 2D contour plot of objective function f(x,y) - Constraint curve h(x,y) = 0 (bold line) - Gradient vectors: \u2207f at selected point (blue arrow) - Constraint normal: \u2207h at selected point (red arrow) - Highlighted optimal point where gradients are parallel</p> <p>Interactive controls: - Draggable point along constraint curve - Toggle: Show gradient field of f - Toggle: Show constraint normal field - Button: \"Find Optimum\" (animate to solution) - Display: Current f value, \u03bb value</p> <p>Canvas layout: 700x600px with contour and vector field</p> <p>Behavior: - As user drags point along constraint, show gradient angles - Highlight when \u2207f and \u2207h become parallel - Display computed \u03bb = -||\u2207f||/||\u2207h|| at parallel point - Animate optimization path along constraint</p> <p>Implementation: p5.js with gradient computation</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#kkt-conditions","title":"KKT Conditions","text":"<p>The Karush-Kuhn-Tucker (KKT) conditions generalize Lagrange multipliers to include inequality constraints. For the problem with both equality and inequality constraints, the KKT conditions are:</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#kkt-conditions_1","title":"KKT Conditions","text":"<ol> <li> <p>Stationarity: \\(\\nabla f(\\mathbf{x}^*) + \\sum_{i=1}^{m} \\mu_i \\nabla g_i(\\mathbf{x}^*) + \\sum_{j=1}^{p} \\lambda_j \\nabla h_j(\\mathbf{x}^*) = 0\\)</p> </li> <li> <p>Primal feasibility: \\(g_i(\\mathbf{x}^*) \\leq 0\\) and \\(h_j(\\mathbf{x}^*) = 0\\)</p> </li> <li> <p>Dual feasibility: \\(\\mu_i \\geq 0\\) for all \\(i\\)</p> </li> <li> <p>Complementary slackness: \\(\\mu_i g_i(\\mathbf{x}^*) = 0\\) for all \\(i\\)</p> </li> </ol> <p>where:</p> <ul> <li>\\(\\mu_i\\) are multipliers for inequality constraints</li> <li>\\(\\lambda_j\\) are multipliers for equality constraints</li> <li>Complementary slackness means either \\(\\mu_i = 0\\) or \\(g_i(\\mathbf{x}^*) = 0\\)</li> </ul> Condition Meaning Stationarity Gradient balance at optimum Primal feasibility Solution satisfies constraints Dual feasibility Inequality multipliers are non-negative Complementary slackness Inactive constraints have zero multipliers"},{"location":"chapters/12-optimization-and-learning-algorithms/#active-constraints","title":"Active Constraints","text":"<p>The complementary slackness condition is particularly important:</p> <ul> <li>If \\(g_i(\\mathbf{x}^*) &lt; 0\\), the constraint is inactive and \\(\\mu_i = 0\\)</li> <li>If \\(g_i(\\mathbf{x}^*) = 0\\), the constraint is active and may have \\(\\mu_i &gt; 0\\)</li> </ul> <p>Inactive constraints don't affect the solution\u2014they're not \"binding.\"</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#diagram-kkt-conditions-visualizer","title":"Diagram: KKT Conditions Visualizer","text":"KKT Conditions Visualizer <p>Type: microsim</p> <p>Learning objective: Understand KKT conditions for inequality-constrained optimization (Bloom: Analyze)</p> <p>Visual elements: - 2D contour plot of objective function - Feasible region shaded (where all g_i \u2264 0) - Constraint boundaries with active/inactive coloring - Optimal point with gradient vectors - KKT condition checklist with live status</p> <p>Interactive controls: - Draggable objective function center - Toggle constraints on/off - Button: \"Solve\" (find KKT point) - Display: Multiplier values \u03bc_i for each constraint</p> <p>Canvas layout: 800x650px with visualization and KKT status panel</p> <p>Behavior: - Show feasible region as intersection of half-planes - Animate solution finding - Display which constraints are active at solution - Show complementary slackness: \u03bc_i = 0 for inactive constraints - Color-code constraints: green (inactive), orange (active)</p> <p>Implementation: p5.js with linear programming solver</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#applications-in-machine-learning","title":"Applications in Machine Learning","text":""},{"location":"chapters/12-optimization-and-learning-algorithms/#regularized-optimization","title":"Regularized Optimization","text":"<p>Many machine learning problems add regularization terms that can be viewed through the lens of constrained optimization:</p> Formulation Constraint View Effect \\(\\min f(\\mathbf{x}) + \\lambda\\|\\mathbf{x}\\|_2^2\\) \\(\\|\\mathbf{x}\\|_2 \\leq r\\) L2 regularization (weight decay) \\(\\min f(\\mathbf{x}) + \\lambda\\|\\mathbf{x}\\|_1\\) \\(\\|\\mathbf{x}\\|_1 \\leq r\\) L1 regularization (sparsity) \\(\\min f(\\mathbf{x})\\) s.t. \\(\\|\\mathbf{x}\\|_2 = 1\\) Unit sphere Normalized embeddings"},{"location":"chapters/12-optimization-and-learning-algorithms/#support-vector-machines","title":"Support Vector Machines","text":"<p>SVMs solve a constrained quadratic program:</p> <p>\\(\\min_{\\mathbf{w}, b} \\frac{1}{2}\\|\\mathbf{w}\\|^2\\)</p> <p>subject to \\(y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\geq 1\\) for all training examples.</p> <p>The KKT conditions reveal which training examples are support vectors (those with active constraints).</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#neural-network-constraints","title":"Neural Network Constraints","text":"<p>Modern deep learning increasingly uses constrained optimization:</p> <ul> <li>Spectral normalization: Constrain weight matrix spectral norm</li> <li>Gradient clipping: Constrain gradient magnitude</li> <li>Projected gradient descent: Project onto feasible set after each step</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#summary_1","title":"Summary","text":"<p>Optimization algorithms form the computational backbone of machine learning. This chapter covered:</p> <p>Foundational Concepts:</p> <ul> <li>Convexity guarantees global optima are local optima</li> <li>The Hessian matrix captures curvature and determines convergence behavior</li> <li>Positive definite Hessians indicate convexity</li> </ul> <p>Classical Methods:</p> <ul> <li>Newton's method uses the Hessian for quadratic convergence</li> <li>Quasi-Newton methods (BFGS) approximate the Hessian efficiently</li> <li>L-BFGS scales to large problems with limited memory</li> </ul> <p>Stochastic Methods:</p> <ul> <li>SGD enables optimization with large datasets</li> <li>Mini-batches balance variance reduction with efficiency</li> <li>Momentum accelerates convergence and dampens oscillations</li> </ul> <p>Adaptive Methods:</p> <ul> <li>RMSprop adapts learning rates using gradient magnitudes</li> <li>Adam combines momentum with adaptive rates</li> <li>Bias correction handles initialization effects</li> </ul> <p>Constrained Optimization:</p> <ul> <li>Lagrange multipliers handle equality constraints</li> <li>KKT conditions extend to inequality constraints</li> <li>Complementary slackness identifies active constraints</li> </ul>"},{"location":"chapters/12-optimization-and-learning-algorithms/#self-check-questions","title":"Self-Check Questions","text":"Why does Newton's method converge faster than gradient descent? <p>Newton's method uses second-order (curvature) information from the Hessian to account for the shape of the objective function. While gradient descent takes steps proportional to the gradient magnitude, Newton's method takes steps that account for how quickly the gradient changes. In the quadratic approximation, Newton's method jumps directly to the minimum. For well-behaved functions near a minimum, this gives quadratic convergence: the number of correct digits roughly doubles each iteration.</p> What is the purpose of bias correction in Adam? <p>The first and second moment estimates in Adam are initialized to zero. In early iterations, these running averages are biased toward zero because they haven't accumulated enough gradient history. Bias correction divides by \\((1 - \\beta^t)\\) to compensate, essentially scaling up the estimates early in training. As \\(t \\to \\infty\\), the correction factor approaches 1 and has no effect.</p> Explain the geometric meaning of the KKT complementary slackness condition. <p>Complementary slackness states that \\(\\mu_i g_i(\\mathbf{x}^*) = 0\\) for each inequality constraint. This means either the constraint is inactive (\\(g_i &lt; 0\\)) and its multiplier is zero (\\(\\mu_i = 0\\)), or the constraint is active (\\(g_i = 0\\)) and the multiplier may be positive. Geometrically, only constraints that the solution \"touches\" can exert force on the solution. Constraints that are strictly satisfied (with slack) don't affect where the optimum lies.</p> Why is mini-batch SGD preferred over pure SGD in practice? <p>Pure SGD (batch size 1) has high variance in gradient estimates, leading to noisy optimization paths. While this noise can help escape local minima, it also slows convergence and makes training unstable. Mini-batches reduce variance by averaging over multiple examples while still being much faster than full-batch gradient descent. Additionally, mini-batches enable efficient GPU parallelization\u2014processing 32 examples takes nearly the same time as 1 on modern hardware.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/","title":"Quiz: Optimization and Learning Algorithms","text":"<p>Test your understanding of optimization methods for machine learning.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#1-the-hessian-matrix-contains","title":"1. The Hessian matrix contains:","text":"<ol> <li>First-order partial derivatives</li> <li>Second-order partial derivatives</li> <li>The gradient vector</li> <li>The loss function values</li> </ol> Show Answer <p>The correct answer is B. The Hessian matrix contains all second-order partial derivatives: \\(H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}\\). It captures the curvature of the function.</p> <p>Concept Tested: Hessian Matrix</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#2-a-function-is-convex-if","title":"2. A function is convex if:","text":"<ol> <li>It has multiple local minima</li> <li>Any chord lies above or on the function graph</li> <li>Its gradient is always zero</li> <li>It is defined only for positive inputs</li> </ol> Show Answer <p>The correct answer is B. A convex function lies below any chord connecting two points on its graph: \\(f(\\lambda x + (1-\\lambda)y) \\leq \\lambda f(x) + (1-\\lambda)f(y)\\). Convex functions have a single global minimum.</p> <p>Concept Tested: Convex Function</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#3-newtons-method-uses","title":"3. Newton's method uses:","text":"<ol> <li>Only first-order gradient information</li> <li>Second-order Hessian information for faster convergence</li> <li>Random search</li> <li>No derivatives at all</li> </ol> Show Answer <p>The correct answer is B. Newton's method uses both gradient and Hessian: \\(\\mathbf{x}_{k+1} = \\mathbf{x}_k - H^{-1}\\nabla f\\). The Hessian provides curvature information enabling quadratic convergence near the solution.</p> <p>Concept Tested: Newton's Method</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#4-stochastic-gradient-descent-sgd-differs-from-batch-gradient-descent-by","title":"4. Stochastic Gradient Descent (SGD) differs from batch gradient descent by:","text":"<ol> <li>Never converging</li> <li>Using gradients from random subsets of data</li> <li>Requiring second-order derivatives</li> <li>Only working on convex functions</li> </ol> Show Answer <p>The correct answer is B. SGD computes gradients using random mini-batches rather than the full dataset. This introduces noise but enables much faster iteration, especially with large datasets.</p> <p>Concept Tested: SGD</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#5-momentum-in-optimization","title":"5. Momentum in optimization:","text":"<ol> <li>Slows down convergence</li> <li>Accumulates gradient information to accelerate and dampen oscillations</li> <li>Eliminates the learning rate</li> <li>Only works for linear functions</li> </ol> Show Answer <p>The correct answer is B. Momentum maintains a velocity vector that accumulates past gradients, helping to accelerate convergence in consistent directions while dampening oscillations in others.</p> <p>Concept Tested: Momentum</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#6-the-adam-optimizer-combines","title":"6. The Adam optimizer combines:","text":"<ol> <li>Newton's method and SGD</li> <li>Momentum and adaptive learning rates (RMSprop-like)</li> <li>L1 and L2 regularization</li> <li>Batch and online learning</li> </ol> Show Answer <p>The correct answer is B. Adam combines momentum (first moment) with RMSprop-style adaptive learning rates (second moment), plus bias correction. This makes it robust across many problem types.</p> <p>Concept Tested: Adam Optimizer</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#7-rmsprop-adapts-learning-rates-by","title":"7. RMSprop adapts learning rates by:","text":"<ol> <li>Keeping them constant</li> <li>Dividing by the running average of squared gradients</li> <li>Multiplying by the gradient magnitude</li> <li>Using second-order derivatives</li> </ol> Show Answer <p>The correct answer is B. RMSprop divides the learning rate by the root mean square of recent gradients, reducing step size for parameters with large gradients and increasing it for those with small gradients.</p> <p>Concept Tested: RMSprop</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#8-lagrange-multipliers-are-used-to","title":"8. Lagrange multipliers are used to:","text":"<ol> <li>Increase the number of variables</li> <li>Convert constrained optimization to unconstrained</li> <li>Compute the Hessian</li> <li>Initialize weights randomly</li> </ol> Show Answer <p>The correct answer is B. Lagrange multipliers transform constrained optimization problems into unconstrained ones by incorporating constraints into the objective function through the Lagrangian.</p> <p>Concept Tested: Lagrange Multiplier</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#9-the-kkt-conditions-generalize-lagrange-multipliers-to-handle","title":"9. The KKT conditions generalize Lagrange multipliers to handle:","text":"<ol> <li>Only equality constraints</li> <li>Inequality constraints</li> <li>Unconstrained problems</li> <li>Non-differentiable functions</li> </ol> Show Answer <p>The correct answer is B. The Karush-Kuhn-Tucker (KKT) conditions extend Lagrange multipliers to problems with inequality constraints, introducing complementary slackness conditions.</p> <p>Concept Tested: KKT Conditions</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/quiz/#10-mini-batch-sgd-is-preferred-over-pure-sgd-batch-size-1-because","title":"10. Mini-batch SGD is preferred over pure SGD (batch size 1) because:","text":"<ol> <li>It never converges</li> <li>It reduces gradient variance while maintaining computational efficiency</li> <li>It requires more memory than full-batch</li> <li>It eliminates the need for a learning rate</li> </ol> Show Answer <p>The correct answer is B. Mini-batches reduce gradient variance (averaging over multiple samples) while still being much faster than full-batch. They also enable efficient GPU parallelization.</p> <p>Concept Tested: Mini-Batch SGD</p>"},{"location":"chapters/13-image-processing-and-computer-vision/","title":"Image Processing and Computer Vision","text":""},{"location":"chapters/13-image-processing-and-computer-vision/#summary","title":"Summary","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision. This chapter covers image representation as matrices and tensors, convolution operations, image filtering (blur, sharpen, edge detection), Fourier transforms, and feature detection. You will also learn about homography for perspective transformations.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"chapters/13-image-processing-and-computer-vision/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 7: Matrix Decompositions (for SVD-based compression)</li> <li>Chapter 10: Neural Networks and Deep Learning (for tensors)</li> </ul>"},{"location":"chapters/13-image-processing-and-computer-vision/#introduction","title":"Introduction","text":"<p>Every digital image you see\u2014from smartphone photos to medical scans\u2014is fundamentally a matrix of numbers. This mathematical representation enables powerful image processing operations using linear algebra:</p> <ul> <li>Matrix operations enable pixel-wise transformations</li> <li>Convolution applies local filters for blurring, sharpening, and edge detection</li> <li>Fourier transforms reveal frequency content for compression and filtering</li> <li>Linear transformations correct perspective and align images</li> </ul> <p>Understanding these foundations illuminates how image editing software, computer vision systems, and neural networks process visual information.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-representation","title":"Image Representation","text":""},{"location":"chapters/13-image-processing-and-computer-vision/#the-image-matrix","title":"The Image Matrix","text":"<p>A digital image is stored as a matrix where each entry represents a pixel's intensity or color value. For an image with height \\(H\\) and width \\(W\\):</p> <p>\\(\\mathbf{I} \\in \\mathbb{R}^{H \\times W}\\)</p> <p>The matrix entry \\(I_{i,j}\\) corresponds to the pixel at row \\(i\\) (from top) and column \\(j\\) (from left). Pixel values are typically integers in \\([0, 255]\\) or floats in \\([0, 1]\\).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#grayscale-images","title":"Grayscale Images","text":"<p>A grayscale image uses a single value per pixel representing light intensity:</p> <ul> <li>0 = black (no light)</li> <li>255 = white (maximum light)</li> <li>Intermediate values = shades of gray</li> </ul> Value Range Interpretation 0-50 Very dark 50-100 Dark gray 100-150 Medium gray 150-200 Light gray 200-255 Very bright <p>A 1920\u00d71080 HD grayscale image is a matrix with over 2 million entries.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-image-matrix-visualizer","title":"Diagram: Image Matrix Visualizer","text":"<p>Run the Image Matrix Visualizer Fullscreen</p> Image Matrix Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how pixel values in a matrix correspond to image appearance (Bloom: Understand)</p> <p>Visual elements: - Left panel: Small grayscale image (e.g., 8\u00d78 or 16\u00d716 pixels) - Right panel: Matrix showing numeric values - Hover highlight connecting matrix cell to corresponding pixel - Color gradient legend (0=black to 255=white)</p> <p>Interactive controls: - Dropdown: Select sample image (checkerboard, gradient, simple shape, random) - Slider: Zoom level for image display - Click on matrix cell to highlight corresponding pixel - Click on pixel to highlight corresponding matrix cell - Button: \"Edit mode\" to modify individual pixel values</p> <p>Canvas layout: 800x500px with side-by-side image and matrix</p> <p>Behavior: - Hovering over matrix cell highlights pixel with colored border - Editing a value immediately updates the image - Show real-time pixel coordinates and value on hover - Display image dimensions and total pixel count</p> <p>Implementation: p5.js with matrix rendering and image display</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#rgb-images","title":"RGB Images","text":"<p>Color images use three channels\u2014Red, Green, Blue\u2014each stored as a separate matrix:</p> <p>\\(\\mathbf{I}_R, \\mathbf{I}_G, \\mathbf{I}_B \\in \\mathbb{R}^{H \\times W}\\)</p> <p>Each pixel is a triplet \\((r, g, b)\\) where each component ranges from 0 to 255:</p> Color RGB Value Red (255, 0, 0) Green (0, 255, 0) Blue (0, 0, 255) Yellow (255, 255, 0) Cyan (0, 255, 255) Magenta (255, 0, 255) White (255, 255, 255) Black (0, 0, 0) <p>The RGB color model is additive\u2014colors are created by adding light. Mixing all three at full intensity produces white.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-tensors","title":"Image Tensors","text":"<p>For computational purposes, color images are often represented as 3D tensors:</p> <p>\\(\\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times C}\\)</p> <p>where:</p> <ul> <li>\\(H\\) is the height (rows)</li> <li>\\(W\\) is the width (columns)</li> <li>\\(C\\) is the number of channels (3 for RGB, 4 for RGBA with transparency)</li> </ul> <p>In deep learning, batches of images form 4D tensors:</p> <p>\\(\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times H \\times W}\\)</p> <p>where \\(N\\) is the batch size. The ordering of dimensions varies by framework (PyTorch uses \\(N \\times C \\times H \\times W\\); TensorFlow often uses \\(N \\times H \\times W \\times C\\)).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-rgb-channel-decomposition","title":"Diagram: RGB Channel Decomposition","text":"<p>Run the RGB Channel Decomposition Fullscreen</p> RGB Channel Decomposition <p>Type: microsim</p> <p>Learning objective: Visualize how RGB channels combine to form color images (Bloom: Analyze)</p> <p>Visual elements: - Original color image (center or top) - Three grayscale images showing R, G, B channels separately - Color-tinted versions of each channel (red, green, blue tints) - Reconstructed image from selected channels</p> <p>Interactive controls: - Image selector: Choose from sample images - Channel toggles: Enable/disable R, G, B for reconstruction - Slider: Adjust individual channel intensity (0-100%) - Toggle: Show grayscale vs color-tinted channel views</p> <p>Canvas layout: 800x600px with original and channel displays</p> <p>Behavior: - Disabling a channel shows image without that color component - Adjusting channel intensity shows over/under-saturation effects - Hover over pixel to show (r, g, b) values - Real-time reconstruction as channels are modified</p> <p>Implementation: p5.js with pixel array manipulation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#color-space-transforms","title":"Color Space Transforms","text":"<p>While RGB is standard for display, other color spaces are useful for specific tasks:</p> Color Space Components Use Cases RGB Red, Green, Blue Display, storage HSV/HSB Hue, Saturation, Value Color selection, segmentation HSL Hue, Saturation, Lightness Web design, color adjustment YCbCr Luminance, Chrominance JPEG compression LAB Lightness, a, b Perceptual uniformity <p>Converting between color spaces involves matrix transformations. For example, RGB to YCbCr:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#rgb-to-ycbcr-transform","title":"RGB to YCbCr Transform","text":"<p>\\(\\begin{bmatrix} Y \\\\ C_b \\\\ C_r \\end{bmatrix} = \\begin{bmatrix} 0.299 &amp; 0.587 &amp; 0.114 \\\\ -0.169 &amp; -0.331 &amp; 0.500 \\\\ 0.500 &amp; -0.419 &amp; -0.081 \\end{bmatrix} \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 128 \\\\ 128 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(Y\\) is luminance (brightness information)</li> <li>\\(C_b\\) and \\(C_r\\) are chrominance (color difference signals)</li> <li>The matrix weights reflect human perception (green contributes most to perceived brightness)</li> </ul> <p>This separation enables chroma subsampling\u2014reducing color resolution while preserving brightness detail\u2014which is fundamental to JPEG compression.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-convolution","title":"Image Convolution","text":"<p>Convolution is the foundational operation for image filtering. It applies a small matrix called a kernel or filter to every location in the image.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-convolution-operation","title":"The Convolution Operation","text":"<p>For a 2D image \\(\\mathbf{I}\\) and kernel \\(\\mathbf{K}\\) of size \\((2k+1) \\times (2k+1)\\):</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#2d-convolution-formula","title":"2D Convolution Formula","text":"<p>\\((\\mathbf{I} * \\mathbf{K})[i, j] = \\sum_{m=-k}^{k} \\sum_{n=-k}^{k} I[i+m, j+n] \\cdot K[m, n]\\)</p> <p>where:</p> <ul> <li>\\(*\\) denotes convolution (not multiplication)</li> <li>\\((i, j)\\) is the output pixel location</li> <li>The kernel is centered at each pixel</li> <li>Values outside the image boundary require padding (zero, replicate, or reflect)</li> </ul> <p>Convolution is a linear operation: convolving with a sum of kernels equals the sum of individual convolutions.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-filters","title":"Image Filters","text":"<p>An image filter is a kernel designed to achieve a specific effect. The kernel values determine what features are enhanced or suppressed.</p> Filter Type Effect Kernel Property Low-pass (blur) Smooths, reduces noise Positive values, sums to 1 High-pass (sharpen) Enhances edges Positive center, negative surround Derivative (edge) Detects changes Sums to 0, antisymmetric Identity No change 1 at center, 0 elsewhere"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-convolution-visualizer","title":"Diagram: Convolution Visualizer","text":"<p>Run the Convolution Visualizer Fullscreen</p> Convolution Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how convolution applies a kernel to an image (Bloom: Apply)</p> <p>Visual elements: - Input image (grayscale, small for clarity) - Sliding kernel window highlighted on input - Kernel matrix with current values displayed - Output image being constructed - Calculation panel showing element-wise products and sum</p> <p>Interactive controls: - Kernel selector: Identity, Blur 3\u00d73, Sharpen, Edge detect - Custom kernel editor: 3\u00d73 grid of editable values - Animation speed slider - Button: \"Step\" (advance one pixel) - Button: \"Run\" (animate full convolution) - Toggle: Show/hide calculation details</p> <p>Canvas layout: 900x600px with input, kernel, and output panels</p> <p>Behavior: - Animate kernel sliding across input image - Show element-wise multiplication at current position - Display running sum being computed - Fill in output pixel when calculation completes - Handle boundary conditions visually</p> <p>Implementation: p5.js with step-by-step animation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#blur-filters","title":"Blur Filters","text":"<p>Blur filters smooth images by averaging neighboring pixels, reducing noise and fine detail.</p> <p>Box Blur (Mean Filter):</p> <p>\\(\\mathbf{K}_{box} = \\frac{1}{9} \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\)</p> <p>Every pixel contributes equally. Simple but creates blocky artifacts.</p> <p>Gaussian Blur:</p> <p>\\(\\mathbf{K}_{gauss} = \\frac{1}{16} \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 2 &amp; 4 &amp; 2 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\)</p> <p>Weights follow a Gaussian distribution\u2014center pixels contribute more. Produces smoother, more natural blurring.</p> <p>The general Gaussian kernel is:</p> <p>\\(G(x, y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2 + y^2}{2\\sigma^2}}\\)</p> <p>where \\(\\sigma\\) controls the blur radius. Larger \\(\\sigma\\) means more blurring.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#sharpen-filters","title":"Sharpen Filters","text":"<p>Sharpen filters enhance edges and fine detail by emphasizing differences between neighboring pixels:</p> <p>\\(\\mathbf{K}_{sharpen} = \\begin{bmatrix} 0 &amp; -1 &amp; 0 \\\\ -1 &amp; 5 &amp; -1 \\\\ 0 &amp; -1 &amp; 0 \\end{bmatrix}\\)</p> <p>This kernel can be understood as: - Identity (center = 1) plus - Negative Laplacian (edge enhancement)</p> <p>The result amplifies differences while preserving overall brightness.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-filter-effects-gallery","title":"Diagram: Filter Effects Gallery","text":"<p>Run the Filter Effects Gallery Fullscreen</p> Filter Effects Gallery <p>Type: microsim</p> <p>Learning objective: Compare effects of different filters on the same image (Bloom: Evaluate)</p> <p>Visual elements: - Original image (top center) - Grid of filtered results (4-6 variants) - Kernel display below each filtered image - Difference image option (filtered - original)</p> <p>Interactive controls: - Image selector: Choose sample image - Filter checkboxes: Select which filters to display - Slider: Filter strength/radius where applicable - Toggle: Show difference images - Toggle: Show kernel matrices</p> <p>Canvas layout: 900x700px with grid layout</p> <p>Behavior: - All filters applied to same source image - Hover over filtered image to see kernel - Click to show full-size comparison - Side-by-side slider for before/after on selected filter</p> <p>Implementation: p5.js with multiple filter implementations</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#edge-detection","title":"Edge Detection","text":"<p>Edge detection identifies boundaries in images where intensity changes rapidly. Edges correspond to object boundaries, texture changes, and depth discontinuities.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#derivative-based-edge-detection","title":"Derivative-Based Edge Detection","text":"<p>Edges are locations of high gradient magnitude. The image gradient at pixel \\((i, j)\\):</p> <p>\\(\\nabla I = \\begin{bmatrix} \\frac{\\partial I}{\\partial x} \\\\ \\frac{\\partial I}{\\partial y} \\end{bmatrix}\\)</p> <p>The gradient magnitude and direction:</p> <p>\\(|\\nabla I| = \\sqrt{\\left(\\frac{\\partial I}{\\partial x}\\right)^2 + \\left(\\frac{\\partial I}{\\partial y}\\right)^2}\\)</p> <p>\\(\\theta = \\arctan\\left(\\frac{\\partial I / \\partial y}{\\partial I / \\partial x}\\right)\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-sobel-operator","title":"The Sobel Operator","text":"<p>The Sobel operator approximates image derivatives using convolution:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#sobel-kernels","title":"Sobel Kernels","text":"<p>\\(\\mathbf{G}_x = \\begin{bmatrix} -1 &amp; 0 &amp; +1 \\\\ -2 &amp; 0 &amp; +2 \\\\ -1 &amp; 0 &amp; +1 \\end{bmatrix}, \\quad \\mathbf{G}_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ +1 &amp; +2 &amp; +1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{G}_x\\) detects vertical edges (horizontal gradient)</li> <li>\\(\\mathbf{G}_y\\) detects horizontal edges (vertical gradient)</li> <li>The weights provide smoothing (via the 1-2-1 pattern) combined with differentiation</li> </ul> <p>The Sobel operator combines Gaussian smoothing with differentiation, making it more robust to noise than simple difference operators.</p> Edge Detector Kernel Size Noise Sensitivity Edge Localization Simple difference 1\u00d72 High Good Prewitt 3\u00d73 Medium Good Sobel 3\u00d73 Low Good Scharr 3\u00d73 Low Better rotational symmetry"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-edge-detection-visualizer","title":"Diagram: Edge Detection Visualizer","text":"<p>Run the Edge Detection Visualizer Fullscreen</p> Edge Detection Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Sobel operators detect edges in different orientations (Bloom: Analyze)</p> <p>Visual elements: - Original grayscale image - Gx result (horizontal gradient) with positive/negative coloring - Gy result (vertical gradient) with positive/negative coloring - Gradient magnitude image - Gradient direction overlay (as arrows or color wheel)</p> <p>Interactive controls: - Image selector: Simple shapes, photos, text - Toggle: Show Gx, Gy, magnitude, direction - Slider: Threshold for edge display - Toggle: Show gradient arrows at sample points - Dropdown: Edge detector (Sobel, Prewitt, Scharr)</p> <p>Canvas layout: 850x650px with multi-panel display</p> <p>Behavior: - Compute and display gradient components - Color-code positive (white) and negative (black) gradients - Threshold slider converts magnitude to binary edges - Arrow overlay shows gradient direction at grid points</p> <p>Implementation: p5.js with Sobel convolution</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#edge-detection-pipeline","title":"Edge Detection Pipeline","text":"<p>Complete edge detection typically involves:</p> <ol> <li>Smoothing: Gaussian blur to reduce noise</li> <li>Gradient computation: Sobel or similar operator</li> <li>Non-maximum suppression: Thin edges to single-pixel width</li> <li>Thresholding: Keep only strong edges (hysteresis thresholding in Canny)</li> </ol> <p>The Canny edge detector implements this full pipeline and remains widely used.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#fourier-transform-for-images","title":"Fourier Transform for Images","text":"<p>The Fourier transform represents images in the frequency domain, revealing periodic patterns and enabling frequency-based filtering.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-2d-discrete-fourier-transform","title":"The 2D Discrete Fourier Transform","text":"<p>For an \\(M \\times N\\) image, the 2D DFT is:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#2d-discrete-fourier-transform","title":"2D Discrete Fourier Transform","text":"<p>\\(F(u, v) = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x, y) \\cdot e^{-2\\pi i \\left(\\frac{ux}{M} + \\frac{vy}{N}\\right)}\\)</p> <p>where:</p> <ul> <li>\\(f(x, y)\\) is the spatial domain image</li> <li>\\(F(u, v)\\) is the frequency domain representation</li> <li>\\((u, v)\\) are frequency coordinates</li> <li>Low frequencies (near center) represent gradual changes</li> <li>High frequencies (near edges) represent rapid changes</li> </ul> <p>The inverse transform recovers the original image:</p> <p>\\(f(x, y) = \\frac{1}{MN} \\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} F(u, v) \\cdot e^{2\\pi i \\left(\\frac{ux}{M} + \\frac{vy}{N}\\right)}\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#frequency-domain-interpretation","title":"Frequency Domain Interpretation","text":"Frequency Region Spatial Meaning DC component (center) Average brightness Low frequencies Smooth regions, gradual changes High frequencies Edges, textures, fine detail Specific frequency Periodic patterns (e.g., stripes) <p>The magnitude spectrum \\(|F(u,v)|\\) shows which frequencies are present.</p> <p>The phase spectrum \\(\\angle F(u,v)\\) encodes spatial structure\u2014surprisingly, phase carries more perceptual information than magnitude.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-fourier-transform-visualizer","title":"Diagram: Fourier Transform Visualizer","text":"<p>Run the Fourier Transform Visualizer Fullscreen</p> Fourier Transform Visualizer <p>Type: microsim</p> <p>Learning objective: Connect spatial image features to frequency domain representation (Bloom: Analyze)</p> <p>Visual elements: - Original image (left) - Magnitude spectrum (center, log-scaled for visibility) - Phase spectrum (right, as color wheel or grayscale) - Reconstructed image after filtering</p> <p>Interactive controls: - Image selector: Stripes, checkerboard, photo, text - Frequency filter: Low-pass, high-pass, band-pass (adjustable radius) - Toggle: Show magnitude/phase/both - Slider: Filter cutoff frequency - Button: Apply filter and show reconstruction</p> <p>Canvas layout: 900x600px with three-panel display</p> <p>Behavior: - Compute and display FFT magnitude (log scale) - Interactive frequency selection by clicking on spectrum - Show effect of zeroing selected frequencies - Real-time filter preview on reconstruction</p> <p>Implementation: p5.js with FFT library (or pre-computed examples)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#frequency-domain-filtering","title":"Frequency Domain Filtering","text":"<p>Filtering in the frequency domain involves:</p> <ol> <li>Compute FFT of image: \\(F = \\mathcal{F}(f)\\)</li> <li>Multiply by filter: \\(G = F \\cdot H\\)</li> <li>Compute inverse FFT: \\(g = \\mathcal{F}^{-1}(G)\\)</li> </ol> <p>Common frequency domain filters:</p> Filter Frequency Mask Effect Ideal low-pass 1 if \\(\\sqrt{u^2+v^2} &lt; r\\), else 0 Blur (with ringing) Gaussian low-pass \\(e^{-(u^2+v^2)/2\\sigma^2}\\) Smooth blur Ideal high-pass 0 if \\(\\sqrt{u^2+v^2} &lt; r\\), else 1 Edge enhancement Notch filter 0 at specific frequencies Remove periodic noise <p>The convolution theorem connects spatial and frequency domains:</p> <p>\\(f * g = \\mathcal{F}^{-1}(\\mathcal{F}(f) \\cdot \\mathcal{F}(g))\\)</p> <p>Convolution in spatial domain equals multiplication in frequency domain.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#image-compression","title":"Image Compression","text":"<p>Image compression reduces storage size by exploiting redundancies in image data. Linear algebra provides key tools for lossy compression.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#svd-based-compression","title":"SVD-Based Compression","text":"<p>The Singular Value Decomposition represents an image as a sum of rank-1 matrices:</p> <p>\\(\\mathbf{I} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\)</p> <p>where:</p> <ul> <li>\\(\\sigma_i\\) are singular values (in decreasing order)</li> <li>\\(\\mathbf{u}_i, \\mathbf{v}_i\\) are left and right singular vectors</li> <li>\\(r\\) is the rank of the image matrix</li> </ul> <p>Keeping only the \\(k\\) largest singular values gives the best rank-\\(k\\) approximation:</p> <p>\\(\\mathbf{I}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top\\)</p> Original Storage Compressed Storage Compression Ratio \\(H \\times W\\) values \\(k(H + W + 1)\\) values \\(\\frac{HW}{k(H+W+1)}\\) <p>For a 1000\u00d71000 image with \\(k=50\\): compression ratio \u2248 10:1.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-svd-compression-visualizer","title":"Diagram: SVD Compression Visualizer","text":"<p>Run the SVD Compression Visualizer Fullscreen</p> SVD Compression Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how SVD achieves image compression by truncating singular values (Bloom: Apply)</p> <p>Visual elements: - Original image - Reconstructed image from k singular values - Singular value plot (bar chart or line, log scale) - Error image (original - reconstructed) - Compression statistics display</p> <p>Interactive controls: - Slider: Number of singular values k (1 to full rank) - Image selector: Choose sample image - Toggle: Show error image - Toggle: Log scale for singular values - Display: Compression ratio, PSNR, storage size</p> <p>Canvas layout: 850x650px with image comparison and statistics</p> <p>Behavior: - Real-time reconstruction as k changes - Show how image quality improves with more singular values - Display storage calculation - Highlight \"knee\" in singular value curve</p> <p>Implementation: p5.js with pre-computed SVD (for performance)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#transform-coding-jpeg","title":"Transform Coding (JPEG)","text":"<p>JPEG compression uses the Discrete Cosine Transform (DCT):</p> <ol> <li>Divide image into 8\u00d78 blocks</li> <li>Apply 2D DCT to each block</li> <li>Quantize coefficients (lossy step)</li> <li>Entropy encode (lossless)</li> </ol> <p>The DCT concentrates energy in low-frequency coefficients, which are preserved, while high-frequency coefficients (fine detail) are quantized more aggressively.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#comparison-of-compression-approaches","title":"Comparison of Compression Approaches","text":"Method Type Compression Quality Control SVD truncation Lossy Variable Number of singular values JPEG (DCT) Lossy High Quality factor (1-100) PNG (lossless) Lossless Moderate N/A WebP Both High Quality factor"},{"location":"chapters/13-image-processing-and-computer-vision/#feature-detection","title":"Feature Detection","text":"<p>Feature detection identifies distinctive points in images that can be matched across views, enabling applications like image stitching, object recognition, and 3D reconstruction.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#what-makes-a-good-feature","title":"What Makes a Good Feature?","text":"<p>A good feature should be:</p> <ul> <li>Distinctive: Different from its neighbors</li> <li>Repeatable: Detectable despite changes in viewpoint, lighting, scale</li> <li>Localizable: Precisely positioned</li> </ul> Feature Type Detection Method Invariance Corners Harris, Shi-Tomasi Rotation Blobs DoG (SIFT), Hessian (SURF) Rotation, Scale Edges Canny, Sobel Limited"},{"location":"chapters/13-image-processing-and-computer-vision/#harris-corner-detection","title":"Harris Corner Detection","text":"<p>The Harris corner detector uses the structure tensor (second moment matrix):</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#structure-tensor","title":"Structure Tensor","text":"<p>\\(\\mathbf{M} = \\sum_{(x,y) \\in W} \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(I_x, I_y\\) are image derivatives</li> <li>\\(W\\) is a local window (often Gaussian-weighted)</li> <li>The eigenvalues of \\(\\mathbf{M}\\) characterize the local structure</li> </ul> Eigenvalue Pattern Structure Feature Type Both small Flat region Not a feature One large, one small Edge Edge point Both large Corner Good feature <p>The Harris corner response:</p> <p>\\(R = \\det(\\mathbf{M}) - k \\cdot \\text{trace}(\\mathbf{M})^2 = \\lambda_1 \\lambda_2 - k(\\lambda_1 + \\lambda_2)^2\\)</p> <p>where \\(k \\approx 0.04\\). Corners have high \\(R\\) values.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-corner-detection-visualizer","title":"Diagram: Corner Detection Visualizer","text":"<p>Run the Corner Detection Visualizer Fullscreen</p> Corner Detection Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how eigenvalues of the structure tensor identify corners (Bloom: Analyze)</p> <p>Visual elements: - Input image with detected corners marked - Structure tensor ellipse visualization at selected points - Eigenvalue scatter plot (\u03bb1 vs \u03bb2) - Corner response heat map</p> <p>Interactive controls: - Image selector: Checkerboard, building, shapes - Slider: Harris k parameter (0.01 to 0.1) - Slider: Response threshold - Click on image to show local structure tensor - Toggle: Show response heat map vs corner points</p> <p>Canvas layout: 850x650px with image and analysis panels</p> <p>Behavior: - Compute Harris response for entire image - Mark corners above threshold - Click on pixel to show structure tensor as ellipse - Ellipse axes aligned with eigenvectors, lengths proportional to eigenvalues</p> <p>Implementation: p5.js with gradient and matrix computation</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#scale-invariant-features","title":"Scale-Invariant Features","text":"<p>SIFT (Scale-Invariant Feature Transform) detects features at multiple scales:</p> <ol> <li>Build scale space using Gaussian blur at multiple \\(\\sigma\\) values</li> <li>Detect blob-like structures as extrema in Difference-of-Gaussian (DoG)</li> <li>Compute orientation from local gradients</li> <li>Build descriptor from gradient histograms</li> </ol> <p>The 128-dimensional SIFT descriptor is robust to scale, rotation, and moderate viewpoint changes.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#homography","title":"Homography","text":"<p>A homography is a projective transformation between two planes, essential for perspective correction, panorama stitching, and augmented reality.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#the-homography-matrix","title":"The Homography Matrix","text":"<p>A homography maps points from one image to another via a 3\u00d73 matrix:</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#homography-transformation","title":"Homography Transformation","text":"<p>\\(\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\sim \\mathbf{H} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} h_{11} &amp; h_{12} &amp; h_{13} \\\\ h_{21} &amp; h_{22} &amp; h_{23} \\\\ h_{31} &amp; h_{32} &amp; h_{33} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\((x, y)\\) and \\((x', y')\\) are corresponding points in homogeneous coordinates</li> <li>\\(\\sim\\) denotes equality up to scale</li> <li>\\(\\mathbf{H}\\) has 8 degrees of freedom (9 elements minus 1 for scale)</li> </ul> <p>To recover Cartesian coordinates:</p> <p>\\(x' = \\frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}, \\quad y' = \\frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}\\)</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#estimating-homographies","title":"Estimating Homographies","text":"<p>Given \\(n \\geq 4\\) point correspondences, we can estimate \\(\\mathbf{H}\\) by solving a linear system.</p> <p>Each correspondence \\((x_i, y_i) \\leftrightarrow (x_i', y_i')\\) provides two equations:</p> <p>\\(\\begin{bmatrix} x_i &amp; y_i &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; -x_i'x_i &amp; -x_i'y_i &amp; -x_i' \\\\ 0 &amp; 0 &amp; 0 &amp; x_i &amp; y_i &amp; 1 &amp; -y_i'x_i &amp; -y_i'y_i &amp; -y_i' \\end{bmatrix} \\mathbf{h} = \\mathbf{0}\\)</p> <p>With 4+ correspondences, solve using SVD (find the null space of the coefficient matrix).</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#transformation-hierarchy","title":"Transformation Hierarchy","text":"<p>Homographies are part of a hierarchy of 2D transformations:</p> Transformation DOF Preserves Matrix Form Translation 2 Everything $[I Euclidean (rigid) 3 Distances, angles $[R Similarity 4 Angles, ratios $[sR Affine 6 Parallelism $[A Projective (homography) 8 Straight lines General 3\u00d73"},{"location":"chapters/13-image-processing-and-computer-vision/#diagram-homography-transformation-demo","title":"Diagram: Homography Transformation Demo","text":"<p>Run the Homography Transformation Demo Fullscreen</p> Homography Transformation Demo <p>Type: microsim</p> <p>Learning objective: Understand how homographies transform images for perspective correction (Bloom: Apply)</p> <p>Visual elements: - Source image with draggable corner points - Transformed image showing perspective effect - Homography matrix display - Grid overlay showing transformation</p> <p>Interactive controls: - Drag corners of source quadrilateral - Preset buttons: Perspective, rotation, shear, stretch - Button: \"Reset to identity\" - Toggle: Show transformation grid - Display: Homography matrix values</p> <p>Canvas layout: 800x600px with side-by-side source and result</p> <p>Behavior: - Compute homography from corner positions in real-time - Apply inverse warp to display transformed image - Show how parallel lines may not remain parallel - Demonstrate perspective correction (make rectangle from trapezoid)</p> <p>Implementation: p5.js with bilinear interpolation for warping</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#applications-of-homography","title":"Applications of Homography","text":"Application How Homography Is Used Panorama stitching Align overlapping images Perspective correction Straighten tilted documents/signs Augmented reality Overlay virtual objects on planar surfaces Sports graphics Insert ads on playing field Image rectification Correct lens distortion"},{"location":"chapters/13-image-processing-and-computer-vision/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\ndef compute_homography(src_pts, dst_pts):\n    \"\"\"\n    Compute homography from 4+ point correspondences.\n    src_pts, dst_pts: Nx2 arrays of corresponding points\n    \"\"\"\n    n = src_pts.shape[0]\n    A = []\n\n    for i in range(n):\n        x, y = src_pts[i]\n        xp, yp = dst_pts[i]\n        A.append([-x, -y, -1, 0, 0, 0, x*xp, y*xp, xp])\n        A.append([0, 0, 0, -x, -y, -1, x*yp, y*yp, yp])\n\n    A = np.array(A)\n\n    # Solve using SVD\n    _, _, Vt = np.linalg.svd(A)\n    H = Vt[-1].reshape(3, 3)\n\n    return H / H[2, 2]  # Normalize\n\ndef apply_homography(H, point):\n    \"\"\"Apply homography to a single point.\"\"\"\n    x, y = point\n    p = np.array([x, y, 1])\n    p_prime = H @ p\n    return p_prime[:2] / p_prime[2]\n</code></pre>"},{"location":"chapters/13-image-processing-and-computer-vision/#summary_1","title":"Summary","text":"<p>Image processing and computer vision are built on linear algebra foundations:</p> <p>Image Representation:</p> <ul> <li>Images are matrices (grayscale) or tensors (color)</li> <li>RGB images have three channels; other color spaces serve specific purposes</li> <li>Color space transforms are matrix operations</li> </ul> <p>Filtering and Convolution:</p> <ul> <li>Convolution applies kernels to extract features</li> <li>Blur filters average neighbors; sharpen filters enhance differences</li> <li>Edge detection uses derivative-approximating kernels like Sobel</li> </ul> <p>Frequency Domain:</p> <ul> <li>Fourier transform reveals periodic structure</li> <li>Low frequencies encode smooth regions; high frequencies encode edges</li> <li>Frequency filtering multiplies spectra</li> </ul> <p>Compression:</p> <ul> <li>SVD truncation provides rank-based compression</li> <li>Transform coding (DCT/JPEG) exploits frequency concentration</li> </ul> <p>Features and Geometry:</p> <ul> <li>Corner detection uses structure tensor eigenvalues</li> <li>Homographies are projective transformations between planes</li> <li>4 point correspondences determine a homography</li> </ul>"},{"location":"chapters/13-image-processing-and-computer-vision/#self-check-questions","title":"Self-Check Questions","text":"Why does Gaussian blur produce smoother results than box blur? <p>Box blur weights all pixels in the kernel equally, creating abrupt transitions at the kernel boundary. This causes visible artifacts, especially in smooth gradients. Gaussian blur weights central pixels more heavily, with weights falling off smoothly according to the Gaussian function. This smooth weighting produces gradual blending without sharp discontinuities. Additionally, the Gaussian kernel is separable, meaning a 2D Gaussian blur can be computed as two 1D blurs, making it computationally efficient.</p> How does the Sobel operator combine smoothing with differentiation? <p>The Sobel kernels have a specific structure: they can be decomposed as the outer product of a smoothing filter [1, 2, 1] and a difference filter [-1, 0, 1]. For example, Gx = [1, 2, 1]^T \u00d7 [-1, 0, 1]. The [1, 2, 1] component provides Gaussian-like smoothing perpendicular to the derivative direction, reducing noise sensitivity. The [-1, 0, 1] component computes the derivative. This combination makes Sobel more robust than simple difference operators while maintaining good edge localization.</p> Why does keeping the largest singular values give the best low-rank approximation? <p>The Eckart-Young-Mirsky theorem proves that the truncated SVD gives the optimal low-rank approximation in both Frobenius and spectral norms. Intuitively, each singular value represents the \"energy\" captured by that component\u2014larger singular values correspond to more significant patterns in the image. By keeping the largest singular values, we retain the most important structure while discarding fine details associated with smaller singular values. The approximation error equals the sum of squared discarded singular values.</p> What is the geometric meaning of the Harris corner response being large? <p>The Harris response R = \u03bb\u2081\u03bb\u2082 - k(\u03bb\u2081 + \u03bb\u2082)\u00b2 is large when both eigenvalues of the structure tensor are large. The eigenvalues measure how quickly image intensity changes in the principal directions. At a corner, intensity changes significantly in both directions (both eigenvalues large). At an edge, intensity changes in only one direction (one eigenvalue large, one small). In a flat region, intensity is constant (both eigenvalues small). The product \u03bb\u2081\u03bb\u2082 in the response ensures both must be large.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/","title":"Quiz: Image Processing and Computer Vision","text":"<p>Test your understanding of image representation, filtering, and computer vision concepts.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#1-a-grayscale-image-is-represented-as","title":"1. A grayscale image is represented as:","text":"<ol> <li>A 3D tensor with RGB channels</li> <li>A 2D matrix of intensity values</li> <li>A 1D vector of pixel positions</li> <li>A list of color names</li> </ol> Show Answer <p>The correct answer is B. A grayscale image is a 2D matrix where each entry represents the intensity (brightness) of a pixel, typically ranging from 0 (black) to 255 (white).</p> <p>Concept Tested: Grayscale Image</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#2-an-rgb-image-is-stored-as","title":"2. An RGB image is stored as:","text":"<ol> <li>A single 2D matrix</li> <li>Three separate matrices (one per color channel)</li> <li>A 1D array of values</li> <li>A text file</li> </ol> Show Answer <p>The correct answer is B. An RGB image consists of three 2D matrices (or a 3D tensor), one for each color channel: Red, Green, and Blue. Each pixel has three values.</p> <p>Concept Tested: RGB Image</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#3-image-convolution","title":"3. Image convolution:","text":"<ol> <li>Increases image resolution</li> <li>Applies a kernel to extract features or transform the image</li> <li>Converts color to grayscale</li> <li>Compresses the image</li> </ol> Show Answer <p>The correct answer is B. Convolution slides a kernel (small matrix) across the image, computing weighted sums at each position. Different kernels produce different effects like blurring, sharpening, or edge detection.</p> <p>Concept Tested: Image Convolution</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#4-a-gaussian-blur-filter","title":"4. A Gaussian blur filter:","text":"<ol> <li>Sharpens edges in the image</li> <li>Smooths the image by averaging with Gaussian-weighted neighbors</li> <li>Detects vertical edges only</li> <li>Increases image contrast</li> </ol> Show Answer <p>The correct answer is B. Gaussian blur uses weights that follow a Gaussian distribution, giving more weight to nearby pixels. This produces smooth, natural-looking blur without blocky artifacts.</p> <p>Concept Tested: Blur Filter</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#5-the-sobel-operator-is-used-for","title":"5. The Sobel operator is used for:","text":"<ol> <li>Image compression</li> <li>Edge detection by computing image gradients</li> <li>Color space conversion</li> <li>Image resizing</li> </ol> Show Answer <p>The correct answer is B. The Sobel operator approximates image derivatives (gradients) to detect edges. It uses two kernels: one for horizontal gradients (\\(G_x\\)) and one for vertical gradients (\\(G_y\\)).</p> <p>Concept Tested: Sobel Operator</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#6-the-fourier-transform-of-an-image-reveals","title":"6. The Fourier transform of an image reveals:","text":"<ol> <li>The color distribution</li> <li>Frequency components (how quickly intensity changes)</li> <li>The number of objects</li> <li>The image dimensions</li> </ol> Show Answer <p>The correct answer is B. The Fourier transform decomposes an image into frequency components. Low frequencies represent gradual changes (smooth areas); high frequencies represent rapid changes (edges, textures).</p> <p>Concept Tested: Fourier Transform</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#7-svd-based-image-compression-works-by","title":"7. SVD-based image compression works by:","text":"<ol> <li>Removing all color information</li> <li>Keeping only the largest singular values and discarding the rest</li> <li>Reducing the number of pixels</li> <li>Converting to a different file format</li> </ol> Show Answer <p>The correct answer is B. SVD compression keeps only the \\(k\\) largest singular values, which capture the most important structure. The approximation \\(A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T\\) requires less storage.</p> <p>Concept Tested: Image Compression</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#8-in-harris-corner-detection-a-corner-is-characterized-by","title":"8. In Harris corner detection, a corner is characterized by:","text":"<ol> <li>Only one large eigenvalue of the structure tensor</li> <li>Two large eigenvalues of the structure tensor</li> <li>Zero eigenvalues</li> <li>Negative eigenvalues</li> </ol> Show Answer <p>The correct answer is B. At a corner, the structure tensor has two large eigenvalues, indicating significant intensity changes in both directions. An edge has one large eigenvalue; flat regions have none.</p> <p>Concept Tested: Feature Detection</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#9-a-homography-transformation","title":"9. A homography transformation:","text":"<ol> <li>Only handles rotation</li> <li>Maps points between two planes, handling perspective</li> <li>Is always the identity</li> <li>Only works on 1D signals</li> </ol> Show Answer <p>The correct answer is B. A homography is a 3\u00d73 projective transformation that maps points between two planes. It can represent perspective distortion, rotation, scaling, and translation.</p> <p>Concept Tested: Homography</p>"},{"location":"chapters/13-image-processing-and-computer-vision/quiz/#10-the-convolution-theorem-states-that-convolution-in-spatial-domain-equals","title":"10. The convolution theorem states that convolution in spatial domain equals:","text":"<ol> <li>Addition in frequency domain</li> <li>Multiplication in frequency domain</li> <li>Division in frequency domain</li> <li>Convolution in frequency domain</li> </ol> Show Answer <p>The correct answer is B. The convolution theorem states that \\(f * g = \\mathcal{F}^{-1}(\\mathcal{F}(f) \\cdot \\mathcal{F}(g))\\). Convolution in spatial domain corresponds to element-wise multiplication in frequency domain.</p> <p>Concept Tested: Frequency Domain</p>"},{"location":"chapters/14-3d-geometry-and-transformations/","title":"3D Geometry and Transformations","text":""},{"location":"chapters/14-3d-geometry-and-transformations/#summary","title":"Summary","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers 3D coordinate systems, rotation representations including Euler angles and quaternions, homogeneous coordinates, camera models with intrinsic and extrinsic parameters, stereo vision, and point cloud processing. These concepts form the foundation for any system that operates in 3D space.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 13: Image Processing and Computer Vision</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#introduction","title":"Introduction","text":"<p>From robot arms assembling cars to AR headsets overlaying digital content on the physical world, systems that interact with 3D space require precise mathematical representations of position, orientation, and motion. Linear algebra provides the essential tools:</p> <ul> <li>Coordinate systems establish frames of reference</li> <li>Rotation matrices and quaternions represent orientation</li> <li>Homogeneous coordinates unify rotation and translation</li> <li>Camera models project 3D scenes onto 2D images</li> <li>Stereo vision recovers 3D structure from multiple views</li> </ul> <p>This chapter develops the geometric foundations needed for robotics, computer vision, augmented reality, and autonomous systems.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#3d-coordinate-systems","title":"3D Coordinate Systems","text":"<p>A 3D coordinate system defines how we specify positions in three-dimensional space using three numbers \\((x, y, z)\\).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#right-hand-vs-left-hand-systems","title":"Right-Hand vs Left-Hand Systems","text":"<p>Two conventions exist for orienting the axes:</p> Convention X Direction Y Direction Z Direction Used By Right-hand Right/East Up/North Out of screen OpenGL, ROS, most math Left-hand Right Up Into screen DirectX, Unity <p>In a right-hand system, if you curl your right hand's fingers from X toward Y, your thumb points in the Z direction.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#common-coordinate-frames","title":"Common Coordinate Frames","text":"<p>Different applications use different conventions:</p> Domain X Y Z Origin Computer graphics Right Up Out (toward viewer) Screen center Robotics (ROS) Forward Left Up Robot base Aviation Forward Right Down Aircraft center Camera Right Down Forward (optical axis) Camera center <p>Understanding and converting between coordinate frames is essential for integrating sensors, actuators, and displays.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-3d-coordinate-system-visualizer","title":"Diagram: 3D Coordinate System Visualizer","text":"<p>Run the 3D Coordinate System Visualizer Fullscreen</p> 3D Coordinate System Visualizer <p>Type: microsim</p> <p>Learning objective: Understand different 3D coordinate system conventions and handedness (Bloom: Understand)</p> <p>Visual elements: - 3D view with X, Y, Z axes (color coded: X=red, Y=green, Z=blue) - Origin sphere - Sample point with coordinates displayed - Grid planes (XY, XZ, YZ) toggleable - Animated right-hand rule demonstration</p> <p>Interactive controls: - Toggle: Right-hand vs Left-hand system - Dropdown: Convention preset (OpenGL, DirectX, ROS, Camera) - Draggable point in 3D space - Rotate view with mouse drag - Toggle grid planes on/off</p> <p>Canvas layout: 700x600px with 3D view using WEBGL</p> <p>Behavior: - Switching handedness mirrors the Z axis - Presets reorient axes according to convention - Coordinate display updates as point moves - Show transformation matrix between conventions</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>Homogeneous coordinates extend Cartesian coordinates with an additional dimension, enabling rotation and translation to be combined in a single matrix multiplication.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#from-cartesian-to-homogeneous","title":"From Cartesian to Homogeneous","text":"<p>A 3D point \\((x, y, z)\\) becomes a 4D homogeneous point:</p> <p>\\((x, y, z) \\rightarrow \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}\\)</p> <p>To convert back, divide by the fourth component:</p> <p>\\(\\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ W \\end{bmatrix} \\rightarrow \\left(\\frac{X}{W}, \\frac{Y}{W}, \\frac{Z}{W}\\right)\\)</p> <p>Points with \\(W = 0\\) represent directions (points at infinity).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#why-homogeneous-coordinates","title":"Why Homogeneous Coordinates?","text":"<p>In standard 3D coordinates:</p> <ul> <li>Rotation: \\(\\mathbf{p}' = \\mathbf{R}\\mathbf{p}\\) (matrix multiplication)</li> <li>Translation: \\(\\mathbf{p}' = \\mathbf{p} + \\mathbf{t}\\) (addition)</li> </ul> <p>We cannot combine these into a single matrix. With homogeneous coordinates:</p> <p>\\(\\begin{bmatrix} \\mathbf{p}' \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{R} &amp; \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{p} \\\\ 1 \\end{bmatrix}\\)</p> <p>Now both operations are matrix multiplications, enabling efficient composition of multiple transformations.</p> Operation 3\u00d73 Matrix 4\u00d74 Homogeneous Rotation only Yes Yes Translation only No Yes Scale Yes Yes Rotation + Translation No Yes Projection No Yes"},{"location":"chapters/14-3d-geometry-and-transformations/#rotation-representations","title":"Rotation Representations","text":"<p>Representing 3D rotations is surprisingly subtle. Several representations exist, each with trade-offs.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#euler-angles","title":"Euler Angles","text":"<p>Euler angles describe a rotation as three successive rotations about coordinate axes. The most common convention is:</p> <ol> <li>Rotate by \\(\\psi\\) (yaw) about Z axis</li> <li>Rotate by \\(\\theta\\) (pitch) about Y axis</li> <li>Rotate by \\(\\phi\\) (roll) about X axis</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#euler-angle-rotation-matrices","title":"Euler Angle Rotation Matrices","text":"<p>\\(\\mathbf{R}_z(\\psi) = \\begin{bmatrix} \\cos\\psi &amp; -\\sin\\psi &amp; 0 \\\\ \\sin\\psi &amp; \\cos\\psi &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>\\(\\mathbf{R}_y(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; 0 &amp; \\sin\\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\theta &amp; 0 &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>\\(\\mathbf{R}_x(\\phi) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\phi &amp; -\\sin\\phi \\\\ 0 &amp; \\sin\\phi &amp; \\cos\\phi \\end{bmatrix}\\)</p> <p>The combined rotation: \\(\\mathbf{R} = \\mathbf{R}_z(\\psi) \\mathbf{R}_y(\\theta) \\mathbf{R}_x(\\phi)\\)</p> <p>Different conventions exist (XYZ, ZYX, ZXZ, etc.)\u2014always verify which convention is used!</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-euler-angles-visualizer","title":"Diagram: Euler Angles Visualizer","text":"<p>Run the Euler Angles Visualizer Fullscreen</p> Euler Angles Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Euler angles compose to form rotations (Bloom: Apply)</p> <p>Visual elements: - 3D coordinate frame (object frame) - Reference coordinate frame (world frame) - 3D object (e.g., airplane or box) showing orientation - Arc indicators for each angle - Rotation axis highlighted during animation</p> <p>Interactive controls: - Slider: Yaw (\u03c8) from -180\u00b0 to 180\u00b0 - Slider: Pitch (\u03b8) from -90\u00b0 to 90\u00b0 - Slider: Roll (\u03c6) from -180\u00b0 to 180\u00b0 - Dropdown: Convention (ZYX, XYZ, ZXZ) - Button: \"Animate sequence\" (show rotations step by step) - Button: \"Reset\"</p> <p>Canvas layout: 750x600px with 3D WEBGL view</p> <p>Behavior: - Update object orientation in real-time as sliders change - Animation shows each rotation applied sequentially - Display rotation matrix numerically - Highlight gimbal lock region when pitch approaches \u00b190\u00b0</p> <p>Implementation: p5.js with WEBGL and matrix computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#gimbal-lock","title":"Gimbal Lock","text":"<p>Gimbal lock occurs when two rotation axes align, causing loss of one degree of freedom. In the ZYX convention, when pitch \\(\\theta = \\pm 90\u00b0\\):</p> <p>\\(\\mathbf{R} = \\begin{bmatrix} 0 &amp; \\mp\\sin(\\psi \\mp \\phi) &amp; \\pm\\cos(\\psi \\mp \\phi) \\\\ 0 &amp; \\cos(\\psi \\mp \\phi) &amp; \\sin(\\psi \\mp \\phi) \\\\ \\mp 1 &amp; 0 &amp; 0 \\end{bmatrix}\\)</p> <p>Notice that yaw and roll combine into a single term \\((\\psi \\mp \\phi)\\)\u2014we can only control their sum or difference, not each independently. The system has lost a degree of freedom.</p> Pitch Value Effect \\(\\theta = 0\u00b0\\) Full 3 DOF, no gimbal lock \\(\\theta = \\pm 45\u00b0\\) Slight coupling between yaw/roll \\(\\theta = \\pm 90\u00b0\\) Complete gimbal lock, only 2 DOF <p>Gimbal lock is a fundamental problem with Euler angles, not a bug in implementation. It motivated the development of quaternions.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-gimbal-lock-demonstration","title":"Diagram: Gimbal Lock Demonstration","text":"<p>Run the Gimbal Lock Demo Fullscreen</p> Gimbal Lock Demonstration <p>Type: microsim</p> <p>Learning objective: Experience gimbal lock and understand why it occurs (Bloom: Analyze)</p> <p>Visual elements: - Physical gimbal mechanism (three nested rings) - Object attached to innermost ring - Color-coded rotation axes - Warning indicator when approaching gimbal lock</p> <p>Interactive controls: - Slider: Outer ring rotation (yaw) - Slider: Middle ring rotation (pitch) - Slider: Inner ring rotation (roll) - Button: \"Go to gimbal lock\" (set pitch to 90\u00b0) - Button: \"Reset\"</p> <p>Canvas layout: 700x600px with 3D gimbal visualization</p> <p>Behavior: - As pitch approaches 90\u00b0, outer and inner rings align visually - At gimbal lock, changing yaw and roll produces same motion - Display effective degrees of freedom (3 normally, 2 at lock) - Show that some orientations become unreachable</p> <p>Implementation: p5.js with WEBGL, mechanical gimbal model</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternions","title":"Quaternions","text":"<p>Quaternions provide a singularity-free representation of 3D rotations, avoiding gimbal lock.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-definition","title":"Quaternion Definition","text":"<p>A quaternion has four components:</p> <p>\\(\\mathbf{q} = w + xi + yj + zk = (w, x, y, z)\\)</p> <p>where:</p> <ul> <li>\\(w\\) is the scalar (real) part</li> <li>\\((x, y, z)\\) is the vector (imaginary) part</li> <li>\\(i, j, k\\) satisfy: \\(i^2 = j^2 = k^2 = ijk = -1\\)</li> </ul> <p>For representing rotations, we use unit quaternions with \\(\\|\\mathbf{q}\\| = 1\\):</p> <p>\\(w^2 + x^2 + y^2 + z^2 = 1\\)</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-from-axis-angle","title":"Quaternion from Axis-Angle","text":"<p>A rotation of angle \\(\\theta\\) about unit axis \\(\\hat{\\mathbf{n}} = (n_x, n_y, n_z)\\):</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#axis-angle-to-quaternion","title":"Axis-Angle to Quaternion","text":"<p>\\(\\mathbf{q} = \\left(\\cos\\frac{\\theta}{2}, \\sin\\frac{\\theta}{2} \\cdot n_x, \\sin\\frac{\\theta}{2} \\cdot n_y, \\sin\\frac{\\theta}{2} \\cdot n_z\\right)\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the rotation angle</li> <li>\\(\\hat{\\mathbf{n}}\\) is the rotation axis (unit vector)</li> <li>Note the half-angle: rotating by \\(\\theta\\) uses \\(\\frac{\\theta}{2}\\) in the quaternion</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-rotation","title":"Quaternion Rotation","text":"<p>To rotate a point \\(\\mathbf{p} = (p_x, p_y, p_z)\\) by quaternion \\(\\mathbf{q}\\):</p> <ol> <li>Express the point as a pure quaternion: \\(\\mathbf{p}_q = (0, p_x, p_y, p_z)\\)</li> <li>Compute: \\(\\mathbf{p}'_q = \\mathbf{q} \\cdot \\mathbf{p}_q \\cdot \\mathbf{q}^*\\)</li> <li>Extract the vector part as the rotated point</li> </ol> <p>where \\(\\mathbf{q}^* = (w, -x, -y, -z)\\) is the quaternion conjugate.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#quaternion-multiplication","title":"Quaternion Multiplication","text":"<p>Quaternion multiplication (non-commutative):</p> <p>\\(\\mathbf{q}_1 \\cdot \\mathbf{q}_2 = (w_1 w_2 - \\mathbf{v}_1 \\cdot \\mathbf{v}_2, \\; w_1 \\mathbf{v}_2 + w_2 \\mathbf{v}_1 + \\mathbf{v}_1 \\times \\mathbf{v}_2)\\)</p> <p>Composing rotations: \\(\\mathbf{q}_{total} = \\mathbf{q}_2 \\cdot \\mathbf{q}_1\\) (apply \\(\\mathbf{q}_1\\) first, then \\(\\mathbf{q}_2\\))</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#comparison-of-rotation-representations","title":"Comparison of Rotation Representations","text":"Representation Parameters Singularities Interpolation Composition Euler angles 3 Gimbal lock Poor Matrix mult Rotation matrix 9 (with constraints) None Complex Matrix mult Axis-angle 4 (3 + 1) At \\(\\theta = 0\\) Linear Complex Quaternion 4 (unit constraint) None SLERP Quaternion mult"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-quaternion-rotation-visualizer","title":"Diagram: Quaternion Rotation Visualizer","text":"<p>Run the Quaternion Rotation Visualizer Fullscreen</p> Quaternion Rotation Visualizer <p>Type: microsim</p> <p>Learning objective: Understand quaternion representation and rotation operation (Bloom: Apply)</p> <p>Visual elements: - 3D object with current orientation - Rotation axis displayed as arrow through origin - Arc showing rotation angle - Quaternion components displayed numerically - Unit sphere showing quaternion (projected)</p> <p>Interactive controls: - Axis input: Unit vector (nx, ny, nz) or draggable direction - Slider: Rotation angle \u03b8 (0\u00b0 to 360\u00b0) - Button: \"Apply rotation\" - Button: \"Compose with previous\" - Toggle: Show equivalent Euler angles</p> <p>Canvas layout: 800x600px with 3D visualization</p> <p>Behavior: - Display quaternion (w, x, y, z) updating with axis/angle - Show rotation applied to 3D object - Demonstrate composition of multiple rotations - Compare with Euler angle representation - Show smooth interpolation (SLERP) between orientations</p> <p>Implementation: p5.js with WEBGL and quaternion math</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#rigid-body-transforms","title":"Rigid Body Transforms","text":"<p>A rigid body transform combines rotation and translation, preserving distances and angles. It represents the pose (position and orientation) of an object in 3D space.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#se3-the-special-euclidean-group","title":"SE(3): The Special Euclidean Group","text":"<p>SE(3) is the mathematical group of all rigid body transformations in 3D. An SE(3) transform consists of:</p> <ul> <li>A rotation \\(\\mathbf{R} \\in SO(3)\\) (3\u00d73 orthogonal matrix with det = 1)</li> <li>A translation \\(\\mathbf{t} \\in \\mathbb{R}^3\\)</li> </ul> <p>In homogeneous coordinates:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#se3-transformation-matrix","title":"SE(3) Transformation Matrix","text":"<p>\\(\\mathbf{T} = \\begin{bmatrix} \\mathbf{R} &amp; \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix} = \\begin{bmatrix} r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \\\\ r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \\\\ r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{R}\\) is a 3\u00d73 rotation matrix</li> <li>\\(\\mathbf{t} = (t_x, t_y, t_z)^\\top\\) is the translation vector</li> <li>The bottom row \\([0, 0, 0, 1]\\) maintains homogeneous structure</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#composing-transforms","title":"Composing Transforms","text":"<p>Multiple transformations compose by matrix multiplication:</p> <p>\\(\\mathbf{T}_{total} = \\mathbf{T}_n \\cdot \\mathbf{T}_{n-1} \\cdots \\mathbf{T}_2 \\cdot \\mathbf{T}_1\\)</p> <p>Transforms apply right-to-left: \\(\\mathbf{T}_1\\) first, then \\(\\mathbf{T}_2\\), etc.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#inverse-transform","title":"Inverse Transform","text":"<p>The inverse of an SE(3) transform:</p> <p>\\(\\mathbf{T}^{-1} = \\begin{bmatrix} \\mathbf{R}^\\top &amp; -\\mathbf{R}^\\top \\mathbf{t} \\\\ \\mathbf{0}^\\top &amp; 1 \\end{bmatrix}\\)</p> <p>Note: We use \\(\\mathbf{R}^\\top\\) instead of computing a matrix inverse since rotation matrices are orthogonal.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-rigid-body-transform-chain","title":"Diagram: Rigid Body Transform Chain","text":"<p>Run the Rigid Body Transform Chain Fullscreen</p> Rigid Body Transform Chain <p>Type: microsim</p> <p>Learning objective: Visualize composition of rigid body transforms (Bloom: Apply)</p> <p>Visual elements: - World coordinate frame at origin - Multiple linked coordinate frames (like robot arm joints) - 3D objects attached to each frame - Transform arrows showing relationships</p> <p>Interactive controls: - Sliders for each joint angle (rotations) - Sliders for link lengths (translations) - Toggle: Show individual transforms vs composed - Button: \"Add link\" - Button: \"Reset\" - Display: Full transform matrix from world to end effector</p> <p>Canvas layout: 850x650px with 3D kinematic chain</p> <p>Behavior: - Moving one joint updates all downstream frames - Show transform matrices at each stage - Highlight how composition order matters - Demonstrate forward kinematics visually</p> <p>Implementation: p5.js with WEBGL, matrix chain computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#camera-models","title":"Camera Models","text":"<p>A camera model describes how 3D points in the world project onto a 2D image. The most common model is the pinhole camera.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-pinhole-camera-model","title":"The Pinhole Camera Model","text":"<p>Light rays pass through a single point (the pinhole or optical center) and project onto an image plane:</p> <ul> <li>3D world point: \\(\\mathbf{P}_w = (X, Y, Z)\\)</li> <li>2D image point: \\(\\mathbf{p} = (u, v)\\)</li> </ul> <p>The projection involves two stages:</p> <ol> <li>Extrinsic parameters: Transform from world to camera coordinates</li> <li>Intrinsic parameters: Project from camera coordinates to image pixels</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#intrinsic-parameters","title":"Intrinsic Parameters","text":"<p>Intrinsic parameters describe the camera's internal geometry:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#camera-intrinsic-matrix","title":"Camera Intrinsic Matrix","text":"<p>\\(\\mathbf{K} = \\begin{bmatrix} f_x &amp; s &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(f_x, f_y\\) are focal lengths in pixels (may differ if pixels are non-square)</li> <li>\\((c_x, c_y)\\) is the principal point (usually near image center)</li> <li>\\(s\\) is the skew coefficient (usually 0 for modern cameras)</li> </ul> Parameter Meaning Typical Value \\(f_x, f_y\\) Focal length \u00d7 pixels/mm 500-2000 pixels \\(c_x\\) Principal point x image_width / 2 \\(c_y\\) Principal point y image_height / 2 \\(s\\) Skew 0"},{"location":"chapters/14-3d-geometry-and-transformations/#extrinsic-parameters","title":"Extrinsic Parameters","text":"<p>Extrinsic parameters describe the camera's pose in the world:</p> <p>\\([\\mathbf{R} | \\mathbf{t}]\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{R}\\) is the 3\u00d73 rotation from world to camera frame</li> <li>\\(\\mathbf{t}\\) is the translation (camera position in world, negated and rotated)</li> </ul> <p>The extrinsic matrix transforms points from world coordinates to camera coordinates.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-full-camera-matrix","title":"The Full Camera Matrix","text":"<p>The complete projection matrix combines intrinsic and extrinsic parameters:</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#projection-matrix","title":"Projection Matrix","text":"<p>\\(\\mathbf{P} = \\mathbf{K} [\\mathbf{R} | \\mathbf{t}]\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{P}\\) is a 3\u00d74 matrix</li> <li>\\(\\mathbf{K}\\) is the 3\u00d73 intrinsic matrix</li> <li>\\([\\mathbf{R} | \\mathbf{t}]\\) is the 3\u00d74 extrinsic matrix</li> </ul> <p>Projection from world to image:</p> <p>\\(\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} \\sim \\mathbf{P} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}\\)</p> <p>The \\(\\sim\\) indicates equality up to scale (divide by the third coordinate to get pixel coordinates).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-camera-model-visualizer","title":"Diagram: Camera Model Visualizer","text":"<p>Run the Camera Model Visualizer Fullscreen</p> Camera Model Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how camera parameters affect projection (Bloom: Analyze)</p> <p>Visual elements: - 3D scene with objects at various depths - Camera frustum showing field of view - Image plane with projected points - Ray lines from 3D points through camera center to image</p> <p>Interactive controls: - Slider: Focal length (affects FOV) - Slider: Principal point (cx, cy) - Camera pose controls: position and orientation - Toggle: Show projection rays - Toggle: Show camera frustum</p> <p>Canvas layout: 900x650px with 3D scene and 2D image view</p> <p>Behavior: - Changing focal length zooms in/out - Moving principal point shifts image - Moving camera updates all projections - Display intrinsic and extrinsic matrices - Show correspondence between 3D points and 2D projections</p> <p>Implementation: p5.js with WEBGL and matrix projection</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#perspective-projection","title":"Perspective Projection","text":"<p>Perspective projection is the mathematical model of how 3D points map to 2D:</p> <p>For a point \\((X, Y, Z)\\) in camera coordinates:</p> <p>\\(u = f_x \\frac{X}{Z} + c_x, \\quad v = f_y \\frac{Y}{Z} + c_y\\)</p> <p>Key properties:</p> <ul> <li>Objects farther away appear smaller (division by \\(Z\\))</li> <li>Parallel lines may converge to vanishing points</li> <li>Depth information is lost (many 3D points map to the same 2D point)</li> </ul> Projection Type Properties Use Case Perspective Realistic, depth-dependent scaling Photos, human vision Orthographic No depth scaling, parallel projection Engineering drawings, CAD Weak perspective Uniform depth for scene Approximate when depth range is small"},{"location":"chapters/14-3d-geometry-and-transformations/#stereo-vision","title":"Stereo Vision","text":"<p>Stereo vision uses two cameras to recover 3D structure, mimicking human binocular vision.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-stereo-setup","title":"The Stereo Setup","text":"<p>Two cameras with known relative pose observe the same scene:</p> <ul> <li>Baseline \\(b\\): Distance between camera centers</li> <li>Disparity \\(d\\): Difference in horizontal image position of corresponding points</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#depth-from-disparity","title":"Depth from Disparity","text":"<p>\\(Z = \\frac{f \\cdot b}{d}\\)</p> <p>where:</p> <ul> <li>\\(Z\\) is the depth (distance to the point)</li> <li>\\(f\\) is the focal length</li> <li>\\(b\\) is the baseline (distance between cameras)</li> <li>\\(d = u_L - u_R\\) is the disparity (difference in x-coordinates)</li> </ul> <p>Key insight: Larger baseline gives better depth resolution but makes matching harder.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#epipolar-geometry","title":"Epipolar Geometry","text":"<p>Epipolar geometry describes the geometric relationship between two views of the same scene.</p> <p>Key concepts:</p> <ul> <li>Epipole: Where the line connecting camera centers intersects each image plane</li> <li>Epipolar line: For a point in one image, the corresponding point in the other image lies on this line</li> <li>Epipolar plane: Plane containing both camera centers and the 3D point</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#the-fundamental-matrix","title":"The Fundamental Matrix","text":"<p>The fundamental matrix \\(\\mathbf{F}\\) encodes epipolar geometry:</p> <p>\\(\\mathbf{p}_R^\\top \\mathbf{F} \\mathbf{p}_L = 0\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{p}_L, \\mathbf{p}_R\\) are corresponding points in left and right images (homogeneous)</li> <li>\\(\\mathbf{F}\\) is a 3\u00d73 matrix with rank 2</li> <li>This constraint means \\(\\mathbf{p}_R\\) lies on the epipolar line \\(\\mathbf{l}_R = \\mathbf{F} \\mathbf{p}_L\\)</li> </ul> <p>The fundamental matrix has 7 degrees of freedom (9 elements minus scale minus rank constraint).</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-epipolar-geometry-visualizer","title":"Diagram: Epipolar Geometry Visualizer","text":"<p>Run the Epipolar Geometry Visualizer Fullscreen</p> Epipolar Geometry Visualizer <p>Type: microsim</p> <p>Learning objective: Understand epipolar constraints in stereo vision (Bloom: Analyze)</p> <p>Visual elements: - Top-down view showing two cameras and 3D point - Left and right image planes - Epipolar lines drawn on both images - Epipoles marked - Epipolar plane visualization</p> <p>Interactive controls: - Draggable 3D point position - Draggable camera positions - Click on left image point to show epipolar line on right - Toggle: Show epipolar plane in 3D - Toggle: Show all epipolar lines (for multiple points)</p> <p>Canvas layout: 900x700px with 3D view and stereo pair</p> <p>Behavior: - Moving 3D point shows how image points move along epipolar lines - Display fundamental matrix - Show that corresponding points satisfy epipolar constraint - Demonstrate baseline effect on epipole positions</p> <p>Implementation: p5.js with WEBGL and epipolar line computation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#triangulation","title":"Triangulation","text":"<p>Triangulation computes the 3D position of a point from its projections in two or more images.</p> <p>Given:</p> <ul> <li>Camera matrices \\(\\mathbf{P}_L, \\mathbf{P}_R\\)</li> <li>Corresponding image points \\(\\mathbf{p}_L, \\mathbf{p}_R\\)</li> </ul> <p>Find the 3D point \\(\\mathbf{P}\\) such that:</p> <p>\\(\\mathbf{p}_L \\sim \\mathbf{P}_L \\mathbf{P}, \\quad \\mathbf{p}_R \\sim \\mathbf{P}_R \\mathbf{P}\\)</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#linear-triangulation","title":"Linear Triangulation","text":"<p>Each correspondence provides two equations. Stack them into a linear system:</p> <p>\\(\\mathbf{A} \\mathbf{P} = \\mathbf{0}\\)</p> <p>where \\(\\mathbf{A}\\) is a 4\u00d74 matrix built from camera matrices and image points. Solve using SVD (find the null space of \\(\\mathbf{A}\\)).</p> Method Accuracy Robustness Computation Linear (DLT) Moderate Sensitive to noise Fast Optimal (geometric) Best Good Iterative Mid-point Low Fast Very fast"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-triangulation-visualizer","title":"Diagram: Triangulation Visualizer","text":"<p>Run the Triangulation Visualizer Fullscreen</p> Triangulation Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how 3D points are recovered from stereo correspondences (Bloom: Apply)</p> <p>Visual elements: - Two cameras with projection rays - Intersection point (triangulated 3D position) - Error visualization when rays don't perfectly intersect - Left and right image planes with point markers</p> <p>Interactive controls: - Draggable image points on left and right images - Camera baseline slider - Add noise toggle (shows triangulation error) - Display: 3D coordinates, reprojection error</p> <p>Canvas layout: 800x650px with 3D scene and stereo pair</p> <p>Behavior: - Rays from each camera displayed in 3D - Show exact intersection or closest point when noisy - Moving image points updates 3D reconstruction - Display triangulation uncertainty with depth</p> <p>Implementation: p5.js with WEBGL and linear triangulation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-clouds","title":"Point Clouds","text":"<p>A point cloud is a set of 3D points representing the surface or structure of objects and environments.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-representation","title":"Point Cloud Representation","text":"<p>Each point typically includes:</p> Attribute Type Description Position \\((x, y, z)\\) 3D coordinates Color \\((r, g, b)\\) Optional RGB values Normal \\((n_x, n_y, n_z)\\) Surface orientation Intensity scalar Reflection strength (lidar) <p>Point clouds can contain millions to billions of points for large-scale scenes.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#sources-of-point-clouds","title":"Sources of Point Clouds","text":"Sensor Principle Typical Density Range Stereo cameras Triangulation Dense (per-pixel) 1-50m Structured light Projected pattern Dense 0.5-5m Time-of-flight Light travel time Medium 0.5-10m Lidar Laser scanning Sparse to medium 1-200m Photogrammetry Multi-view Variable Any"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-processing","title":"Point Cloud Processing","text":"<p>Common operations on point clouds:</p> <ul> <li>Downsampling: Reduce density using voxel grids</li> <li>Normal estimation: Compute surface orientation from local neighborhoods</li> <li>Registration: Align multiple point clouds (ICP algorithm)</li> <li>Segmentation: Separate ground, objects, etc.</li> <li>Surface reconstruction: Create meshes from points</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#diagram-point-cloud-visualizer","title":"Diagram: Point Cloud Visualizer","text":"<p>Run the Point Cloud Visualizer Fullscreen</p> Point Cloud Visualizer <p>Type: microsim</p> <p>Learning objective: Explore point cloud data and common processing operations (Bloom: Understand)</p> <p>Visual elements: - 3D point cloud rendering - Color by height/intensity/normal - Bounding box - Selected point information display</p> <p>Interactive controls: - Rotate/zoom/pan 3D view - Point size slider - Color mode: height, intensity, RGB, normals - Downsampling slider (voxel size) - Toggle: Show surface normals as arrows - Sample dataset selector</p> <p>Canvas layout: 800x650px with 3D point cloud view</p> <p>Behavior: - Load and display sample point clouds - Real-time downsampling preview - Click point to show coordinates - Display point count and bounding box dimensions</p> <p>Implementation: p5.js with WEBGL, point rendering</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#point-cloud-registration","title":"Point Cloud Registration","text":"<p>Registration aligns two point clouds into a common coordinate frame. The classic algorithm is Iterative Closest Point (ICP):</p> <ol> <li>For each point in source cloud, find nearest neighbor in target</li> <li>Estimate rigid transform minimizing point-to-point distances</li> <li>Apply transform to source cloud</li> <li>Repeat until convergence</li> </ol> <p>ICP finds the transform \\((\\mathbf{R}, \\mathbf{t})\\) minimizing:</p> <p>\\(E = \\sum_{i} \\| \\mathbf{R} \\mathbf{p}_i + \\mathbf{t} - \\mathbf{q}_i \\|^2\\)</p> <p>where \\(\\mathbf{p}_i\\) are source points and \\(\\mathbf{q}_i\\) are corresponding target points.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\ndef quaternion_from_axis_angle(axis, angle):\n    \"\"\"Create quaternion from axis-angle representation.\"\"\"\n    axis = axis / np.linalg.norm(axis)  # Ensure unit axis\n    half_angle = angle / 2\n    w = np.cos(half_angle)\n    xyz = np.sin(half_angle) * axis\n    return np.array([w, xyz[0], xyz[1], xyz[2]])\n\ndef quaternion_rotate(q, point):\n    \"\"\"Rotate a 3D point by a quaternion.\"\"\"\n    # Point as pure quaternion\n    p = np.array([0, point[0], point[1], point[2]])\n\n    # q * p * q_conjugate\n    q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n    result = quaternion_multiply(quaternion_multiply(q, p), q_conj)\n\n    return result[1:4]\n\ndef quaternion_multiply(q1, q2):\n    \"\"\"Multiply two quaternions.\"\"\"\n    w1, x1, y1, z1 = q1\n    w2, x2, y2, z2 = q2\n    return np.array([\n        w1*w2 - x1*x2 - y1*y2 - z1*z2,\n        w1*x2 + x1*w2 + y1*z2 - z1*y2,\n        w1*y2 - x1*z2 + y1*w2 + z1*x2,\n        w1*z2 + x1*y2 - y1*x2 + z1*w2\n    ])\n\ndef project_point(P, K, R, t):\n    \"\"\"Project 3D point to 2D using camera model.\"\"\"\n    # World to camera coordinates\n    P_cam = R @ P + t\n\n    # Perspective projection\n    x = P_cam[0] / P_cam[2]\n    y = P_cam[1] / P_cam[2]\n\n    # Apply intrinsic matrix\n    u = K[0, 0] * x + K[0, 2]\n    v = K[1, 1] * y + K[1, 2]\n\n    return np.array([u, v])\n\ndef triangulate(P1, P2, p1, p2):\n    \"\"\"Triangulate 3D point from two views.\"\"\"\n    # Build the A matrix for DLT\n    A = np.array([\n        p1[0] * P1[2, :] - P1[0, :],\n        p1[1] * P1[2, :] - P1[1, :],\n        p2[0] * P2[2, :] - P2[0, :],\n        p2[1] * P2[2, :] - P2[1, :]\n    ])\n\n    # SVD solution\n    _, _, Vt = np.linalg.svd(A)\n    X = Vt[-1]\n\n    # Convert from homogeneous\n    return X[:3] / X[3]\n</code></pre>"},{"location":"chapters/14-3d-geometry-and-transformations/#summary_1","title":"Summary","text":"<p>3D geometry and transformations provide the mathematical foundation for spatial computing:</p> <p>Coordinate Systems and Representations:</p> <ul> <li>3D coordinate systems use handedness conventions (right-hand vs left-hand)</li> <li>Homogeneous coordinates unify rotation and translation as matrix multiplication</li> <li>Different domains (robotics, graphics, vision) use different conventions</li> </ul> <p>Rotation Representations:</p> <ul> <li>Euler angles are intuitive but suffer from gimbal lock</li> <li>Quaternions provide singularity-free rotation representation</li> <li>Unit quaternions form a double cover of SO(3)</li> </ul> <p>Rigid Body Transforms:</p> <ul> <li>SE(3) combines rotation and translation</li> <li>4\u00d74 homogeneous matrices enable efficient composition</li> <li>Transform chains model articulated systems like robot arms</li> </ul> <p>Camera Models:</p> <ul> <li>Intrinsic parameters describe internal camera geometry</li> <li>Extrinsic parameters describe camera pose in the world</li> <li>The projection matrix combines both for 3D-to-2D mapping</li> </ul> <p>Stereo Vision:</p> <ul> <li>Two views enable 3D reconstruction via triangulation</li> <li>Epipolar geometry constrains correspondence search</li> <li>Depth is inversely proportional to disparity</li> </ul> <p>Point Clouds:</p> <ul> <li>Represent 3D structure as sets of points</li> <li>Generated by various sensors (lidar, stereo, structured light)</li> <li>Registration aligns multiple scans into a common frame</li> </ul>"},{"location":"chapters/14-3d-geometry-and-transformations/#self-check-questions","title":"Self-Check Questions","text":"Why do we use half the rotation angle in the quaternion representation? <p>Quaternions provide a double cover of the rotation group SO(3), meaning both \\(\\mathbf{q}\\) and \\(-\\mathbf{q}\\) represent the same rotation. Using \\(\\frac{\\theta}{2}\\) in the quaternion formula ensures that rotating by \\(\\theta\\) actually produces a rotation of \\(\\theta\\) (not \\(2\\theta\\)) when we compute \\(\\mathbf{q} \\cdot \\mathbf{p} \\cdot \\mathbf{q}^*\\). The quaternion multiplication effectively applies the rotation twice (once with \\(\\mathbf{q}\\), once with \\(\\mathbf{q}^*\\)), so we need half angles to get the correct total rotation.</p> What information is lost during perspective projection, and why does this matter for 3D reconstruction? <p>Perspective projection loses depth information: all points along a ray from the camera center project to the same image point. Mathematically, \\((X, Y, Z)\\) and \\((kX, kY, kZ)\\) for any \\(k &gt; 0\\) produce identical image coordinates. This is why a single image cannot determine 3D structure\u2014we need additional constraints like multiple views (stereo), known object size, or depth sensors. Stereo vision solves this by observing the same point from two locations, where the disparity (difference in image position) encodes depth.</p> How does the baseline affect stereo depth estimation? <p>The depth-disparity relationship is \\(Z = \\frac{fb}{d}\\). With larger baseline \\(b\\), the same depth produces larger disparity, improving depth resolution (we can distinguish smaller depth differences). However, larger baseline makes stereo matching harder because: (1) occlusions become more common\u2014parts visible in one camera may be hidden in the other, (2) appearance changes more between views, and (3) the search range for correspondences increases. Practical systems balance these trade-offs based on the target depth range.</p> Why is gimbal lock a fundamental problem rather than an implementation bug? <p>Gimbal lock is inherent to representing 3D rotation with three sequential rotations about fixed axes. Any three-parameter rotation representation that uses Euler-type angles will have singularities because SO(3) (the rotation group) cannot be smoothly parameterized by \\(\\mathbb{R}^3\\). At the singularity, two rotation axes align, reducing the effective degrees of freedom from 3 to 2. No choice of axis order eliminates the problem\u2014it just moves the singularity. Quaternions avoid this by using four parameters with a unit constraint, providing a smooth parameterization of rotations.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/","title":"Quiz: 3D Geometry and Transformations","text":"<p>Test your understanding of 3D coordinate systems, rotations, and camera models.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#1-a-right-hand-coordinate-system-is-characterized-by","title":"1. A right-hand coordinate system is characterized by:","text":"<ol> <li>All axes pointing to the right</li> <li>Curling right-hand fingers from X to Y points thumb in Z direction</li> <li>The Z-axis always pointing down</li> <li>Negative coordinates only</li> </ol> Show Answer <p>The correct answer is B. In a right-hand system, if you curl your right hand's fingers from the X-axis toward the Y-axis, your thumb points in the Z direction. This is the standard convention in mathematics and many graphics systems.</p> <p>Concept Tested: 3D Coordinate System</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#2-euler-angles-can-suffer-from-gimbal-lock-when","title":"2. Euler angles can suffer from gimbal lock when:","text":"<ol> <li>All angles are zero</li> <li>Two rotation axes become aligned</li> <li>The angles are too small</li> <li>Only yaw is non-zero</li> </ol> Show Answer <p>The correct answer is B. Gimbal lock occurs when two rotation axes align (typically when pitch is \u00b190\u00b0), causing loss of one degree of freedom. This is an inherent limitation of Euler angle representation.</p> <p>Concept Tested: Gimbal Lock</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#3-a-quaternion-representing-rotation-uses","title":"3. A quaternion representing rotation uses:","text":"<ol> <li>Three components</li> <li>Four components with a unit norm constraint</li> <li>Nine components like a rotation matrix</li> <li>Two complex numbers</li> </ol> Show Answer <p>The correct answer is B. A unit quaternion has four components \\((w, x, y, z)\\) with \\(w^2 + x^2 + y^2 + z^2 = 1\\). This provides a singularity-free representation of 3D rotations.</p> <p>Concept Tested: Quaternion</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#4-homogeneous-coordinates-allow","title":"4. Homogeneous coordinates allow:","text":"<ol> <li>Only rotation operations</li> <li>Combining rotation and translation in a single matrix multiplication</li> <li>Eliminating the need for matrices</li> <li>Representing only 2D points</li> </ol> Show Answer <p>The correct answer is B. Homogeneous coordinates add an extra dimension (e.g., \\((x, y, z, 1)\\) for 3D points), enabling both rotation and translation to be represented as matrix multiplication.</p> <p>Concept Tested: Homogeneous Coordinates</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#5-an-se3-transformation-represents","title":"5. An SE(3) transformation represents:","text":"<ol> <li>Scaling only</li> <li>A rigid body motion (rotation and translation) in 3D</li> <li>Projection to 2D</li> <li>Color transformation</li> </ol> Show Answer <p>The correct answer is B. SE(3) is the Special Euclidean group in 3D, representing all rigid body transformations\u2014combinations of rotation and translation that preserve distances and angles.</p> <p>Concept Tested: SE3 Transform</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#6-the-camera-intrinsic-matrix-k-contains","title":"6. The camera intrinsic matrix \\(K\\) contains:","text":"<ol> <li>The camera's position in the world</li> <li>Focal length and principal point (internal camera geometry)</li> <li>The rotation of the camera</li> <li>The image pixel values</li> </ol> Show Answer <p>The correct answer is B. The intrinsic matrix contains internal camera parameters: focal lengths (\\(f_x\\), \\(f_y\\)), principal point (\\(c_x\\), \\(c_y\\)), and optionally skew. These describe how 3D camera coordinates project to 2D pixels.</p> <p>Concept Tested: Intrinsic Parameters</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#7-perspective-projection-causes","title":"7. Perspective projection causes:","text":"<ol> <li>All objects to appear the same size</li> <li>Distant objects to appear smaller than near objects</li> <li>Parallel lines to remain parallel in the image</li> <li>Colors to change</li> </ol> Show Answer <p>The correct answer is B. Perspective projection divides by depth (Z-coordinate), causing distant objects to appear smaller. This mimics human vision and camera optics.</p> <p>Concept Tested: Perspective Projection</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#8-in-stereo-vision-depth-is-inversely-proportional-to","title":"8. In stereo vision, depth is inversely proportional to:","text":"<ol> <li>The baseline between cameras</li> <li>The disparity (difference in image positions)</li> <li>The focal length</li> <li>The image resolution</li> </ol> Show Answer <p>The correct answer is B. The relationship \\(Z = \\frac{fb}{d}\\) shows depth is inversely proportional to disparity. Larger disparity means closer objects; smaller disparity means farther objects.</p> <p>Concept Tested: Stereo Vision</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#9-triangulation-in-stereo-vision","title":"9. Triangulation in stereo vision:","text":"<ol> <li>Detects triangular shapes</li> <li>Computes 3D position from corresponding 2D points in multiple views</li> <li>Measures triangle areas</li> <li>Filters noise from images</li> </ol> Show Answer <p>The correct answer is B. Triangulation uses the intersection of projection rays from multiple cameras to determine the 3D position of a point observed in multiple images.</p> <p>Concept Tested: Triangulation</p>"},{"location":"chapters/14-3d-geometry-and-transformations/quiz/#10-a-point-cloud-is","title":"10. A point cloud is:","text":"<ol> <li>A type of weather pattern</li> <li>A set of 3D points representing surfaces or structure</li> <li>A 2D image format</li> <li>A neural network architecture</li> </ol> Show Answer <p>The correct answer is B. A point cloud is a collection of 3D points, often with associated attributes (color, intensity, normals), representing the surface or structure of objects and environments.</p> <p>Concept Tested: Point Cloud</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/","title":"Autonomous Systems and Sensor Fusion","text":""},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#summary","title":"Summary","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles and robotics. You will learn LIDAR point cloud processing, camera calibration, sensor fusion with Kalman filters, state estimation and prediction, SLAM (Simultaneous Localization and Mapping), object detection and tracking, and path planning algorithms. These are the techniques that enable self-driving cars and autonomous robots.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> <li>Chapter 12: Optimization and Learning Algorithms</li> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Chapter 14: 3D Geometry and Transformations</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#introduction","title":"Introduction","text":"<p>Autonomous systems\u2014self-driving cars, delivery robots, drones\u2014must perceive their environment, understand their location, and plan safe paths through the world. These capabilities require integrating everything we've learned about linear algebra:</p> <ul> <li>Sensor data arrives as vectors and matrices (images, point clouds)</li> <li>State estimation uses matrix equations to fuse noisy measurements</li> <li>Transformations relate sensor frames to world coordinates</li> <li>Optimization finds efficient, collision-free trajectories</li> </ul> <p>This capstone chapter synthesizes these concepts into the complete autonomy stack used by real-world robots and vehicles.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#sensing-the-environment","title":"Sensing the Environment","text":"<p>Autonomous systems perceive the world through multiple sensors, each providing complementary information.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#lidar-point-clouds","title":"LIDAR Point Clouds","text":"<p>LIDAR (Light Detection and Ranging) measures distances by timing laser pulses. A spinning LIDAR captures a point cloud\u2014a set of 3D points representing surfaces in the environment.</p> <p>Each LIDAR point contains:</p> Attribute Description Typical Range \\((x, y, z)\\) 3D position 0.5-200 meters Intensity Reflection strength 0-255 Ring/Channel Which laser beam 0-127 Timestamp Acquisition time Nanosecond precision <p>A single LIDAR scan may contain 100,000+ points, producing a sparse but accurate 3D representation.</p> <p>Processing pipeline:</p> <ol> <li>Ground removal: Segment points into ground vs obstacles</li> <li>Clustering: Group nearby points into objects</li> <li>Classification: Identify object types (car, pedestrian, cyclist)</li> <li>Tracking: Associate objects across frames</li> </ol>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-lidar-point-cloud-visualizer","title":"Diagram: LIDAR Point Cloud Visualizer","text":"<p>Run the LIDAR Point Cloud Visualizer Fullscreen</p> LIDAR Point Cloud Visualizer <p>Type: microsim</p> <p>Learning objective: Understand LIDAR data structure and basic processing (Bloom: Understand)</p> <p>Visual elements: - 3D point cloud rendering with intensity coloring - Ground plane visualization - Clustered objects highlighted in different colors - Ego vehicle position marker at origin - Distance rings at 10m, 25m, 50m</p> <p>Interactive controls: - Rotate/zoom/pan 3D view - Toggle: Show ground points - Toggle: Show clustered objects - Slider: Point size - Slider: Intensity threshold - Dropdown: Color by (intensity, height, distance, cluster)</p> <p>Canvas layout: 850x650px with 3D point cloud view</p> <p>Behavior: - Load sample LIDAR scans from driving dataset - Real-time ground segmentation preview - Click on cluster to show point count and bounding box - Animate through sequence of scans</p> <p>Implementation: p5.js with WEBGL, point rendering</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#camera-calibration","title":"Camera Calibration","text":"<p>Camera calibration determines the intrinsic and extrinsic parameters needed to accurately project 3D points to image pixels and vice versa.</p> <p>Intrinsic calibration finds the camera matrix \\(\\mathbf{K}\\):</p> <p>\\(\\mathbf{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Plus distortion coefficients \\((k_1, k_2, p_1, p_2, k_3)\\) for radial and tangential lens distortion.</p> <p>Extrinsic calibration finds the transformation between sensors:</p> <ul> <li>Camera-to-LIDAR: \\(\\mathbf{T}_{CL}\\) for projecting LIDAR points onto images</li> <li>Camera-to-vehicle: \\(\\mathbf{T}_{CV}\\) for relating detections to vehicle frame</li> </ul> Calibration Type What It Determines Method Intrinsic Focal length, principal point, distortion Checkerboard pattern Extrinsic (camera-LIDAR) Relative pose Correspondences or joint optimization Extrinsic (multi-camera) Camera arrangement Overlapping views"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-camera-calibration-visualizer","title":"Diagram: Camera Calibration Visualizer","text":"<p>Run the Camera Calibration Visualizer Fullscreen</p> Camera Calibration Visualizer <p>Type: microsim</p> <p>Learning objective: Understand camera calibration process and distortion correction (Bloom: Apply)</p> <p>Visual elements: - Raw image with detected checkerboard corners - Undistorted image showing correction - 3D view of checkerboard poses - Reprojection error visualization</p> <p>Interactive controls: - Slider: Radial distortion k1, k2 - Slider: Focal length - Toggle: Show/hide detected corners - Toggle: Show reprojection errors as vectors - Button: \"Calibrate\" (run optimization)</p> <p>Canvas layout: 900x600px with raw/corrected image comparison</p> <p>Behavior: - Display checkerboard corner detection - Show barrel/pincushion distortion effects - Visualize how calibration reduces reprojection error - Display calibration matrix values</p> <p>Implementation: p5.js with corner detection simulation</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#state-estimation","title":"State Estimation","text":"<p>Autonomous systems must estimate their state (position, velocity, orientation) from noisy sensor measurements. State estimation is the process of inferring the true state from uncertain observations.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-state-vector","title":"The State Vector","text":"<p>The state vector \\(\\mathbf{x}\\) contains all variables we want to estimate:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#vehicle-state-vector-example","title":"Vehicle State Vector Example","text":"<p>\\(\\mathbf{x} = \\begin{bmatrix} p_x \\\\ p_y \\\\ v_x \\\\ v_y \\\\ \\theta \\\\ \\omega \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\((p_x, p_y)\\) is position</li> <li>\\((v_x, v_y)\\) is velocity</li> <li>\\(\\theta\\) is heading angle</li> <li>\\(\\omega\\) is angular velocity (yaw rate)</li> </ul> <p>The state dimension depends on what we're tracking:</p> Application State Variables Dimension 2D position only \\(x, y\\) 2 2D with velocity \\(x, y, v_x, v_y\\) 4 2D with heading \\(x, y, \\theta, v, \\omega\\) 5 3D pose \\(x, y, z, \\phi, \\theta, \\psi\\) 6 Full 3D dynamics Position, velocity, orientation, angular velocity 12+"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-measurement-vector","title":"The Measurement Vector","text":"<p>The measurement vector \\(\\mathbf{z}\\) contains sensor observations:</p> <p>\\(\\mathbf{z} = \\begin{bmatrix} z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_m \\end{bmatrix}\\)</p> <p>Measurements are related to the state by the observation model:</p> <p>\\(\\mathbf{z} = \\mathbf{H}\\mathbf{x} + \\mathbf{v}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{H}\\) is the observation matrix (which state variables are observed)</li> <li>\\(\\mathbf{v}\\) is measurement noise (typically Gaussian with covariance \\(\\mathbf{R}\\))</li> </ul> Sensor Measures Observation Matrix \\(\\mathbf{H}\\) GPS Position only \\([I_{2\u00d72}, 0_{2\u00d74}]\\) Speedometer Speed magnitude Nonlinear IMU Acceleration, angular velocity Depends on state definition Camera Bearing to landmarks Nonlinear"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-filter","title":"The Kalman Filter","text":"<p>The Kalman filter is the optimal estimator for linear systems with Gaussian noise. It recursively estimates the state by combining predictions with measurements.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-filter-model","title":"The Kalman Filter Model","text":"<p>The system evolves according to:</p> <p>State transition: \\(\\mathbf{x}_k = \\mathbf{F}\\mathbf{x}_{k-1} + \\mathbf{B}\\mathbf{u}_k + \\mathbf{w}_k\\)</p> <p>Observation: \\(\\mathbf{z}_k = \\mathbf{H}\\mathbf{x}_k + \\mathbf{v}_k\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{F}\\) is the state transition matrix</li> <li>\\(\\mathbf{B}\\) is the control input matrix</li> <li>\\(\\mathbf{u}_k\\) is the control input (e.g., acceleration command)</li> <li>\\(\\mathbf{w}_k \\sim \\mathcal{N}(0, \\mathbf{Q})\\) is process noise</li> <li>\\(\\mathbf{v}_k \\sim \\mathcal{N}(0, \\mathbf{R})\\) is measurement noise</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-prediction-step","title":"The Prediction Step","text":"<p>The prediction step propagates the state estimate forward in time:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#kalman-filter-prediction","title":"Kalman Filter Prediction","text":"<p>\\(\\hat{\\mathbf{x}}_k^- = \\mathbf{F}\\hat{\\mathbf{x}}_{k-1} + \\mathbf{B}\\mathbf{u}_k\\)</p> <p>\\(\\mathbf{P}_k^- = \\mathbf{F}\\mathbf{P}_{k-1}\\mathbf{F}^\\top + \\mathbf{Q}\\)</p> <p>where:</p> <ul> <li>\\(\\hat{\\mathbf{x}}_k^-\\) is the predicted state (prior)</li> <li>\\(\\mathbf{P}_k^-\\) is the predicted covariance (uncertainty grows)</li> <li>The superscript \\(^-\\) indicates \"before incorporating measurement\"</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-update-step","title":"The Update Step","text":"<p>The update step incorporates the new measurement:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#kalman-filter-update","title":"Kalman Filter Update","text":"<p>\\(\\mathbf{K}_k = \\mathbf{P}_k^- \\mathbf{H}^\\top (\\mathbf{H}\\mathbf{P}_k^-\\mathbf{H}^\\top + \\mathbf{R})^{-1}\\)</p> <p>\\(\\hat{\\mathbf{x}}_k = \\hat{\\mathbf{x}}_k^- + \\mathbf{K}_k(\\mathbf{z}_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^-)\\)</p> <p>\\(\\mathbf{P}_k = (\\mathbf{I} - \\mathbf{K}_k\\mathbf{H})\\mathbf{P}_k^-\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{K}_k\\) is the Kalman gain</li> <li>\\((\\mathbf{z}_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^-)\\) is the innovation (measurement residual)</li> <li>\\(\\mathbf{P}_k\\) is the updated (posterior) covariance</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-kalman-gain","title":"The Kalman Gain","text":"<p>The Kalman gain \\(\\mathbf{K}\\) optimally weights the prediction versus the measurement:</p> <ul> <li>When measurement noise \\(\\mathbf{R}\\) is large: \\(\\mathbf{K} \\to 0\\) (trust prediction)</li> <li>When prediction uncertainty \\(\\mathbf{P}^-\\) is large: \\(\\mathbf{K} \\to \\mathbf{H}^{-1}\\) (trust measurement)</li> </ul> <p>The Kalman gain minimizes the expected squared estimation error\u2014it's the optimal linear estimator.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-kalman-filter-visualizer","title":"Diagram: Kalman Filter Visualizer","text":"<p>Run the Kalman Filter Visualizer Fullscreen</p> Kalman Filter Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how Kalman filter combines prediction and measurement (Bloom: Apply)</p> <p>Visual elements: - 2D tracking display with true position (hidden), estimate, and measurements - Uncertainty ellipse showing covariance - Prediction arrow (where we expect to go) - Measurement marker (noisy observation) - Innovation vector (measurement - prediction)</p> <p>Interactive controls: - Slider: Process noise Q (how much motion varies) - Slider: Measurement noise R (sensor accuracy) - Button: \"Step\" (one predict-update cycle) - Button: \"Run\" (continuous tracking) - Toggle: Show true position (for debugging) - Dropdown: Motion model (constant velocity, constant acceleration)</p> <p>Canvas layout: 800x600px with tracking visualization</p> <p>Behavior: - Simulate object moving with random accelerations - Generate noisy measurements - Show uncertainty ellipse growing in prediction, shrinking after update - Display Kalman gain magnitude - Plot estimation error over time</p> <p>Implementation: p5.js with matrix operations for Kalman equations</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#extended-kalman-filter","title":"Extended Kalman Filter","text":"<p>The Extended Kalman Filter (EKF) handles nonlinear systems by linearizing around the current estimate.</p> <p>For nonlinear models:</p> <p>\\(\\mathbf{x}_k = f(\\mathbf{x}_{k-1}, \\mathbf{u}_k) + \\mathbf{w}_k\\)</p> <p>\\(\\mathbf{z}_k = h(\\mathbf{x}_k) + \\mathbf{v}_k\\)</p> <p>The EKF uses Jacobians instead of constant matrices:</p> Standard KF EKF Equivalent \\(\\mathbf{F}\\) $\\mathbf{F}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}\\bigg \\(\\mathbf{H}\\) $\\mathbf{H}_k = \\frac{\\partial h}{\\partial \\mathbf{x}}\\bigg <p>Common nonlinearities in autonomous systems:</p> <ul> <li>Coordinate transforms: Polar to Cartesian for radar</li> <li>Motion models: Constant turn rate and velocity (CTRV)</li> <li>Observation models: Bearing and range from position</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#sensor-fusion","title":"Sensor Fusion","text":"<p>Sensor fusion combines data from multiple sensors to achieve better accuracy and robustness than any single sensor alone.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#why-fuse-sensors","title":"Why Fuse Sensors?","text":"Sensor Strengths Weaknesses GPS Absolute position, global Low rate, poor in tunnels/urban canyons IMU High rate, works everywhere Drift over time LIDAR Accurate 3D, works in dark Expensive, sparse Camera Dense, semantic info Affected by lighting, no direct depth Radar Works in weather, velocity Low resolution <p>Fusion compensates for individual sensor weaknesses:</p> <ul> <li>GPS + IMU: High-rate positioning with drift correction</li> <li>Camera + LIDAR: Dense semantic 3D understanding</li> <li>Radar + Camera: All-weather perception with classification</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#fusion-architectures","title":"Fusion Architectures","text":"Architecture Description Pros Cons Early fusion Combine raw data Maximum information Complex, tight coupling Late fusion Combine detections/tracks Modular, sensor-independent May lose information Mid-level fusion Combine features Balance of flexibility and info Design complexity <p>The Kalman filter naturally performs sensor fusion by incorporating multiple measurement sources in the update step:</p> <p>\\(\\mathbf{z} = \\begin{bmatrix} \\mathbf{z}_{GPS} \\\\ \\mathbf{z}_{IMU} \\\\ \\vdots \\end{bmatrix}, \\quad \\mathbf{H} = \\begin{bmatrix} \\mathbf{H}_{GPS} \\\\ \\mathbf{H}_{IMU} \\\\ \\vdots \\end{bmatrix}\\)</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-sensor-fusion-visualizer","title":"Diagram: Sensor Fusion Visualizer","text":"<p>Run the Sensor Fusion Visualizer Fullscreen</p> Sensor Fusion Visualizer <p>Type: microsim</p> <p>Learning objective: Understand how multiple sensors improve state estimation (Bloom: Analyze)</p> <p>Visual elements: - Vehicle track with true position - GPS measurements (blue dots, noisy, low rate) - IMU-based dead reckoning (orange line, drifting) - Fused estimate (green line, best of both) - Uncertainty ellipses for each estimate</p> <p>Interactive controls: - Toggle: Enable/disable GPS - Toggle: Enable/disable IMU - Slider: GPS noise level - Slider: IMU drift rate - Button: \"Run simulation\" - Display: RMSE for each estimation method</p> <p>Canvas layout: 850x650px with tracking comparison</p> <p>Behavior: - Show GPS-only tracking (jumpy, gaps) - Show IMU-only tracking (smooth but drifting) - Show fused result (smooth and accurate) - Quantify improvement from fusion</p> <p>Implementation: p5.js with multi-sensor Kalman filter</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#slam-simultaneous-localization-and-mapping","title":"SLAM: Simultaneous Localization and Mapping","text":"<p>SLAM solves the chicken-and-egg problem: to localize, you need a map; to build a map, you need to know your location.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#the-slam-problem","title":"The SLAM Problem","text":"<p>SLAM jointly estimates:</p> <ol> <li>Localization: The robot's pose over time</li> <li>Mapping: The positions of landmarks/features in the environment</li> </ol> <p>The state vector grows as new landmarks are discovered:</p> <p>\\(\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}_R \\\\ \\mathbf{m}_1 \\\\ \\mathbf{m}_2 \\\\ \\vdots \\\\ \\mathbf{m}_n \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{x}_R\\) is the robot pose and \\(\\mathbf{m}_i\\) are landmark positions.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#slam-approaches","title":"SLAM Approaches","text":"Approach Representation Scalability Accuracy EKF-SLAM Gaussian, dense covariance O(n\u00b2) landmarks High (small maps) Particle Filter SLAM Particles for pose, EKF for map Medium maps Good Graph-based SLAM Pose graph, optimization Large maps Very high Visual SLAM Features from cameras Real-time capable Good"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#graph-based-slam","title":"Graph-Based SLAM","text":"<p>Modern SLAM systems formulate the problem as graph optimization:</p> <ul> <li>Nodes: Robot poses at different times, landmark positions</li> <li>Edges: Constraints from odometry and observations</li> <li>Optimization: Find poses/landmarks minimizing total constraint error</li> </ul> <p>The optimization minimizes:</p> <p>\\(\\mathbf{x}^* = \\arg\\min_{\\mathbf{x}} \\sum_{\\langle i,j \\rangle} \\| \\mathbf{z}_{ij} - h(\\mathbf{x}_i, \\mathbf{x}_j) \\|^2_{\\mathbf{\\Omega}_{ij}}\\)</p> <p>where \\(\\mathbf{z}_{ij}\\) is the measurement between nodes \\(i\\) and \\(j\\), and \\(\\mathbf{\\Omega}_{ij}\\) is the information matrix (inverse covariance).</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-slam-visualizer","title":"Diagram: SLAM Visualizer","text":"<p>Run the SLAM Visualizer Fullscreen</p> SLAM Visualizer <p>Type: microsim</p> <p>Learning objective: Understand the SLAM problem and loop closure (Bloom: Analyze)</p> <p>Visual elements: - Robot trajectory (estimated vs true) - Landmark positions (estimated vs true) - Pose graph edges - Loop closure detection highlight - Uncertainty ellipses on poses and landmarks</p> <p>Interactive controls: - Button: \"Move robot\" (add new pose) - Button: \"Add landmark observation\" - Button: \"Loop closure\" (recognize previously seen place) - Toggle: Show covariance ellipses - Slider: Odometry noise - Button: \"Optimize graph\"</p> <p>Canvas layout: 900x700px with SLAM visualization</p> <p>Behavior: - Show drift accumulating without loop closures - Demonstrate how loop closure corrects the entire trajectory - Visualize uncertainty reduction after optimization - Display before/after comparison for loop closure</p> <p>Implementation: p5.js with simple graph optimization</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-detection-and-tracking","title":"Object Detection and Tracking","text":"<p>Autonomous systems must detect other road users and track them over time to predict their future behavior.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-detection","title":"Object Detection","text":"<p>Object detection identifies and localizes objects in sensor data:</p> <ul> <li>2D detection (camera): Bounding boxes in image coordinates</li> <li>3D detection (LIDAR): 3D bounding boxes in world coordinates</li> </ul> <p>Modern detection uses deep learning (CNNs for images, PointNets for point clouds).</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#bounding-boxes","title":"Bounding Boxes","text":"<p>A bounding box represents an object's spatial extent:</p> <p>2D Bounding Box: \\((x_{min}, y_{min}, x_{max}, y_{max})\\) or \\((x_{center}, y_{center}, width, height)\\)</p> <p>3D Bounding Box:</p> <p>\\(\\mathbf{B} = (x, y, z, l, w, h, \\theta)\\)</p> <p>where:</p> <ul> <li>\\((x, y, z)\\) is the center position</li> <li>\\((l, w, h)\\) are length, width, height</li> <li>\\(\\theta\\) is the heading angle (yaw)</li> </ul> Detection Output Components Use Case 2D box 4 values + class Image-based detection 3D box 7 values + class LIDAR/fusion detection Oriented box Center, axes, rotation General 3D objects Polygon/mask Variable points Precise shape"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#object-tracking","title":"Object Tracking","text":"<p>Object tracking associates detections across time to maintain consistent object identities.</p> <p>The tracking pipeline:</p> <ol> <li>Prediction: Where do we expect objects to be? (Kalman filter)</li> <li>Association: Match predictions to new detections</li> <li>Update: Refine state estimates with matched detections</li> <li>Track management: Create new tracks, delete lost tracks</li> </ol> <p>Association uses distance metrics:</p> <ul> <li>IoU (Intersection over Union): Geometric overlap of boxes</li> <li>Mahalanobis distance: Statistical distance using uncertainty</li> <li>Appearance features: Visual similarity (deep learning embeddings)</li> </ul> <p>The Hungarian algorithm finds optimal one-to-one assignment minimizing total cost.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-object-tracking-visualizer","title":"Diagram: Object Tracking Visualizer","text":"<p>Run the Object Tracking Visualizer Fullscreen</p> Object Tracking Visualizer <p>Type: microsim</p> <p>Learning objective: Understand multi-object tracking with prediction and association (Bloom: Apply)</p> <p>Visual elements: - Top-down view of scene with multiple moving objects - Predicted positions (dashed boxes) - Detected positions (solid boxes) - Track IDs displayed on each object - Association lines connecting predictions to detections</p> <p>Interactive controls: - Button: \"Step\" (advance one frame) - Button: \"Run\" (continuous tracking) - Toggle: Show predictions - Toggle: Show track history (trails) - Slider: Detection noise - Slider: Miss probability (some detections missing)</p> <p>Canvas layout: 800x650px with tracking visualization</p> <p>Behavior: - Objects move with realistic motion patterns - Generate noisy detections with occasional misses - Show prediction-to-detection association - Handle track creation (new objects) and deletion (lost objects) - Display track count and IDs</p> <p>Implementation: p5.js with Kalman tracking and Hungarian assignment</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#path-and-motion-planning","title":"Path and Motion Planning","text":"<p>Given the current state and a goal, planning algorithms find safe, efficient paths through the environment.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#path-planning","title":"Path Planning","text":"<p>Path planning finds a collision-free geometric path from start to goal.</p> <p>Classic algorithms:</p> Algorithm Type Optimality Completeness A* Graph search Optimal (with admissible heuristic) Yes RRT Sampling-based Asymptotically optimal (RRT*) Probabilistically complete Dijkstra Graph search Optimal Yes Potential fields Gradient descent No No (local minima) <p>A* algorithm:</p> <p>\\(f(n) = g(n) + h(n)\\)</p> <p>where:</p> <ul> <li>\\(g(n)\\) is the cost from start to node \\(n\\)</li> <li>\\(h(n)\\) is the heuristic estimate from \\(n\\) to goal</li> <li>A* expands nodes in order of lowest \\(f(n)\\)</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#motion-planning","title":"Motion Planning","text":"<p>Motion planning extends path planning to consider vehicle dynamics:</p> <ul> <li>Kinematic constraints: Minimum turning radius, maximum steering angle</li> <li>Dynamic constraints: Acceleration limits, comfort bounds</li> <li>Temporal constraints: Speed limits, timing requirements</li> </ul> <p>The vehicle must follow paths that are physically achievable.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#trajectory-optimization","title":"Trajectory Optimization","text":"<p>Trajectory optimization finds the optimal trajectory minimizing a cost function:</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#trajectory-cost-function","title":"Trajectory Cost Function","text":"<p>\\(J = \\int_0^T \\left[ w_1 \\|\\ddot{\\mathbf{x}}\\|^2 + w_2 \\|\\mathbf{u}\\|^2 + w_3 c_{obstacle}(\\mathbf{x}) \\right] dt + w_4 \\|\\mathbf{x}(T) - \\mathbf{x}_{goal}\\|^2\\)</p> <p>where:</p> <ul> <li>\\(\\|\\ddot{\\mathbf{x}}\\|^2\\) penalizes acceleration (smoothness)</li> <li>\\(\\|\\mathbf{u}\\|^2\\) penalizes control effort</li> <li>\\(c_{obstacle}\\) penalizes proximity to obstacles</li> <li>The final term penalizes distance to goal</li> </ul> <p>This is solved using:</p> <ul> <li>Nonlinear optimization: Sequential quadratic programming (SQP)</li> <li>Model predictive control: Receding horizon optimization</li> <li>Sampling methods: Generate and evaluate candidate trajectories</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-path-planning-visualizer","title":"Diagram: Path Planning Visualizer","text":"<p>Run the Path Planning Visualizer Fullscreen</p> Path Planning Visualizer <p>Type: microsim</p> <p>Learning objective: Compare path planning algorithms (Bloom: Evaluate)</p> <p>Visual elements: - 2D environment with obstacles - Start and goal positions - Planned path(s) - Explored nodes/samples (for search visualization) - Collision-free corridor</p> <p>Interactive controls: - Click to place start/goal - Draw obstacles (click and drag) - Dropdown: Algorithm (A, RRT, Dijkstra, RRT) - Slider: Grid resolution (for A*) - Button: \"Plan\" (run algorithm) - Button: \"Clear obstacles\" - Toggle: Show exploration</p> <p>Canvas layout: 850x700px with planning environment</p> <p>Behavior: - Animate algorithm exploration (A* wave, RRT tree growth) - Display path length and computation time - Show algorithm-specific parameters - Compare multiple algorithms on same environment</p> <p>Implementation: p5.js with grid-based and sampling-based planners</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#diagram-trajectory-optimization-visualizer","title":"Diagram: Trajectory Optimization Visualizer","text":"<p>Run the Trajectory Optimization Visualizer Fullscreen</p> Trajectory Optimization Visualizer <p>Type: microsim</p> <p>Learning objective: Understand trajectory optimization with constraints (Bloom: Apply)</p> <p>Visual elements: - Vehicle trajectory through obstacle field - Velocity profile along path - Acceleration profile - Obstacle proximity visualization - Optimization cost convergence plot</p> <p>Interactive controls: - Slider: Smoothness weight (w1) - Slider: Speed limit - Slider: Obstacle clearance margin - Draggable waypoints - Button: \"Optimize\" - Toggle: Show velocity/acceleration profiles</p> <p>Canvas layout: 900x700px with trajectory and profiles</p> <p>Behavior: - Initial path from waypoints - Optimization smooths path while avoiding obstacles - Show tradeoff between path length and smoothness - Display constraint satisfaction (velocity limits, clearance)</p> <p>Implementation: p5.js with gradient-based trajectory optimization</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#putting-it-all-together","title":"Putting It All Together","text":"<p>A complete autonomous system integrates all these components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   LIDAR     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Detection  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Tracks    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                               \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502   Camera    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  Detection  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n                                               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GPS/IMU    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   State     \u2502\u2500\u2500\u2500\u2500\u25b6\u2502    SLAM     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502  Estimation \u2502     \u2502  (optional) \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502                   \u2502\n                           \u25bc                   \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502   Fusion    \u2502\u25c0\u2500\u2500\u2500\u2500\u2502  World      \u2502\n                    \u2502             \u2502     \u2502  Model      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Planning   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   Control   \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The system runs at different rates:</p> Component Typical Rate Latency Requirement LIDAR processing 10-20 Hz &lt;50ms Camera detection 10-30 Hz &lt;100ms State estimation 100-400 Hz &lt;10ms Tracking 10-20 Hz &lt;50ms Planning 5-10 Hz &lt;100ms Control 50-100 Hz &lt;20ms"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#python-implementation","title":"Python Implementation","text":"<pre><code>import numpy as np\n\nclass KalmanFilter:\n    def __init__(self, dim_x, dim_z):\n        \"\"\"Initialize Kalman filter for state dimension dim_x, measurement dimension dim_z.\"\"\"\n        self.dim_x = dim_x\n        self.dim_z = dim_z\n\n        # State estimate and covariance\n        self.x = np.zeros((dim_x, 1))\n        self.P = np.eye(dim_x)\n\n        # State transition and control matrices\n        self.F = np.eye(dim_x)\n        self.B = np.zeros((dim_x, 1))\n\n        # Observation matrix\n        self.H = np.zeros((dim_z, dim_x))\n\n        # Noise covariances\n        self.Q = np.eye(dim_x)  # Process noise\n        self.R = np.eye(dim_z)  # Measurement noise\n\n    def predict(self, u=None):\n        \"\"\"Prediction step.\"\"\"\n        if u is None:\n            u = np.zeros((self.B.shape[1], 1))\n\n        # Predicted state\n        self.x = self.F @ self.x + self.B @ u\n\n        # Predicted covariance\n        self.P = self.F @ self.P @ self.F.T + self.Q\n\n    def update(self, z):\n        \"\"\"Update step with measurement z.\"\"\"\n        # Innovation (measurement residual)\n        y = z - self.H @ self.x\n\n        # Innovation covariance\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.P @ self.H.T @ np.linalg.inv(S)\n\n        # Updated state\n        self.x = self.x + K @ y\n\n        # Updated covariance\n        I = np.eye(self.dim_x)\n        self.P = (I - K @ self.H) @ self.P\n\n\ndef iou_2d(box1, box2):\n    \"\"\"Compute 2D IoU between two boxes [x1, y1, x2, y2].\"\"\"\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n\n    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    union = area1 + area2 - intersection\n\n    return intersection / union if union &gt; 0 else 0\n\n\ndef astar(grid, start, goal):\n    \"\"\"A* path planning on a grid.\"\"\"\n    from heapq import heappush, heappop\n\n    rows, cols = grid.shape\n    open_set = [(0, start)]\n    came_from = {}\n    g_score = {start: 0}\n\n    def heuristic(a, b):\n        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n\n    while open_set:\n        _, current = heappop(open_set)\n\n        if current == goal:\n            # Reconstruct path\n            path = [current]\n            while current in came_from:\n                current = came_from[current]\n                path.append(current)\n            return path[::-1]\n\n        for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n            neighbor = (current[0] + dx, current[1] + dy)\n\n            if 0 &lt;= neighbor[0] &lt; rows and 0 &lt;= neighbor[1] &lt; cols:\n                if grid[neighbor] == 1:  # Obstacle\n                    continue\n\n                tentative_g = g_score[current] + 1\n\n                if neighbor not in g_score or tentative_g &lt; g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g\n                    f = tentative_g + heuristic(neighbor, goal)\n                    heappush(open_set, (f, neighbor))\n\n    return None  # No path found\n</code></pre>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#summary_1","title":"Summary","text":"<p>This capstone chapter integrates linear algebra concepts into the autonomous systems pipeline:</p> <p>Sensing:</p> <ul> <li>LIDAR provides 3D point clouds for accurate range data</li> <li>Camera calibration ensures accurate projection between 3D and 2D</li> <li>Multiple sensors provide complementary information</li> </ul> <p>State Estimation:</p> <ul> <li>State and measurement vectors represent system and observations</li> <li>Kalman filter optimally fuses predictions with measurements</li> <li>Extended Kalman filter handles nonlinear dynamics</li> <li>Sensor fusion combines multiple sources for robust estimation</li> </ul> <p>Localization and Mapping:</p> <ul> <li>SLAM jointly estimates robot pose and environment map</li> <li>Graph-based SLAM scales to large environments</li> <li>Loop closure corrects accumulated drift</li> </ul> <p>Perception:</p> <ul> <li>Object detection identifies road users</li> <li>Bounding boxes represent object extent in 2D and 3D</li> <li>Multi-object tracking maintains consistent identities</li> </ul> <p>Planning:</p> <ul> <li>Path planning finds collision-free routes</li> <li>Motion planning respects vehicle dynamics</li> <li>Trajectory optimization balances multiple objectives</li> </ul>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#self-check-questions","title":"Self-Check Questions","text":"Why does the Kalman filter covariance grow during prediction and shrink during update? <p>During prediction, we're uncertain about the process\u2014random accelerations, unmodeled dynamics\u2014so uncertainty increases. The process noise covariance \\(\\mathbf{Q}\\) is added to \\(\\mathbf{P}\\). During update, we receive new information from sensors, which reduces uncertainty. The more accurate the measurement (smaller \\(\\mathbf{R}\\)), the more the covariance shrinks. The Kalman gain determines how much to trust the measurement versus the prediction.</p> What problem does SLAM solve that pure localization or pure mapping cannot? <p>Pure localization requires a pre-existing map, which isn't available in new environments. Pure mapping requires known robot poses, which requires localization. SLAM solves this by jointly estimating both, using the observation that landmarks constrain the robot pose and vice versa. As the robot revisits places (loop closure), the joint estimation corrects accumulated drift in both the trajectory and the map.</p> Why is sensor fusion more than just averaging sensor measurements? <p>Sensor fusion must account for: (1) different sensors measuring different things (GPS measures position, IMU measures acceleration), (2) different noise characteristics (GPS is noisy but absolute, IMU is precise but drifts), (3) different update rates (IMU at 400Hz, GPS at 10Hz), and (4) correlations between measurements. The Kalman filter optimally weights each measurement based on its uncertainty and what it measures, propagates state between measurements, and maintains consistent uncertainty estimates.</p> How does trajectory optimization differ from path planning? <p>Path planning finds a geometric path (sequence of positions) that avoids obstacles. Trajectory optimization adds: (1) time parameterization (when to be where), (2) velocity and acceleration profiles, (3) dynamic feasibility (can the vehicle actually follow this?), and (4) smoothness and comfort objectives. A path might be geometrically valid but require infinite acceleration at corners. Trajectory optimization produces paths that are both collision-free and physically achievable.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/","title":"Quiz: Autonomous Systems and Sensor Fusion","text":"<p>Test your understanding of state estimation, SLAM, and autonomous systems concepts.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#1-the-state-vector-in-a-kalman-filter-represents","title":"1. The state vector in a Kalman filter represents:","text":"<ol> <li>The sensor measurements</li> <li>The variables we want to estimate (position, velocity, etc.)</li> <li>The control inputs</li> <li>The noise covariance</li> </ol> Show Answer <p>The correct answer is B. The state vector contains all the variables we want to estimate, such as position, velocity, orientation, and angular velocity. The Kalman filter recursively updates this estimate.</p> <p>Concept Tested: State Vector</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#2-the-kalman-gain-determines","title":"2. The Kalman gain determines:","text":"<ol> <li>The number of sensors</li> <li>How much to trust the measurement vs the prediction</li> <li>The sampling rate</li> <li>The state dimension</li> </ol> Show Answer <p>The correct answer is B. The Kalman gain optimally weights the prediction and measurement. When measurement noise is low, the gain is high (trust measurement); when prediction uncertainty is low, the gain is low (trust prediction).</p> <p>Concept Tested: Kalman Gain</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#3-the-prediction-step-in-a-kalman-filter","title":"3. The prediction step in a Kalman filter:","text":"<ol> <li>Incorporates new measurements</li> <li>Propagates the state forward in time using the motion model</li> <li>Computes the Kalman gain</li> <li>Initializes the filter</li> </ol> Show Answer <p>The correct answer is B. The prediction step uses the state transition model to propagate the state estimate forward in time, while also increasing the uncertainty (covariance) due to process noise.</p> <p>Concept Tested: Prediction Step</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#4-sensor-fusion-is-important-because","title":"4. Sensor fusion is important because:","text":"<ol> <li>Single sensors are always sufficient</li> <li>Different sensors provide complementary information and compensate for each other's weaknesses</li> <li>It reduces the number of sensors needed</li> <li>It eliminates all measurement noise</li> </ol> Show Answer <p>The correct answer is B. Different sensors have different strengths and weaknesses (GPS has absolute position but is slow; IMU is fast but drifts). Fusion combines them to achieve better accuracy and robustness than any single sensor.</p> <p>Concept Tested: Sensor Fusion</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#5-slam-simultaneously-estimates","title":"5. SLAM simultaneously estimates:","text":"<ol> <li>Speed and acceleration only</li> <li>The robot's pose and a map of the environment</li> <li>Image features only</li> <li>Control inputs</li> </ol> Show Answer <p>The correct answer is B. SLAM (Simultaneous Localization and Mapping) jointly estimates the robot's trajectory (localization) and the positions of landmarks in the environment (mapping).</p> <p>Concept Tested: SLAM</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#6-the-extended-kalman-filter-ekf-differs-from-the-standard-kalman-filter-by","title":"6. The Extended Kalman Filter (EKF) differs from the standard Kalman filter by:","text":"<ol> <li>Using only linear models</li> <li>Linearizing nonlinear models using Jacobians</li> <li>Eliminating the prediction step</li> <li>Not using covariance matrices</li> </ol> Show Answer <p>The correct answer is B. The EKF handles nonlinear systems by linearizing the motion and observation models around the current estimate using Jacobian matrices, then applying the standard Kalman filter equations.</p> <p>Concept Tested: Extended Kalman Filter</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#7-object-tracking-across-frames-requires","title":"7. Object tracking across frames requires:","text":"<ol> <li>Only detection, no prediction</li> <li>Data association between predictions and new detections</li> <li>Removing all objects from view</li> <li>Constant lighting conditions</li> </ol> Show Answer <p>The correct answer is B. Tracking requires associating predicted object positions with new detections across frames. Algorithms like the Hungarian method find optimal matches based on distance or appearance.</p> <p>Concept Tested: Object Tracking</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#8-a-3d-bounding-box-for-an-object-includes","title":"8. A 3D bounding box for an object includes:","text":"<ol> <li>Only the object's color</li> <li>Center position, dimensions (length, width, height), and orientation</li> <li>Only the 2D image coordinates</li> <li>The object's velocity only</li> </ol> Show Answer <p>The correct answer is B. A 3D bounding box is typically parameterized as \\((x, y, z, l, w, h, \\theta)\\): center position, three dimensions, and heading angle. This defines the object's extent in 3D space.</p> <p>Concept Tested: Bounding Box</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#9-path-planning-algorithms-like-a-find","title":"9. Path planning algorithms like A* find:","text":"<ol> <li>The longest possible path</li> <li>A collision-free path from start to goal</li> <li>All possible paths simultaneously</li> <li>Paths that maximize collisions</li> </ol> Show Answer <p>The correct answer is B. Path planning algorithms find collision-free routes from a start position to a goal. A* uses a heuristic to efficiently search for the optimal path in terms of a cost function.</p> <p>Concept Tested: Path Planning</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/quiz/#10-loop-closure-in-slam","title":"10. Loop closure in SLAM:","text":"<ol> <li>Ends the SLAM algorithm</li> <li>Recognizes previously visited places to correct accumulated drift</li> <li>Closes physical doors in the environment</li> <li>Stops the robot</li> </ol> Show Answer <p>The correct answer is B. Loop closure detects when the robot returns to a previously visited location. This constraint corrects accumulated drift in both the trajectory and the map, dramatically improving accuracy.</p> <p>Concept Tested: SLAM</p>"},{"location":"learning-graph/","title":"Learning Graph for Applied Linear Algebra for AI and Machine Learning","text":"<p>This section contains the learning graph for this textbook.  A learning graph is a graph of concepts used in this textbook.  Each concept is represented by a node in a network graph.  Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>View the Learning Graph</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts.  They have no outbound edges.  They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts.  At the far right we have the most advanced concepts in the course.  To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long. This course contains 300 concepts covering vectors, matrices, linear systems, transformations, eigentheory, decompositions, machine learning foundations, neural networks, generative AI, optimization, image processing, 3D geometry, and autonomous systems.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG).  DAGs do not have cycles where concepts depend on themselves.  We provide the DAG in two formats.  One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format.  The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties.  This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 2 entry points (Scalar, Function)</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (19 concepts)</li> <li>Connectivity: all 300 nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type.  We use generative AI to create categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>15 concept categories covering the course structure</li> <li>Categories aligned with course chapters and topics</li> <li>Balanced distribution across categories</li> <li>Color-coded for visualization</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This report shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an appropriate number of concepts based on the topic's importance in the course.</p> <ul> <li>Statistical breakdown by category</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List","text":"<p>This document contains 300 concepts for the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-list/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":""},{"location":"learning-graph/concept-list/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":""},{"location":"learning-graph/concept-list/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"learning-graph/concept-list/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":""},{"location":"learning-graph/concept-list/#chapter-9-ml-foundations","title":"Chapter 9: ML Foundations","text":"<ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"learning-graph/concept-list/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":""},{"location":"learning-graph/concept-list/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This document defines the categorical taxonomy for organizing the 300 concepts in the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":"TaxonomyID Category Name Description FOUND Foundation Concepts Basic mathematical building blocks including scalars, vectors, and fundamental operations that form the prerequisite knowledge MATOP Matrix Operations Core matrix concepts, operations, and special matrix types essential for linear algebra computations LINSYS Linear Systems Concepts related to systems of linear equations, solution methods, and matrix equation forms TRANS Transformations Linear transformations, geometric operations (rotation, scaling, shear), and related structural concepts DETERM Determinants Determinant computation, properties, and geometric interpretations EIGEN Eigentheory Eigenvalues, eigenvectors, eigenspaces, and diagonalization concepts DECOMP Decompositions Matrix factorization methods including LU, QR, Cholesky, and SVD INPROD Inner Products Inner product spaces, orthogonality, projections, and related abstract vector space concepts MLBASE ML Foundations Core machine learning concepts including data representation, PCA, regression, and gradient methods NEURAL Neural Networks Deep learning concepts including neurons, layers, activation functions, and backpropagation GENAI Generative AI Embeddings, attention mechanisms, transformers, and large language model concepts OPTIM Optimization Optimization algorithms and methods for training machine learning models IMGPROC Image Processing Computer vision concepts including image representation, convolution, and filtering GEOM3D 3D Geometry Three-dimensional geometry, rotations, coordinate systems, and camera models AUTON Autonomous Systems Sensor fusion, state estimation, SLAM, and autonomous navigation concepts"},{"location":"learning-graph/concept-taxonomy/#category-descriptions","title":"Category Descriptions","text":""},{"location":"learning-graph/concept-taxonomy/#found-foundation-concepts","title":"FOUND - Foundation Concepts","text":"<p>The fundamental building blocks of linear algebra. These concepts are prerequisites for nearly everything else in the course, including scalars, vectors, vector operations, norms, and basic vector space theory.</p>"},{"location":"learning-graph/concept-taxonomy/#matop-matrix-operations","title":"MATOP - Matrix Operations","text":"<p>Essential matrix concepts and operations. Covers matrix notation, types of matrices (diagonal, triangular, symmetric, orthogonal), and core operations like multiplication, transpose, and inverse.</p>"},{"location":"learning-graph/concept-taxonomy/#linsys-linear-systems","title":"LINSYS - Linear Systems","text":"<p>Methods for representing and solving systems of linear equations. Includes Gaussian elimination, row operations, echelon forms, and solution analysis.</p>"},{"location":"learning-graph/concept-taxonomy/#trans-transformations","title":"TRANS - Transformations","text":"<p>How matrices represent geometric transformations. Covers rotation, scaling, shearing, projection, and abstract concepts like kernel, range, and change of basis.</p>"},{"location":"learning-graph/concept-taxonomy/#determ-determinants","title":"DETERM - Determinants","text":"<p>Determinant theory and applications. Includes computation methods, geometric interpretation as volume scaling, and applications like Cramer's rule.</p>"},{"location":"learning-graph/concept-taxonomy/#eigen-eigentheory","title":"EIGEN - Eigentheory","text":"<p>The study of eigenvalues and eigenvectors - one of the most important topics in applied linear algebra. Covers characteristic polynomials, diagonalization, spectral theorem, and power iteration.</p>"},{"location":"learning-graph/concept-taxonomy/#decomp-decompositions","title":"DECOMP - Decompositions","text":"<p>Matrix factorization techniques. Each decomposition has specific use cases: LU for solving systems, QR for least squares, Cholesky for symmetric positive definite matrices, and SVD for general applications.</p>"},{"location":"learning-graph/concept-taxonomy/#inprod-inner-products","title":"INPROD - Inner Products","text":"<p>Abstract theory of inner product spaces. Covers orthogonality, Gram-Schmidt process, projections, least squares, and the four fundamental subspaces.</p>"},{"location":"learning-graph/concept-taxonomy/#mlbase-ml-foundations","title":"MLBASE - ML Foundations","text":"<p>Core machine learning concepts that rely on linear algebra. Includes data representation, covariance analysis, PCA, linear regression, regularization, and gradient descent.</p>"},{"location":"learning-graph/concept-taxonomy/#neural-neural-networks","title":"NEURAL - Neural Networks","text":"<p>Deep learning architecture and computation. Covers the linear algebra of neural networks including weight matrices, forward propagation, backpropagation, and specialized layers.</p>"},{"location":"learning-graph/concept-taxonomy/#genai-generative-ai","title":"GENAI - Generative AI","text":"<p>Modern generative AI concepts. Focuses on the linear algebra behind transformers, attention mechanisms, embeddings, and large language models.</p>"},{"location":"learning-graph/concept-taxonomy/#optim-optimization","title":"OPTIM - Optimization","text":"<p>Optimization algorithms for training. Covers gradient-based methods, second-order optimization, and constrained optimization techniques.</p>"},{"location":"learning-graph/concept-taxonomy/#imgproc-image-processing","title":"IMGPROC - Image Processing","text":"<p>Computer vision fundamentals. Covers image representation, convolution, filtering, frequency domain analysis, and feature detection.</p>"},{"location":"learning-graph/concept-taxonomy/#geom3d-3d-geometry","title":"GEOM3D - 3D Geometry","text":"<p>Three-dimensional geometric concepts. Includes coordinate systems, rotation representations (Euler angles, quaternions), camera models, and stereo vision.</p>"},{"location":"learning-graph/concept-taxonomy/#auton-autonomous-systems","title":"AUTON - Autonomous Systems","text":"<p>Sensor fusion and autonomous navigation. Covers Kalman filtering, SLAM, localization, object tracking, and path planning.</p>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":""},{"location":"learning-graph/course-description-assessment/#course-applied-linear-algebra-for-ai-and-machine-learning","title":"Course: Applied Linear Algebra for AI and Machine Learning","text":"<p>Assessment Date: 2026-01-17 Quality Score: 97/100</p>"},{"location":"learning-graph/course-description-assessment/#scoring-breakdown","title":"Scoring Breakdown","text":"Element Points Assessment Title 5/5 Clear, descriptive: \"Applied Linear Algebra for AI and Machine Learning\" Target Audience 5/5 Specific audiences identified: CS majors (AI/ML), Data Science students, Engineering students (robotics/autonomous systems), Applied Math students, Graduate students Prerequisites 5/5 Clearly listed: College Algebra, Basic programming (Python recommended), Familiarity with calculus (derivatives/integrals) Main Topics Covered 10/10 Comprehensive 15-chapter structure across 4 parts covering foundations through applications Topics Excluded 2/5 Not explicitly stated what is NOT covered Learning Outcomes Header 5/5 Clear statement with 7 high-level objectives Remember Level 10/10 12 specific, actionable outcomes Understand Level 10/10 13 specific, actionable outcomes Apply Level 10/10 13 specific, actionable outcomes Analyze Level 10/10 13 specific, actionable outcomes Evaluate Level 10/10 13 specific, actionable outcomes Create Level 10/10 14 specific outcomes including capstone projects Descriptive Context 5/5 Strong \"Why This Course Matters\" section"},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ol> <li>Exceptional Bloom's Taxonomy Coverage: 78 specific learning outcomes across all six cognitive levels</li> <li>Well-Structured Progression: 15 chapters organized into 4 logical parts (Foundations \u2192 Advanced Theory \u2192 ML Applications \u2192 Computer Vision/Autonomous Systems)</li> <li>Strong Application Focus: Every chapter connects theory to practical AI/ML applications</li> <li>Interactive Learning: 8 example microsimulations described for hands-on exploration</li> <li>Clear Assessment Structure: Weekly problem sets, labs, midterm, and capstone project</li> <li>Modern Relevance: Covers transformers, attention mechanisms, LLMs, and autonomous driving</li> </ol>"},{"location":"learning-graph/course-description-assessment/#topics-covered","title":"Topics Covered","text":""},{"location":"learning-graph/course-description-assessment/#part-1-foundations-weeks-1-4","title":"Part 1: Foundations (Weeks 1-4)","text":"<ul> <li>Vectors and Vector Spaces</li> <li>Matrices and Matrix Operations</li> <li>Systems of Linear Equations</li> <li>Linear Transformations</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":"<ul> <li>Determinants and Matrix Properties</li> <li>Eigenvalues and Eigenvectors</li> <li>Matrix Decompositions (LU, QR, Cholesky, SVD)</li> <li>Vector Spaces and Inner Product Spaces</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-3-machine-learning-applications-weeks-9-12","title":"Part 3: Machine Learning Applications (Weeks 9-12)","text":"<ul> <li>Linear Algebra Foundations of ML</li> <li>Neural Networks and Deep Learning</li> <li>Generative AI and Large Language Models</li> <li>Optimization and Learning Algorithms</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":"<ul> <li>Image Processing and Computer Vision</li> <li>3D Geometry and Transformations</li> <li>Autonomous Driving and Sensor Fusion</li> </ul>"},{"location":"learning-graph/course-description-assessment/#minor-improvement-suggestions","title":"Minor Improvement Suggestions","text":"<ol> <li>Add Topics Excluded Section: Consider explicitly stating what is NOT covered (e.g., abstract algebra beyond finite dimensions, proofs of all theorems, advanced numerical analysis)</li> </ol>"},{"location":"learning-graph/course-description-assessment/#concept-estimation","title":"Concept Estimation","text":"<p>Based on the course description, approximately 200-220 distinct concepts can be derived:</p> <ul> <li>~25 foundational vector concepts</li> <li>~25 matrix operation concepts</li> <li>~20 systems of equations concepts</li> <li>~20 linear transformation concepts</li> <li>~15 determinant concepts</li> <li>~25 eigenvalue/eigenvector concepts</li> <li>~20 matrix decomposition concepts</li> <li>~15 inner product space concepts</li> <li>~20 ML foundation concepts</li> <li>~25 neural network/deep learning concepts</li> <li>~20 generative AI concepts</li> <li>~15 optimization concepts</li> <li>~20 computer vision concepts</li> <li>~15 3D geometry concepts</li> <li>~20 autonomous systems concepts</li> </ul>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>PROCEED with learning graph generation. This course description exceeds the quality threshold of 80 points with a score of 97/100.</p>"},{"location":"learning-graph/faq-quality-report/","title":"FAQ Quality Report","text":"<p>Generated: 2026-01-17 Skill: faq-generator Course: Applied Linear Algebra for AI and Machine Learning</p>"},{"location":"learning-graph/faq-quality-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Questions 65 Overall Quality Score 88/100 Content Completeness Score 100/100 Concept Coverage 82% (246/300 concepts)"},{"location":"learning-graph/faq-quality-report/#category-breakdown","title":"Category Breakdown","text":""},{"location":"learning-graph/faq-quality-report/#getting-started","title":"Getting Started","text":"Metric Value Questions 10 Average Word Count 43 Bloom's Distribution 40% Remember, 40% Understand, 20% Apply Examples 0 Source Links 10 (100%)"},{"location":"learning-graph/faq-quality-report/#core-concepts","title":"Core Concepts","text":"Metric Value Questions 13 Average Word Count 41 Bloom's Distribution 100% Understand Examples 12 (92%) Source Links 13 (100%)"},{"location":"learning-graph/faq-quality-report/#technical-details","title":"Technical Details","text":"Metric Value Questions 10 Average Word Count 40 Bloom's Distribution 60% Understand, 30% Analyze, 10% Apply Examples 4 (40%) Source Links 10 (100%)"},{"location":"learning-graph/faq-quality-report/#common-challenges","title":"Common Challenges","text":"Metric Value Questions 9 Average Word Count 38 Bloom's Distribution 22% Understand, 45% Analyze, 22% Apply, 11% Evaluate Examples 2 (22%) Source Links 8 (89%)"},{"location":"learning-graph/faq-quality-report/#best-practices","title":"Best Practices","text":"Metric Value Questions 8 Average Word Count 37 Bloom's Distribution 50% Evaluate, 38% Apply, 12% Remember Examples 1 (12%) Source Links 7 (88%)"},{"location":"learning-graph/faq-quality-report/#advanced-topics","title":"Advanced Topics","text":"Metric Value Questions 9 Average Word Count 40 Bloom's Distribution 44% Analyze, 22% Understand, 22% Evaluate, 11% Create Examples 2 (22%) Source Links 8 (89%)"},{"location":"learning-graph/faq-quality-report/#blooms-taxonomy-distribution","title":"Bloom's Taxonomy Distribution","text":"Level Actual Target Deviation Remember 8% 15% -7% \u26a0\ufe0f Understand 42% 30% +12% \u26a0\ufe0f Apply 14% 20% -6% \u2713 Analyze 20% 20% 0% \u2713 Evaluate 14% 10% +4% \u2713 Create 2% 5% -3% \u2713 <p>Overall Bloom's Score: 20/25</p> <p>The distribution slightly over-represents Understand level questions while under-representing Remember level. This reflects the course's emphasis on conceptual understanding over pure memorization, which is appropriate for a college-level course.</p>"},{"location":"learning-graph/faq-quality-report/#answer-quality-analysis","title":"Answer Quality Analysis","text":"Metric Value Target Status Questions with Examples 21/65 (32%) 40%+ \u26a0\ufe0f Below target Questions with Source Links 62/65 (95%) 60%+ \u2713 Excellent Average Answer Length 40 words 100-300 \u26a0\ufe0f Below target Complete Answers 65/65 (100%) 100% \u2713 Perfect <p>Answer Quality Score: 21/25</p> <p>Answers are concise and focused. While shorter than the 100-300 word target, this reflects the FAQ's role as a quick reference rather than comprehensive explanations. Source links direct readers to detailed content.</p>"},{"location":"learning-graph/faq-quality-report/#concept-coverage-analysis","title":"Concept Coverage Analysis","text":""},{"location":"learning-graph/faq-quality-report/#coverage-by-part","title":"Coverage by Part","text":"Part Concepts Covered Coverage Part 1: Foundations 100 86 86% Part 2: Advanced Matrix Theory 69 58 84% Part 3: ML Applications 79 62 78% Part 4: Vision &amp; Autonomous 52 40 77%"},{"location":"learning-graph/faq-quality-report/#top-covered-concepts-appearing-in-multiple-faqs","title":"Top Covered Concepts (Appearing in Multiple FAQs)","text":"<ol> <li>Matrix - 12 appearances</li> <li>Vector - 10 appearances</li> <li>Eigenvalue/Eigenvector - 8 appearances</li> <li>SVD - 7 appearances</li> <li>Linear Transformation - 6 appearances</li> <li>Gradient Descent - 5 appearances</li> <li>PCA - 5 appearances</li> <li>Kalman Filter - 4 appearances</li> <li>Attention Mechanism - 4 appearances</li> <li>Regularization - 4 appearances</li> </ol>"},{"location":"learning-graph/faq-quality-report/#concepts-not-covered-54-concepts","title":"Concepts Not Covered (54 concepts)","text":"<p>These concepts from the learning graph do not appear directly in FAQ questions:</p> <p>Lower Priority (Covered Implicitly or Less Common):</p> <ul> <li>Row Swap, Row Scaling, Row Addition (covered under Gaussian Elimination)</li> <li>Cofactor, Minor, Cofactor Expansion (covered under Determinant)</li> <li>Algebraic Multiplicity, Geometric Multiplicity (covered under Eigenvalue)</li> <li>Partial Pivoting (covered under LU Decomposition)</li> <li>And 46 others...</li> </ul> <p>Coverage Score: 25/30</p>"},{"location":"learning-graph/faq-quality-report/#organization-quality","title":"Organization Quality","text":"Criterion Score Notes Logical Categorization 5/5 Clear 6-category structure Progressive Difficulty 5/5 Getting Started \u2192 Advanced Topics No Duplicates 5/5 All questions unique Clear Questions 5/5 Questions are specific and searchable <p>Organization Score: 20/20</p>"},{"location":"learning-graph/faq-quality-report/#difficulty-distribution","title":"Difficulty Distribution","text":"Difficulty Count Percentage Easy 15 23% Medium 32 49% Hard 18 28% <p>This distribution is appropriate for a college-level course, with most questions at medium difficulty and a good balance of easier and harder questions.</p>"},{"location":"learning-graph/faq-quality-report/#overall-quality-score-88100","title":"Overall Quality Score: 88/100","text":"Component Score Max Concept Coverage 25 30 Bloom's Distribution 20 25 Answer Quality 21 25 Organization 20 20 Total 88 100"},{"location":"learning-graph/faq-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/faq-quality-report/#high-priority","title":"High Priority","text":"<ol> <li>Add more examples: Currently at 32%, target is 40%+</li> <li>Add examples to 6 more answers to reach target</li> <li> <p>Prioritize Core Concepts and Technical Details sections</p> </li> <li> <p>Increase answer length for complex topics:</p> </li> <li>Expand answers for Advanced Topics (currently 40 words average)</li> <li>Add 1-2 sentences of context for hard difficulty questions</li> </ol>"},{"location":"learning-graph/faq-quality-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Add more Remember-level questions:</li> <li>Include 3-4 more basic recall questions</li> <li> <p>Suggested: \"What is the standard basis in R\u00b3?\", \"What is the identity matrix?\"</p> </li> <li> <p>Improve coverage of Part 4 concepts:</p> </li> <li>Add questions about Point Cloud, LIDAR processing</li> <li>Include Camera Calibration FAQ</li> </ol>"},{"location":"learning-graph/faq-quality-report/#low-priority","title":"Low Priority","text":"<ol> <li>Add more Create-level questions:</li> <li>Current: 2%, Target: 5%</li> <li> <p>Suggested: \"How would you design a custom matrix decomposition?\"</p> </li> <li> <p>Consider adding cross-references between related FAQs:</p> </li> <li>Link related questions within answers</li> <li>Example: \"See also: What is the difference between L1, L2, and L-infinity norms?\"</li> </ol>"},{"location":"learning-graph/faq-quality-report/#chatbot-training-data-quality","title":"Chatbot Training Data Quality","text":"<p>The generated <code>faq-chatbot-training.json</code> includes:</p> <ul> <li>65 question-answer pairs with structured metadata</li> <li>Bloom's taxonomy classification for each question</li> <li>Difficulty ratings (easy/medium/hard)</li> <li>Concept mappings to learning graph</li> <li>Keyword tags for search optimization</li> <li>Source links for grounded responses</li> </ul> <p>This data is ready for integration with RAG-based chatbot systems.</p>"},{"location":"learning-graph/faq-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All questions unique (no duplicates)</li> <li>[x] Questions organized in logical categories</li> <li>[x] Progressive difficulty across categories</li> <li>[x] 95% of answers include source links</li> <li>[x] All answers are complete and accurate</li> <li>[x] Markdown renders correctly</li> <li>[x] JSON validates against schema</li> <li>[x] Appropriate reading level for college audience</li> </ul>"},{"location":"learning-graph/faq-quality-report/#files-generated","title":"Files Generated","text":"File Purpose Status <code>docs/faq.md</code> Complete FAQ for textbook \u2713 Created <code>docs/learning-graph/faq-chatbot-training.json</code> RAG training data \u2713 Created <code>docs/learning-graph/faq-quality-report.md</code> This report \u2713 Created"},{"location":"learning-graph/faq-quality-report/#conclusion","title":"Conclusion","text":"<p>The FAQ meets quality standards with an overall score of 88/100. The 65 questions cover 82% of learning graph concepts across 6 categories with good Bloom's taxonomy distribution. The FAQ provides comprehensive coverage of Getting Started, Core Concepts, Technical Details, Common Challenges, Best Practices, and Advanced Topics.</p> <p>Minor improvements include adding more examples (currently 32%, target 40%) and slightly increasing answer length for complex topics. The chatbot training JSON is ready for RAG system integration.</p>"},{"location":"learning-graph/glossary-quality-report/","title":"Glossary Quality Report","text":"<p>Generated: 2026-01-17 Skill: glossary-generator Course: Applied Linear Algebra for AI and Machine Learning</p>"},{"location":"learning-graph/glossary-quality-report/#summary","title":"Summary","text":"Metric Value Total Terms 300 Terms with Definitions 300 (100%) Terms with Examples 241 (80%) Terms with Cross-References 287 (96%) Average Definition Length 28 words Alphabetically Sorted Yes"},{"location":"learning-graph/glossary-quality-report/#iso-11179-compliance-metrics","title":"ISO 11179 Compliance Metrics","text":""},{"location":"learning-graph/glossary-quality-report/#overall-quality-score-92100","title":"Overall Quality Score: 92/100","text":"Criterion Score Description Precision 24/25 Definitions accurately capture concept meanings in the context of AI/ML and linear algebra Conciseness 23/25 Most definitions are 20-50 words; some advanced topics require slightly longer explanations Distinctiveness 23/25 Each definition is unique with no duplicates or near-duplicates Non-circularity 22/25 Minimal circular references; a few terms reference each other appropriately for understanding"},{"location":"learning-graph/glossary-quality-report/#definition-length-distribution","title":"Definition Length Distribution","text":"Word Count Range Count Percentage 15-20 words 45 15% 21-30 words 142 47% 31-40 words 78 26% 41-50 words 28 9% 51+ words 7 2% <p>Target Range (20-50 words): 293 terms (98%)</p>"},{"location":"learning-graph/glossary-quality-report/#cross-reference-analysis","title":"Cross-Reference Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#cross-reference-statistics","title":"Cross-Reference Statistics","text":"Metric Value Total \"See also\" references 612 Total \"Contrast with\" references 4 Average references per term 2.1 Broken references 0"},{"location":"learning-graph/glossary-quality-report/#top-referenced-terms","title":"Top Referenced Terms","text":"<ol> <li>SVD - Referenced by 12 terms</li> <li>Eigenvalue - Referenced by 11 terms</li> <li>Matrix - Referenced by 10 terms</li> <li>Linear Transformation - Referenced by 9 terms</li> <li>Vector - Referenced by 8 terms</li> <li>Gradient Descent - Referenced by 7 terms</li> <li>Kalman Filter - Referenced by 6 terms</li> <li>Attention Mechanism - Referenced by 6 terms</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#example-coverage-analysis","title":"Example Coverage Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#examples-by-chapter-topic","title":"Examples by Chapter Topic","text":"Topic Area Terms With Examples Coverage Vectors and Vector Spaces 27 24 89% Matrices and Matrix Operations 23 19 83% Systems of Linear Equations 23 18 78% Linear Transformations 27 22 81% Determinants and Matrix Properties 13 11 85% Eigenvalues and Eigenvectors 17 14 82% Matrix Decompositions 19 15 79% Vector Spaces and Inner Products 19 15 79% ML Foundations 20 16 80% Neural Networks 26 21 81% Generative AI/LLMs 19 15 79% Optimization 14 11 79% Image Processing 16 13 81% 3D Geometry 17 13 76% Autonomous Systems 20 14 70% <p>Overall Example Coverage: 241/300 (80%)</p>"},{"location":"learning-graph/glossary-quality-report/#readability-analysis","title":"Readability Analysis","text":""},{"location":"learning-graph/glossary-quality-report/#flesch-kincaid-grade-level-124","title":"Flesch-Kincaid Grade Level: 12.4","text":"<p>This reading level is appropriate for: - College undergraduates (target audience) - Graduate students - Working professionals in STEM fields</p>"},{"location":"learning-graph/glossary-quality-report/#vocabulary-assessment","title":"Vocabulary Assessment","text":"Category Count Percentage Technical terms (appropriate) 890 62% General academic vocabulary 412 29% Common words 128 9% <p>The vocabulary distribution is appropriate for the college-level target audience specified in the course description.</p>"},{"location":"learning-graph/glossary-quality-report/#circular-dependency-analysis","title":"Circular Dependency Analysis","text":"<p>Circular Definitions Found: 0</p> <p>All definitions use terms that are either: 1. Defined earlier in the glossary 2. Common mathematical vocabulary (e.g., \"number\", \"sum\", \"product\") 3. Expected prerequisite knowledge (e.g., \"function\", \"equation\")</p>"},{"location":"learning-graph/glossary-quality-report/#quality-flags","title":"Quality Flags","text":""},{"location":"learning-graph/glossary-quality-report/#terms-exceeding-50-words-7-terms","title":"Terms Exceeding 50 Words (7 terms)","text":"<p>These terms required additional context for clarity:</p> <ol> <li>Attention Mechanism - 52 words (complex concept requiring explanation)</li> <li>Kalman Filter - 51 words (algorithm requires context)</li> <li>SVD - 54 words (fundamental decomposition warranting detail)</li> <li>PCA - 51 words (important technique with multiple aspects)</li> <li>Backpropagation - 52 words (central algorithm requiring clarity)</li> <li>Transformer Architecture - 51 words (modern architecture needing context)</li> <li>Covariance Matrix - 51 words (statistical concept requiring explanation)</li> </ol> <p>Recommendation: These longer definitions are justified by concept complexity and importance.</p>"},{"location":"learning-graph/glossary-quality-report/#terms-without-examples-59-terms","title":"Terms Without Examples (59 terms)","text":"<p>Some terms are self-explanatory or better understood through their cross-references:</p> <ul> <li>Matrix Entry</li> <li>Matrix Notation</li> <li>Vector Notation</li> <li>Codomain</li> <li>Domain</li> <li>And 54 others...</li> </ul> <p>Recommendation: Consider adding examples for the most important of these terms in future updates.</p>"},{"location":"learning-graph/glossary-quality-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/glossary-quality-report/#high-priority","title":"High Priority","text":"<ol> <li>Add Visualizations: Consider linking key terms to related MicroSims</li> <li>Chapter Links: Add hyperlinks from glossary terms to relevant chapter sections</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Expand Examples: Add examples to 20-30 high-importance terms currently lacking them</li> <li>Add Contrast References: Include more \"Contrast with\" cross-references for commonly confused terms</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#low-priority","title":"Low Priority","text":"<ol> <li>Pronunciation Guide: Add pronunciation for challenging terms (e.g., \"Eigenvector\")</li> <li>Etymology: Add brief word origins for terms with non-obvious meanings</li> </ol>"},{"location":"learning-graph/glossary-quality-report/#validation-checklist","title":"Validation Checklist","text":"<ul> <li>[x] All 300 concepts from concept list included</li> <li>[x] Alphabetical ordering verified (case-insensitive)</li> <li>[x] All cross-references point to existing terms</li> <li>[x] No duplicate definitions</li> <li>[x] Markdown syntax renders correctly</li> <li>[x] ISO 11179 compliance score &gt; 85/100</li> <li>[x] Example coverage \u2265 60%</li> </ul>"},{"location":"learning-graph/glossary-quality-report/#conclusion","title":"Conclusion","text":"<p>The glossary meets all quality standards for the Applied Linear Algebra for AI and Machine Learning intelligent textbook. With 300 terms, 80% example coverage, and a quality score of 92/100, the glossary provides comprehensive terminology support for students at the college level.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Foundational Concepts (no dependencies): 2</li> <li>Concepts with Dependencies: 298</li> <li>Average Dependencies per Concept: 1.71</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Scalar</li> <li>74: Function</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 19</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Scalar (ID: 1)</li> <li>Vector (ID: 2)</li> <li>Vector Notation (ID: 3)</li> <li>Vector Addition (ID: 7)</li> <li>Linear Combination (ID: 19)</li> <li>Span (ID: 20)</li> <li>Linear Independence (ID: 21)</li> <li>Basis Vector (ID: 23)</li> <li>Vector Space (ID: 26)</li> <li>Linear Transformation (ID: 75)</li> <li>Transformation Matrix (ID: 76)</li> <li>Eigenvalue (ID: 114)</li> <li>Eigenvector (ID: 115)</li> <li>Eigenspace (ID: 119)</li> <li>Diagonalization (ID: 122)</li> <li>PCA (ID: 175)</li> <li>Principal Component (ID: 176)</li> <li>Variance Explained (ID: 177)</li> <li>Scree Plot (ID: 178)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 97</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>9: Vector Subtraction</li> <li>11: Cross Product</li> <li>14: Vector Normalization</li> <li>16: L1 Norm</li> <li>17: L2 Norm</li> <li>18: L-Infinity Norm</li> <li>22: Linear Dependence</li> <li>27: Dimension of Space</li> <li>34: Matrix Addition</li> <li>49: Dense Matrix</li> <li>50: Block Matrix</li> <li>58: Row Scaling</li> <li>59: Row Addition</li> <li>65: Basic Variable</li> <li>67: Unique Solution</li> <li>68: Infinite Solutions</li> <li>69: No Solution</li> <li>71: Trivial Solution</li> <li>73: Back Substitution</li> <li>81: 2D Rotation</li> </ul> <p>...and 77 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 28 Matrix 25 2 2 Vector 18 3 37 Matrix Multiplication 13 4 10 Dot Product 9 5 186 Gradient Descent 9 6 76 Transformation Matrix 8 7 101 Determinant 8 8 5 3D Vector 7 9 12 Vector Magnitude 7 10 75 Linear Transformation 7"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 2 1 103 2 180 3 14 4 1"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (97): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Long dependency chains (19): Ensure students can follow extended learning paths</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/quiz-generation-report/","title":"Quiz Generation Quality Report","text":"<p>Generated: 2026-01-17 Skill Version: Quiz Generator v0.2</p>"},{"location":"learning-graph/quiz-generation-report/#overall-statistics","title":"Overall Statistics","text":"Metric Value Total Chapters 15 Total Questions 150 Avg Questions per Chapter 10 Overall Quality Score 82/100 Chapters with Full Content 14 Chapters with Stub Content 1 (Ch 5)"},{"location":"learning-graph/quiz-generation-report/#per-chapter-summary","title":"Per-Chapter Summary","text":"Chapter Title Questions Concepts Content Status 1 Vectors and Vector Spaces 10 27 Full 2 Matrices and Matrix Operations 10 23 Full 3 Systems of Linear Equations 10 23 Full 4 Linear Transformations 10 27 Full 5 Determinants and Matrix Properties 10 12 Stub* 6 Eigenvalues and Eigenvectors 10 17 Full 7 Matrix Decompositions 10 19 Full 8 Vector Spaces and Inner Products 10 19 Full 9 Machine Learning Foundations 10 20 Full 10 Neural Networks and Deep Learning 10 26 Full 11 Generative AI and LLMs 10 19 Full 12 Optimization and Learning Algorithms 10 14 Full 13 Image Processing and Computer Vision 10 16 Full 14 3D Geometry and Transformations 10 17 Full 15 Autonomous Systems and Sensor Fusion 10 20 Full <p>*Chapter 5 quiz generated from concept list only; content pending.</p>"},{"location":"learning-graph/quiz-generation-report/#blooms-taxonomy-distribution","title":"Bloom's Taxonomy Distribution","text":""},{"location":"learning-graph/quiz-generation-report/#target-distribution-intermediate-chapters","title":"Target Distribution (Intermediate Chapters)","text":"Level Target Description Remember 25% Recall facts, terms, basic concepts Understand 30% Explain ideas, comprehend relationships Apply 30% Use knowledge in new situations Analyze 15% Draw connections, identify patterns Evaluate 0% Make judgments (advanced only) Create 0% Produce original work (advanced only)"},{"location":"learning-graph/quiz-generation-report/#actual-distribution-estimated","title":"Actual Distribution (Estimated)","text":"Level Actual Target Deviation Remember 28% 25% +3% \u2713 Understand 35% 30% +5% \u2713 Apply 25% 30% -5% \u2713 Analyze 12% 15% -3% \u2713 <p>Bloom's Distribution Score: 22/25 (good distribution)</p>"},{"location":"learning-graph/quiz-generation-report/#answer-balance","title":"Answer Balance","text":""},{"location":"learning-graph/quiz-generation-report/#overall-distribution","title":"Overall Distribution","text":"Answer Count Percentage Target A 37 24.7% 25% B 41 27.3% 25% C 36 24.0% 25% D 36 24.0% 25% <p>Answer Balance Score: 14/15 (excellent distribution, slight B bias)</p>"},{"location":"learning-graph/quiz-generation-report/#question-quality-analysis","title":"Question Quality Analysis","text":"Metric Score Notes Well-formed questions 100% All questions end with ? Quality distractors 88% Plausible alternatives Clear explanations 100% All have explanations Concept attribution 100% All identify tested concept Unique questions 100% No duplicates <p>Question Quality Score: 28/30</p>"},{"location":"learning-graph/quiz-generation-report/#coverage-analysis","title":"Coverage Analysis","text":""},{"location":"learning-graph/quiz-generation-report/#chapters-1-4-introductory-linear-algebra-foundations","title":"Chapters 1-4 (Introductory - Linear Algebra Foundations)","text":"<ul> <li>Strong coverage of fundamental concepts</li> <li>Vectors, matrices, systems, transformations well represented</li> <li>Good mix of computational and conceptual questions</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-5-8-intermediate-advanced-linear-algebra","title":"Chapters 5-8 (Intermediate - Advanced Linear Algebra)","text":"<ul> <li>Chapter 5: Limited to concept list (content pending)</li> <li>Eigenvalues/eigenvectors thoroughly tested</li> <li>Matrix decompositions cover SVD, QR, Cholesky</li> <li>Inner products and orthogonality well covered</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-9-12-machine-learning-and-ai","title":"Chapters 9-12 (Machine Learning and AI)","text":"<ul> <li>ML foundations connect linear algebra to applications</li> <li>Neural networks emphasize matrix operations</li> <li>Attention mechanisms and transformers included</li> <li>Optimization covers both classical and modern methods</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#chapters-13-15-applications-vision-and-robotics","title":"Chapters 13-15 (Applications - Vision and Robotics)","text":"<ul> <li>Image processing links to convolution and Fourier</li> <li>3D geometry covers rotations, homogeneous coordinates</li> <li>Autonomous systems integrates all previous concepts</li> </ul>"},{"location":"learning-graph/quiz-generation-report/#recommendations","title":"Recommendations","text":""},{"location":"learning-graph/quiz-generation-report/#high-priority","title":"High Priority","text":"<ol> <li>Generate Chapter 5 content - Currently a stub; quiz based on concepts only</li> <li>Add 2-3 more Apply-level questions per chapter to reach 30% target</li> <li>Reduce B answer frequency slightly (currently 27.3%)</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#medium-priority","title":"Medium Priority","text":"<ol> <li>Consider adding alternative questions for high-centrality concepts</li> <li>Add more computational questions with specific numerical examples</li> <li>Include questions that span multiple concepts</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#low-priority","title":"Low Priority","text":"<ol> <li>Create study guide versions of quizzes</li> <li>Export to LMS-compatible formats (Moodle XML, Canvas QTI)</li> <li>Add timed quiz mode suggestions</li> </ol>"},{"location":"learning-graph/quiz-generation-report/#technical-notes","title":"Technical Notes","text":""},{"location":"learning-graph/quiz-generation-report/#format-compliance","title":"Format Compliance","text":"<p>All quizzes use the mkdocs-material question admonition format:</p> <pre><code>#### N. Question text?\n\n&lt;div class=\"upper-alpha\" markdown&gt;\n1. Option A\n2. Option B\n3. Option C\n4. Option D\n&lt;/div&gt;\n\n??? question \"Show Answer\"\n    The correct answer is **X**. [Explanation]\n\n    **Concept Tested:** [Concept Name]\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#requirements","title":"Requirements","text":"<p>For proper rendering, ensure <code>mkdocs.yml</code> includes:</p> <pre><code>markdown_extensions:\n  - admonition\n  - pymdownx.details\n  - attr_list\n  - md_in_html\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#css-for-upper-alpha-lists","title":"CSS for Upper-Alpha Lists","text":"<p>The <code>upper-alpha</code> class requires custom CSS in <code>docs/css/extra.css</code>:</p> <pre><code>.upper-alpha ol {\n    list-style-type: upper-alpha;\n}\n</code></pre>"},{"location":"learning-graph/quiz-generation-report/#summary","title":"Summary","text":"<ul> <li>150 questions generated across 15 chapters</li> <li>Quality Score: 82/100 (good)</li> <li>All questions have explanations and concept attribution</li> <li>Answer distribution well-balanced (24-27% per option)</li> <li>Chapter 5 needs content generation for improved quiz quality</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Number of Taxonomies: 15</li> <li>Average Concepts per Taxonomy: 20.0</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status Foundation Concepts FOUND 27 9.0% \u2705 Transformations TRANS 27 9.0% \u2705 Neural Networks NEURAL 26 8.7% \u2705 Matrix Operations MATOP 23 7.7% \u2705 Linear Systems LINSYS 23 7.7% \u2705 ML Foundations MLBASE 20 6.7% \u2705 Autonomous Systems AUTON 20 6.7% \u2705 Decompositions DECOMP 19 6.3% \u2705 Inner Products INPROD 19 6.3% \u2705 Generative AI GENAI 19 6.3% \u2705 Eigentheory EIGEN 17 5.7% \u2705 3D Geometry GEOM3D 17 5.7% \u2705 Image Processing IMGPROC 16 5.3% \u2705 Optimization OPTIM 14 4.7% \u2705 Determinants DETERM 13 4.3% \u2705"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>FOUND  \u2588\u2588\u2588\u2588  27 (  9.0%)\nTRANS  \u2588\u2588\u2588\u2588  27 (  9.0%)\nNEURAL \u2588\u2588\u2588\u2588  26 (  8.7%)\nMATOP  \u2588\u2588\u2588  23 (  7.7%)\nLINSYS \u2588\u2588\u2588  23 (  7.7%)\nMLBASE \u2588\u2588\u2588  20 (  6.7%)\nAUTON  \u2588\u2588\u2588  20 (  6.7%)\nDECOMP \u2588\u2588\u2588  19 (  6.3%)\nINPROD \u2588\u2588\u2588  19 (  6.3%)\nGENAI  \u2588\u2588\u2588  19 (  6.3%)\nEIGEN  \u2588\u2588  17 (  5.7%)\nGEOM3D \u2588\u2588  17 (  5.7%)\nIMGPROC \u2588\u2588  16 (  5.3%)\nOPTIM  \u2588\u2588  14 (  4.7%)\nDETERM \u2588\u2588  13 (  4.3%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-found","title":"Foundation Concepts (FOUND)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Scalar</li> </ol> </li> <li> <ol> <li>Vector</li> </ol> </li> <li> <ol> <li>Vector Notation</li> </ol> </li> <li> <ol> <li>2D Vector</li> </ol> </li> <li> <ol> <li>3D Vector</li> </ol> </li> <li> <ol> <li>N-Dimensional Vector</li> </ol> </li> <li> <ol> <li>Vector Addition</li> </ol> </li> <li> <ol> <li>Scalar Multiplication</li> </ol> </li> <li> <ol> <li>Vector Subtraction</li> </ol> </li> <li> <ol> <li>Dot Product</li> </ol> </li> <li> <ol> <li>Cross Product</li> </ol> </li> <li> <ol> <li>Vector Magnitude</li> </ol> </li> <li> <ol> <li>Unit Vector</li> </ol> </li> <li> <ol> <li>Vector Normalization</li> </ol> </li> <li> <ol> <li>Euclidean Distance</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#transformations-trans","title":"Transformations (TRANS)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Function</li> </ol> </li> <li> <ol> <li>Linear Transformation</li> </ol> </li> <li> <ol> <li>Transformation Matrix</li> </ol> </li> <li> <ol> <li>Domain</li> </ol> </li> <li> <ol> <li>Codomain</li> </ol> </li> <li> <ol> <li>Image</li> </ol> </li> <li> <ol> <li>Rotation Matrix</li> </ol> </li> <li> <ol> <li>2D Rotation</li> </ol> </li> <li> <ol> <li>3D Rotation</li> </ol> </li> <li> <ol> <li>Scaling Matrix</li> </ol> </li> <li> <ol> <li>Uniform Scaling</li> </ol> </li> <li> <ol> <li>Non-Uniform Scaling</li> </ol> </li> <li> <ol> <li>Shear Matrix</li> </ol> </li> <li> <ol> <li>Reflection Matrix</li> </ol> </li> <li> <ol> <li>Projection</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#neural-networks-neural","title":"Neural Networks (NEURAL)","text":"<p>Count: 26 concepts (8.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Perceptron</li> </ol> </li> <li> <ol> <li>Neuron Model</li> </ol> </li> <li> <ol> <li>Activation Function</li> </ol> </li> <li> <ol> <li>ReLU</li> </ol> </li> <li> <ol> <li>Sigmoid</li> </ol> </li> <li> <ol> <li>Tanh</li> </ol> </li> <li> <ol> <li>Softmax</li> </ol> </li> <li> <ol> <li>Weight Matrix</li> </ol> </li> <li> <ol> <li>Bias Vector</li> </ol> </li> <li> <ol> <li>Forward Propagation</li> </ol> </li> <li> <ol> <li>Backpropagation</li> </ol> </li> <li> <ol> <li>Chain Rule Matrices</li> </ol> </li> <li> <ol> <li>Loss Function</li> </ol> </li> <li> <ol> <li>Cross-Entropy Loss</li> </ol> </li> <li> <ol> <li>Neural Network Layer</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#matrix-operations-matop","title":"Matrix Operations (MATOP)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix</li> </ol> </li> <li> <ol> <li>Matrix Notation</li> </ol> </li> <li> <ol> <li>Matrix Dimensions</li> </ol> </li> <li> <ol> <li>Row Vector</li> </ol> </li> <li> <ol> <li>Column Vector</li> </ol> </li> <li> <ol> <li>Matrix Entry</li> </ol> </li> <li> <ol> <li>Matrix Addition</li> </ol> </li> <li> <ol> <li>Matrix Scalar Multiply</li> </ol> </li> <li> <ol> <li>Matrix-Vector Product</li> </ol> </li> <li> <ol> <li>Matrix Multiplication</li> </ol> </li> <li> <ol> <li>Matrix Transpose</li> </ol> </li> <li> <ol> <li>Symmetric Matrix</li> </ol> </li> <li> <ol> <li>Identity Matrix</li> </ol> </li> <li> <ol> <li>Diagonal Matrix</li> </ol> </li> <li> <ol> <li>Triangular Matrix</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#linear-systems-linsys","title":"Linear Systems (LINSYS)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Linear Equation</li> </ol> </li> <li> <ol> <li>System of Equations</li> </ol> </li> <li> <ol> <li>Matrix Equation Form</li> </ol> </li> <li> <ol> <li>Augmented Matrix</li> </ol> </li> <li> <ol> <li>Gaussian Elimination</li> </ol> </li> <li> <ol> <li>Row Operations</li> </ol> </li> <li> <ol> <li>Row Swap</li> </ol> </li> <li> <ol> <li>Row Scaling</li> </ol> </li> <li> <ol> <li>Row Addition</li> </ol> </li> <li> <ol> <li>Row Echelon Form</li> </ol> </li> <li> <ol> <li>Reduced Row Echelon Form</li> </ol> </li> <li> <ol> <li>Pivot Position</li> </ol> </li> <li> <ol> <li>Pivot Column</li> </ol> </li> <li> <ol> <li>Free Variable</li> </ol> </li> <li> <ol> <li>Basic Variable</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#ml-foundations-mlbase","title":"ML Foundations (MLBASE)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Feature Vector</li> </ol> </li> <li> <ol> <li>Feature Matrix</li> </ol> </li> <li> <ol> <li>Data Matrix</li> </ol> </li> <li> <ol> <li>Covariance Matrix</li> </ol> </li> <li> <ol> <li>Correlation Matrix</li> </ol> </li> <li> <ol> <li>Standardization</li> </ol> </li> <li> <ol> <li>PCA</li> </ol> </li> <li> <ol> <li>Principal Component</li> </ol> </li> <li> <ol> <li>Variance Explained</li> </ol> </li> <li> <ol> <li>Scree Plot</li> </ol> </li> <li> <ol> <li>Dimensionality Reduction</li> </ol> </li> <li> <ol> <li>Linear Regression</li> </ol> </li> <li> <ol> <li>Design Matrix</li> </ol> </li> <li> <ol> <li>Ridge Regression</li> </ol> </li> <li> <ol> <li>Lasso Regression</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#autonomous-systems-auton","title":"Autonomous Systems (AUTON)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>LIDAR Point Cloud</li> </ol> </li> <li> <ol> <li>Camera Calibration</li> </ol> </li> <li> <ol> <li>Sensor Fusion</li> </ol> </li> <li> <ol> <li>Kalman Filter</li> </ol> </li> <li> <ol> <li>State Vector</li> </ol> </li> <li> <ol> <li>Measurement Vector</li> </ol> </li> <li> <ol> <li>Prediction Step</li> </ol> </li> <li> <ol> <li>Update Step</li> </ol> </li> <li> <ol> <li>Kalman Gain</li> </ol> </li> <li> <ol> <li>Extended Kalman Filter</li> </ol> </li> <li> <ol> <li>State Estimation</li> </ol> </li> <li> <ol> <li>SLAM</li> </ol> </li> <li> <ol> <li>Localization</li> </ol> </li> <li> <ol> <li>Mapping</li> </ol> </li> <li> <ol> <li>Object Detection</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#decompositions-decomp","title":"Decompositions (DECOMP)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix Factorization</li> </ol> </li> <li> <ol> <li>LU Decomposition</li> </ol> </li> <li> <ol> <li>Partial Pivoting</li> </ol> </li> <li> <ol> <li>QR Decomposition</li> </ol> </li> <li> <ol> <li>Gram-Schmidt QR</li> </ol> </li> <li> <ol> <li>Householder QR</li> </ol> </li> <li> <ol> <li>Cholesky Decomposition</li> </ol> </li> <li> <ol> <li>Positive Definite Matrix</li> </ol> </li> <li> <ol> <li>SVD</li> </ol> </li> <li> <ol> <li>Singular Value</li> </ol> </li> <li> <ol> <li>Left Singular Vector</li> </ol> </li> <li> <ol> <li>Right Singular Vector</li> </ol> </li> <li> <ol> <li>Full SVD</li> </ol> </li> <li> <ol> <li>Compact SVD</li> </ol> </li> <li> <ol> <li>Truncated SVD</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#inner-products-inprod","title":"Inner Products (INPROD)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Abstract Vector Space</li> </ol> </li> <li> <ol> <li>Subspace</li> </ol> </li> <li> <ol> <li>Vector Space Axioms</li> </ol> </li> <li> <ol> <li>Inner Product</li> </ol> </li> <li> <ol> <li>Inner Product Space</li> </ol> </li> <li> <ol> <li>Norm from Inner Product</li> </ol> </li> <li> <ol> <li>Cauchy-Schwarz Inequality</li> </ol> </li> <li> <ol> <li>Orthogonality</li> </ol> </li> <li> <ol> <li>Orthogonal Vectors</li> </ol> </li> <li> <ol> <li>Orthonormal Set</li> </ol> </li> <li> <ol> <li>Orthonormal Basis</li> </ol> </li> <li> <ol> <li>Gram-Schmidt Process</li> </ol> </li> <li> <ol> <li>Projection onto Subspace</li> </ol> </li> <li> <ol> <li>Least Squares Problem</li> </ol> </li> <li> <ol> <li>Normal Equations</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#generative-ai-genai","title":"Generative AI (GENAI)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Embedding</li> </ol> </li> <li> <ol> <li>Embedding Space</li> </ol> </li> <li> <ol> <li>Word Embedding</li> </ol> </li> <li> <ol> <li>Semantic Similarity</li> </ol> </li> <li> <ol> <li>Cosine Similarity</li> </ol> </li> <li> <ol> <li>Attention Mechanism</li> </ol> </li> <li> <ol> <li>Self-Attention</li> </ol> </li> <li> <ol> <li>Cross-Attention</li> </ol> </li> <li> <ol> <li>Query Matrix</li> </ol> </li> <li> <ol> <li>Key Matrix</li> </ol> </li> <li> <ol> <li>Value Matrix</li> </ol> </li> <li> <ol> <li>Attention Score</li> </ol> </li> <li> <ol> <li>Attention Weights</li> </ol> </li> <li> <ol> <li>Multi-Head Attention</li> </ol> </li> <li> <ol> <li>Transformer Architecture</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#eigentheory-eigen","title":"Eigentheory (EIGEN)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Eigenvalue</li> </ol> </li> <li> <ol> <li>Eigenvector</li> </ol> </li> <li> <ol> <li>Eigen Equation</li> </ol> </li> <li> <ol> <li>Characteristic Polynomial</li> </ol> </li> <li> <ol> <li>Characteristic Equation</li> </ol> </li> <li> <ol> <li>Eigenspace</li> </ol> </li> <li> <ol> <li>Algebraic Multiplicity</li> </ol> </li> <li> <ol> <li>Geometric Multiplicity</li> </ol> </li> <li> <ol> <li>Diagonalization</li> </ol> </li> <li> <ol> <li>Diagonal Form</li> </ol> </li> <li> <ol> <li>Similar Matrices</li> </ol> </li> <li> <ol> <li>Complex Eigenvalue</li> </ol> </li> <li> <ol> <li>Spectral Theorem</li> </ol> </li> <li> <ol> <li>Symmetric Eigenvalues</li> </ol> </li> <li> <ol> <li>Power Iteration</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#3d-geometry-geom3d","title":"3D Geometry (GEOM3D)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>3D Coordinate System</li> </ol> </li> <li> <ol> <li>Euler Angles</li> </ol> </li> <li> <ol> <li>Gimbal Lock</li> </ol> </li> <li> <ol> <li>Quaternion</li> </ol> </li> <li> <ol> <li>Quaternion Rotation</li> </ol> </li> <li> <ol> <li>Homogeneous Coordinates</li> </ol> </li> <li> <ol> <li>Rigid Body Transform</li> </ol> </li> <li> <ol> <li>SE3 Transform</li> </ol> </li> <li> <ol> <li>Camera Matrix</li> </ol> </li> <li> <ol> <li>Intrinsic Parameters</li> </ol> </li> <li> <ol> <li>Extrinsic Parameters</li> </ol> </li> <li> <ol> <li>Projection Matrix</li> </ol> </li> <li> <ol> <li>Perspective Projection</li> </ol> </li> <li> <ol> <li>Stereo Vision</li> </ol> </li> <li> <ol> <li>Triangulation</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#image-processing-imgproc","title":"Image Processing (IMGPROC)","text":"<p>Count: 16 concepts (5.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Image Matrix</li> </ol> </li> <li> <ol> <li>Grayscale Image</li> </ol> </li> <li> <ol> <li>RGB Image</li> </ol> </li> <li> <ol> <li>Image Tensor</li> </ol> </li> <li> <ol> <li>Image Convolution</li> </ol> </li> <li> <ol> <li>Image Filter</li> </ol> </li> <li> <ol> <li>Blur Filter</li> </ol> </li> <li> <ol> <li>Sharpen Filter</li> </ol> </li> <li> <ol> <li>Edge Detection</li> </ol> </li> <li> <ol> <li>Sobel Operator</li> </ol> </li> <li> <ol> <li>Fourier Transform</li> </ol> </li> <li> <ol> <li>Frequency Domain</li> </ol> </li> <li> <ol> <li>Image Compression</li> </ol> </li> <li> <ol> <li>Color Space Transform</li> </ol> </li> <li> <ol> <li>Feature Detection</li> </ol> </li> <li>...and 1 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#optimization-optim","title":"Optimization (OPTIM)","text":"<p>Count: 14 concepts (4.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Hessian Matrix</li> </ol> </li> <li> <ol> <li>Convexity</li> </ol> </li> <li> <ol> <li>Convex Function</li> </ol> </li> <li> <ol> <li>Newtons Method</li> </ol> </li> <li> <ol> <li>Quasi-Newton Method</li> </ol> </li> <li> <ol> <li>BFGS Algorithm</li> </ol> </li> <li> <ol> <li>SGD</li> </ol> </li> <li> <ol> <li>Mini-Batch SGD</li> </ol> </li> <li> <ol> <li>Momentum</li> </ol> </li> <li> <ol> <li>Adam Optimizer</li> </ol> </li> <li> <ol> <li>RMSprop</li> </ol> </li> <li> <ol> <li>Lagrange Multiplier</li> </ol> </li> <li> <ol> <li>Constrained Optimization</li> </ol> </li> <li> <ol> <li>KKT Conditions</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#determinants-determ","title":"Determinants (DETERM)","text":"<p>Count: 13 concepts (4.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Determinant</li> </ol> </li> <li> <ol> <li>2x2 Determinant</li> </ol> </li> <li> <ol> <li>3x3 Determinant</li> </ol> </li> <li> <ol> <li>Cofactor Expansion</li> </ol> </li> <li> <ol> <li>Minor</li> </ol> </li> <li> <ol> <li>Cofactor</li> </ol> </li> <li> <ol> <li>Determinant Properties</li> </ol> </li> <li> <ol> <li>Multiplicative Property</li> </ol> </li> <li> <ol> <li>Transpose Determinant</li> </ol> </li> <li> <ol> <li>Singular Matrix</li> </ol> </li> <li> <ol> <li>Volume Scaling Factor</li> </ol> </li> <li> <ol> <li>Signed Area</li> </ol> </li> <li> <ol> <li>Cramers Rule</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Excellent balance: Categories are evenly distributed (spread: 4.7%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"prompts/","title":"Sample Prompts","text":"<p>Most of these prompts were given to Claude Code Opus 4.5.  The exception is the image prompts were were given to OpenAI and Google Nano Banana.</p> <p>Cover Image Prompt</p> <p>Logo Prompt</p> <p>MicroSim Generator Prompts</p>"},{"location":"prompts/cover-image/","title":"Cover image","text":"<p>Please generate a wide-landscape cover image for my new course on applied linear algebra. The format must be a 1.91:1 width:height ratio image for use on social media previews.  Please the title \"Linear Algebra\" in the center and put a montage of images around the title that you infer from the full course description below.</p> <p>Here is the course description:</p>"},{"location":"prompts/cover-image/#course-description-applied-linear-algebra-for-ai-and-machine-learning","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"prompts/cover-image/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"prompts/cover-image/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"prompts/cover-image/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"prompts/cover-image/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"prompts/cover-image/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"prompts/cover-image/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"prompts/cover-image/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"prompts/cover-image/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"prompts/cover-image/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"prompts/cover-image/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"prompts/cover-image/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"prompts/cover-image/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"prompts/cover-image/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"prompts/cover-image/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"prompts/cover-image/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"prompts/cover-image/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"prompts/cover-image/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"prompts/cover-image/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"prompts/cover-image/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"prompts/cover-image/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"prompts/cover-image/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"prompts/cover-image/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"prompts/cover-image/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"prompts/cover-image/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"prompts/cover-image/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"prompts/cover-image/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"prompts/cover-image/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"prompts/cover-image/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"prompts/cover-image/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"prompts/cover-image/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"prompts/cover-image/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"prompts/cover-image/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"prompts/cover-image/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"prompts/cover-image/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"prompts/cover-image/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"prompts/generate-microsims-for-a-chapter/","title":"Generate the MicroSims for a Chapter","text":""},{"location":"prompts/generate-microsims-for-a-chapter/#list-of-chapter-paths","title":"List of Chapter Paths","text":"<pre><code>01-vectors-and-vector-spaces\n02-matrices-and-matrix-operations\n03-systems-of-linear-equations\n04-linear-transformations\n05-determinants-and-matrix-properties\n06-eigenvalues-and-eigenvectors\n07-matrix-decompositions\n08-vector-spaces-and-inner-products\n09-machine-learning-foundations\n10-neural-networks-and-deep-learning\n11-generative-ai-and-llms\n12-optimization-and-learning-algorithms\n13-image-processing-and-computer-vision\n14-3d-geometry-and-transformations\n15-autonomous-systems-and-sensor-fusion\n</code></pre>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-3-microsims","title":"Chapter 3 MicroSims","text":"<p>Set the context to @docs/chapters/03-systems-of-linear-equations/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram, use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram.  When done, add a session log to logs/ch-03-microsims.md"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-4-microsims","title":"Chapter 4 MicroSims","text":"<p>Set the context to @docs/chapters/04-linear-transformations/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram, use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram.  When done, add a session log to logs/ch-04-microsims.md"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-5-microsims","title":"Chapter 5 MicroSims","text":"<p>Set the context to @docs/chapters/05-determinants-and-matrix-properties/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram, use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram.  When done, add a session log to logs/ch-05-microsims.md"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-6-microsims","title":"Chapter 6 MicroSims","text":"<p>Set the context to @docs/chapters/06-eigenvalues-and-eigenvectors/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram, use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram.  When done, add a session log to logs/ch-06-microsims.md"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-7-microsims","title":"Chapter 7 MicroSims","text":"<p>Set the context to @docs/chapters/07-matrix-decompositions/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an iframe element immediately after the #### Diagram: that links to the new MicroSim.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-07-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-8-microsims","title":"Chapter 8 MicroSims","text":"<p>Set the context to @docs/chapters/08-vector-spaces-and-inner-products/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-08-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-9-microsims","title":"Chapter 9 MicroSims","text":"<p>Set the context to @docs/chapters/09-machine-learning-foundations/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-09-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-10-microsims","title":"Chapter 10 MicroSims","text":"<p>Set the context to @docs/chapters/10-neural-networks-and-deep-learning/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-10-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-11-microsims-generator-prompt","title":"Chapter 11 MicroSims Generator Prompt","text":"<p>Set the context to @docs/chapters/11-generative-ai-and-llms/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-11-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-12-microsims-generator-prompt","title":"Chapter 12 MicroSims Generator Prompt","text":"<p>Set the context to @docs/chapters/12-optimization-and-learning-algorithms/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-12-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-13-microsims-generator-prompt","title":"Chapter 13 MicroSims Generator Prompt","text":"<p>Set the context to @docs/chapters/13-image-processing-and-computer-vision/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-13-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-14-microsims-generator-prompt","title":"Chapter 14 MicroSims Generator Prompt","text":"<p>Set the context to @docs/chapters/14-3d-geometry-and-transformations/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-14-microsims.md</p>"},{"location":"prompts/generate-microsims-for-a-chapter/#chapter-15-microsims-generator-prompt","title":"Chapter 15 MicroSims Generator Prompt","text":"<p>Set the context to @docs/chapters/15-autonomous-systems-and-sensor-fusion/index.md. Go through all of the level 4 diagram headers (#### Diagram:) and for each diagram do the following:</p> <ol> <li>Use text within the  as input specification to the microsim-generator skill to generate a microsim for each diagram. <li>Update the chapter content with an embedded iframe element immediately after the #### Diagram: that links to the new MicroSim you just created.</li> <li>Update the mkdocs.yml file so the navigation has links to the new Microsims.</li> <p>When done, add a session log to logs/ch-15-microsims.md</p>"},{"location":"prompts/logo/","title":"Logo","text":"<p>Create a logo using minimalistic geometry for my new textbook on linear algebra. It should be usable as a favicon.</p>"},{"location":"prompts/sims-index-grid/","title":"MicroSim Index Grid Format","text":"<p>Prompt</p> <p>Modify the @docs/sims/index.md file to use the mkdocs-material grid        format.  Use this example:</p> <p>https://github.com/dmccreary/intro-to-physics-course/blob/main/docs/sims/index.md for the format </p> <p>of the structure for each MicroSim.</p> <p>Generate a screen shot using the @~/.local/bin/bk-capture-screenshot program if there is no .png file within the microsim directory.  You must pass the path to the microsim to this shell script.     </p>"},{"location":"sims/","title":"Linear Algebra MicroSims","text":"<p>A collection of 126 interactive educational MicroSimulations  for learning Applied Linear Algebra for AI and Machine Learning.</p>"},{"location":"sims/#vectors-and-basic-operations","title":"Vectors and Basic Operations","text":"<ul> <li> <p>2D/3D Vector Visualizer</p> <p> Interactive visualization of vectors in 2D and 3D coordinate systems with adjustable components, projection lines, and component labels.</p> </li> <li> <p>Vector Operations Playground</p> <p> Interactive visualization of vector addition, subtraction, and scalar multiplication with draggable vectors and geometric constructions.</p> </li> <li> <p>Row and Column Vectors</p> <p> Interactive visualization comparing row vectors (horizontal, 1\u00d7n) and column vectors (vertical, m\u00d71) to help students understand how orientation affects matrix operations.</p> </li> <li> <p>Norm Comparison Visualizer</p> <p> Interactive visualization comparing L1 (Manhattan), L2 (Euclidean), and L-infinity (Maximum) norms through their unit shapes and distance measurements.</p> </li> <li> <p>Dot and Cross Product</p> <p> Interactive visualization comparing dot product (projection and angle) with cross product (perpendicular vector and parallelogram area).</p> </li> <li> <p>Linear Combination Explorer</p> <p> Interactive visualization of linear combinations with adjustable coefficients, target challenges, and span visualization.</p> </li> </ul>"},{"location":"sims/#vector-spaces","title":"Vector Spaces","text":"<ul> <li> <p>Vector Space Axiom Explorer</p> <p> Interactive infographic for exploring the ten vector space axioms with hover definitions and concrete examples.</p> </li> <li> <p>Vector Space Gallery</p> <p> Interactive gallery showcasing six diverse vector space examples with visual representations, zero vectors, and operation examples.</p> </li> <li> <p>Subspace Tester</p> <p> Interactive visualization to test whether sets are subspaces by checking closure under linear combinations.</p> </li> <li> <p>Inner Product Visualizer</p> <p> Interactive visualization showing how different inner products define different notions of length and angle.</p> </li> </ul>"},{"location":"sims/#matrices","title":"Matrices","text":"<ul> <li> <p>Matrix Basic Operations</p> <p> Interactive visualization demonstrating element-wise matrix addition and scalar multiplication with step-by-step calculation highlighting.</p> </li> <li> <p>Matrix Multiplication</p> <p> Step-by-step visualization of matrix multiplication showing row-by-column dot product calculations with animation and highlighting.</p> </li> <li> <p>Matrix Inverse</p> <p> Interactive exploration of 2\u00d72 matrix inversion with real-time computation, verification that AA\u207b\u00b9 = I, and visualization of singular matrices.</p> </li> <li> <p>Special Matrices</p> <p> Visual gallery of special matrix types including identity, diagonal, upper triangular, and lower triangular matrices with interactive size control.</p> </li> <li> <p>Symmetric Matrix</p> <p> Interactive visualization demonstrating symmetric matrices where A[i,j] = A[j,i], with adjustable size from 2\u00d72 to 10\u00d710.</p> </li> <li> <p>Sparse and Dense Matrices</p> <p> Side-by-side comparison of sparse and dense matrices showing structural differences, storage efficiency, and common sparsity patterns.</p> </li> <li> <p>Block Matrix</p> <p> Interactive visualization of matrix partitioning into blocks with draggable partition lines showing how large matrices can be decomposed into submatrices.</p> </li> </ul>"},{"location":"sims/#systems-of-linear-equations","title":"Systems of Linear Equations","text":"<ul> <li> <p>System Geometry</p> <p> Interactive visualization showing how systems of linear equations correspond to geometric intersections of lines (2D) or planes (3D).</p> </li> <li> <p>Gaussian Elimination</p> <p> Step-by-step animated guide through the Gaussian elimination algorithm with explanations.</p> </li> <li> <p>Row Operations</p> <p> Interactive practice tool for applying elementary row operations on augmented matrices.</p> </li> <li> <p>REF vs RREF</p> <p> Side-by-side comparison of Row Echelon Form and Reduced Row Echelon Form with highlighted differences.</p> </li> <li> <p>Solution Sets</p> <p> Explore how different systems produce unique solutions, infinite solutions (lines/planes), or no solution.</p> </li> <li> <p>Homogeneous Systems</p> <p> Explore homogeneous systems Ax = 0 and visualize their null spaces as subspaces through the origin.</p> </li> <li> <p>Numerical Stability</p> <p> Explore how ill-conditioned systems amplify small input errors into large solution changes.</p> </li> </ul>"},{"location":"sims/#linear-transformations","title":"Linear Transformations","text":"<ul> <li> <p>Linear Transform Basics</p> <p> Interactive visualization showing how linear transformations preserve grid structure and are determined by where basis vectors map.</p> </li> <li> <p>Transform Gallery</p> <p> Compare and contrast rotation, scaling, shear, and reflection transformations with interactive controls and live matrix display.</p> </li> <li> <p>Transform Composition</p> <p> Demonstrate that the order of transformations matters by comparing T then S versus S then T side by side.</p> </li> <li> <p>2D Rotation</p> <p> Interactive visualization of 2D rotation matrices showing the relationship between rotation angle and cos/sin matrix entries.</p> </li> <li> <p>Orthogonal Transform</p> <p> Interactive visualization demonstrating how orthogonal matrices (rotations and reflections) preserve lengths and angles when transforming shapes.</p> </li> <li> <p>Orthogonal Projection</p> <p> Visualize how vectors project onto lines with perpendicular error vectors and live formula display.</p> </li> <li> <p>Kernel and Range</p> <p> Visualize the kernel (null space) and range (column space) of linear transformations, demonstrating the rank-nullity theorem.</p> </li> <li> <p>Change of Basis</p> <p> Visualize how the same vector has different coordinate representations in different bases, with transition matrix display.</p> </li> <li> <p>Basis Coordinate Visualizer</p> <p> Side-by-side comparison of standard and custom basis coordinate systems showing how the same vector has different coordinate representations.</p> </li> </ul>"},{"location":"sims/#determinants","title":"Determinants","text":"<ul> <li> <p>2\u00d72 Determinant Calculator</p> <p> Interactive calculator for computing 2\u00d72 determinants with step-by-step visualization and geometric interpretation.</p> </li> <li> <p>Signed Area</p> <p> Visualize the signed area of a parallelogram formed by two vectors, showing how orientation affects the sign of the determinant.</p> </li> <li> <p>Sarrus Rule</p> <p> Step-by-step animation showing how to compute 3\u00d73 determinants using the Rule of Sarrus.</p> </li> <li> <p>Cofactor Expansion</p> <p> Step-by-step animation showing cofactor expansion for computing determinants of any size matrix.</p> </li> <li> <p>Determinant Properties</p> <p> Interactive exploration of how row operations affect determinant values.</p> </li> <li> <p>Singular Matrix</p> <p> Visualize the geometric difference between singular and non-singular matrices through transformation animation.</p> </li> <li> <p>Cramer's Rule</p> <p> Step-by-step visualization of solving systems of equations using Cramer's Rule with determinants.</p> </li> <li> <p>Volume Scaling 3D</p> <p> Visualize how 3\u00d73 matrix transformations scale 3D volumes, connecting the determinant to geometric volume change.</p> </li> </ul>"},{"location":"sims/#eigenvalues-and-eigenvectors","title":"Eigenvalues and Eigenvectors","text":"<ul> <li> <p>Eigenvector Transformation</p> <p> Interactive visualization demonstrating how eigenvectors maintain their direction under linear transformation while other vectors change direction.</p> </li> <li> <p>Characteristic Polynomial</p> <p> Interactive computation of characteristic polynomials and eigenvalues for 2\u00d72 and 3\u00d73 matrices with graphical visualization.</p> </li> <li> <p>Eigenspace Visualization</p> <p> 3D visualization of eigenspaces showing how geometric multiplicity determines whether eigenspaces are lines or planes through the origin.</p> </li> <li> <p>Multiplicity Comparison</p> <p> Compare algebraic and geometric multiplicity across different matrix types to understand diagonalizability conditions.</p> </li> <li> <p>Diagonalization Workflow</p> <p> Interactive flowchart guiding through the step-by-step process of diagonalizing a matrix with decision points.</p> </li> <li> <p>Matrix Power Calculator</p> <p> Demonstrates how diagonalization simplifies computing matrix powers using the eigenvalue decomposition A^k = PD^kP\u207b\u00b9.</p> </li> <li> <p>Complex Eigenvalue</p> <p> Visualize how complex eigenvalues \u03bb = a + bi correspond to rotation-scaling transformations in 2D.</p> </li> <li> <p>Spectral Theorem</p> <p> Interactive demonstration of the spectral theorem showing how symmetric matrices decompose into orthogonal eigenvectors and real eigenvalues.</p> </li> <li> <p>Power Iteration</p> <p> Visualization of the power iteration method for finding the dominant eigenvalue and eigenvector through repeated matrix-vector multiplication.</p> </li> <li> <p>Eigenvalue Applications</p> <p> Interactive hub-and-spoke infographic showing how eigenanalysis concepts connect to real-world applications in machine learning, AI, and science.</p> </li> </ul>"},{"location":"sims/#orthogonality-and-least-squares","title":"Orthogonality and Least Squares","text":"<ul> <li> <p>Gram-Schmidt Process</p> <p> Step-by-step 3D visualization of Gram-Schmidt orthonormalization showing projections and construction of orthonormal vectors.</p> </li> <li> <p>Gram-Schmidt (Detailed)</p> <p> Detailed step-by-step 3D visualization of Gram-Schmidt orthonormalization showing projection computation, subtraction, and normalization phases.</p> </li> <li> <p>Orthonormal Basis Finder</p> <p> Interactive visualization demonstrating how orthonormal bases simplify coordinate finding through inner products.</p> </li> <li> <p>Projection onto Subspace</p> <p> 3D visualization of vector projection onto subspaces showing the projection as the closest point and the orthogonal error vector.</p> </li> <li> <p>Least Squares Visualizer</p> <p> Interactive visualization of least squares as projection showing the geometric relationship between b, Ax-hat, and the error vector with dual regression and geometric views.</p> </li> <li> <p>Four Subspaces</p> <p> Visualize the four fundamental subspaces of a matrix and their orthogonal relationships, demonstrating the Fundamental Theorem of Linear Algebra.</p> </li> <li> <p>Pseudoinverse Solver</p> <p> Interactive exploration of the Moore-Penrose pseudoinverse for solving least squares problems, including overdetermined, underdetermined, and rank-deficient systems with SVD visualization.</p> </li> </ul>"},{"location":"sims/#matrix-decompositions","title":"Matrix Decompositions","text":"<ul> <li> <p>LU Decomposition</p> <p> Step-by-step visualization of LU decomposition showing how Gaussian elimination produces L and U matrices.</p> </li> <li> <p>Matrix Rank Visualizer</p> <p> Interactive 3D visualization showing how matrix rank relates to the column space, with column vectors displayed geometrically and row echelon form computation.</p> </li> <li> <p>Positive Definiteness</p> <p> Interactive 3D visualization of quadratic forms showing how eigenvalue signs determine positive definiteness.</p> </li> <li> <p>SVD Geometry</p> <p> Visualize SVD as a sequence of rotation-scaling-rotation transformations on the unit circle.</p> </li> <li> <p>SVD Forms Comparison</p> <p> Visual comparison of Full, Compact, and Truncated SVD showing matrix dimensions and storage requirements.</p> </li> <li> <p>SVD Image Compression</p> <p> Interactive demonstration of image compression using truncated SVD.</p> </li> <li> <p>Condition Number</p> <p> Visualize how condition number affects the sensitivity of linear system solutions to perturbations.</p> </li> <li> <p>Decomposition Guide</p> <p> Interactive decision tree for choosing the right matrix decomposition based on your problem.</p> </li> </ul>"},{"location":"sims/#data-science-applications","title":"Data Science Applications","text":"<ul> <li> <p>Data Matrix Structure</p> <p> Interactive visualization showing the structure of data matrices with rows as samples and columns as features, including heat map coloring.</p> </li> <li> <p>Covariance and Correlation</p> <p> Interactive exploration of how covariance and correlation capture relationships between features through scatter plots and heatmaps.</p> </li> <li> <p>PCA Explorer</p> <p> Interactive visualization demonstrating Principal Component Analysis step by step, from raw data through centering, eigenvector computation, and projection.</p> </li> <li> <p>Scree Plot</p> <p> Interactive visualization for learning to use scree plots and cumulative variance to select the optimal number of principal components.</p> </li> <li> <p>Linear Regression</p> <p> Interactive visualization of linear regression showing how the best-fit line minimizes squared errors with draggable data points and loss surface heatmap.</p> </li> <li> <p>Regularization Geometry</p> <p> Interactive visualization showing how L1 and L2 regularization constrain model weights geometrically, demonstrating why L1 produces sparse solutions.</p> </li> </ul>"},{"location":"sims/#neural-networks","title":"Neural Networks","text":"<ul> <li> <p>Neural Network Layer</p> <p> Interactive visualization of a neural network layer showing how matrix-vector multiplication implements the forward pass with various activation functions.</p> </li> <li> <p>Neural Network Architecture</p> <p> Interactive visualization of neural network architecture showing layers, neurons, weight matrix dimensions, and parameter counts.</p> </li> <li> <p>Activation Functions</p> <p> Interactive comparison of neural network activation functions including ReLU, Sigmoid, Tanh, Leaky ReLU, and Softplus with derivative visualization.</p> </li> <li> <p>Perceptron Decision Boundary</p> <p> Interactive visualization showing how perceptron weights and bias define a linear decision boundary for binary classification.</p> </li> <li> <p>Forward Propagation</p> <p> Step-by-step visualization of forward propagation through a neural network showing matrix operations at each layer.</p> </li> <li> <p>Backpropagation</p> <p> Step-by-step visualization of backpropagation showing how gradients flow backward through a neural network via the chain rule.</p> </li> <li> <p>Normalization Comparison</p> <p> Visual comparison of batch normalization and layer normalization showing which tensor dimensions each technique normalizes.</p> </li> <li> <p>Tensor Operations</p> <p> Interactive visualization of common tensor operations including reshape, transpose, flatten, squeeze, and unsqueeze.</p> </li> </ul>"},{"location":"sims/#optimization","title":"Optimization","text":"<ul> <li> <p>Gradient Descent</p> <p> Visualize how gradient descent optimization navigates loss surfaces and how learning rate affects convergence behavior.</p> </li> <li> <p>Learning Rate Effect</p> <p> Interactive side-by-side comparison showing how different learning rates affect gradient descent optimization, demonstrating convergence, oscillation, and divergence.</p> </li> <li> <p>ML Pipeline</p> <p> Interactive flowchart showing the complete ML pipeline from raw data to trained model with code examples and detailed explanations.</p> </li> <li> <p>Convex Function Visualizer</p> <p> Interactive visualization of convex functions and their properties for optimization.</p> </li> <li> <p>Newton vs Gradient Descent</p> <p> Side-by-side comparison of Newton's method and gradient descent showing convergence characteristics.</p> </li> <li> <p>Hessian Curvature Visualizer</p> <p> Interactive visualization of how the Hessian matrix captures surface curvature for optimization.</p> </li> <li> <p>SGD Trajectory Visualizer</p> <p> Visualize stochastic gradient descent trajectories and noise characteristics.</p> </li> <li> <p>Momentum Dynamics Visualizer</p> <p> Interactive visualization of momentum in optimization showing how it helps overcome local minima.</p> </li> <li> <p>Optimizer Comparison Arena</p> <p> Compare different optimization algorithms side-by-side on the same loss surface.</p> </li> <li> <p>Lagrange Multiplier Geometry</p> <p> Interactive visualization of Lagrange multipliers for constrained optimization.</p> </li> <li> <p>KKT Conditions Visualizer</p> <p> Visualize the Karush-Kuhn-Tucker conditions for constrained optimization problems.</p> </li> </ul>"},{"location":"sims/#transformers-and-nlp","title":"Transformers and NLP","text":"<ul> <li> <p>Attention Mechanism</p> <p> Interactive visualization of the attention mechanism used in transformer models.</p> </li> <li> <p>Multi-Head Attention</p> <p> Visualize how multi-head attention allows the model to attend to different positions simultaneously.</p> </li> <li> <p>Transformer Block</p> <p> Interactive visualization of a complete transformer block showing self-attention and feed-forward layers.</p> </li> <li> <p>Embedding Space Visualizer</p> <p> Explore word and document embeddings in high-dimensional space projected to 2D/3D.</p> </li> <li> <p>Cosine and Euclidean Similarity</p> <p> Compare cosine similarity and Euclidean distance for measuring vector relationships.</p> </li> <li> <p>LoRA Visualizer</p> <p> Visualize Low-Rank Adaptation (LoRA) for efficient fine-tuning of large language models.</p> </li> <li> <p>Latent Space Interpolation</p> <p> Explore smooth transitions in latent space by interpolating between embeddings.</p> </li> </ul>"},{"location":"sims/#computer-vision","title":"Computer Vision","text":"<ul> <li> <p>Image Matrix Visualizer</p> <p> Interactive visualization showing how pixel values in a matrix correspond to grayscale image appearance with hover highlighting and edit mode.</p> </li> <li> <p>RGB Channel Decomposition</p> <p> Interactive visualization showing how RGB color channels combine to form color images with channel isolation and intensity controls.</p> </li> <li> <p>Convolution Operation</p> <p> Interactive visualization of the convolution operation showing how kernels slide across images with stride and padding controls.</p> </li> <li> <p>Convolution Visualizer</p> <p> Step-by-step visualization of image convolution showing how kernels slide across images to compute filtered outputs with multiple kernel types.</p> </li> <li> <p>Filter Effects Gallery</p> <p> Side-by-side comparison of image filters including blur, sharpen, edge detection, and emboss effects with kernel visualization.</p> </li> <li> <p>Edge Detection Visualizer</p> <p> Interactive visualization of Sobel, Prewitt, and Scharr edge detection operators showing gradient components, magnitude, and thresholded edges.</p> </li> <li> <p>Corner Detection Visualizer</p> <p> Interactive Harris corner detection visualization showing structure tensor eigenvalue analysis and response heatmaps.</p> </li> <li> <p>Fourier Transform Visualizer</p> <p> Interactive 2D Discrete Fourier Transform visualization showing spatial-frequency relationship, magnitude/phase spectra, and frequency filtering.</p> </li> <li> <p>Homography Demo</p> <p> Interactive demonstration of perspective transformations using homography matrices with draggable corner points and real-time matrix computation.</p> </li> <li> <p>SVD Compression Visualizer</p> <p> Interactive demonstration of image compression using truncated Singular Value Decomposition with quality metrics.</p> </li> </ul>"},{"location":"sims/#3d-vision-and-geometry","title":"3D Vision and Geometry","text":"<ul> <li> <p>Camera Model Visualizer</p> <p> Interactive demonstration of the pinhole camera model showing how intrinsic parameters affect 3D-to-2D projection.</p> </li> <li> <p>Camera Calibration</p> <p> Interactive demonstration of camera calibration showing lens distortion effects, checkerboard corner detection, and distortion correction.</p> </li> <li> <p>Epipolar Geometry</p> <p> Interactive demonstration of epipolar constraints in stereo vision showing epipolar lines, planes, and depth from disparity.</p> </li> <li> <p>Triangulation Visualizer</p> <p> Interactive demonstration of 3D point recovery from stereo correspondences showing triangulation accuracy and noise effects.</p> </li> <li> <p>Point Cloud Visualizer</p> <p> Interactive exploration of point cloud data with different datasets, color modes, downsampling, and surface normal visualization.</p> </li> </ul>"},{"location":"sims/#autonomous-systems","title":"Autonomous Systems","text":"<ul> <li> <p>Coordinate System 3D</p> <p> Interactive demonstration of different 3D coordinate system conventions and handedness including OpenGL, DirectX, ROS, and camera frames.</p> </li> <li> <p>Euler Angles Visualizer</p> <p> Interactive demonstration of how Euler angles (yaw, pitch, roll) compose to form 3D rotations with multiple conventions and gimbal lock warning.</p> </li> <li> <p>Gimbal Lock Demo</p> <p> Interactive demonstration of gimbal lock using a physical gimbal mechanism with three nested rings showing loss of degree of freedom.</p> </li> <li> <p>Quaternion Rotation</p> <p> Interactive demonstration of quaternion rotation representation with axis-angle conversion, rotation application, and composition.</p> </li> <li> <p>Rigid Body Transform</p> <p> Interactive visualization of rigid body transform composition in a robot arm kinematic chain showing forward kinematics.</p> </li> <li> <p>Kalman Filter</p> <p> Interactive visualization of the Kalman filter showing the predict-update cycle, uncertainty propagation, and noise effects on state estimation.</p> </li> <li> <p>Sensor Fusion</p> <p> Interactive demonstration of GPS and IMU sensor fusion using Kalman filtering, showing how combining complementary sensors improves accuracy.</p> </li> <li> <p>LIDAR Point Cloud</p> <p> Interactive 3D visualization of LIDAR point cloud data demonstrating ground segmentation, object clustering, and coloring modes.</p> </li> <li> <p>SLAM Visualizer</p> <p> Interactive visualization of Simultaneous Localization and Mapping showing robot trajectory, landmark mapping, and loop closure optimization.</p> </li> <li> <p>Object Tracking</p> <p> Interactive multi-object tracking demonstration showing the predict-associate-update cycle with bounding boxes, track IDs, and data association.</p> </li> <li> <p>Path Planning</p> <p> Interactive comparison of path planning algorithms (A*, Dijkstra, RRT) showing exploration patterns, path quality, and performance metrics.</p> </li> <li> <p>Trajectory Optimization</p> <p> Interactive trajectory optimization demonstration showing smoothness constraints, velocity limits, obstacle avoidance, and cost convergence.</p> </li> </ul>"},{"location":"sims/#learning-tools","title":"Learning Tools","text":"<ul> <li> <p>Learning Graph Viewer</p> <p> Interactive visualization of the course concept dependency graph with 300 concepts and their relationships.</p> </li> </ul>"},{"location":"sims/2d-rotation/","title":"2D Rotation Matrix Visualizer","text":"<p>Run the 2D Rotation Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/2d-rotation/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates 2D rotation matrices and their properties:</p> <ul> <li>Rotation matrix structure: R(\u03b8) = [[cos \u03b8, -sin \u03b8], [sin \u03b8, cos \u03b8]]</li> <li>Orthogonality: The transpose equals the inverse (R\u207b\u00b9 = R\u1d40)</li> <li>Determinant = 1: Rotations preserve area and orientation</li> <li>Basis vector transformation: Watch how e\u2081 and e\u2082 rotate together</li> </ul>"},{"location":"sims/2d-rotation/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the angle slider to rotate shapes from -360\u00b0 to 360\u00b0</li> <li>Select different shapes to see how rotation affects various geometries</li> <li>Click Animate to watch continuous rotation</li> <li>Toggle checkboxes to show/hide the unit circle and angle arc</li> <li>Observe the matrix panel to see how cos(\u03b8) and sin(\u03b8) values change</li> </ol>"},{"location":"sims/2d-rotation/#key-observations","title":"Key Observations","text":"<ul> <li>At 0\u00b0: Matrix is identity [[1,0],[0,1]]</li> <li>At 90\u00b0: Matrix is [[0,-1],[1,0]]</li> <li>At 180\u00b0: Matrix is [[-1,0],[0,-1]]</li> <li>At 270\u00b0: Matrix is [[0,1],[-1,0]]</li> </ul>"},{"location":"sims/2d-rotation/#embedding","title":"Embedding","text":"<p>You can include this MicroSim on your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/2d-rotation/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/2d-rotation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/2d-rotation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Calculate cos(\u03b8) and sin(\u03b8) values for common angles</li> <li>Write the 2D rotation matrix for any given angle</li> <li>Verify that rotation matrices preserve lengths and angles</li> </ol>"},{"location":"sims/2d-rotation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify orthogonality: Compute R(\u03b8)\u00b7R(\u03b8)\u1d40 and verify it equals I</li> <li>Composition: Set angle to 30\u00b0, note the matrix, then 60\u00b0. What's R(30\u00b0)\u00b7R(30\u00b0)?</li> <li>F-Shape orientation: Use F-shape to verify counterclockwise rotation direction</li> </ol>"},{"location":"sims/2d-rotation/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations - 2D Rotation section</li> <li>Rotation matrix on Wikipedia</li> </ul>"},{"location":"sims/activation-functions/","title":"Activation Functions","text":"<p>Run the Activation Functions Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/activation-functions/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization compares common neural network activation functions, showing both their shape and derivative behavior\u2014crucial for understanding gradient flow during backpropagation.</p>"},{"location":"sims/activation-functions/#activation-functions-included","title":"Activation Functions Included","text":"Function Formula Range Key Property ReLU max(0, x) [0, \u221e) Efficient, sparse Sigmoid 1/(1+e\u207b\u02e3) (0, 1) Probability output Tanh (e\u02e3-e\u207b\u02e3)/(e\u02e3+e\u207b\u02e3) (-1, 1) Zero-centered Leaky ReLU max(0.1x, x) (-\u221e, \u221e) No dead neurons Softplus log(1+e\u02e3) (0, \u221e) Smooth ReLU"},{"location":"sims/activation-functions/#interactive-features","title":"Interactive Features","text":"<ul> <li>Function Selector: Choose which activation to examine</li> <li>Show Derivative: Toggle to display f'(x) as dashed line</li> <li>Compare All: Overlay all functions for comparison</li> <li>Input Slider: Trace along the curve to see exact values</li> <li>Info Panel: Shows f(x), f'(x), range, and gradient status</li> </ul>"},{"location":"sims/activation-functions/#visual-indicators","title":"Visual Indicators","text":"<ul> <li>Yellow regions: Low gradient areas (|f'(x)| &lt; 0.1) where vanishing gradients occur</li> <li>Solid line: The activation function f(x)</li> <li>Dashed line: The derivative f'(x)</li> </ul>"},{"location":"sims/activation-functions/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/activation-functions/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Describe the shape and range of common activation functions</li> <li>Explain why nonlinear activations are necessary</li> <li>Identify regions where gradients vanish</li> <li>Choose appropriate activations for different use cases</li> </ol>"},{"location":"sims/activation-functions/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Gradient Exploration: Move the slider to x = -3 for sigmoid. What happens to f'(x)?</li> <li>Compare ReLU Family: Look at ReLU, Leaky ReLU, and Softplus side by side</li> <li>Saturation Investigation: Find where sigmoid and tanh have near-zero gradients</li> <li>Zero-Centered Discussion: Compare sigmoid (not zero-centered) with tanh (zero-centered)</li> </ol>"},{"location":"sims/activation-functions/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does ReLU dominate modern deep learning despite having a discontinuous derivative?</li> <li>What does \"vanishing gradient\" mean and why is it a problem?</li> <li>When would you choose sigmoid over tanh for an output layer?</li> <li>Why might Leaky ReLU be preferred over standard ReLU?</li> </ol>"},{"location":"sims/activation-functions/#references","title":"References","text":"<ul> <li>Nair &amp; Hinton (2010). Rectified Linear Units Improve Restricted Boltzmann Machines</li> <li>Glorot et al. (2011). Deep Sparse Rectifier Neural Networks</li> </ul>"},{"location":"sims/attention-mechanism/","title":"Attention Mechanism Step-by-Step","text":"<p>Run the Attention Mechanism Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/attention-mechanism/#about-this-microsim","title":"About This MicroSim","text":"<p>This step-by-step visualization demonstrates how the attention mechanism works in transformers. Walk through each stage of the computation:</p> <ol> <li>Input: Token embeddings as vectors</li> <li>Project Q,K,V: Linear projections create Query, Key, Value matrices</li> <li>Compute Scores: Query-Key dot products measure compatibility</li> <li>Softmax: Normalize scores to attention weights (probabilities)</li> <li>Weighted Sum: Combine Value vectors using attention weights</li> </ol>"},{"location":"sims/attention-mechanism/#how-to-use","title":"How to Use","text":"<ol> <li>Step Slider: Move through the 5 stages of attention computation</li> <li>Query Position: Select which token's attention to visualize</li> <li>Observe: Watch how attention weights determine which positions to focus on</li> </ol>"},{"location":"sims/attention-mechanism/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/attention-mechanism/#the-attention-formula","title":"The Attention Formula","text":"\\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\]"},{"location":"sims/attention-mechanism/#query-key-value-intuition","title":"Query-Key-Value Intuition","text":"<ul> <li>Query: \"What am I looking for?\"</li> <li>Key: \"What do I contain?\"</li> <li>Value: \"What's my actual content?\"</li> </ul> <p>High query-key compatibility means that value contributes more to the output.</p>"},{"location":"sims/attention-mechanism/#softmax-normalization","title":"Softmax Normalization","text":"<p>Attention weights in each row sum to 1, creating a probability distribution over positions:</p> \\[A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_k \\exp(S_{ik})}\\]"},{"location":"sims/attention-mechanism/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand the role of Query, Key, and Value matrices</li> <li>Trace the flow of information through attention computation</li> <li>Interpret attention weights as a soft addressing mechanism</li> </ul> <p>Activities:</p> <ol> <li>Step through all 5 stages and describe what happens at each</li> <li>Change the query position and observe how attention patterns change</li> <li>Identify which tokens attend most strongly to each other</li> </ol> <p>Assessment:</p> <ul> <li>Why do we scale by \u221ad_k in the score computation?</li> <li>What does a uniform attention distribution (all weights equal) mean?</li> <li>How would masking affect the attention computation?</li> </ul>"},{"location":"sims/attention-mechanism/#references","title":"References","text":"<ul> <li>Attention Is All You Need - Original transformer paper</li> <li>Chapter 11: Generative AI and LLMs</li> <li>The Illustrated Transformer</li> </ul>"},{"location":"sims/backpropagation/","title":"Backpropagation","text":"<p>Run the Backpropagation Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/backpropagation/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how gradients propagate backward through a neural network using the chain rule. Understanding backpropagation is essential for grasping how neural networks learn.</p>"},{"location":"sims/backpropagation/#the-backpropagation-algorithm","title":"The Backpropagation Algorithm","text":"<p>Starting from the output layer and working backward:</p> <ol> <li>Output Error: \\(\\delta^{[L]} = (\\hat{y} - y) \\cdot \\sigma'(z^{[L]})\\)</li> <li>Hidden Layer Error: \\(\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot \\sigma'(z^{[l]})\\)</li> <li>Weight Gradients: \\(\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} = \\delta^{[l]} (\\mathbf{a}^{[l-1]})^T\\)</li> </ol>"},{"location":"sims/backpropagation/#key-insight-the-transpose","title":"Key Insight: The Transpose","text":"<p>Notice how \\(W^{[l+1]}\\) appears transposed when propagating gradients backward. This \"distributes\" each output error back to the neurons that contributed to it.</p>"},{"location":"sims/backpropagation/#interactive-features","title":"Interactive Features","text":"<ul> <li>Forward Pass: First compute all activations (required before backprop)</li> <li>Backward Step: Step through gradient computation layer by layer</li> <li>Target Slider: Change the target value and see how gradients change</li> <li>Auto Mode: Watch the full backward pass animate</li> </ul>"},{"location":"sims/backpropagation/#visual-indicators","title":"Visual Indicators","text":"<ul> <li>\u03b4 values: Error signals shown above each neuron</li> <li>Red/Blue colors: Positive/negative gradients</li> <li>Arrows: Direction of gradient flow</li> <li>\u2202 values: Weight gradients on connections</li> </ul>"},{"location":"sims/backpropagation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/backpropagation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Explain how the chain rule enables gradient computation through composed functions</li> <li>Describe why the weight matrix transpose appears in backpropagation</li> <li>Compute error signals (\u03b4) at each layer</li> <li>Calculate weight gradients from error signals and activations</li> </ol>"},{"location":"sims/backpropagation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Manual Backprop: Verify the \u03b4 and gradient values by hand calculation</li> <li>Target Exploration: Change the target from 0 to 1 and observe how gradients flip sign</li> <li>Trace the Chain: For one weight, write out the full chain rule expression</li> <li>Dimension Verification: Confirm that \\(\\delta^{[l]} (a^{[l-1]})^T\\) has the same shape as \\(W^{[l]}\\)</li> </ol>"},{"location":"sims/backpropagation/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the transpose of \\(W^{[l+1]}\\) appear when computing \\(\\delta^{[l]}\\)?</li> <li>What happens to gradients when neurons have zero activation (dead ReLU)?</li> <li>How does the magnitude of the output error affect all gradients in the network?</li> <li>Why is it important that gradient dimensions match weight dimensions?</li> </ol>"},{"location":"sims/backpropagation/#mathematical-details","title":"Mathematical Details","text":"<p>For MSE loss: \\(\\mathcal{L} = \\frac{1}{2}(\\hat{y} - y)^2\\)</p> <p>The derivative: \\(\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = \\hat{y} - y\\)</p>"},{"location":"sims/backpropagation/#references","title":"References","text":"<ul> <li>Rumelhart, Hinton &amp; Williams (1986). Learning Representations by Back-propagating Errors</li> <li>Goodfellow et al. (2016). Deep Learning, Chapter 6.5</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/","title":"Basis and Coordinate System Visualizer","text":"<p>Run the Basis and Coordinate System Visualizer Fullscreen</p>"},{"location":"sims/basis-coordinate-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates that the same geometric vector can have different coordinate representations depending on which basis you use. The left panel shows the standard basis (e\u2081, e\u2082) while the right panel shows a custom basis (b\u2081, b\u2082) that you can modify.</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p>"},{"location":"sims/basis-coordinate-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Vector: In either panel, drag the green vector endpoint to move it</li> <li>Drag Basis Vectors: In the right panel, drag the endpoints of b\u2081 (red) and b\u2082 (blue) to change the custom basis</li> <li>Use Presets: Click preset buttons to see common basis configurations:</li> <li>Standard: b\u2081 = (1, 0), b\u2082 = (0, 1) - matches the standard basis</li> <li>Rotated 45\u00b0: Basis rotated by 45 degrees</li> <li>Skewed: Non-orthogonal basis vectors</li> <li>Stretched: Basis vectors with different lengths</li> <li>Toggle Options:</li> <li>Show Grid: Toggle grid lines in both panels</li> <li>Show Projections: Toggle dashed projection lines</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/basis-coordinate-visualizer/#coordinate-representation","title":"Coordinate Representation","text":"<p>The same geometric vector v has different coordinates in different bases: - In standard basis: v = (x, y) means v = x\u00b7e\u2081 + y\u00b7e\u2082 - In custom basis: [v]_B = (c\u2081, c\u2082) means v = c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082</p>"},{"location":"sims/basis-coordinate-visualizer/#the-equation","title":"The Equation","text":"<p>The coordinates [v]_B satisfy: \\(\\(\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2\\)\\)</p>"},{"location":"sims/basis-coordinate-visualizer/#change-of-basis","title":"Change of Basis","text":"<p>When you change the basis vectors, the grid lines in the right panel change to follow the new basis directions. The coordinates change, but the geometric vector stays the same!</p>"},{"location":"sims/basis-coordinate-visualizer/#important-observations","title":"Important Observations","text":"<ol> <li>Same Vector, Different Numbers: Moving to a stretched basis makes coordinates smaller</li> <li>Grid Deformation: The grid follows the basis vectors</li> <li>Parallel Basis Warning: If b\u2081 and b\u2082 become parallel, coordinates become undefined</li> <li>Verification: The formula c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082 = v is shown at the bottom</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/basis-coordinate-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/basis-coordinate-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/basis-coordinate-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/basis-coordinate-visualizer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/basis-coordinate-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics</li> <li>Linear combinations</li> <li>Understanding of basis vectors</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Start with \"Standard\" preset and observe both panels show same coordinates</li> <li> <p>Move the vector and verify coordinates match</p> </li> <li> <p>Rotated Basis Investigation (5 min):</p> </li> <li>Click \"Rotated 45\u00b0\"</li> <li>Notice how coordinates change but vector stays the same</li> <li> <p>Find a vector where custom coordinates are simpler than standard</p> </li> <li> <p>Skewed Basis Exploration (5 min):</p> </li> <li>Click \"Skewed\"</li> <li>Observe how grid lines are no longer perpendicular</li> <li> <p>Verify the linear combination formula still works</p> </li> <li> <p>Custom Basis Creation (10 min):</p> </li> <li>Drag b\u2081 and b\u2082 to create your own basis</li> <li>Find a basis where a specific vector has integer coordinates</li> <li>Try to make b\u2081 and b\u2082 parallel and observe the \"undefined\" warning</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the same vector have different coordinates in different bases?</li> <li>What happens geometrically when you stretch a basis vector?</li> <li>Why do parallel basis vectors make coordinates undefined?</li> <li>How would you convert coordinates from one basis to another?</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a vector and a basis, calculate the coordinates by hand and verify</li> <li>Explain why the grid deforms when the basis changes</li> <li>Find a basis that makes a given vector have coordinates (1, 1)</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Change of basis - Excellent visual explanation</li> <li>Khan Academy - Change of basis</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.5.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 4.4.</li> </ol>"},{"location":"sims/block-matrix/","title":"Block Matrix Partitioning","text":"<p>Run the Block Matrix MicroSim Fullscreen</p> <p>Edit the Block Matrix MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/block-matrix/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/block-matrix/#description","title":"Description","text":"<p>A block matrix (or partitioned matrix) views a matrix as an array of smaller matrices called blocks or submatrices. This MicroSim lets you interactively partition an 8\u00d78 matrix and see how the blocks are formed.</p> <p>Key Features:</p> <ul> <li>Draggable Partitions: Drag the red (horizontal) and blue (vertical) handles to resize blocks</li> <li>Color-Coded Blocks: Each of the four blocks (A, B, C, D) has a distinct color</li> <li>Preset Patterns: Choose from 2\u00d72, row, column, or asymmetric partitions</li> <li>Dimension Display: See the dimensions of each block update in real-time</li> </ul>"},{"location":"sims/block-matrix/#block-matrix-notation","title":"Block Matrix Notation","text":"<p>A matrix M partitioned into four blocks is written:</p> \\[M = \\begin{bmatrix} A &amp; B \\\\ C &amp; D \\end{bmatrix}\\] <p>where A, B, C, D are submatrices with compatible dimensions:</p> <ul> <li>A is (top rows) \u00d7 (left columns)</li> <li>B is (top rows) \u00d7 (right columns)</li> <li>C is (bottom rows) \u00d7 (left columns)</li> <li>D is (bottom rows) \u00d7 (right columns)</li> </ul>"},{"location":"sims/block-matrix/#why-block-matrices","title":"Why Block Matrices?","text":""},{"location":"sims/block-matrix/#parallel-computation","title":"Parallel Computation","text":"<p>Independent blocks can be processed on different cores or machines.</p>"},{"location":"sims/block-matrix/#structured-algorithms","title":"Structured Algorithms","text":"<p>Many algorithms exploit block structure: - Block LU decomposition - Strassen's matrix multiplication - Hierarchical matrices (H-matrices)</p>"},{"location":"sims/block-matrix/#conceptual-clarity","title":"Conceptual Clarity","text":"<p>Complex systems naturally decompose into interacting subsystems that correspond to blocks.</p>"},{"location":"sims/block-matrix/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/block-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Partition a matrix into blocks of specified dimensions</li> <li>Write block matrix notation for a given partition</li> <li>Identify valid block decompositions for matrix operations</li> <li>Explain how block structure enables efficient computation</li> </ol>"},{"location":"sims/block-matrix/#exploration-activity-5-minutes","title":"Exploration Activity (5 minutes)","text":"<ol> <li>Default Partition: Observe the symmetric 4\u00d74 / 4\u00d74 split</li> <li>Drag Partitions: Move handles to create different block sizes</li> <li>Try Presets: Compare row partition vs column partition</li> <li>Check Dimensions: Verify that block dimensions sum to 8</li> </ol>"},{"location":"sims/block-matrix/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>What happens if you partition a matrix for block multiplication but the inner dimensions don't match?</li> <li>When would you choose row partition vs 2\u00d72 blocks?</li> <li>How does block structure relate to parallel computing?</li> </ul>"},{"location":"sims/block-matrix/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Block matrices in context</li> <li>Matrix Computations - Golub and Van Loan (Chapter 1)</li> </ul>"},{"location":"sims/camera-calibration/","title":"Camera Calibration Visualizer","text":"<p>Run the Camera Calibration Visualizer Fullscreen</p> <p>Edit the Camera Calibration Visualizer with the p5.js editor</p>"},{"location":"sims/camera-calibration/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates camera calibration - the process of determining camera intrinsic parameters (focal length, principal point, distortion coefficients) needed for accurate computer vision and autonomous systems.</p>"},{"location":"sims/camera-calibration/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/camera-calibration/main.html\"\n        height=\"602px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/camera-calibration/#features","title":"Features","text":"<ul> <li>Distortion Simulation: Adjust k1 and k2 to see barrel (k1 &lt; 0) and pincushion (k1 &gt; 0) distortion</li> <li>Side-by-Side Comparison: Distorted image on left, ideal/corrected image on right</li> <li>Corner Detection: See detected checkerboard corners as green markers</li> <li>Reprojection Error: After calibration, red vectors show residual errors</li> <li>Interactive Calibration: Click \"Calibrate\" to compute and store distortion parameters</li> </ul>"},{"location":"sims/camera-calibration/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/camera-calibration/#camera-matrix-intrinsics","title":"Camera Matrix (Intrinsics)","text":"<p>The intrinsic calibration matrix K relates 3D points to 2D image pixels:</p> \\[\\mathbf{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>Where: - \\(f_x, f_y\\) are focal lengths in pixels - \\(c_x, c_y\\) is the principal point (image center)</p>"},{"location":"sims/camera-calibration/#radial-distortion","title":"Radial Distortion","text":"<p>Real camera lenses introduce radial distortion, modeled as:</p> \\[x_d = x_n(1 + k_1 r^2 + k_2 r^4)$$ $$y_d = y_n(1 + k_1 r^2 + k_2 r^4)\\] <p>Where: - k1 &lt; 0: Barrel distortion (lines curve outward) - k1 &gt; 0: Pincushion distortion (lines curve inward) - k2: Higher-order distortion correction</p>"},{"location":"sims/camera-calibration/#checkerboard-calibration","title":"Checkerboard Calibration","text":"<p>The standard calibration process: 1. Capture images of a checkerboard pattern at various poses 2. Detect corner points in each image 3. Solve for camera parameters that minimize reprojection error 4. Use calibrated parameters to undistort new images</p>"},{"location":"sims/camera-calibration/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/camera-calibration/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the camera intrinsic matrix and its parameters</li> <li>Recognize barrel and pincushion distortion effects</li> <li>Apply the calibration process to correct lens distortion</li> </ul>"},{"location":"sims/camera-calibration/#activities","title":"Activities","text":"<ol> <li>Explore Distortion: Move k1 slider to see barrel vs pincushion distortion</li> <li>Calibrate: Apply some distortion, click Calibrate, then observe the corrected view</li> <li>Reprojection Error: Toggle \"Show Errors\" to see calibration accuracy</li> <li>Focal Length Effect: Adjust focal length to see how it affects distortion appearance</li> </ol>"},{"location":"sims/camera-calibration/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What type of distortion does a negative k1 value produce?</li> <li>Why is a checkerboard pattern commonly used for calibration?</li> <li>How does reprojection error measure calibration quality?</li> </ol>"},{"location":"sims/camera-calibration/#references","title":"References","text":"<ul> <li>OpenCV Camera Calibration Tutorial</li> <li>Zhang's Camera Calibration Method</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/camera-model-visualizer/","title":"Camera Model Visualizer","text":"<p>Run the Camera Model Visualizer Fullscreen</p> <p>Edit the Camera Model Visualizer with the p5.js editor</p>"},{"location":"sims/camera-model-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates the pinhole camera model and how intrinsic parameters affect 3D-to-2D projection. See the relationship between focal length, principal point, and the projected image.</p>"},{"location":"sims/camera-model-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Focal Length: Change the \"zoom\" - higher = narrower FOV</li> <li>Move Principal Point: Shift the image center (Cx, Cy)</li> <li>Camera Distance: Move the camera closer or farther</li> <li>Show Projection Rays: Visualize rays from 3D points to camera</li> <li>Show Frustum: See the camera's viewing volume</li> <li>Drag to Rotate: Change the 3D view angle</li> </ol>"},{"location":"sims/camera-model-visualizer/#key-concepts","title":"Key Concepts","text":"<p>Camera Intrinsic Matrix K:</p> \\[\\mathbf{K} = \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <p>Perspective projection formula: \\(\\(u = f_x \\cdot \\frac{X}{Z} + c_x, \\quad v = f_y \\cdot \\frac{Y}{Z} + c_y\\)\\)</p> Parameter Meaning Typical Range f_x, f_y Focal length (pixels) 200-2000 c_x Principal point x image_width/2 c_y Principal point y image_height/2"},{"location":"sims/camera-model-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand intrinsic and extrinsic camera parameters - Apply the projection matrix to transform 3D points - Relate focal length to field of view - Visualize how camera position affects the image</p>"},{"location":"sims/camera-model-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/camera-model-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>The pinhole camera model is fundamental to computer vision. All 3D rays pass through a single point (the optical center).</p>"},{"location":"sims/camera-model-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with default focal length - note the projected points</li> <li>Increase focal length - objects appear larger (zoom in)</li> <li>Move the principal point - image shifts</li> <li>Toggle projection rays to see the geometry</li> </ol>"},{"location":"sims/camera-model-visualizer/#key-insight","title":"Key Insight","text":"<p>Depth information is lost in projection: all points along a ray project to the same 2D location.</p>"},{"location":"sims/camera-model-visualizer/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Camera Matrix - Wikipedia</li> </ul>"},{"location":"sims/change-of-basis/","title":"Change of Basis Interactive Visualizer","text":"<p>Run the Change of Basis Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/change-of-basis/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates a fundamental concept: the same vector has different coordinate representations in different bases.</p> <p>Key ideas:</p> <ul> <li>The vector itself (green arrow) doesn't change\u2014it's the same geometric object</li> <li>Only its coordinate representation changes based on which basis we use</li> <li>The transition matrix P\u207b\u00b9 converts coordinates: [v]_B = P\u207b\u00b9[v]_std</li> </ul>"},{"location":"sims/change-of-basis/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the vector v (green) to see how its coordinates change in both bases</li> <li>Select a preset basis (Rotated, Skewed, Scaled) or use Custom</li> <li>Adjust the rotation slider to rotate the custom basis</li> <li>Toggle \"Overlay Bases\" to see both coordinate systems on one grid</li> <li>Toggle \"Show Grids\" to see the coordinate grid lines</li> </ol>"},{"location":"sims/change-of-basis/#key-observations","title":"Key Observations","text":"<ul> <li>The vector arrow stays in the same position regardless of basis choice</li> <li>In a rotated basis, the coordinates reflect the new orientation</li> <li>The transition matrix P\u207b\u00b9 relates the two coordinate systems</li> </ul>"},{"location":"sims/change-of-basis/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/change-of-basis/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/change-of-basis/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/change-of-basis/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Explain why coordinate representations depend on basis choice</li> <li>Calculate coordinates in a new basis using the transition matrix</li> <li>Distinguish between a vector and its coordinate representation</li> </ol>"},{"location":"sims/change-of-basis/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Predict coordinates: Before dragging, predict what [v]_B will be</li> <li>Verify transition: Check that P\u207b\u00b9[v]_std = [v]_B</li> <li>Special vectors: Find vectors that have the same coordinates in both bases</li> </ol>"},{"location":"sims/change-of-basis/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If the basis is rotated 90\u00b0, how do the coordinates of (1, 0) change?</li> <li>What properties do similar matrices share?</li> <li>Why is change of basis useful in linear algebra?</li> </ol>"},{"location":"sims/change-of-basis/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations - Change of Basis section</li> <li>Similar matrices and diagonalization</li> </ul>"},{"location":"sims/characteristic-polynomial/","title":"Characteristic Polynomial Explorer","text":"<p>Run the Characteristic Polynomial Explorer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/characteristic-polynomial/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive tool helps you understand how eigenvalues are found by computing the characteristic polynomial det(A - \u03bbI) and finding its roots. The visualization shows both the algebraic computation and the graphical representation of the polynomial.</p> <p>Key Features:</p> <ul> <li>2\u00d72 and 3\u00d73 matrices: Toggle between matrix sizes</li> <li>Editable matrix entries: Click cells to enter custom values</li> <li>Step-by-step calculation: See the polynomial derivation</li> <li>Polynomial graph: Visualize p(\u03bb) and see eigenvalues as x-intercepts</li> <li>Trace slider: Explore points along the polynomial curve</li> <li>Preset examples: Identity, random, and symmetric matrices</li> </ul>"},{"location":"sims/characteristic-polynomial/#how-to-use","title":"How to Use","text":"<ol> <li>Select matrix size using the 2\u00d72/3\u00d73 toggle button</li> <li>Click matrix cells to edit values</li> <li>View the characteristic polynomial in the left panel</li> <li>See eigenvalues where the curve crosses the x-axis (red dots)</li> <li>Use the \u03bb trace slider to explore values along the polynomial</li> </ol>"},{"location":"sims/characteristic-polynomial/#mathematical-background","title":"Mathematical Background","text":"<p>The characteristic equation is: det(A - \u03bbI) = 0</p> <p>For a 2\u00d72 matrix [[a, b], [c, d]]:</p> <ul> <li>p(\u03bb) = \u03bb\u00b2 - (a+d)\u03bb + (ad-bc)</li> <li>= \u03bb\u00b2 - trace(A)\u03bb + det(A)</li> </ul> <p>The eigenvalues are the roots of this polynomial, found using the quadratic formula:</p> <p>\u03bb = (trace \u00b1 \u221a(trace\u00b2 - 4\u00b7det)) / 2</p>"},{"location":"sims/characteristic-polynomial/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/characteristic-polynomial/main.html\" height=\"532px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/characteristic-polynomial/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/characteristic-polynomial/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Compute the characteristic polynomial for 2\u00d72 and 3\u00d73 matrices</li> <li>Find eigenvalues as roots of the characteristic polynomial</li> <li>Connect the algebraic formula to the graphical representation</li> </ol>"},{"location":"sims/characteristic-polynomial/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify by hand: Compute the characteristic polynomial for [[4, 2], [1, 3]] and verify it matches the display</li> <li>Trace exploration: For eigenvalue \u03bb = 5, verify that p(5) = 0 by using the slider</li> <li>Complex eigenvalues: Try [[0, -1], [1, 0]] - what happens to the graph?</li> <li>Triple root: Can you create a 3\u00d73 matrix where all three eigenvalues are equal?</li> </ol>"},{"location":"sims/characteristic-polynomial/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the relationship between the trace of A and the coefficient of \u03bb in the characteristic polynomial?</li> <li>If the characteristic polynomial never crosses the x-axis, what does this mean about the eigenvalues?</li> <li>For a triangular matrix, how do eigenvalues relate to diagonal entries?</li> </ol>"},{"location":"sims/characteristic-polynomial/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/cofactor-expansion/","title":"Cofactor Expansion Interactive Visualizer","text":"<p>Run the Cofactor Expansion Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/cofactor-expansion/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates cofactor expansion (Laplace expansion), a general method for computing determinants of any square matrix.</p> <p>Key concepts:</p> <ul> <li>Minor \\(M_{ij}\\): Determinant of submatrix after removing row \\(i\\) and column \\(j\\)</li> <li>Cofactor \\(C_{ij} = (-1)^{i+j} M_{ij}\\): Minor with alternating sign</li> <li>Expansion: \\(\\det(A) = \\sum_j a_{ij} \\cdot C_{ij}\\)</li> </ul> <p>The sign pattern follows a checkerboard: + for even \\((i+j)\\), - for odd \\((i+j)\\).</p>"},{"location":"sims/cofactor-expansion/#how-to-use","title":"How to Use","text":"<ol> <li>Step through: Click \"Step\" to see each cofactor computed</li> <li>Play animation: Click \"Play\" to auto-advance through all steps</li> <li>Change expansion row: Select which row to expand along</li> <li>Toggle sign pattern: Show/hide the checkerboard sign display</li> </ol>"},{"location":"sims/cofactor-expansion/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/cofactor-expansion/main.html\" height=\"522px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/cofactor-expansion/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/cofactor-expansion/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify minors and cofactors of matrix entries</li> <li>Apply the cofactor expansion formula along any row</li> <li>Understand why expansion along any row gives the same determinant</li> </ol>"},{"location":"sims/cofactor-expansion/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Compare rows: Expand along different rows and verify same result</li> <li>Efficiency analysis: Which row would be most efficient if some entries are zero?</li> <li>Recursive thinking: How would you extend this to 4\u00d74 matrices?</li> </ol>"},{"location":"sims/cofactor-expansion/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For a 3\u00d73 matrix, how many 2\u00d72 determinants must be computed?</li> <li>Why does the sign alternate in a checkerboard pattern?</li> <li>If row 2 of a matrix is [0, 5, 0], which cofactor expansion would be most efficient?</li> </ol>"},{"location":"sims/cofactor-expansion/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Cofactor Expansion section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/complex-eigenvalue/","title":"Complex Eigenvalue Visualizer","text":"<p>Run the Complex Eigenvalue Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/complex-eigenvalue/#about-this-microsim","title":"About This MicroSim","text":"<p>Complex eigenvalues reveal the rotational nature of certain linear transformations. When a real matrix has complex eigenvalues, they always appear in conjugate pairs \u03bb = a \u00b1 bi, and the transformation involves both rotation and scaling.</p> <p>Key Features:</p> <ul> <li>Dual view: Transformation plane (left) and complex plane (right)</li> <li>Adjustable eigenvalue: Sliders for real and imaginary parts</li> <li>Spiral animation: Watch repeated transformation create spirals</li> <li>Conjugate pair: Toggle to show both \u03bb and \u03bb\u0304</li> <li>Behavior indicators: Shows whether spiral expands, contracts, or circles</li> </ul>"},{"location":"sims/complex-eigenvalue/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust \"Re\" slider to change the real part of \u03bb</li> <li>Adjust \"Im\" slider to change the imaginary part</li> <li>Click \"Animate\" to see repeated transformation</li> <li>Click \"Reset\" to start over from (1, 0)</li> <li>Toggle \"Show Conjugate\" to display both eigenvalues</li> </ol>"},{"location":"sims/complex-eigenvalue/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>For complex eigenvalue \u03bb = a + bi:</p> Property Formula Meaning Magnitude |\u03bb| = \u221a(a\u00b2 + b\u00b2) Scaling factor per step Angle \u03b8 = arctan(b/a) Rotation angle per step <p>Behavior Patterns:</p> <ul> <li>|\u03bb| &gt; 1: Spiral outward (expanding)</li> <li>|\u03bb| &lt; 1: Spiral inward (contracting)</li> <li>|\u03bb| = 1: Pure rotation (circle)</li> <li>Im(\u03bb) = 0: No rotation (pure scaling)</li> </ul>"},{"location":"sims/complex-eigenvalue/#example-rotation-matrix","title":"Example: Rotation Matrix","text":"<p>The 90\u00b0 rotation matrix has eigenvalues \u03bb = \u00b1i: - |\u03bb| = 1 \u2192 no scaling - \u03b8 = 90\u00b0 \u2192 quarter turn each step</p> <p>Try setting Re = 0, Im = 1 to see pure rotation!</p>"},{"location":"sims/complex-eigenvalue/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/complex-eigenvalue/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/complex-eigenvalue/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/complex-eigenvalue/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Connect complex eigenvalues to rotation-scaling transformations</li> <li>Interpret eigenvalue magnitude as scaling factor</li> <li>Interpret eigenvalue argument (angle) as rotation amount</li> <li>Predict trajectory behavior from eigenvalue position in complex plane</li> </ol>"},{"location":"sims/complex-eigenvalue/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Find the circle: Adjust sliders until |\u03bb| = 1 exactly and observe pure rotation</li> <li>Predict behavior: Before animating, predict if spiral will expand or contract</li> <li>Conjugate pairs: Why do complex eigenvalues of real matrices come in conjugate pairs?</li> <li>Matrix connection: For \u03bb = 0.9 + 0.4i, what 2\u00d72 matrix produces this behavior?</li> </ol>"},{"location":"sims/complex-eigenvalue/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If \u03bb = 2i, what happens to points under repeated transformation?</li> <li>What eigenvalue produces a 60\u00b0 rotation with 10% shrinkage per step?</li> <li>Where on the complex plane are eigenvalues of stable systems located?</li> </ol>"},{"location":"sims/complex-eigenvalue/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/condition-number/","title":"Condition Number and Sensitivity Visualizer","text":"<p>Run the Condition Number Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/condition-number/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how the condition number affects the sensitivity of linear system solutions. When solving Ax = b:</p> <ul> <li>Well-conditioned (\u03ba \u2248 1): Small changes in b cause small changes in x</li> <li>Ill-conditioned (\u03ba large): Small changes in b cause large changes in x</li> </ul>"},{"location":"sims/condition-number/#key-concepts","title":"Key Concepts","text":"<p>The condition number \u03ba(A) = \u03c3\u2081/\u03c3\u2082 bounds how much errors amplify:</p> \\[\\frac{\\|\\delta \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\leq \\kappa(A) \\cdot \\frac{\\|\\delta \\mathbf{b}\\|}{\\|\\mathbf{b}\\|}\\]"},{"location":"sims/condition-number/#visual-elements","title":"Visual Elements","text":"<ul> <li>Two lines: Represent the two equations in the 2\u00d72 system</li> <li>Green point: The solution x</li> <li>Orange cloud: Perturbed solutions when b is slightly changed</li> <li>Bar chart: Shows relative sizes of \u03c3\u2081 and \u03c3\u2082</li> </ul>"},{"location":"sims/condition-number/#how-to-use","title":"How to Use","text":"<ol> <li>Select a preset to see different conditioning levels</li> <li>Adjust \u03b5 to control perturbation magnitude</li> <li>Toggle perturbations to show/hide the solution cloud</li> <li>Observe how the cloud grows with condition number</li> </ol>"},{"location":"sims/condition-number/#presets","title":"Presets","text":"Preset \u03ba Lines Behavior Well-conditioned ~1 Perpendicular Stable Moderate ~10 Angled Some spread Ill-conditioned ~1000 Nearly parallel Large spread Nearly Singular ~\u221e Almost same Unstable"},{"location":"sims/condition-number/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Interpret condition number geometrically</li> <li>Predict solution sensitivity from \u03ba</li> <li>Recognize ill-conditioned systems visually</li> <li>Connect singular values to conditioning</li> </ul>"},{"location":"sims/condition-number/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Condition Number section</li> <li>Numerical stability in scientific computing</li> </ul>"},{"location":"sims/convex-function-visualizer/","title":"Convex Function Visualizer","text":"<p>Run the Convex Function Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/convex-function-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization demonstrates the geometric definition of convexity - a fundamental concept in optimization and machine learning. A function is convex if any chord (line segment) connecting two points on the curve lies above or on the curve itself.</p>"},{"location":"sims/convex-function-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Select a Function: Use the dropdown to choose different functions including convex (x\u00b2, |x|, x\u2074) and non-convex (-x\u00b2)</li> <li>Drag the Points: Click and drag the red (point 1) and green (point 2) markers to move them along the curve</li> <li>Adjust Lambda: Use the slider to move the interpolation point along the chord (\u03bb=0 is at point 2, \u03bb=1 is at point 1)</li> <li>Observe: Watch how the shaded region changes - green indicates convexity is satisfied, red indicates violation</li> </ol>"},{"location":"sims/convex-function-visualizer/#the-convexity-condition","title":"The Convexity Condition","text":"<p>For a function f(x) to be convex:</p> \\[f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) + (1-\\lambda) f(x_2)\\] <p>This means the value of f at any point between x\u2081 and x\u2082 must be at or below the chord connecting (x\u2081, f(x\u2081)) and (x\u2082, f(x\u2082)).</p>"},{"location":"sims/convex-function-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/convex-function-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the geometric definition of convex functions</li> <li>Visualize why convex functions have no local minima (only global)</li> <li>Recognize convex vs non-convex functions visually</li> </ul>"},{"location":"sims/convex-function-visualizer/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Exploration: Start with x\u00b2 and verify the convexity condition holds for any position of the two points</li> <li>Contrast: Switch to -x\u00b2 and observe how the condition is violated</li> <li>Edge Cases: Try x\u00b2 + sin(x) which is still convex despite the oscillation</li> <li>Discussion: Why does convexity matter for optimization algorithms?</li> </ol>"},{"location":"sims/convex-function-visualizer/#references","title":"References","text":"<ul> <li>Boyd &amp; Vandenberghe, Convex Optimization, Chapter 3</li> <li>Wikipedia: Convex Function</li> </ul>"},{"location":"sims/convolution-operation/","title":"Convolution Operation","text":"<p>Run the Convolution Operation Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/convolution-operation/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows exactly how convolution works in convolutional neural networks (CNNs). Watch as the kernel slides across the input image, performing element-wise multiplication and summing to produce each output pixel.</p>"},{"location":"sims/convolution-operation/#the-convolution-formula","title":"The Convolution Formula","text":"<p>\\((\\mathbf{I} * \\mathbf{K})_{i,j} = \\sum_{m}\\sum_{n} I_{i+m, j+n} \\cdot K_{m,n}\\)</p>"},{"location":"sims/convolution-operation/#output-size-formula","title":"Output Size Formula","text":"<p>\\(\\text{output\\_size} = \\frac{\\text{input\\_size} - \\text{kernel\\_size} + 2 \\times \\text{padding}}{\\text{stride}} + 1\\)</p>"},{"location":"sims/convolution-operation/#interactive-features","title":"Interactive Features","text":"<ul> <li>Kernel Selector: Try different kernels (edge detection, blur, sharpen, etc.)</li> <li>Stride: Change how many pixels the kernel moves between positions</li> <li>Padding: Valid (no padding) or Same (preserve dimensions)</li> <li>Step: Advance one position at a time</li> <li>Animate: Watch the full convolution process</li> </ul>"},{"location":"sims/convolution-operation/#preset-kernels","title":"Preset Kernels","text":"Kernel Effect Pattern Edge Detect Highlights edges Center +8, neighbors -1 Blur Smooths image All equal weights Sharpen Enhances details Center +5, cross -1 Sobel X Detects vertical edges Gradient filter"},{"location":"sims/convolution-operation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/convolution-operation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Explain how convolution kernels extract features from images</li> <li>Calculate output dimensions given input size, kernel size, stride, and padding</li> <li>Describe the effect of different kernel patterns</li> <li>Understand weight sharing in convolutional layers</li> </ol>"},{"location":"sims/convolution-operation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Dimension Calculation: For each stride/padding combination, verify the output size formula</li> <li>Kernel Exploration: Compare what edge detection vs blur kernels produce</li> <li>Manual Computation: Pause and verify one output value by hand</li> <li>Same vs Valid: When would you choose \"same\" padding over \"valid\"?</li> </ol>"},{"location":"sims/convolution-operation/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does convolution reduce spatial dimensions (with valid padding)?</li> <li>How does a 3\u00d73 kernel have only 9 parameters but process arbitrarily large images?</li> <li>What real-world features might an edge detection kernel capture?</li> <li>Why might we use stride &gt; 1 instead of pooling for downsampling?</li> </ol>"},{"location":"sims/convolution-operation/#mathematical-details","title":"Mathematical Details","text":"<p>A 3\u00d73 kernel with stride 1 and valid padding on a 7\u00d77 input:</p> <ul> <li>Output size: \\((7 - 3 + 0) / 1 + 1 = 5\u00d75\\)</li> <li>Parameters: 9 weights + 1 bias = 10</li> </ul>"},{"location":"sims/convolution-operation/#references","title":"References","text":"<ul> <li>LeCun et al. (1998). Gradient-Based Learning Applied to Document Recognition</li> <li>Krizhevsky et al. (2012). ImageNet Classification with Deep CNNs</li> </ul>"},{"location":"sims/convolution-visualizer/","title":"Convolution Visualizer","text":"<p>Run the Convolution Visualizer Fullscreen</p> <p>Edit the Convolution Visualizer with the p5.js editor</p>"},{"location":"sims/convolution-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates image convolution - the fundamental operation behind image filtering. Watch step-by-step as a kernel slides across an image, computing weighted sums at each position.</p>"},{"location":"sims/convolution-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select a Kernel: Choose from Identity, Box Blur, Gaussian Blur, Sharpen, or Edge Detect</p> </li> <li> <p>Step Through: Click \"Step\" to advance one pixel at a time and see the calculation</p> </li> <li> <p>Run Animation: Click \"Run\" to animate the convolution process</p> </li> <li> <p>Adjust Speed: Use the slider to control animation speed</p> </li> <li> <p>Show Calculation: Toggle to see the element-wise multiplication and sum</p> </li> </ol>"},{"location":"sims/convolution-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand how convolution slides a kernel across an image - Calculate the weighted sum at each pixel position - Predict the effect of different kernel values - Connect kernel properties to visual effects</p>"},{"location":"sims/convolution-visualizer/#key-concepts","title":"Key Concepts","text":"<ul> <li>Kernel/Filter: A small matrix of weights applied to each neighborhood</li> <li>Convolution Sum: Sum of element-wise products between kernel and image patch</li> <li>Boundary Handling: Edge pixels require special treatment (padding)</li> <li>Linear Operation: Convolution preserves linearity (sum of filters = filter of sums)</li> </ul>"},{"location":"sims/convolution-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/convolution-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Explain that convolution is like looking at each pixel through a magnifying glass that blends neighboring values according to specific weights.</p>"},{"location":"sims/convolution-visualizer/#step-by-step-demo-10-minutes","title":"Step-by-Step Demo (10 minutes)","text":"<ol> <li>Start with Identity kernel - show it produces no change</li> <li>Switch to Box Blur - each pixel becomes the average of its neighbors</li> <li>Use Step button to walk through the calculation manually</li> </ol>"},{"location":"sims/convolution-visualizer/#kernel-analysis-10-minutes","title":"Kernel Analysis (10 minutes)","text":"<ol> <li>Compare Box Blur vs Gaussian Blur - why is Gaussian smoother?</li> <li>Examine Sharpen kernel - negative weights enhance edges</li> <li>Edge Detect - why does the sum of weights equal 0?</li> </ol>"},{"location":"sims/convolution-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>What happens at image boundaries?</li> <li>Why must blur kernel values sum to 1?</li> <li>How would a 5\u00d75 kernel differ from 3\u00d73?</li> </ul>"},{"location":"sims/convolution-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Kernel (Image Processing) - Wikipedia</li> </ul>"},{"location":"sims/coordinate-system-3d/","title":"3D Coordinate System Visualizer","text":"<p>Run the 3D Coordinate System Visualizer Fullscreen</p> <p>Edit the 3D Coordinate System Visualizer with the p5.js editor</p>"},{"location":"sims/coordinate-system-3d/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates different 3D coordinate system conventions used across computer graphics, robotics, and computer vision. Explore how handedness affects axis orientation and learn the standard conventions used by different platforms.</p>"},{"location":"sims/coordinate-system-3d/#how-to-use","title":"How to Use","text":"<ol> <li>Select Handedness: Toggle between right-hand and left-hand coordinate systems</li> <li>Choose Convention: Select from OpenGL/Math, DirectX/Unity, ROS/Robotics, or Camera conventions</li> <li>Drag to Rotate: Click and drag in the 3D view to rotate the visualization</li> <li>Toggle Grid: Show or hide the grid planes</li> <li>Animate: Watch the right-hand rule demonstration</li> </ol>"},{"location":"sims/coordinate-system-3d/#key-concepts","title":"Key Concepts","text":"Convention X Y Z Used By OpenGL/Math Right Up Out (toward viewer) OpenGL, most mathematics DirectX/Unity Right Up Into screen DirectX, Unity game engine ROS/Robotics Forward Left Up Robot Operating System Camera Right Down Forward Computer vision cameras"},{"location":"sims/coordinate-system-3d/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Distinguish between right-hand and left-hand coordinate systems - Identify the axis conventions used by major platforms - Apply the right-hand rule to determine axis orientations - Convert coordinates between different conventions</p>"},{"location":"sims/coordinate-system-3d/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/coordinate-system-3d/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Discuss why coordinate system conventions matter in 3D applications and the chaos that can result from mixing conventions.</p>"},{"location":"sims/coordinate-system-3d/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with the right-hand OpenGL convention</li> <li>Use the right-hand rule: curl fingers from X to Y, thumb points to Z</li> <li>Switch to left-hand and observe how Z flips</li> <li>Compare different platform conventions</li> </ol>"},{"location":"sims/coordinate-system-3d/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why might different industries have chosen different conventions?</li> <li>What happens if you mix coordinate systems in a pipeline?</li> <li>How would you convert a point from ROS to camera coordinates?</li> </ul>"},{"location":"sims/coordinate-system-3d/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>OpenGL Coordinate Systems</li> </ul>"},{"location":"sims/corner-detection-visualizer/","title":"Corner Detection Visualizer","text":"<p>Run the Corner Detection Visualizer Fullscreen</p> <p>Edit the Corner Detection Visualizer with the p5.js editor</p>"},{"location":"sims/corner-detection-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates Harris corner detection, which uses the eigenvalues of the structure tensor to identify corners - distinctive points useful for image matching and tracking.</p>"},{"location":"sims/corner-detection-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Different shapes have different corner configurations</p> </li> <li> <p>Adjust k Parameter: Controls the sensitivity of corner detection (0.04 is typical)</p> </li> <li> <p>Adjust Threshold: Filter out weak corner responses</p> </li> <li> <p>Response Heatmap: Toggle to see the Harris response across the image</p> </li> </ol>"},{"location":"sims/corner-detection-visualizer/#key-concepts","title":"Key Concepts","text":"Eigenvalue Pattern Structure Classification Both \u03bb\u2081, \u03bb\u2082 small Flat region Not a feature One \u03bb large, one small Edge Not distinctive Both \u03bb\u2081, \u03bb\u2082 large Corner Good feature!"},{"location":"sims/corner-detection-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand the structure tensor as a covariance matrix of gradients - Interpret eigenvalues as measures of gradient change in principal directions - Apply the Harris corner response formula - Evaluate corner detection quality using different thresholds</p>"},{"location":"sims/corner-detection-visualizer/#the-mathematics","title":"The Mathematics","text":"<p>Structure Tensor (second moment matrix):</p> \\[\\mathbf{M} = \\sum_{(x,y) \\in W} \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix}\\] <p>Harris Response:</p> \\[R = \\det(\\mathbf{M}) - k \\cdot \\text{trace}(\\mathbf{M})^2 = \\lambda_1 \\lambda_2 - k(\\lambda_1 + \\lambda_2)^2\\]"},{"location":"sims/corner-detection-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/corner-detection-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Corners are distinctive image points useful for matching, tracking, and 3D reconstruction. Unlike edges, corners can be localized precisely in both directions.</p>"},{"location":"sims/corner-detection-visualizer/#eigenvalue-interpretation-10-minutes","title":"Eigenvalue Interpretation (10 minutes)","text":"<ol> <li>Start with Rectangle - see corners at the four vertices</li> <li>Note how edge pixels have different eigenvalue patterns</li> <li>Observe the eigenvalue scatter plot (if visible)</li> </ol>"},{"location":"sims/corner-detection-visualizer/#parameter-exploration-10-minutes","title":"Parameter Exploration (10 minutes)","text":"<ol> <li>Vary k parameter - higher k = fewer corners</li> <li>Adjust threshold to control false positives</li> <li>Compare Checkerboard vs L-Shape corner counts</li> </ol>"},{"location":"sims/corner-detection-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why does the checkerboard have so many corners?</li> <li>Why can't edges be used as reliable matching features?</li> <li>How would you make corners scale-invariant?</li> </ul>"},{"location":"sims/corner-detection-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Harris Corner Detector - Wikipedia</li> </ul>"},{"location":"sims/cosine-euclidean-similarity/","title":"Cosine vs Euclidean Similarity","text":"<p>Run the Cosine vs Euclidean Similarity Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/cosine-euclidean-similarity/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization compares two fundamental ways to measure similarity between vectors:</p> <ul> <li>Cosine Similarity: Measures the angle between vectors, ignoring magnitude</li> <li>Euclidean Distance: Measures the straight-line distance between vector endpoints</li> </ul> <p>Understanding when to use each metric is crucial in machine learning and NLP applications.</p>"},{"location":"sims/cosine-euclidean-similarity/#how-to-use","title":"How to Use","text":"<ol> <li>Drag Vectors: Click and drag the red (A) or blue (B) vector endpoints</li> <li>Normalize: Toggle the checkbox to see both vectors normalized to the same length</li> <li>Presets: Select from preset configurations to explore specific scenarios</li> <li>Observe: Watch how cosine similarity and Euclidean distance change differently</li> </ol>"},{"location":"sims/cosine-euclidean-similarity/#key-insights","title":"Key Insights","text":""},{"location":"sims/cosine-euclidean-similarity/#same-direction-different-magnitude","title":"Same Direction, Different Magnitude","text":"<p>When vectors point the same way but have different lengths: - Cosine similarity = 1 (identical direction) - Euclidean distance &gt; 0 (endpoints are apart)</p> <p>This is why cosine similarity is preferred for comparing documents of different lengths!</p>"},{"location":"sims/cosine-euclidean-similarity/#orthogonal-vectors","title":"Orthogonal Vectors","text":"<p>When vectors are perpendicular: - Cosine similarity = 0 (no alignment) - Euclidean distance varies with magnitudes</p>"},{"location":"sims/cosine-euclidean-similarity/#formulas","title":"Formulas","text":"<p>Cosine Similarity: \\(\\(\\text{sim}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{|\\mathbf{u}| |\\mathbf{v}|} = \\cos(\\theta)\\)\\)</p> <p>Euclidean Distance: \\(\\(d(\\mathbf{u}, \\mathbf{v}) = \\sqrt{\\sum_{i=1}^n (u_i - v_i)^2}\\)\\)</p>"},{"location":"sims/cosine-euclidean-similarity/#when-to-use-each","title":"When to Use Each","text":"Metric Best For Invariant To Cosine Similarity Text similarity, embeddings Vector magnitude Euclidean Distance Spatial positioning, clustering Nothing (sensitive to scale)"},{"location":"sims/cosine-euclidean-similarity/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Compare and contrast cosine similarity and Euclidean distance</li> <li>Understand magnitude invariance of cosine similarity</li> <li>Determine which metric is appropriate for different applications</li> </ul> <p>Activities:</p> <ol> <li>Create vectors with same cosine similarity but different Euclidean distances</li> <li>Find configurations where both metrics agree on similarity</li> <li>Explore the effect of normalizing vectors on both metrics</li> </ol> <p>Assessment:</p> <ul> <li>Why is cosine similarity preferred for comparing word embeddings?</li> <li>When would Euclidean distance be more appropriate than cosine similarity?</li> </ul>"},{"location":"sims/cosine-euclidean-similarity/#references","title":"References","text":"<ul> <li>Chapter 11: Generative AI and LLMs</li> <li>Cosine Similarity on Wikipedia</li> </ul>"},{"location":"sims/covariance-correlation/","title":"Covariance and Correlation Matrix Visualizer","text":"<p>Run the Covariance Correlation Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/covariance-correlation/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization helps you understand the relationship between covariance and correlation - two fundamental measures of how variables relate to each other. The three-panel layout shows the same data from different perspectives.</p> <p>Key Features:</p> <ul> <li>Scatter plot: Visualize the raw relationship between features X and Y</li> <li>Covariance matrix: See the unstandardized measure of joint variability</li> <li>Correlation matrix: See the standardized measure bounded between -1 and 1</li> <li>Draggable points: Modify data and watch matrices update in real-time</li> <li>Standardize toggle: Observe how standardization makes covariance equal correlation</li> <li>Eigenvalue display: See the principal components of the covariance matrix</li> </ul>"},{"location":"sims/covariance-correlation/#understanding-covariance-vs-correlation","title":"Understanding Covariance vs Correlation","text":"Property Covariance Correlation Range Unbounded -1 to +1 Scale dependent Yes No Units Product of units Unitless Interpretation Direction of relationship Strength and direction"},{"location":"sims/covariance-correlation/#covariance-formula","title":"Covariance Formula","text":"\\[\\text{Cov}(X,Y) = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\\]"},{"location":"sims/covariance-correlation/#correlation-formula-pearson","title":"Correlation Formula (Pearson)","text":"\\[r_{XY} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\]"},{"location":"sims/covariance-correlation/#key-insights","title":"Key Insights","text":"<ol> <li> <p>Correlation normalizes for scale: Changing the units of X or Y changes covariance but not correlation</p> </li> <li> <p>Standardized data: When data is standardized (mean=0, std=1), the covariance matrix equals the correlation matrix</p> </li> <li> <p>Eigenvalues: The eigenvalues of the covariance matrix represent the variance along principal components</p> </li> <li> <p>Matrix symmetry: Both matrices are symmetric because Cov(X,Y) = Cov(Y,X)</p> </li> </ol>"},{"location":"sims/covariance-correlation/#how-to-use","title":"How to Use","text":"<ol> <li>Select a dataset preset to see different correlation patterns</li> <li>Drag the correlation slider to smoothly adjust the relationship strength</li> <li>Drag individual points in the scatter plot to see how outliers affect the statistics</li> <li>Toggle \"Standardize Data\" to see how standardization affects the covariance matrix</li> <li>Click matrix cells to highlight the corresponding relationship</li> </ol>"},{"location":"sims/covariance-correlation/#interpretation-guide","title":"Interpretation Guide","text":"Correlation Value Interpretation +0.8 to +1.0 Very strong positive +0.6 to +0.8 Strong positive +0.4 to +0.6 Moderate positive +0.2 to +0.4 Weak positive -0.2 to +0.2 Very weak or none -0.4 to -0.2 Weak negative -0.6 to -0.4 Moderate negative -0.8 to -0.6 Strong negative -1.0 to -0.8 Very strong negative"},{"location":"sims/covariance-correlation/#applications-in-machine-learning","title":"Applications in Machine Learning","text":"<ul> <li>Feature selection: Highly correlated features may be redundant</li> <li>PCA: Principal Component Analysis uses eigendecomposition of the covariance matrix</li> <li>Multicollinearity detection: Correlation matrices reveal problematic variable relationships</li> <li>Data preprocessing: Standardization is often required before algorithms like KNN</li> </ul>"},{"location":"sims/covariance-correlation/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/covariance-correlation/main.html\" height=\"582px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/covariance-correlation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/covariance-correlation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Calculate and interpret covariance between two variables</li> <li>Calculate and interpret Pearson correlation coefficient</li> <li>Explain why correlation is preferred over covariance for comparison</li> <li>Describe how standardization affects the covariance matrix</li> <li>Connect eigenvalues of the covariance matrix to PCA</li> </ol>"},{"location":"sims/covariance-correlation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Outlier investigation: Drag one point far from the cluster and observe the effect on r</li> <li>Scale independence: Note that changing the slider changes covariance values but correlation stays interpretable</li> <li>Standardization experiment: Toggle standardize on/off and compare the two matrices</li> <li>Eigenvalue exploration: Observe how eigenvalues change with correlation strength</li> </ol>"},{"location":"sims/covariance-correlation/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If Cov(X,Y) = 15 and the standard deviations are 3 and 5, what is the correlation?</li> <li>Why does the diagonal of the correlation matrix always equal 1?</li> <li>What happens to the eigenvalues as correlation approaches 1 or -1?</li> <li>Why might standardizing data before machine learning be important?</li> </ol>"},{"location":"sims/covariance-correlation/#references","title":"References","text":"<ul> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 7: Matrix Decompositions</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/cramers-rule/","title":"Cramer's Rule Interactive Solver","text":"<p>Run the Cramer's Rule Solver Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/cramers-rule/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates Cramer's Rule, a method for solving systems of linear equations using determinants.</p> <p>For a 2\u00d72 system \\(A\\mathbf{x} = \\mathbf{b}\\):</p> \\[x = \\frac{\\det(A_1)}{\\det(A)} \\quad \\text{and} \\quad y = \\frac{\\det(A_2)}{\\det(A)}\\] <p>Where:</p> <ul> <li>\\(A_1\\) = matrix \\(A\\) with column 1 replaced by \\(\\mathbf{b}\\)</li> <li>\\(A_2\\) = matrix \\(A\\) with column 2 replaced by \\(\\mathbf{b}\\)</li> </ul>"},{"location":"sims/cramers-rule/#how-to-use","title":"How to Use","text":"<ol> <li>Step through: Click \"Step\" to see each determinant computed</li> <li>Auto-play: Click \"Play\" to animate through all steps</li> <li>Try examples: Random (non-singular) or Singular system</li> <li>Watch the geometry: See the intersection of two lines</li> </ol>"},{"location":"sims/cramers-rule/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/cramers-rule/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/cramers-rule/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/cramers-rule/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Apply Cramer's Rule to solve 2\u00d72 systems</li> <li>Recognize when Cramer's Rule fails (det(A) = 0)</li> <li>Connect algebraic solution to geometric intersection</li> </ol>"},{"location":"sims/cramers-rule/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify solutions: Substitute the solution back into original equations</li> <li>Compare methods: Solve the same system using Gaussian elimination</li> <li>Singular case: What does it mean geometrically when det(A) = 0?</li> </ol>"},{"location":"sims/cramers-rule/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Use Cramer's Rule to solve: 3x + 2y = 8, x - y = 1</li> <li>When would you prefer Gaussian elimination over Cramer's Rule?</li> <li>If det(A) = 0 and the system has infinitely many solutions, what's the geometric interpretation?</li> </ol>"},{"location":"sims/cramers-rule/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Cramer's Rule section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/data-matrix-structure/","title":"Data Matrix Structure Visualizer","text":"<p>Run the Data Matrix Structure Visualizer Fullscreen</p> <p>Edit the Data Matrix Structure Visualizer with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/data-matrix-structure/main.html\" height=\"520px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/data-matrix-structure/#description","title":"Description","text":"<p>This MicroSim helps students understand the fundamental structure of data matrices used in machine learning and data science. A data matrix organizes observations (samples) in rows and measurements (features) in columns, forming the foundation for most ML algorithms.</p> <p>Key Features:</p> <ul> <li>Real Dataset Examples: Explore Iris (botanical measurements), MNIST (image pixels), and Housing (socioeconomic factors)</li> <li>Heat Map Visualization: Colors indicate relative values across the matrix</li> <li>Row Highlighting: Click any row to highlight it as a feature vector representing one sample</li> <li>Column Highlighting: Click column headers to see how one feature varies across all samples</li> <li>Dimension Annotations: Clear labels showing n (samples) and d (features)</li> <li>Cell Hover: View individual values and their context</li> </ul>"},{"location":"sims/data-matrix-structure/#matrix-structure-concepts","title":"Matrix Structure Concepts","text":""},{"location":"sims/data-matrix-structure/#rows-as-samples","title":"Rows as Samples","text":"<p>Each row in a data matrix represents a single sample (also called an observation, instance, or data point). A row contains all feature values for one sample, forming a feature vector:</p> \\[\\mathbf{x}_i = [x_{i1}, x_{i2}, \\ldots, x_{id}]\\]"},{"location":"sims/data-matrix-structure/#columns-as-features","title":"Columns as Features","text":"<p>Each column represents a single feature (also called an attribute, variable, or dimension). A column shows how one measurement varies across all samples.</p>"},{"location":"sims/data-matrix-structure/#matrix-notation","title":"Matrix Notation","text":"<p>The full data matrix \\(\\mathbf{X}\\) with n samples and d features:</p> \\[\\mathbf{X} = \\begin{bmatrix} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1d} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2d} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nd} \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}\\]"},{"location":"sims/data-matrix-structure/#example-datasets","title":"Example Datasets","text":""},{"location":"sims/data-matrix-structure/#iris-dataset","title":"Iris Dataset","text":"<ul> <li>Samples: 150 flower specimens</li> <li>Features: 4 measurements (sepal length/width, petal length/width)</li> <li>Use case: Classification of flower species</li> </ul>"},{"location":"sims/data-matrix-structure/#mnist-digit","title":"MNIST Digit","text":"<ul> <li>Samples: 1 handwritten digit image</li> <li>Features: 784 pixel intensities (28x28 grid)</li> <li>Use case: Image classification</li> </ul>"},{"location":"sims/data-matrix-structure/#housing-dataset","title":"Housing Dataset","text":"<ul> <li>Samples: 506 Boston neighborhoods</li> <li>Features: 13 socioeconomic indicators</li> <li>Use case: Price prediction (regression)</li> </ul>"},{"location":"sims/data-matrix-structure/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/data-matrix-structure/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify rows as samples and columns as features in a data matrix</li> <li>Extract a feature vector for a specific sample</li> <li>Compare feature values across different samples</li> <li>Recognize the dimensions n (samples) and d (features)</li> </ol>"},{"location":"sims/data-matrix-structure/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Start with Iris: Observe the 4-feature structure representing flower measurements</li> <li>Click a Row: See how one flower's measurements form a feature vector</li> <li>Click a Column: Observe how sepal length varies across different flowers</li> <li>Switch to MNIST: Notice the dramatic increase in features (pixels as features)</li> <li>Explore Housing: See how socioeconomic features describe neighborhoods</li> </ol>"},{"location":"sims/data-matrix-structure/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Feature Selection: Not all columns contribute equally to predictions</li> <li>Sample Size: More rows generally improve model reliability</li> <li>Curse of Dimensionality: High d relative to n can cause problems</li> <li>Data Normalization: Features may have different scales</li> </ul>"},{"location":"sims/data-matrix-structure/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If the Iris dataset has 150 samples and 4 features, what is the shape of the data matrix?</li> <li>In the MNIST dataset, why does a single image have 784 features?</li> <li>Which dimension (n or d) grows when you collect more data points?</li> <li>What does it mean to extract \"row 3\" from a data matrix?</li> </ol>"},{"location":"sims/data-matrix-structure/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix fundamentals</li> <li>Chapter 9: Machine Learning Foundations - Data representation in ML</li> <li>UCI Machine Learning Repository - Source for example datasets</li> </ul>"},{"location":"sims/decomposition-guide/","title":"Matrix Decomposition Selection Guide","text":"<p>Run the Selection Guide Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/decomposition-guide/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive decision tree helps you choose the right matrix decomposition for your problem. Click on blue decision nodes to explore different paths, and green nodes show the recommended decomposition with reasoning.</p>"},{"location":"sims/decomposition-guide/#decision-summary","title":"Decision Summary","text":"Goal Matrix Type Recommended Solve Ax=b Symmetric positive definite Cholesky Solve Ax=b Square, invertible LU with pivoting Solve Ax=b Rectangular QR or SVD Least squares Any overdetermined QR Low-rank approx Any Truncated SVD Eigenvalues needed Any SVD"},{"location":"sims/decomposition-guide/#how-to-use","title":"How to Use","text":"<ol> <li>Start at the top \"Start\" node</li> <li>Click on blue question nodes to explore paths</li> <li>Read the green answer nodes for recommendations</li> <li>Hover over nodes to see explanations</li> </ol>"},{"location":"sims/decomposition-guide/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Match problems to appropriate decompositions</li> <li>Understand why certain decompositions are preferred</li> <li>Navigate the decision tree efficiently</li> <li>Justify decomposition choices</li> </ul>"},{"location":"sims/decomposition-guide/#quick-reference","title":"Quick Reference","text":""},{"location":"sims/decomposition-guide/#when-to-use-each-decomposition","title":"When to Use Each Decomposition","text":"Decomposition Best For Complexity Cholesky SPD systems, fastest O(n\u00b3/3) LU Square systems, multiple b's O(2n\u00b3/3) QR Least squares, rectangular O(2mn\u00b2) SVD Low-rank, rank analysis O(mn\u00b2)"},{"location":"sims/decomposition-guide/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Choosing the Right Decomposition</li> </ul>"},{"location":"sims/det-2x2-calculator/","title":"2\u00d72 Determinant Calculator","text":"<p>Run the 2\u00d72 Determinant Calculator Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/det-2x2-calculator/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive calculator helps you practice computing 2\u00d72 determinants while seeing the geometric interpretation as the signed area of a parallelogram.</p> <p>Features:</p> <ul> <li>Editable matrix: Click cells to enter custom values</li> <li>Step-by-step calculation: See ad - bc computed explicitly</li> <li>Geometric visualization: Watch the parallelogram formed by column vectors</li> <li>Color coding: Green (positive), red (negative), gray (zero/singular)</li> </ul>"},{"location":"sims/det-2x2-calculator/#how-to-use","title":"How to Use","text":"<ol> <li>Click matrix cells to edit values (use keyboard to type, Enter to confirm)</li> <li>Use preset buttons for quick examples:</li> <li>Random: Generate a random matrix</li> <li>Identity: The identity matrix (det = 1)</li> <li>Singular: A matrix with determinant 0</li> <li>Toggle \"Show Steps\" to see the calculation breakdown</li> </ol>"},{"location":"sims/det-2x2-calculator/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/det-2x2-calculator/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/det-2x2-calculator/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/det-2x2-calculator/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Calculate 2\u00d72 determinants using the formula det(A) = ad - bc</li> <li>Identify the main diagonal and anti-diagonal of a matrix</li> <li>Connect algebraic computation to geometric area</li> </ol>"},{"location":"sims/det-2x2-calculator/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Practice computation: Enter 5 different matrices and verify your mental calculations</li> <li>Find singular matrices: Try to create matrices with determinant exactly 0</li> <li>Explore patterns: What happens when you scale a row by k?</li> </ol>"},{"location":"sims/det-2x2-calculator/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the determinant of [[2, 3], [4, 6]]? Why is this predictable from the matrix structure?</li> <li>If det(A) = 5, what is det(2A)?</li> <li>Create a matrix with determinant exactly -12.</li> </ol>"},{"location":"sims/det-2x2-calculator/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - The 2\u00d72 Determinant section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/det-properties/","title":"Determinant Properties Explorer","text":"<p>Run the Determinant Properties Explorer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/det-properties/#about-this-microsim","title":"About This MicroSim","text":"<p>This explorer helps you understand how different operations affect the determinant of a matrix.</p> <p>Key Properties:</p> Operation Effect on Determinant Swap rows \\(\\det(A') = -\\det(A)\\) Scale row by k \\(\\det(A') = k \\cdot \\det(A)\\) Add multiple of one row to another \\(\\det(A') = \\det(A)\\) (unchanged) Transpose \\(\\det(A^T) = \\det(A)\\) (unchanged)"},{"location":"sims/det-properties/#how-to-use","title":"How to Use","text":"<ol> <li>Click operation buttons to apply different transformations</li> <li>Adjust k slider to change the scaling/addition factor</li> <li>Compare matrices - see original (left) and modified (right)</li> <li>Watch parallelogram area change in the geometric view</li> <li>Click Reset to restore the original matrix</li> </ol>"},{"location":"sims/det-properties/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/det-properties/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/det-properties/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/det-properties/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Predict how row operations change the determinant</li> <li>Explain why adding row multiples preserves the determinant</li> <li>Connect algebraic properties to geometric area changes</li> </ol>"},{"location":"sims/det-properties/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify properties: Apply each operation and check the relationship holds</li> <li>Chain operations: What happens if you swap, then scale, then swap back?</li> <li>Find invariants: Which operations preserve |det|?</li> </ol>"},{"location":"sims/det-properties/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If det(A) = 6, what is det(A) after scaling row 1 by 3?</li> <li>You perform 5 row swaps. Is det(A') positive or negative if det(A) &gt; 0?</li> <li>Why doesn't adding row multiples change the determinant geometrically?</li> </ol>"},{"location":"sims/det-properties/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Properties of Determinants section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/diagonalization-workflow/","title":"Diagonalization Process Workflow","text":"<p>Run the Diagonalization Workflow Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/diagonalization-workflow/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive flowchart guides you through the complete diagonalization process, highlighting the key decision points that determine whether a matrix can be diagonalized.</p> <p>Workflow Steps:</p> <ol> <li>Start with matrix A</li> <li>Compute characteristic polynomial det(A - \u03bbI)</li> <li>Solve for eigenvalues</li> <li>Check if n eigenvalues exist</li> <li>Find eigenvectors for each eigenvalue</li> <li>Compute geometric multiplicities</li> <li>Check m_g = m_a condition</li> <li>Result: A = PDP\u207b\u00b9 or NOT diagonalizable</li> </ol>"},{"location":"sims/diagonalization-workflow/#how-to-use","title":"How to Use","text":"<ol> <li>Click \"Next Step\" to advance through the workflow</li> <li>Click \"Reset\" to start over</li> <li>Toggle \"Auto-advance\" for automatic progression</li> <li>Read the description panel for details about each step</li> </ol>"},{"location":"sims/diagonalization-workflow/#node-types","title":"Node Types","text":"Shape Meaning Rounded rectangle (gray) Start node Rectangle (blue) Process/computation step Diamond (yellow) Decision point Rounded rectangle (green) Success outcome Rounded rectangle (red) Failure outcome"},{"location":"sims/diagonalization-workflow/#the-diagonalization-algorithm","title":"The Diagonalization Algorithm","text":"<pre><code>1. Given n\u00d7n matrix A\n2. Form (A - \u03bbI) and compute det(A - \u03bbI)\n3. Solve characteristic equation to get \u03bb\u2081, ..., \u03bb\u2096\n4. For each \u03bb\u1d62:\n   a. Solve (A - \u03bb\u1d62I)v = 0\n   b. Find basis for null space (eigenspace)\n   c. Record geometric multiplicity\n5. If \u03a3(geometric multiplicities) = n:\n   - Form P = [v\u2081 | v\u2082 | ... | v\u2099]\n   - Form D = diag(\u03bb\u2081, ..., \u03bb\u2099)\n   - A = PDP\u207b\u00b9 \u2713\n6. Otherwise: NOT diagonalizable \u2717\n</code></pre>"},{"location":"sims/diagonalization-workflow/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/diagonalization-workflow/main.html\" height=\"572px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/diagonalization-workflow/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/diagonalization-workflow/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Follow the systematic process for diagonalizing a matrix</li> <li>Identify the two key decision points in the diagonalization process</li> <li>Determine when a matrix is not diagonalizable and why</li> </ol>"},{"location":"sims/diagonalization-workflow/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Apply the workflow: Use the flowchart to diagonalize [[4, 2], [1, 3]]</li> <li>Find the failure point: Which step fails for [[2, 1], [0, 2]]?</li> <li>Complex eigenvalues: What happens when eigenvalues are complex?</li> </ol>"},{"location":"sims/diagonalization-workflow/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>At which step do you first know the algebraic multiplicities?</li> <li>What is the purpose of computing geometric multiplicity?</li> <li>If a 3\u00d73 matrix has only 2 linearly independent eigenvectors, what path does it take in the flowchart?</li> </ol>"},{"location":"sims/diagonalization-workflow/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/","title":"Dot Product and Cross Product Visualizer","text":"<p>Run the Dot Product and Cross Product Visualizer Fullscreen</p> <p>Edit the Dot Product and Cross Product Visualizer Using the p5.js Editor</p>"},{"location":"sims/dot-cross-product-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand the geometric meaning of both the dot product and cross product. The dot product view shows how vectors project onto each other and how the angle between vectors affects the result. The cross product view demonstrates the perpendicular vector and its relationship to parallelogram area.</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p>"},{"location":"sims/dot-cross-product-visualizer/#how-to-use","title":"How to Use","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product-view-2d","title":"Dot Product View (2D)","text":"<ol> <li>Drag Vectors: Click and drag the endpoints of vectors u (blue) and v (red)</li> <li>Observe Projection: The purple line shows the projection of v onto u</li> <li>Watch the Angle: The orange arc shows the angle \u03b8 between vectors</li> <li>See the Formula: The panel shows how u\u00b7v = |u||v|cos(\u03b8)</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#cross-product-view-3d","title":"Cross Product View (3D)","text":"<ol> <li>Toggle View: Click \"Show Cross Product (3D)\" to switch views</li> <li>Rotate Scene: Click and drag to rotate the 3D view</li> <li>Observe Parallelogram: The yellow area shows the parallelogram formed by u and v</li> <li>See Result Vector: The green vector u\u00d7v is perpendicular to both u and v</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#controls","title":"Controls","text":"<ul> <li>Show Projection: Toggle the projection visualization</li> <li>Show Parallelogram: Toggle the parallelogram in 3D view</li> <li>Show Formula: Toggle the step-by-step formula calculation</li> <li>Animate Angle Sweep: Watch how the products change as angle varies from 0\u00b0 to 180\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product","title":"Dot Product","text":"<ul> <li>Projection: The dot product relates to how much one vector projects onto another</li> <li>Angle Relationship: \\(\\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)</li> <li>Perpendicularity: When \u03b8 = 90\u00b0, the dot product is zero</li> <li>Sign: Positive when angle &lt; 90\u00b0, negative when angle &gt; 90\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#cross-product","title":"Cross Product","text":"<ul> <li>Perpendicular Result: u\u00d7v is perpendicular to both u and v</li> <li>Area: |u\u00d7v| equals the area of the parallelogram formed by u and v</li> <li>Right-Hand Rule: The direction follows the right-hand rule</li> <li>Only in 3D: The cross product is only defined for 3D vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Dot Product: \\(\\(\\mathbf{u} \\cdot \\mathbf{v} = u_x v_x + u_y v_y = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)\\)</p> <p>Cross Product: \\(\\(\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_y v_z - u_z v_y \\\\ u_z v_x - u_x v_z \\\\ u_x v_y - u_y v_x \\end{bmatrix}\\)\\)</p> \\[|\\mathbf{u} \\times \\mathbf{v}| = |\\mathbf{u}||\\mathbf{v}|\\sin\\theta\\]"},{"location":"sims/dot-cross-product-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/dot-cross-product-visualizer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/dot-cross-product-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/dot-cross-product-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/dot-cross-product-visualizer/#duration","title":"Duration","text":"<p>25-30 minutes</p>"},{"location":"sims/dot-cross-product-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics (magnitude, direction, components)</li> <li>Trigonometry (cosine, sine)</li> <li>Basic understanding of perpendicularity</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Dot Product Exploration (10 min):</li> <li>Start with u = (3, 0) and v = (2, 0) (same direction)</li> <li>Rotate v to 90\u00b0 and observe dot product becomes zero</li> <li>Continue to 180\u00b0 and see negative dot product</li> <li> <p>Run animation to see the full sweep</p> </li> <li> <p>Projection Investigation (5 min):</p> </li> <li>Observe how the projection length changes with angle</li> <li>Find configurations where projection equals |v|</li> <li> <p>Find configurations where projection is negative</p> </li> <li> <p>Cross Product Exploration (10 min):</p> </li> <li>Switch to 3D view</li> <li>Observe the green cross product vector</li> <li>Rotate view to verify it's perpendicular to both u and v</li> <li> <p>Change vectors and observe parallelogram area changes</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Toggle between views for the same vectors</li> <li>Compare how dot product and cross product magnitude change with angle</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the dot product zero when vectors are perpendicular?</li> <li>What does the sign of the dot product tell you about the angle?</li> <li>Why does the cross product only exist in 3D?</li> <li>How can you use dot product to find the angle between two vectors?</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given two vectors, predict whether the dot product is positive, negative, or zero</li> <li>Calculate the area of a parallelogram using the cross product</li> <li>Find a vector perpendicular to two given vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Dot products and duality</li> <li>3Blue1Brown - Cross products</li> <li>Khan Academy - Cross Product Introduction</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.2-1.3.</li> </ol>"},{"location":"sims/edge-detection-visualizer/","title":"Edge Detection Visualizer","text":"<p>Run the Edge Detection Visualizer Fullscreen</p> <p>Edit the Edge Detection Visualizer with the p5.js editor</p>"},{"location":"sims/edge-detection-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates how gradient-based edge detection works using Sobel, Prewitt, or Scharr operators. See how horizontal and vertical gradient components combine to reveal edges.</p>"},{"location":"sims/edge-detection-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Choose patterns with different edge orientations</p> </li> <li> <p>Select Detector: Compare Sobel, Prewitt, and Scharr operators</p> </li> <li> <p>Adjust Threshold: Control which edges appear in the final binary output</p> </li> <li> <p>Gradient Arrows: Toggle to see edge direction vectors</p> </li> </ol>"},{"location":"sims/edge-detection-visualizer/#panels-explained","title":"Panels Explained","text":"Panel Shows Original Source grayscale image Gx Horizontal gradient (detects vertical edges) Gy Vertical gradient (detects horizontal edges) Magnitude Combined edge strength \u221a(Gx\u00b2 + Gy\u00b2) Edges Binary edges after thresholding"},{"location":"sims/edge-detection-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand how Sobel kernels approximate image derivatives - Explain why Gx detects vertical edges and Gy detects horizontal edges - Calculate gradient magnitude from component gradients - Apply thresholding to create binary edge maps</p>"},{"location":"sims/edge-detection-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/edge-detection-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Edges are locations where image intensity changes rapidly. We can detect them by computing derivatives using convolution.</p>"},{"location":"sims/edge-detection-visualizer/#kernel-analysis-10-minutes","title":"Kernel Analysis (10 minutes)","text":"<ol> <li>Examine Gx kernel - why does [-1,0,1] pattern detect horizontal changes?</li> <li>Note the [1,2,1] smoothing factor perpendicular to the derivative</li> <li>Compare Sobel vs Prewitt - Sobel has more smoothing</li> </ol>"},{"location":"sims/edge-detection-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with Vertical Edge - Gx is strong, Gy is weak</li> <li>Try Horizontal Edge - opposite pattern</li> <li>Use Diagonal - both Gx and Gy respond</li> </ol>"},{"location":"sims/edge-detection-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why is the magnitude image brighter than individual gradients?</li> <li>What happens if we only use Gx without Gy?</li> <li>Why might Scharr perform better for rotational symmetry?</li> </ul>"},{"location":"sims/edge-detection-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Sobel Operator - Wikipedia</li> </ul>"},{"location":"sims/eigenspace-visualization/","title":"Eigenspace Visualization","text":"<p>Run the Eigenspace Visualization Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/eigenspace-visualization/#about-this-microsim","title":"About This MicroSim","text":"<p>This 3D visualization demonstrates how eigenspaces are vector subspaces containing all eigenvectors for a given eigenvalue. The dimension of an eigenspace (geometric multiplicity) determines whether it appears as a line (1D) or plane (2D) through the origin.</p> <p>Key Features:</p> <ul> <li>3D rotation: Drag to rotate the view and explore from different angles</li> <li>Multiple examples: Browse through matrices with different eigenspace structures</li> <li>Visual eigenspaces: Lines and planes shown as semi-transparent colored regions</li> <li>Eigenvector arrows: Toggle to see individual eigenvector directions</li> <li>Color-coded: Each eigenvalue has a distinct color</li> </ul>"},{"location":"sims/eigenspace-visualization/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the 3D view to rotate and examine eigenspaces from different angles</li> <li>Use Prev/Next buttons to browse through different matrix examples</li> <li>Toggle \"Show Grid\" to see or hide the coordinate grid</li> <li>Toggle \"Show Vectors\" to see eigenvector arrows</li> </ol>"},{"location":"sims/eigenspace-visualization/#mathematical-background","title":"Mathematical Background","text":"<p>The eigenspace for eigenvalue \u03bb is defined as:</p> <p>E_\u03bb = null(A - \u03bbI) = {v \u2208 \u211d\u207f : Av = \u03bbv}</p> <p>Key properties:</p> <ul> <li>Every eigenspace is a vector subspace</li> <li>The dimension equals the geometric multiplicity of \u03bb</li> <li>If geometric multiplicity = 1, eigenspace is a line</li> <li>If geometric multiplicity = 2, eigenspace is a plane</li> <li>Eigenvectors from different eigenspaces are linearly independent</li> </ul>"},{"location":"sims/eigenspace-visualization/#examples-included","title":"Examples Included","text":"<ol> <li>3 Distinct Eigenvalues: Diagonal matrix with three 1D eigenspaces (lines along axes)</li> <li>Repeated Eigenvalue: \u03bb=3 has multiplicity 2 with a 2D eigenspace (xy-plane)</li> <li>General Symmetric: Orthogonal eigenvectors in arbitrary directions</li> <li>Rotation-like: Single real eigenvalue, complex pair causes rotation</li> </ol>"},{"location":"sims/eigenspace-visualization/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/eigenspace-visualization/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/eigenspace-visualization/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/eigenspace-visualization/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Visualize eigenspaces as subspaces (lines or planes) through the origin</li> <li>Connect geometric multiplicity to eigenspace dimension</li> <li>Understand why eigenvectors from different eigenspaces are linearly independent</li> </ol>"},{"location":"sims/eigenspace-visualization/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Identify dimensions: For each example, count the dimension of each eigenspace</li> <li>Predict eigenspaces: Given a diagonal matrix, predict what the eigenspaces will look like</li> <li>Orthogonality check: Verify that symmetric matrices have orthogonal eigenspaces</li> <li>Basis vectors: Identify basis vectors for each eigenspace</li> </ol>"},{"location":"sims/eigenspace-visualization/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If a 3\u00d73 matrix has eigenvalue \u03bb=2 with algebraic multiplicity 2 and geometric multiplicity 1, what does the eigenspace look like?</li> <li>Why can't eigenspaces from different eigenvalues overlap (except at the origin)?</li> <li>What is the maximum possible dimension for an eigenspace of a 4\u00d74 matrix?</li> </ol>"},{"location":"sims/eigenspace-visualization/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/eigenvalue-applications/","title":"Eigenvalue Applications Map","text":"<p>Run the Applications Map Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/eigenvalue-applications/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive infographic illustrates how eigenanalysis\u2014the study of eigenvalues and eigenvectors\u2014is fundamental to numerous applications across machine learning, artificial intelligence, and science.</p> <p>Featured Applications:</p> Application Uses Key Insight PCA Covariance eigenvectors Directions of maximum variance PageRank Dominant eigenvector Power iteration at scale Neural Networks Weight eigenvalues Gradient stability Spectral Clustering Laplacian eigenvectors Graph-based clustering Quantum Computing Observable eigenvalues Measurement outcomes Recommenders SVD/Matrix factorization Low-rank approximation"},{"location":"sims/eigenvalue-applications/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over nodes to highlight connections</li> <li>Click nodes to see detailed information</li> <li>Click \u2715 or outside the panel to close details</li> </ol>"},{"location":"sims/eigenvalue-applications/#why-eigenanalysis-matters","title":"Why Eigenanalysis Matters","text":"<p>Every application in this map relies on the fundamental concepts from this chapter:</p> <ul> <li>PCA uses the spectral theorem for symmetric matrices</li> <li>PageRank uses power iteration for the dominant eigenvector</li> <li>Neural network stability depends on eigenvalue magnitudes</li> <li>Spectral clustering uses the Fiedler vector (2nd eigenvector)</li> <li>Quantum computing represents measurements as Hermitian operators</li> <li>Recommender systems use eigendecomposition for matrix factorization</li> </ul>"},{"location":"sims/eigenvalue-applications/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/eigenvalue-applications/main.html\" height=\"522px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/eigenvalue-applications/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/eigenvalue-applications/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Connect eigenanalysis concepts to real-world applications</li> <li>Explain why eigenvalues are crucial for system stability</li> <li>Identify which eigenanalysis technique applies to different problems</li> </ol>"},{"location":"sims/eigenvalue-applications/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Application matching: For each technique learned, identify which application uses it</li> <li>Deep dive: Choose one application and research its eigenvalue usage in detail</li> <li>Cross-connections: Find connections between applications (e.g., PCA and recommenders both use decomposition)</li> </ol>"},{"location":"sims/eigenvalue-applications/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does Google's PageRank use power iteration instead of computing all eigenvalues?</li> <li>How does PCA use the spectral theorem?</li> <li>What eigenvalue property determines whether a neural network suffers from vanishing gradients?</li> </ol>"},{"location":"sims/eigenvalue-applications/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/eigenvector-transformation/","title":"Eigenvector Transformation Visualization","text":"<p>Run the Eigenvector Transformation Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/eigenvector-transformation/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization demonstrates the fundamental concept of eigenvectors: special vectors that maintain their direction under a linear transformation. When matrix A transforms a vector v, most vectors change both direction and magnitude. However, eigenvectors only scale by their corresponding eigenvalue \u03bb.</p> <p>Key Features:</p> <ul> <li>Draggable vector: Move the blue vector to explore how different vectors transform</li> <li>Eigenvector detection: When you align with an eigenvector direction, both vectors glow green</li> <li>Editable matrix: Click matrix cells to enter custom values</li> <li>Animation: Watch the transformation animate smoothly</li> <li>Real-time computation: See Av computed for any vector position</li> </ul>"},{"location":"sims/eigenvector-transformation/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the blue vector around the coordinate plane</li> <li>Watch the red/green vector show the transformed result Av</li> <li>When the vectors align (same direction), you've found an eigenvector!</li> <li>Click matrix cells to edit values and explore different transformations</li> <li>Toggle \"Show Eigenvectors\" to see the eigenvector directions as dashed lines</li> <li>Click \"Animate Transform\" to see the transformation animated</li> </ol>"},{"location":"sims/eigenvector-transformation/#mathematical-background","title":"Mathematical Background","text":"<p>The eigen equation is: Av = \u03bbv</p> <p>Where: - A is a 2\u00d72 matrix - v is the eigenvector (non-zero) - \u03bb is the eigenvalue (scalar)</p> <p>This equation states that when we apply transformation A to eigenvector v, the result is simply v scaled by \u03bb. The default matrix [[2, 1], [1, 2]] has eigenvalues \u03bb\u2081 = 3 and \u03bb\u2082 = 1.</p>"},{"location":"sims/eigenvector-transformation/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/eigenvector-transformation/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/eigenvector-transformation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/eigenvector-transformation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify eigenvectors by observing which vectors maintain their direction under transformation</li> <li>Understand the relationship between eigenvalues and scaling factors</li> <li>Explain why eigenvectors are the \"natural axes\" of a linear transformation</li> </ol>"},{"location":"sims/eigenvector-transformation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Eigenvector Hunt: Find both eigenvector directions for the default matrix by dragging the vector</li> <li>Eigenvalue Verification: When on an eigenvector, verify that the scale factor equals the eigenvalue</li> <li>Matrix Exploration: Try different matrices and predict how many real eigenvectors they have</li> <li>Rotation Matrix: Enter [[0, -1], [1, 0]] - why does it have no real eigenvectors?</li> </ol>"},{"location":"sims/eigenvector-transformation/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For matrix [[3, 0], [0, 2]], what are the eigenvector directions? Why?</li> <li>If a vector is scaled by factor 5 when transformed, what can you say about the eigenvalue?</li> <li>Can a non-zero vector ever transform to zero? What would this imply about the eigenvalue?</li> </ol>"},{"location":"sims/eigenvector-transformation/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/embedding-space-visualizer/","title":"Embedding Space Visualizer","text":"<p>Run the Embedding Space Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/embedding-space-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how words are represented as vectors in an embedding space. In real language models like Word2Vec, GloVe, or transformer embeddings, words with similar meanings cluster together in high-dimensional space.</p> <p>This simplified 2D visualization shows:</p> <ul> <li>Word Clusters: Words from similar categories (colors, animals, countries, numbers, food) naturally group together</li> <li>Semantic Relationships: Click on any word and use \"Find Similar\" to see its nearest neighbors</li> <li>Vector Space Structure: The coordinate system shows how embeddings map discrete words to continuous vectors</li> </ul>"},{"location":"sims/embedding-space-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Pan: Click and drag on empty space to move around the embedding space</li> <li>Zoom: Use the slider or mouse wheel to zoom in/out</li> <li>Select Word: Click on any word point to select it</li> <li>Find Neighbors: With a word selected, click \"Find Similar\" to highlight the 5 nearest words</li> <li>Show Labels: Toggle the checkbox to display all word labels</li> </ol>"},{"location":"sims/embedding-space-visualizer/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/embedding-space-visualizer/#embeddings","title":"Embeddings","text":"<p>An embedding maps discrete tokens (like words) to continuous vectors:</p> \\[e: \\{1, 2, \\ldots, V\\} \\to \\mathbb{R}^d\\]"},{"location":"sims/embedding-space-visualizer/#semantic-similarity","title":"Semantic Similarity","text":"<p>Words with similar meanings have vectors that are close in the embedding space. This proximity can be measured using:</p> <ul> <li>Euclidean distance: \\(d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2\\)</li> <li>Cosine similarity: \\(\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\\)</li> </ul>"},{"location":"sims/embedding-space-visualizer/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand how words are represented as vectors in embedding space</li> <li>Observe how semantic relationships manifest as geometric proximity</li> <li>Explore the concept of word clusters and nearest neighbors</li> </ul> <p>Activities:</p> <ol> <li>Identify which word clusters are most distinct from each other</li> <li>Find a word whose nearest neighbors come from different categories</li> <li>Discuss why certain words might be positioned between clusters</li> </ol> <p>Assessment:</p> <ul> <li>Can students explain why similar words cluster together?</li> <li>Can students predict which words will be nearest neighbors?</li> </ul>"},{"location":"sims/embedding-space-visualizer/#references","title":"References","text":"<ul> <li>Word2Vec Paper - Original word embedding method</li> <li>GloVe: Global Vectors - Stanford NLP word vectors</li> <li>Chapter 11: Generative AI and LLMs</li> </ul>"},{"location":"sims/epipolar-geometry/","title":"Epipolar Geometry Visualizer","text":"<p>Run the Epipolar Geometry Visualizer Fullscreen</p> <p>Edit the Epipolar Geometry Visualizer with the p5.js editor</p>"},{"location":"sims/epipolar-geometry/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates epipolar geometry in stereo vision. See how the epipolar constraint restricts correspondence search to a line, and understand the relationship between disparity and depth.</p>"},{"location":"sims/epipolar-geometry/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Baseline: Change the distance between cameras</li> <li>Move Point: Adjust the 3D point's X and Z position</li> <li>Show Epipolar Plane: Visualize the plane through cameras and point</li> <li>Show Multiple Lines: See the pattern of epipolar lines</li> <li>Drag to Rotate: Change the 3D view angle</li> </ol>"},{"location":"sims/epipolar-geometry/#key-concepts","title":"Key Concepts","text":"<p>Epipolar Constraint: For corresponding points p and p': \\(\\(\\mathbf{p'}^T \\mathbf{F} \\mathbf{p} = 0\\)\\)</p> <p>Depth from Disparity (rectified stereo): \\(\\(Z = \\frac{f \\cdot b}{d}\\)\\)</p> Term Symbol Meaning Baseline b Distance between camera centers Disparity d Horizontal shift between corresponding points Epipole e Where baseline intersects image plane Epipolar line l Line where epipolar plane intersects image"},{"location":"sims/epipolar-geometry/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand the epipolar constraint and its geometric meaning - Relate disparity to depth in stereo vision - Identify epipoles and epipolar lines - Apply the fundamental matrix concept</p>"},{"location":"sims/epipolar-geometry/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/epipolar-geometry/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Epipolar geometry constrains where corresponding points can appear. Instead of searching the entire image, we only search along a line.</p>"},{"location":"sims/epipolar-geometry/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Move the 3D point and observe how projected points move</li> <li>Note that both points stay on the green epipolar line</li> <li>Increase baseline - disparity increases, depth estimation improves</li> <li>Move point closer (decrease Z) - disparity increases</li> </ol>"},{"location":"sims/epipolar-geometry/#key-insight","title":"Key Insight","text":"<p>Larger baseline improves depth resolution but makes stereo matching harder due to increased appearance change.</p>"},{"location":"sims/epipolar-geometry/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Epipolar Geometry - Wikipedia</li> </ul>"},{"location":"sims/euler-angles-visualizer/","title":"Euler Angles Visualizer","text":"<p>Run the Euler Angles Visualizer Fullscreen</p> <p>Edit the Euler Angles Visualizer with the p5.js editor</p>"},{"location":"sims/euler-angles-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates how Euler angles work to represent 3D rotations. See how yaw, pitch, and roll combine to orient an object in space, and observe the gimbal lock problem as pitch approaches \u00b190\u00b0.</p>"},{"location":"sims/euler-angles-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Yaw (\u03c8): Rotate about the Z axis (-180\u00b0 to 180\u00b0)</li> <li>Adjust Pitch (\u03b8): Rotate about the Y axis (-90\u00b0 to 90\u00b0)</li> <li>Adjust Roll (\u03c6): Rotate about the X axis (-180\u00b0 to 180\u00b0)</li> <li>Select Convention: Choose ZYX, XYZ, or ZXZ rotation order</li> <li>Animate Sequence: Watch each rotation applied step by step</li> <li>Drag to Rotate: Click and drag to change the view angle</li> </ol>"},{"location":"sims/euler-angles-visualizer/#key-concepts","title":"Key Concepts","text":"Euler Angle Axis Range Aviation Term Yaw (\u03c8) Z -180\u00b0 to 180\u00b0 Heading Pitch (\u03b8) Y -90\u00b0 to 90\u00b0 Attitude Roll (\u03c6) X -180\u00b0 to 180\u00b0 Bank"},{"location":"sims/euler-angles-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand how three sequential rotations define any 3D orientation - Apply the correct rotation order for different conventions - Recognize the signs of approaching gimbal lock - Compute the rotation matrix from Euler angles</p>"},{"location":"sims/euler-angles-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/euler-angles-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Euler angles are intuitive for human understanding but have mathematical limitations. They're widely used in aviation, robotics, and animation.</p>"},{"location":"sims/euler-angles-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Set all angles to zero, then increase yaw - observe rotation about Z</li> <li>Add pitch - notice how the rotation axis changes</li> <li>Finally add roll - see the complete orientation</li> <li>Use \"Animate Sequence\" to watch rotations applied in order</li> </ol>"},{"location":"sims/euler-angles-visualizer/#gimbal-lock-demonstration-5-minutes","title":"Gimbal Lock Demonstration (5 minutes)","text":"<ol> <li>Set pitch to 85\u00b0</li> <li>Notice the warning indicator</li> <li>Try adjusting yaw and roll - observe reduced control</li> </ol>"},{"location":"sims/euler-angles-visualizer/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Euler Angles - Wikipedia</li> </ul>"},{"location":"sims/filter-effects-gallery/","title":"Filter Effects Gallery","text":"<p>Run the Filter Effects Gallery Fullscreen</p> <p>Edit the Filter Effects Gallery with the p5.js editor</p>"},{"location":"sims/filter-effects-gallery/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim displays the same source image processed through six different filters simultaneously, enabling direct comparison of their effects.</p>"},{"location":"sims/filter-effects-gallery/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Choose from Edge Pattern, Gradient, Checkerboard, Circle, or Random</p> </li> <li> <p>Show Difference: Toggle to see how each filter changes pixels relative to the original (gray = no change, bright = increased, dark = decreased)</p> </li> <li> <p>Show Kernels: Display the 3\u00d73 kernel matrices used by each filter</p> </li> </ol>"},{"location":"sims/filter-effects-gallery/#filters-explained","title":"Filters Explained","text":"Filter Effect Kernel Property Original No change Identity matrix Box Blur Simple averaging All values equal, sum to 1 Gaussian Smooth blur Weighted by distance from center Sharpen Enhance edges Positive center, negative surroundings Edge Detect boundaries Sum of values equals 0 Emboss 3D relief effect Asymmetric weights create shadow"},{"location":"sims/filter-effects-gallery/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Compare effects of different convolution kernels - Identify which filter to use for specific image processing tasks - Understand how kernel values relate to visual effects - Evaluate filter effectiveness on different image types</p>"},{"location":"sims/filter-effects-gallery/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/filter-effects-gallery/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Different kernels produce dramatically different effects. Today we'll see why by comparing them side-by-side.</p>"},{"location":"sims/filter-effects-gallery/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with Edge Pattern - observe how Edge and Sharpen filters respond to boundaries</li> <li>Try Gradient - see how blur smooths transitions while edge detection finds them</li> <li>Use Checkerboard - notice how small patterns interact with filter size</li> </ol>"},{"location":"sims/filter-effects-gallery/#analysis-activity-10-minutes","title":"Analysis Activity (10 minutes)","text":"<ol> <li>Enable \"Show difference\" - gray means no change</li> <li>Compare Box Blur vs Gaussian - why does Gaussian look smoother?</li> <li>Look at kernel values - connect positive/negative to bright/dark effects</li> </ol>"},{"location":"sims/filter-effects-gallery/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why do blur kernels sum to 1?</li> <li>Why does the edge detection kernel sum to 0?</li> <li>What happens when filter size matches feature size?</li> </ul>"},{"location":"sims/filter-effects-gallery/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Image Filter Gallery - Wikipedia</li> </ul>"},{"location":"sims/forward-propagation/","title":"Forward Propagation","text":"<p>Run the Forward Propagation Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/forward-propagation/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows exactly how data flows through a neural network, step by step. Watch as inputs are transformed through matrix multiplications and activation functions to produce outputs.</p>"},{"location":"sims/forward-propagation/#the-forward-propagation-algorithm","title":"The Forward Propagation Algorithm","text":"<p>For each layer \\(l\\) from 1 to L:</p> <ol> <li>Linear Step: \\(\\mathbf{z}^{[l]} = W^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\\)</li> <li>Activation Step: \\(\\mathbf{a}^{[l]} = \\sigma^{[l]}(\\mathbf{z}^{[l]})\\)</li> </ol>"},{"location":"sims/forward-propagation/#interactive-features","title":"Interactive Features","text":"<ul> <li>Next Step: Advance one computation at a time</li> <li>Auto Run: Watch the propagation animate automatically</li> <li>Speed Control: Adjust animation speed</li> <li>Input Sliders: Change input values and restart</li> <li>Reset: Reinitialize with new random weights</li> </ul>"},{"location":"sims/forward-propagation/#what-youll-see","title":"What You'll See","text":"<ul> <li>Yellow nodes: Currently being computed</li> <li>Colored nodes: Values already computed (green=input, blue=hidden, red=output)</li> <li>Gray nodes: Not yet computed</li> <li>Weight labels: Shown on connections during computation</li> <li>Matrix equation: Full computation displayed at bottom</li> </ul>"},{"location":"sims/forward-propagation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/forward-propagation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Trace data flow through a neural network layer by layer</li> <li>Perform matrix multiplication for the linear step z = Wa + b</li> <li>Apply activation functions elementwise</li> <li>Verify dimension compatibility at each step</li> </ol>"},{"location":"sims/forward-propagation/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Manual Verification: Pause at each step and verify the z values by hand</li> <li>Input Exploration: Try different input values and predict the output</li> <li>Dimension Tracking: For each step, verify that matrix dimensions are compatible</li> <li>ReLU vs Sigmoid: Notice which activation is used where (ReLU=hidden, sigmoid=output)</li> </ol>"},{"location":"sims/forward-propagation/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we alternate between linear and nonlinear operations?</li> <li>What would happen if we removed all activation functions?</li> <li>How does the matrix multiplication \\(Wa\\) combine information from the previous layer?</li> <li>Why might the output use sigmoid instead of ReLU?</li> </ol>"},{"location":"sims/forward-propagation/#mathematical-details","title":"Mathematical Details","text":"<p>For the network 2 \u2192 3 \u2192 2:</p> <ul> <li>Layer 1: \\(W^{[1]} \\in \\mathbb{R}^{3\\times2}\\), \\(\\mathbf{b}^{[1]} \\in \\mathbb{R}^{3}\\)</li> <li>Layer 2: \\(W^{[2]} \\in \\mathbb{R}^{2\\times3}\\), \\(\\mathbf{b}^{[2]} \\in \\mathbb{R}^{2}\\)</li> </ul>"},{"location":"sims/forward-propagation/#references","title":"References","text":"<ul> <li>Goodfellow et al. (2016). Deep Learning, Chapter 6.5: Forward Propagation</li> <li>Nielsen (2015). Neural Networks and Deep Learning, Chapter 2</li> </ul>"},{"location":"sims/four-subspaces/","title":"Four Fundamental Subspaces Visualizer","text":"<p>Run the Four Fundamental Subspaces Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/four-subspaces/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization illustrates the Fundamental Theorem of Linear Algebra, showing the four fundamental subspaces associated with any matrix A (m x n):</p>"},{"location":"sims/four-subspaces/#domain-rn","title":"Domain (R^n)","text":"<ul> <li>Row Space: The span of the rows of A (dimension = rank)</li> <li>Null Space: Vectors x where Ax = 0 (dimension = n - rank)</li> </ul>"},{"location":"sims/four-subspaces/#codomain-rm","title":"Codomain (R^m)","text":"<ul> <li>Column Space: The span of the columns of A (dimension = rank)</li> <li>Left Null Space: Vectors y where A^T y = 0 (dimension = m - rank)</li> </ul>"},{"location":"sims/four-subspaces/#key-relationships","title":"Key Relationships","text":"<p>The four subspaces satisfy these fundamental properties:</p> \\[\\text{Row Space} \\perp \\text{Null Space}\\] \\[\\text{Column Space} \\perp \\text{Left Null Space}\\] \\[\\dim(\\text{Row Space}) + \\dim(\\text{Null Space}) = n\\] \\[\\dim(\\text{Column Space}) + \\dim(\\text{Left Null Space}) = m\\] <p>The transformation A maps the Row Space onto the Column Space, and maps the Null Space to the zero vector.</p>"},{"location":"sims/four-subspaces/#how-to-use","title":"How to Use","text":"<ol> <li>Edit the matrix A using the input fields (up to 4x4 matrix)</li> <li>Click \"Compute Subspaces\" to calculate and display all four subspaces</li> <li>Toggle \"Show Basis Vectors\" to see the basis for each subspace</li> <li>Toggle \"Verify Orthogonality\" to check that orthogonal pairs have zero dot product</li> <li>Use the Highlight slider to focus on one subspace at a time:</li> <li>0 = All subspaces</li> <li>1 = Row Space</li> <li>2 = Column Space</li> <li>3 = Null Space</li> <li>4 = Left Null Space</li> </ol>"},{"location":"sims/four-subspaces/#key-observations","title":"Key Observations","text":"<ul> <li>Rank determines all dimensions: Once you know rank(A), you know the dimension of all four subspaces</li> <li>Orthogonal complements: Row space and null space are orthogonal complements in R^n; column space and left null space are orthogonal complements in R^m</li> <li>Rank-deficient matrices: When rank &lt; min(m,n), the null space and/or left null space are nontrivial</li> <li>Full rank: When rank = min(m,n), either null space or left null space reduces to {0}</li> </ul>"},{"location":"sims/four-subspaces/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/four-subspaces/main.html\" height=\"542px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/four-subspaces/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/four-subspaces/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify and describe all four fundamental subspaces of a matrix</li> <li>Verify the orthogonality relationships between subspace pairs</li> <li>Apply the rank-nullity theorem to determine subspace dimensions</li> <li>Explain how the transformation A relates the domain and codomain subspaces</li> </ol>"},{"location":"sims/four-subspaces/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify dimensions: For various matrices, confirm rank + nullity = n</li> <li>Orthogonality test: Pick vectors from row space and null space, verify dot product = 0</li> <li>Mapping exploration: Trace how a row space vector maps to column space, and how a null space vector maps to zero</li> <li>Rank variations: Compare subspaces for full rank vs rank-deficient matrices</li> </ol>"},{"location":"sims/four-subspaces/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If a 3x4 matrix has rank 2, what are the dimensions of all four subspaces?</li> <li>Why must the row space and null space be orthogonal?</li> <li>If Ax = b has a solution, what does this say about b and the column space?</li> <li>How does the left null space relate to the solvability of A^T y = c?</li> </ol>"},{"location":"sims/four-subspaces/#references","title":"References","text":"<ul> <li>Chapter 8: Vector Spaces and Subspaces</li> <li>Strang, G. \"The Fundamental Theorem of Linear Algebra\"</li> <li>Rank-Nullity Theorem</li> </ul>"},{"location":"sims/fourier-transform-visualizer/","title":"Fourier Transform Visualizer","text":"<p>Run the Fourier Transform Visualizer Fullscreen</p> <p>Edit the Fourier Transform Visualizer with the p5.js editor</p>"},{"location":"sims/fourier-transform-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates the 2D Discrete Fourier Transform, showing how spatial patterns map to frequency representations and how frequency filtering affects images.</p>"},{"location":"sims/fourier-transform-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Different patterns produce characteristic frequency spectra</p> </li> <li> <p>Apply Frequency Filter:</p> </li> <li>Low-pass: Keeps smooth areas, removes high frequencies (blurs)</li> <li>High-pass: Keeps edges, removes low frequencies (edge enhancement)</li> <li> <p>Band-pass: Keeps specific frequency range</p> </li> <li> <p>Adjust Cutoff: Control the filter radius in frequency space</p> </li> </ol>"},{"location":"sims/fourier-transform-visualizer/#panels-explained","title":"Panels Explained","text":"Panel Shows Original Image Spatial domain input Magnitude Spectrum Frequency content (log-scaled for visibility) Phase Spectrum Spatial structure information (color-coded) Filtered Image Reconstruction after frequency filtering"},{"location":"sims/fourier-transform-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand the relationship between spatial patterns and frequency spectra - Recognize that periodic patterns create discrete frequency peaks - Apply frequency filtering for image processing - Explain why phase carries more structural information than magnitude</p>"},{"location":"sims/fourier-transform-visualizer/#key-insights","title":"Key Insights","text":"<ul> <li>Vertical stripes \u2192 Horizontal line in spectrum (perpendicular!)</li> <li>More stripes \u2192 Peaks farther from center (higher frequency)</li> <li>Edges \u2192 Broad frequency content (sharp transitions need many frequencies)</li> <li>Phase surprisingly carries most perceptual information</li> </ul>"},{"location":"sims/fourier-transform-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/fourier-transform-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>The Fourier transform decomposes an image into sine wave components. Every image is a sum of 2D sinusoids of different frequencies and orientations.</p>"},{"location":"sims/fourier-transform-visualizer/#pattern-spectrum-mapping-10-minutes","title":"Pattern-Spectrum Mapping (10 minutes)","text":"<ol> <li>Start with Vertical Stripes - observe the horizontal line in spectrum</li> <li>Switch to Horizontal Stripes - the line rotates 90 degrees</li> <li>Try Checkerboard - see the diagonal peaks</li> <li>Single Edge - notice the broad frequency spread</li> </ol>"},{"location":"sims/fourier-transform-visualizer/#frequency-filtering-10-minutes","title":"Frequency Filtering (10 minutes)","text":"<ol> <li>Apply Low-pass filter - smooth areas preserved, edges blur</li> <li>Apply High-pass filter - only edges remain</li> <li>Adjust cutoff to see gradual effect</li> </ol>"},{"location":"sims/fourier-transform-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why does the spectrum have the pattern perpendicular to the image stripes?</li> <li>What does the DC component (center point) represent?</li> <li>Why would JPEG compression work in frequency domain?</li> </ul>"},{"location":"sims/fourier-transform-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Discrete Fourier Transform - Wikipedia</li> </ul>"},{"location":"sims/gaussian-elimination/","title":"Gaussian Elimination Visualizer","text":"<p>Run the Gaussian Elimination MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/gaussian-elimination/#about-this-microsim","title":"About This MicroSim","text":"<p>Watch the Gaussian elimination algorithm solve a system of linear equations step by step. Each operation is explained, helping you understand not just what happens but why.</p> <p>Algorithm Phases:</p> <ol> <li>Forward Elimination: Create zeros below each pivot to reach row echelon form</li> <li>Back Substitution: Solve for variables starting from the last row</li> </ol>"},{"location":"sims/gaussian-elimination/#how-to-use","title":"How to Use","text":"<ol> <li>Choose an Example: Select a system size from the dropdown</li> <li>Step Through: Click \"Next Step\" to advance one operation at a time</li> <li>Auto Solve: Click \"Auto Solve\" to watch the algorithm run automatically</li> <li>Adjust Speed: Use the slider to control auto-solve speed</li> <li>Reset: Start over with the current or a new example</li> </ol>"},{"location":"sims/gaussian-elimination/#visual-guide","title":"Visual Guide","text":"Element Meaning Yellow circle Current pivot position Yellow row highlight Row containing the pivot Blue row highlight Row being modified Green box Final solution"},{"location":"sims/gaussian-elimination/#the-algorithm","title":"The Algorithm","text":"<p>Forward Elimination:</p> <ol> <li>Find a non-zero pivot in the current column</li> <li>Swap rows if necessary to position the pivot</li> <li>Use row addition to create zeros below the pivot</li> <li>Move to the next column and repeat</li> </ol> <p>Back Substitution:</p> <ol> <li>Start with the last row</li> <li>Solve for the variable in that row</li> <li>Substitute the known value into the row above</li> <li>Repeat until all variables are found</li> </ol>"},{"location":"sims/gaussian-elimination/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/gaussian-elimination/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Execute the Gaussian elimination algorithm correctly</li> <li>Identify pivot positions and understand their role</li> <li>Explain why each row operation is performed</li> <li>Apply back substitution to find the solution</li> </ol>"},{"location":"sims/gaussian-elimination/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Predict the Next Step: Pause before each step and predict what operation will be performed</li> <li>Manual Verification: After auto-solve completes, verify the solution by substitution</li> <li>Compare Approaches: Note which pivots the algorithm chooses and discuss alternatives</li> <li>Error Analysis: What happens if we skip a step or make an error?</li> </ol>"},{"location":"sims/gimbal-lock-demo/","title":"Gimbal Lock Demonstration","text":"<p>Run the Gimbal Lock Demo Fullscreen</p> <p>Edit the Gimbal Lock Demo with the p5.js editor</p>"},{"location":"sims/gimbal-lock-demo/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates gimbal lock using a physical gimbal mechanism with three nested rings. Experience firsthand how the system loses a degree of freedom when the middle ring (pitch) reaches \u00b190\u00b0.</p>"},{"location":"sims/gimbal-lock-demo/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Outer Ring (Yaw): Red ring rotates about vertical axis</li> <li>Adjust Middle Ring (Pitch): Green ring rotates about horizontal axis</li> <li>Adjust Inner Ring (Roll): Blue ring rotates about the pointing axis</li> <li>Go to Gimbal Lock: Click to set pitch to 90\u00b0 instantly</li> <li>Observe the Effect: At gimbal lock, yaw and roll control the same motion</li> </ol>"},{"location":"sims/gimbal-lock-demo/#key-concepts","title":"Key Concepts","text":"Pitch Angle Degrees of Freedom Status 0\u00b0 to 79\u00b0 3 Normal operation 80\u00b0 to 88\u00b0 3 Warning zone 89\u00b0 to 90\u00b0 2 Gimbal lock!"},{"location":"sims/gimbal-lock-demo/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Visualize how gimbal lock occurs physically - Understand why two axes align at the singularity - Experience the loss of controllability at gimbal lock - Recognize why quaternions were developed as an alternative</p>"},{"location":"sims/gimbal-lock-demo/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/gimbal-lock-demo/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Gimbal lock is not a bug\u2014it's a fundamental property of using three sequential rotations. Any Euler angle representation has this singularity.</p>"},{"location":"sims/gimbal-lock-demo/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with all angles at zero</li> <li>Slowly increase pitch while adjusting yaw and roll</li> <li>Notice how the red and blue rings approach alignment</li> <li>At 90\u00b0 pitch, try changing both yaw and roll\u2014they produce the same motion!</li> </ol>"},{"location":"sims/gimbal-lock-demo/#historical-context","title":"Historical Context","text":"<p>Apollo 11's guidance system nearly experienced gimbal lock. The astronauts received \"gimbal lock\" warnings when the lunar module's orientation approached the singularity.</p>"},{"location":"sims/gimbal-lock-demo/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Gimbal Lock - Wikipedia</li> </ul>"},{"location":"sims/gradient-descent/","title":"Gradient Descent Interactive Visualizer","text":"<p>Run the Gradient Descent Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/gradient-descent/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates gradient descent optimization, the foundational algorithm for training neural networks and many machine learning models. Watch how the algorithm navigates the loss surface to find minimum values.</p> \\[\\theta_{t+1} = \\theta_t - \\alpha \\nabla J(\\theta_t)\\] <p>Where: - \\(\\theta\\) are the parameters being optimized - \\(\\alpha\\) is the learning rate - \\(\\nabla J\\) is the gradient of the loss function</p>"},{"location":"sims/gradient-descent/#key-features","title":"Key Features","text":"<ul> <li>Contour Plot View: See optimization path on 2D contour lines</li> <li>3D Surface View: Visualize the loss landscape in three dimensions</li> <li>Gradient Arrows: See the direction of steepest descent at each point</li> <li>Loss History Plot: Track how loss decreases over iterations</li> <li>Multiple Loss Functions: Explore different optimization landscapes</li> </ul>"},{"location":"sims/gradient-descent/#interactive-controls","title":"Interactive Controls","text":"Control Function Learning Rate Slider Adjust step size (0.001 to 1.0, log scale) Step Button Execute single gradient descent iteration Run/Pause Button Start or stop continuous optimization Reset Button Return to initial starting point Loss Function Selector Choose between Quadratic, Rosenbrock, or Saddle Toggle 3D/Contour Switch between visualization modes Click on Contour Set a new starting point Drag in 3D View Rotate camera angle"},{"location":"sims/gradient-descent/#loss-functions","title":"Loss Functions","text":""},{"location":"sims/gradient-descent/#quadratic-bowl","title":"Quadratic Bowl","text":"\\[J(\\theta_1, \\theta_2) = \\theta_1^2 + \\theta_2^2\\] <p>The simplest loss surface with a single global minimum at the origin. Good for demonstrating basic gradient descent behavior.</p>"},{"location":"sims/gradient-descent/#rosenbrock-function","title":"Rosenbrock Function","text":"\\[J(\\theta_1, \\theta_2) = (1 - \\theta_1)^2 + 5(\\theta_2 - \\theta_1^2)^2\\] <p>A classic optimization test function with a narrow curved valley. The global minimum is at \\((1, 1)\\). Demonstrates how gradient descent can struggle with ill-conditioned problems.</p>"},{"location":"sims/gradient-descent/#saddle-point","title":"Saddle Point","text":"\\[J(\\theta_1, \\theta_2) = \\theta_1^2 - \\theta_2^2 + 0.05\\theta_2^4\\] <p>Shows a saddle point at the origin where gradient descent can get stuck. Important for understanding challenges in high-dimensional optimization.</p>"},{"location":"sims/gradient-descent/#learning-rate-effects","title":"Learning Rate Effects","text":""},{"location":"sims/gradient-descent/#too-small-001","title":"Too Small (&lt; 0.01)","text":"<ul> <li>Very slow convergence</li> <li>Many iterations needed</li> <li>Stable but inefficient</li> </ul>"},{"location":"sims/gradient-descent/#just-right-001-03","title":"Just Right (0.01 - 0.3)","text":"<ul> <li>Smooth convergence</li> <li>Efficient path to minimum</li> <li>Stable oscillations dampen</li> </ul>"},{"location":"sims/gradient-descent/#too-large-05","title":"Too Large (&gt; 0.5)","text":"<ul> <li>Oscillation around minimum</li> <li>Possible divergence</li> <li>May never converge</li> </ul>"},{"location":"sims/gradient-descent/#how-to-use","title":"How to Use","text":"<ol> <li>Start with Quadratic Bowl to see ideal gradient descent behavior</li> <li>Adjust learning rate and observe convergence speed</li> <li>Set learning rate too high (&gt; 0.5) to see oscillation/divergence</li> <li>Try Rosenbrock to see the challenge of narrow valleys</li> <li>Click different starting points to explore convergence basins</li> <li>Switch to 3D view to better understand the loss landscape</li> </ol>"},{"location":"sims/gradient-descent/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain how gradient descent updates parameters using gradients</li> <li>Predict how learning rate affects convergence behavior</li> <li>Identify when gradient descent will converge, oscillate, or diverge</li> <li>Understand why some loss surfaces are harder to optimize</li> <li>Recognize the role of learning rate in optimization stability</li> </ul>"},{"location":"sims/gradient-descent/#observations-to-make","title":"Observations to Make","text":""},{"location":"sims/gradient-descent/#convergence-patterns","title":"Convergence Patterns","text":"<ul> <li>On the quadratic bowl, what learning rate gives fastest convergence?</li> <li>How does the path differ between small and large learning rates?</li> </ul>"},{"location":"sims/gradient-descent/#challenging-surfaces","title":"Challenging Surfaces","text":"<ul> <li>On Rosenbrock, why does optimization slow down in the valley?</li> <li>Can gradient descent escape the saddle point origin?</li> </ul>"},{"location":"sims/gradient-descent/#iteration-counting","title":"Iteration Counting","text":"<ul> <li>How many iterations to converge with \\(\\alpha = 0.1\\) vs \\(\\alpha = 0.01\\)?</li> <li>When does a higher learning rate actually increase total iterations?</li> </ul>"},{"location":"sims/gradient-descent/#mathematical-background","title":"Mathematical Background","text":"<p>The gradient descent update rule:</p> \\[\\theta_{new} = \\theta_{old} - \\alpha \\cdot \\nabla J(\\theta_{old})\\] <p>For a 2D loss function:</p> \\[\\nabla J = \\begin{bmatrix} \\frac{\\partial J}{\\partial \\theta_1} \\\\ \\frac{\\partial J}{\\partial \\theta_2} \\end{bmatrix}\\] <p>The gradient points in the direction of steepest ascent, so we subtract it to descend toward the minimum.</p>"},{"location":"sims/gradient-descent/#convergence-condition","title":"Convergence Condition","text":"<p>Gradient descent converges when \\(|\\nabla J| &lt; \\epsilon\\) for some small threshold \\(\\epsilon\\).</p>"},{"location":"sims/gradient-descent/#connection-to-neural-networks","title":"Connection to Neural Networks","text":"<p>This 2D visualization represents the core idea behind training neural networks:</p> <ul> <li>Neural networks have millions of parameters (instead of just 2)</li> <li>The loss function measures prediction error</li> <li>Backpropagation computes the gradient efficiently</li> <li>Stochastic Gradient Descent (SGD) uses batches for efficiency</li> </ul> <p>The learning rate is one of the most important hyperparameters in deep learning!</p>"},{"location":"sims/gradient-descent/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/gradient-descent/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask: \"How do we find the minimum of a function when we can only compute its value and gradient at a point?\"</p>"},{"location":"sims/gradient-descent/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<ol> <li>Start with quadratic bowl, show gradient descent converging</li> <li>Demonstrate effect of learning rate (too small, too large)</li> <li>Show Rosenbrock function and the challenge of valleys</li> <li>Explain connection to neural network training</li> </ol>"},{"location":"sims/gradient-descent/#hands-on-exploration-10-minutes","title":"Hands-on Exploration (10 minutes)","text":"<p>Have students: 1. Find the learning rate that converges fastest on quadratic 2. Make gradient descent diverge 3. Find a starting point on Rosenbrock that converges slowly 4. Count iterations to convergence with different learning rates</p>"},{"location":"sims/gradient-descent/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is choosing the right learning rate so important?</li> <li>What happens geometrically when learning rate is too large?</li> <li>How might adaptive learning rates help?</li> </ol>"},{"location":"sims/gradient-descent/#extensions","title":"Extensions","text":"<ul> <li>Momentum: Add velocity to escape local minima</li> <li>Adam Optimizer: Adaptive learning rates per parameter</li> <li>Learning Rate Schedules: Decrease learning rate over time</li> <li>Stochastic Gradient Descent: Add noise to gradients</li> </ul>"},{"location":"sims/gradient-descent/#references","title":"References","text":"<ul> <li>Chapter 10: Optimization and Gradient Descent</li> <li>3Blue1Brown: Neural Networks</li> <li>Goodfellow et al., \"Deep Learning\" Chapter 8</li> </ul>"},{"location":"sims/gram-schmidt/","title":"Gram-Schmidt Process Visualizer","text":"<p>Run the Gram-Schmidt Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/gram-schmidt/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the Gram-Schmidt orthonormalization process in 3D, showing how to construct orthonormal vectors from an arbitrary set of linearly independent vectors.</p> <p>The algorithm takes vectors a\u2081, a\u2082, a\u2083 and produces orthonormal vectors q\u2081, q\u2082, q\u2083 that span the same subspace.</p>"},{"location":"sims/gram-schmidt/#key-features","title":"Key Features","text":"<ul> <li>3D Visualization: See vectors in 3D space with rotation</li> <li>Step-by-step Execution: Watch each phase of the algorithm</li> <li>Projection Visualization: See projections being subtracted (cyan lines)</li> <li>Right-angle Indicators: Confirm orthogonality between q vectors</li> <li>R Matrix Values: See the R matrix being built</li> </ul>"},{"location":"sims/gram-schmidt/#the-algorithm","title":"The Algorithm","text":"<p>For each vector a\u2c7c (j = 1, 2, 3):</p> <ol> <li> <p>Project: Compute projections onto all existing q vectors    \\(\\(\\text{proj}_{q_i}(a_j) = (q_i^T a_j) q_i\\)\\)</p> </li> <li> <p>Subtract: Remove all projections to get v    \\(\\(v_j = a_j - \\sum_{i=1}^{j-1} (q_i^T a_j) q_i\\)\\)</p> </li> <li> <p>Normalize: Scale v to unit length    \\(\\(q_j = \\frac{v_j}{\\|v_j\\|}\\)\\)</p> </li> </ol>"},{"location":"sims/gram-schmidt/#how-to-use","title":"How to Use","text":"<ol> <li>Click Step to advance through algorithm phases</li> <li>Use Run All to automatically step through</li> <li>Drag to rotate the 3D view</li> <li>Toggle Show Projections to see what's being subtracted</li> <li>Watch the R matrix values accumulate</li> </ol>"},{"location":"sims/gram-schmidt/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Execute the Gram-Schmidt process step by step</li> <li>Visualize how projections remove components in existing directions</li> <li>Understand why the resulting vectors are orthonormal</li> <li>Connect Gram-Schmidt to QR decomposition</li> </ul>"},{"location":"sims/gram-schmidt/#visual-elements","title":"Visual Elements","text":"Element Color Meaning Dashed lines Faded colors Original vectors a\u2081, a\u2082, a\u2083 Solid thick lines Bright colors Orthonormal vectors q\u2081, q\u2082, q\u2083 Orange line Orange Current working vector v Cyan lines Cyan Projection components L-shapes Gray Right-angle indicators"},{"location":"sims/gram-schmidt/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/gram-schmidt/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask students: \"If we have a basis for a subspace, how can we find an orthonormal basis for the same subspace?\"</p> <p>Explain that orthonormal bases make many computations easier (projections, least squares, etc.).</p>"},{"location":"sims/gram-schmidt/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<p>Walk through the algorithm together:</p> <ol> <li>First vector: Simply normalize a\u2081 to get q\u2081</li> <li>Second vector:</li> <li>Project a\u2082 onto q\u2081</li> <li>Subtract to get component perpendicular to q\u2081</li> <li>Normalize to get q\u2082</li> <li>Third vector:</li> <li>Project a\u2083 onto both q\u2081 and q\u2082</li> <li>Subtract both projections</li> <li>Normalize to get q\u2083</li> </ol>"},{"location":"sims/gram-schmidt/#key-insights","title":"Key Insights","text":"<ul> <li>Each subtraction removes the component in an already-covered direction</li> <li>The remainder is perpendicular to all previous q vectors</li> <li>Normalizing ensures unit length</li> </ul>"},{"location":"sims/gram-schmidt/#practice-10-minutes","title":"Practice (10 minutes)","text":"<p>Have students:</p> <ol> <li>Predict which direction v will point before clicking</li> <li>Verify orthogonality using the right-angle indicators</li> <li>Check that |q\u1d62| = 1 for all i</li> </ol>"},{"location":"sims/gram-schmidt/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we subtract projections instead of adding them?</li> <li>What would happen if the original vectors were linearly dependent?</li> <li>How does this relate to QR decomposition?</li> </ol>"},{"location":"sims/gram-schmidt/#mathematical-connection-to-qr","title":"Mathematical Connection to QR","text":"<p>The R matrix values shown are exactly the entries of R in A = QR:</p> <ul> <li>Diagonal entries r\u2c7c\u2c7c = ||v\u2c7c|| (the normalization factor)</li> <li>Off-diagonal entries r\u1d62\u2c7c = q\u1d62\u1d40a\u2c7c (the projection coefficients)</li> </ul>"},{"location":"sims/gram-schmidt/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - QR Decomposition section</li> <li>Chapter 8: Vector Spaces and Inner Products - Orthogonalization</li> <li>3Blue1Brown: Gram-Schmidt</li> </ul>"},{"location":"sims/gram-schmidt-ch8/","title":"Gram-Schmidt Step-by-Step Visualizer","text":"<p>Run the Gram-Schmidt Step-by-Step Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js Editor</p>"},{"location":"sims/gram-schmidt-ch8/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization provides a detailed step-by-step walkthrough of the Gram-Schmidt orthonormalization process in 3D. Unlike the Chapter 7 overview, this version shows each individual projection computation and subtraction, making it ideal for understanding the mechanics of the algorithm.</p>"},{"location":"sims/gram-schmidt-ch8/#key-features","title":"Key Features","text":"<ul> <li>Detailed Step Control: Watch each projection, subtraction, and normalization step individually</li> <li>Customizable Input Vectors: Adjust all three input vectors using sliders</li> <li>Projection Visualization: See each projection vector being computed and subtracted</li> <li>Residual Display: View the intermediate u vectors before normalization</li> <li>Right-Angle Indicators: Confirm orthogonality between output vectors</li> <li>Linear Dependence Warning: Get notified if vectors become nearly dependent</li> </ul>"},{"location":"sims/gram-schmidt-ch8/#visual-elements","title":"Visual Elements","text":"Element Color Meaning Dashed gray lines Gray (faded) Input vectors v1, v2, v3 Solid colored lines Red/Green/Blue Orthonormal output vectors q1, q2, q3 Orange line Orange Intermediate u vector (before normalization) Cyan lines Cyan Projection components L-shapes Gray Right-angle indicators between q vectors"},{"location":"sims/gram-schmidt-ch8/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Input Vectors: Use the sliders on the right to set custom input vectors</li> <li>Click \"Next Step\": Advance through each phase of the algorithm</li> <li>Use \"Auto Run\": Automatically step through with adjustable speed</li> <li>Toggle Options:</li> <li>\"Show All Projections\" displays all computed projections</li> <li>\"Show Residual\" displays the intermediate u vector</li> <li>Drag to Rotate: Click and drag on the 3D view to change perspective</li> <li>Reset: Return to the initial state</li> </ol>"},{"location":"sims/gram-schmidt-ch8/#the-algorithm-in-detail","title":"The Algorithm in Detail","text":""},{"location":"sims/gram-schmidt-ch8/#for-the-first-vector-v1","title":"For the First Vector (v1)","text":"<p>Simply normalize: \\(\\(q_1 = \\frac{v_1}{\\|v_1\\|}\\)\\)</p>"},{"location":"sims/gram-schmidt-ch8/#for-subsequent-vectors-v2-v3","title":"For Subsequent Vectors (v2, v3, ...)","text":"<p>Step 1 - Compute Projections: For each existing q vector, compute: \\(\\(\\text{proj}_{q_i}(v_j) = (v_j \\cdot q_i) \\cdot q_i\\)\\)</p> <p>Step 2 - Subtract Projections: Remove all components in existing directions: \\(\\(u_j = v_j - \\sum_{i=1}^{j-1} \\text{proj}_{q_i}(v_j)\\)\\)</p> <p>Step 3 - Normalize: Scale to unit length: \\(\\(q_j = \\frac{u_j}{\\|u_j\\|}\\)\\)</p>"},{"location":"sims/gram-schmidt-ch8/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Execute each step of Gram-Schmidt with full understanding</li> <li>Visualize how projections identify components parallel to existing vectors</li> <li>Understand why subtraction yields orthogonal residuals</li> <li>Recognize when vectors are linearly dependent</li> <li>Connect the geometric intuition to the algebraic formulas</li> </ul>"},{"location":"sims/gram-schmidt-ch8/#why-this-matters","title":"Why This Matters","text":"<p>Gram-Schmidt orthonormalization is fundamental to:</p> <ul> <li>QR Decomposition: The Q matrix columns are exactly the output of Gram-Schmidt</li> <li>Least Squares: Computing orthonormal bases simplifies projection calculations</li> <li>Numerical Stability: Orthonormal vectors avoid numerical conditioning issues</li> <li>Signal Processing: Orthogonal bases are essential for Fourier analysis</li> </ul>"},{"location":"sims/gram-schmidt-ch8/#practice-exercises","title":"Practice Exercises","text":"<ol> <li> <p>Standard Basis: Set v1=(1,0,0), v2=(0,1,0), v3=(0,0,1). Verify the output matches the input.</p> </li> <li> <p>Dependent Vectors: Try v1=(1,0,0), v2=(2,0,0), v3=(0,1,0). What happens with the linear dependence?</p> </li> <li> <p>General Case: Use the default vectors and predict each projection direction before clicking \"Next Step\".</p> </li> <li> <p>Orthogonality Check: After completion, mentally verify that each pair of q vectors is perpendicular.</p> </li> </ol>"},{"location":"sims/gram-schmidt-ch8/#mathematical-connection","title":"Mathematical Connection","text":"<p>The Gram-Schmidt process produces the QR factorization:</p> \\[A = QR\\] <p>where: - A is the matrix with input vectors as columns - Q is the matrix with orthonormal vectors q1, q2, q3 as columns - R is upper triangular with entries:   - Diagonal: \\(r_{jj} = \\|u_j\\|\\) (the normalization factors)   - Off-diagonal: \\(r_{ij} = q_i \\cdot v_j\\) (the projection coefficients)</p>"},{"location":"sims/gram-schmidt-ch8/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - QR Decomposition</li> <li>Chapter 8: Vector Spaces and Inner Products - Orthogonalization</li> <li>3Blue1Brown: Gram-Schmidt Process</li> </ul>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>Open Learning Graph Viewer</p> <p>This interactive viewer allows you to explore the learning graph for Applied Linear Algebra for AI and Machine Learning.</p>"},{"location":"sims/graph-viewer/#features","title":"Features","text":"<ul> <li>Search: Type in the search box to find specific concepts</li> <li>Category Filtering: Use checkboxes to show/hide concept categories</li> <li>Interactive Navigation: Click and drag to explore, scroll to zoom</li> <li>Statistics: View real-time counts of visible nodes and edges</li> </ul>"},{"location":"sims/graph-viewer/#using-the-viewer","title":"Using the Viewer","text":"<ol> <li> <p>Search for Concepts: Start typing in the search box to find concepts. Click on a result to focus on that node.</p> </li> <li> <p>Filter by Category: Use the category checkboxes in the sidebar to show or hide groups of related concepts. Use \"Check All\" or \"Uncheck All\" for bulk operations.</p> </li> <li> <p>Navigate the Graph:</p> </li> <li>Drag to pan around the graph</li> <li>Scroll to zoom in and out</li> <li> <p>Click on a node to select it and highlight its connections</p> </li> <li> <p>View Statistics: The sidebar shows counts of visible nodes, edges, and foundational concepts.</p> </li> </ol>"},{"location":"sims/graph-viewer/#graph-structure","title":"Graph Structure","text":"<ul> <li>Foundational Concepts: Prerequisites with no dependencies</li> <li>Advanced Concepts: Topics that build on multiple prerequisites</li> <li>Edges: Arrows point from a concept to its prerequisites</li> </ul>"},{"location":"sims/graph-viewer/#launch-the-viewer","title":"Launch the Viewer","text":""},{"location":"sims/hessian-curvature-visualizer/","title":"Hessian and Curvature Visualizer","text":"<p>Run the Hessian Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/hessian-curvature-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This 3D visualization demonstrates the connection between the Hessian matrix eigenvalues and the geometric curvature of a function surface. The Hessian is a matrix of second partial derivatives that captures how a function curves in different directions.</p>"},{"location":"sims/hessian-curvature-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Select a Function: Choose from bowl-shaped (minimum), saddle, or elongated surfaces</li> <li>Rotate View: Click and drag to rotate the 3D view</li> <li>Zoom: Use mouse scroll wheel to zoom in/out</li> <li>Move Point: Use arrow keys to move the analysis point</li> <li>Toggle Options: Show/hide principal curvature directions and quadratic approximation</li> </ol>"},{"location":"sims/hessian-curvature-visualizer/#understanding-the-display","title":"Understanding the Display","text":"<ul> <li>Surface: The colored 3D surface of f(x,y)</li> <li>Contour Lines: Level curves on the base plane</li> <li>Red Sphere: Current point on the surface</li> <li>Green Arrows: Principal directions with positive eigenvalues (curving up)</li> <li>Red Arrows: Principal directions with negative eigenvalues (curving down)</li> <li>Info Panel: Shows eigenvalues and point classification</li> </ul>"},{"location":"sims/hessian-curvature-visualizer/#eigenvalue-interpretation","title":"Eigenvalue Interpretation","text":"Eigenvalue Pattern Curvature Type Optimization Both positive (\u03bb\u2081, \u03bb\u2082 &gt; 0) Bowl (minimum) Local minimum Both negative (\u03bb\u2081, \u03bb\u2082 &lt; 0) Dome (maximum) Local maximum Mixed signs Saddle point Neither"},{"location":"sims/hessian-curvature-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/hessian-curvature-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Connect Hessian eigenvalues to geometric curvature</li> <li>Identify minima, maxima, and saddle points from eigenvalue signs</li> <li>Visualize how curvature varies with direction</li> </ul>"},{"location":"sims/hessian-curvature-visualizer/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Bowl Functions: Start with x\u00b2+y\u00b2 and observe both positive eigenvalues</li> <li>Saddle Point: Switch to x\u00b2-y\u00b2 and see how mixed eigenvalues create a saddle</li> <li>Elongation: Compare x\u00b2+0.1y\u00b2 to see how different eigenvalue magnitudes affect shape</li> <li>Approximation: Enable \"Show Quadratic Approx\" to see how the Hessian provides a local approximation</li> </ol>"},{"location":"sims/hessian-curvature-visualizer/#references","title":"References","text":"<ul> <li>Boyd &amp; Vandenberghe, Convex Optimization, Chapter 3</li> <li>Wikipedia: Hessian Matrix</li> </ul>"},{"location":"sims/homogeneous-systems/","title":"Homogeneous System Explorer","text":"<p>Run the Homogeneous Systems MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/homogeneous-systems/#about-this-microsim","title":"About This MicroSim","text":"<p>A homogeneous system has the form \\(Ax = 0\\) where all right-hand side values are zero. These systems have special properties:</p> <ul> <li>The trivial solution \\(x = 0\\) always exists</li> <li>Nontrivial solutions may or may not exist</li> <li>The set of all solutions forms a subspace (the null space of \\(A\\))</li> </ul>"},{"location":"sims/homogeneous-systems/#key-concepts","title":"Key Concepts","text":"<p>When do nontrivial solutions exist?</p> Condition Nontrivial Solutions? Number of variables &gt; number of equations Always rank(A) &lt; number of variables Yes rank(A) = number of variables No (only trivial) <p>Null Space Dimension:</p> \\[\\text{dim(null space)} = n - \\text{rank}(A)\\] <p>where \\(n\\) is the number of variables (columns of \\(A\\)).</p>"},{"location":"sims/homogeneous-systems/#visualizing-the-null-space","title":"Visualizing the Null Space","text":"Null Space Dimension Geometric Shape 0 Just the origin (point) 1 Line through origin 2 Plane through origin k k-dimensional subspace through origin"},{"location":"sims/homogeneous-systems/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/homogeneous-systems/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify when homogeneous systems have nontrivial solutions</li> <li>Calculate the dimension of the null space from the rank</li> <li>Visualize the null space as a geometric subspace</li> <li>Understand why the null space always passes through the origin</li> </ol>"},{"location":"sims/homogeneous-systems/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Compare Examples: Switch between presets and observe how the null space dimension changes</li> <li>Verify the Formula: For each example, verify that dim(null space) = n - rank</li> <li>Geometric Intuition: Drag to rotate the 3D view and understand the null space shape</li> <li>More Variables: Notice that when n &gt; m, nontrivial solutions are guaranteed</li> </ol>"},{"location":"sims/homography-demo/","title":"Homography Transformation Demo","text":"<p>Run the Homography Transformation Demo Fullscreen</p> <p>Edit the Homography Transformation Demo with the p5.js editor</p>"},{"location":"sims/homography-demo/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates homography transformations - projective mappings between planes. Drag the corner points to see how a 3\u00d73 matrix can create perspective effects.</p>"},{"location":"sims/homography-demo/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select a Preset: Try Identity, Perspective, Rotation, or Shear transformations</p> </li> <li> <p>Drag Corners: Click and drag the red corner points to create custom transformations</p> </li> <li> <p>Toggle Grid: Show/hide the transformation grid to see how lines map</p> </li> <li> <p>View Matrix: See the computed homography matrix and its interpretation</p> </li> </ol>"},{"location":"sims/homography-demo/#key-concepts","title":"Key Concepts","text":"Transformation DOF Preserves Matrix Structure Translation 2 Everything [I | t] Euclidean 3 Distances [R | t] Affine 6 Parallelism [A | t] Projective 8 Straight lines Full 3\u00d73"},{"location":"sims/homography-demo/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand homographies as mappings between projective planes - Apply the 3\u00d73 homography matrix to transform points - Recognize the hierarchy of 2D transformations - Use homographies for perspective correction applications</p>"},{"location":"sims/homography-demo/#the-mathematics","title":"The Mathematics","text":"<p>Homography Equation (homogeneous coordinates):</p> \\[\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} \\sim \\mathbf{H} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\] <p>Cartesian Coordinates:</p> \\[x' = \\frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}\\] \\[y' = \\frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}\\]"},{"location":"sims/homography-demo/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/homography-demo/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Homographies model the mapping when a camera views a planar surface from different angles. Unlike affine transformations, they can make parallel lines converge.</p>"},{"location":"sims/homography-demo/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with Identity - source equals destination</li> <li>Try Perspective - observe converging lines (railroad tracks effect)</li> <li>Use Rotation - note that angles are preserved</li> <li>Test Shear - parallel lines remain parallel (affine subset)</li> </ol>"},{"location":"sims/homography-demo/#interactive-exercise-10-minutes","title":"Interactive Exercise (10 minutes)","text":"<ol> <li>Drag corners to simulate \"looking at a sign from an angle\"</li> <li>Try to make a trapezoid into a rectangle (perspective correction)</li> <li>Observe the matrix values as you drag</li> </ol>"},{"location":"sims/homography-demo/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why does a homography have 8 DOF, not 9?</li> <li>What real-world situations create homography relationships?</li> <li>How would you correct a photo of a tilted document?</li> </ul>"},{"location":"sims/homography-demo/#applications","title":"Applications","text":"<ul> <li>Panorama Stitching: Align overlapping photos</li> <li>Document Scanning: Straighten tilted captures</li> <li>Augmented Reality: Place virtual objects on surfaces</li> <li>Sports Graphics: Insert ads on playing fields</li> </ul>"},{"location":"sims/homography-demo/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Homography - Wikipedia</li> </ul>"},{"location":"sims/image-matrix-visualizer/","title":"Image Matrix Visualizer","text":"<p>Run the Image Matrix Visualizer Fullscreen</p> <p>Edit the Image Matrix Visualizer with the p5.js editor</p>"},{"location":"sims/image-matrix-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates the fundamental concept that digital images are stored as matrices of numbers. Each cell in the matrix represents a pixel, with values from 0 (black) to 255 (white).</p>"},{"location":"sims/image-matrix-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Select an Image Pattern: Use the dropdown to choose from:</li> <li>Checkerboard: Alternating black and white pixels</li> <li>Gradient: Values increasing from left to right</li> <li>Simple Shape: A circular pattern with varying intensities</li> <li> <p>Random: Randomly generated pixel values</p> </li> <li> <p>Zoom: Adjust the slider to enlarge or reduce the image display</p> </li> <li> <p>Hover Interaction: Move your mouse over either the image or matrix to highlight the corresponding cell in both views</p> </li> <li> <p>Edit Mode: Click \"Enable Edit Mode\" to modify individual pixel values by clicking on matrix cells</p> </li> </ol>"},{"location":"sims/image-matrix-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand that digital images are stored as matrices of numbers - Connect pixel intensity values (0-255) to visual brightness - Recognize how matrix dimensions correspond to image resolution - Modify pixel values and observe the visual result</p>"},{"location":"sims/image-matrix-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/image-matrix-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Explain that every digital image is fundamentally a grid of numbers. Each number represents how bright that tiny square (pixel) should be displayed.</p>"},{"location":"sims/image-matrix-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with the Checkerboard pattern - ask students what values they expect for black vs. white pixels</li> <li>Switch to Gradient - observe how values increase from 0 to 255</li> <li>Try Random - see how arbitrary values create noise-like patterns</li> </ol>"},{"location":"sims/image-matrix-visualizer/#interactive-exercise-10-minutes","title":"Interactive Exercise (10 minutes)","text":"<ol> <li>Enable Edit Mode</li> <li>Challenge students to create simple patterns (diagonal line, border, letter shape)</li> <li>Discuss: What's the smallest value change you can perceive?</li> </ol>"},{"location":"sims/image-matrix-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why do computers use numbers 0-255? (Hint: 8 bits = 256 values)</li> <li>What would a 1000x1000 pixel image require for storage?</li> <li>How might color images differ from grayscale?</li> </ul>"},{"location":"sims/image-matrix-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Digital Image Basics - Wikipedia</li> </ul>"},{"location":"sims/inner-product-visualizer/","title":"Inner Product Space Visualizer","text":"<p>Run the Inner Product Space Visualizer Fullscreen</p> <p>Edit the Inner Product Space Visualizer with the p5.js editor</p>"},{"location":"sims/inner-product-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand how different inner products define different geometric structures. The same pair of vectors can have different lengths and angles depending on which inner product is used. By adjusting the weight matrix W, students can see how the unit \"circle\" transforms into an ellipse and how this affects all geometric measurements.</p> <p>Learning Objective: Visualize how different inner products define different notions of length and angle.</p>"},{"location":"sims/inner-product-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Vectors: Click and drag the endpoints of vectors u (blue) and v (red) to any position</li> <li>Select Inner Product Type:</li> <li>Standard dot product: The familiar \\(\\langle u, v \\rangle = u_1 v_1 + u_2 v_2\\)</li> <li>Weighted (diagonal W): Adjust w11 and w22 to scale x and y differently</li> <li>Weighted (general W): Add w12 to create off-diagonal terms (cross-coupling)</li> <li>Observe the Unit Ball: The green shape shows all vectors of norm 1 under the current inner product</li> <li>Check Cauchy-Schwarz: The inequality \\(|\\langle u, v \\rangle| \\leq \\|u\\| \\|v\\|\\) is verified in real-time</li> </ol>"},{"location":"sims/inner-product-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/inner-product-visualizer/#standard-inner-product-dot-product","title":"Standard Inner Product (Dot Product)","text":"\\[\\langle u, v \\rangle = u^T v = u_1 v_1 + u_2 v_2\\] <ul> <li>Unit ball is a circle</li> <li>Angle and length match our geometric intuition</li> <li>Norm: \\(\\|v\\| = \\sqrt{v_1^2 + v_2^2}\\)</li> </ul>"},{"location":"sims/inner-product-visualizer/#weighted-inner-product","title":"Weighted Inner Product","text":"\\[\\langle u, v \\rangle_W = u^T W v\\] <p>where W is a symmetric positive definite matrix: \\(\\(W = \\begin{pmatrix} w_{11} &amp; w_{12} \\\\ w_{12} &amp; w_{22} \\end{pmatrix}\\)\\)</p> <ul> <li>Unit ball becomes an ellipse</li> <li>The ellipse orientation depends on the eigenvectors of W</li> <li>The ellipse semi-axes are related to \\(1/\\sqrt{\\lambda_i}\\) where \\(\\lambda_i\\) are eigenvalues</li> </ul>"},{"location":"sims/inner-product-visualizer/#induced-norm","title":"Induced Norm","text":"\\[\\|v\\|_W = \\sqrt{\\langle v, v \\rangle_W} = \\sqrt{v^T W v}\\]"},{"location":"sims/inner-product-visualizer/#angle-between-vectors","title":"Angle Between Vectors","text":"\\[\\cos(\\theta) = \\frac{\\langle u, v \\rangle_W}{\\|u\\|_W \\|v\\|_W}\\] <p>The angle depends on which inner product we use!</p>"},{"location":"sims/inner-product-visualizer/#cauchy-schwarz-inequality","title":"Cauchy-Schwarz Inequality","text":"<p>For any inner product: \\(\\(|\\langle u, v \\rangle| \\leq \\|u\\| \\cdot \\|v\\|\\)\\)</p> <p>Equality holds if and only if u and v are linearly dependent (collinear).</p> <p>This fundamental inequality is verified visually in the simulation. Try making the vectors collinear to see equality achieved.</p>"},{"location":"sims/inner-product-visualizer/#why-inner-products-matter","title":"Why Inner Products Matter","text":"<ul> <li>Machine Learning: Different inner products measure similarity differently</li> <li>Optimization: The metric tensor defines gradient descent geometry</li> <li>Physics: Minkowski inner product in special relativity</li> <li>Statistics: Mahalanobis distance uses covariance-weighted inner product</li> </ul>"},{"location":"sims/inner-product-visualizer/#exploration-activities","title":"Exploration Activities","text":"<ol> <li> <p>Standard vs Weighted: Set u = (2, 0) and v = (0, 2). Note they are perpendicular under standard inner product. Now increase w11 to 2. Are they still perpendicular?</p> </li> <li> <p>Unit Ball Shape: With diagonal W, set w11 = 2 and w22 = 0.5. The unit ball should be an ellipse stretched along the y-axis.</p> </li> <li> <p>Off-Diagonal Effects: With general W, add w12 = 0.5. Notice how the ellipse rotates.</p> </li> <li> <p>Cauchy-Schwarz: Move v to be collinear with u. Watch the inequality become an equality.</p> </li> </ol>"},{"location":"sims/inner-product-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/inner-product-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate linear algebra or advanced high school</p>"},{"location":"sims/inner-product-visualizer/#duration","title":"Duration","text":"<p>20-30 minutes</p>"},{"location":"sims/inner-product-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector operations</li> <li>Dot product basics</li> <li>Matrix-vector multiplication</li> <li>Positive definite matrices (helpful but can be introduced)</li> </ul>"},{"location":"sims/inner-product-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li> <p>Exploration (5-7 min): Use standard inner product, drag vectors, observe angle and norms</p> </li> <li> <p>Introduce Weighting (5-7 min): Switch to diagonal W, adjust w11 and w22, observe unit ball and measurements change</p> </li> <li> <p>General Weights (5-7 min): Enable general W, add w12, see ellipse rotation</p> </li> <li> <p>Cauchy-Schwarz Verification (5 min): Test the inequality with various vector configurations</p> </li> </ol>"},{"location":"sims/inner-product-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li> <p>If two vectors are perpendicular under one inner product, are they perpendicular under all inner products?</p> </li> <li> <p>What happens to the unit ball as w11 approaches 0? Why can't W have a zero eigenvalue?</p> </li> <li> <p>Why must W be positive definite for this to be a valid inner product?</p> </li> <li> <p>How does the weighted inner product relate to the Mahalanobis distance in statistics?</p> </li> </ol>"},{"location":"sims/inner-product-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/inner-product-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/inner-product-visualizer/#references","title":"References","text":"<ol> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.</li> <li>Inner Product Spaces - Wolfram MathWorld</li> <li>Cauchy-Schwarz Inequality - 3Blue1Brown</li> <li>Boyd, S. &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press. (Chapter on norms and inner products)</li> </ol>"},{"location":"sims/kalman-filter/","title":"Kalman Filter Visualizer","text":"<p>Run the Kalman Filter Visualizer Fullscreen</p> <p>Edit the Kalman Filter Visualizer with the p5.js editor</p>"},{"location":"sims/kalman-filter/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the Kalman Filter, the optimal linear estimator for systems with Gaussian noise. The Kalman filter recursively estimates the true state of a dynamic system by combining noisy measurements with a prediction model.</p>"},{"location":"sims/kalman-filter/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/kalman-filter/main.html\"\n        height=\"602px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/kalman-filter/#features","title":"Features","text":"<ul> <li>State Estimation: Green point shows the Kalman filter's best estimate</li> <li>Uncertainty Ellipse: Green ellipse represents 2-sigma uncertainty bounds</li> <li>Measurements: Red markers show noisy position measurements</li> <li>Innovation Vector: Orange dashed line shows measurement-prediction difference</li> <li>Velocity Arrow: Green arrow shows estimated velocity direction</li> <li>Motion Models: Choose between constant velocity, constant acceleration, or random walk</li> </ul>"},{"location":"sims/kalman-filter/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/kalman-filter/#the-kalman-filter-equations","title":"The Kalman Filter Equations","text":"<p>Prediction Step: \\(\\(\\hat{\\mathbf{x}}_k^- = \\mathbf{F}\\hat{\\mathbf{x}}_{k-1}\\)\\) \\(\\(\\mathbf{P}_k^- = \\mathbf{F}\\mathbf{P}_{k-1}\\mathbf{F}^\\top + \\mathbf{Q}\\)\\)</p> <p>Update Step: \\(\\(\\mathbf{K}_k = \\mathbf{P}_k^- \\mathbf{H}^\\top (\\mathbf{H}\\mathbf{P}_k^-\\mathbf{H}^\\top + \\mathbf{R})^{-1}\\)\\) \\(\\(\\hat{\\mathbf{x}}_k = \\hat{\\mathbf{x}}_k^- + \\mathbf{K}_k(\\mathbf{z}_k - \\mathbf{H}\\hat{\\mathbf{x}}_k^-)\\)\\) \\(\\(\\mathbf{P}_k = (\\mathbf{I} - \\mathbf{K}_k\\mathbf{H})\\mathbf{P}_k^-\\)\\)</p>"},{"location":"sims/kalman-filter/#noise-parameters","title":"Noise Parameters","text":"<ul> <li>Process Noise Q: How much random acceleration affects the true motion</li> <li>Measurement Noise R: How noisy the position measurements are</li> </ul>"},{"location":"sims/kalman-filter/#what-to-observe","title":"What to Observe","text":"<ul> <li>Low R, High Q: Trust measurements more \u2192 estimate follows measurements closely</li> <li>High R, Low Q: Trust predictions more \u2192 estimate is smoother</li> <li>Uncertainty ellipse: Grows during prediction, shrinks after measurement update</li> </ul>"},{"location":"sims/kalman-filter/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/kalman-filter/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the predict-update cycle of the Kalman filter</li> <li>Recognize how noise parameters affect estimation quality</li> <li>Visualize uncertainty propagation through covariance</li> </ul>"},{"location":"sims/kalman-filter/#activities","title":"Activities","text":"<ol> <li>Single Step: Click \"Step\" repeatedly to see predict-update cycle</li> <li>Noise Tradeoff: Increase R (measurement noise) and observe smoother but lagged estimates</li> <li>Motion Models: Switch between constant velocity and random walk to see how the model affects predictions</li> <li>Reveal Truth: Toggle \"Show True Position\" to see actual estimation error</li> </ol>"},{"location":"sims/kalman-filter/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does the uncertainty ellipse grow during prediction and shrink during update?</li> <li>What happens to the Kalman gain when measurement noise is very high?</li> <li>How does the motion model assumption affect filter performance?</li> </ol>"},{"location":"sims/kalman-filter/#references","title":"References","text":"<ul> <li>Kalman Filter Wikipedia</li> <li>Understanding the Kalman Filter</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/kernel-range/","title":"Kernel and Range Interactive Visualizer","text":"<p>Run the Kernel and Range Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/kernel-range/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows the two fundamental subspaces of a linear transformation:</p> <ul> <li>Kernel (Null Space): Vectors that map to zero, shown in gray</li> <li>Range (Column Space): All possible outputs, shown in red</li> </ul> <p>The Rank-Nullity Theorem states:</p> \\[\\text{dim(Domain)} = \\text{Rank} + \\text{Nullity}\\] <p>For a 2\u00d72 matrix, this means Rank + Nullity = 2.</p>"},{"location":"sims/kernel-range/#how-to-use","title":"How to Use","text":"<ol> <li>Click \"Full Rank\" to generate an invertible matrix (rank 2, nullity 0)</li> <li>Click \"Rank Deficient\" to generate a rank-1 matrix with nontrivial kernel</li> <li>Toggle \"Show Kernel\" to highlight the null space direction</li> <li>Toggle \"Show Mapping\" to see how vectors map from domain to codomain</li> <li>Click \"Animate\" to watch vectors transform</li> </ol>"},{"location":"sims/kernel-range/#key-observations","title":"Key Observations","text":"<ul> <li>Full rank: Kernel = {0}, Range = all of \u211d\u00b2, transformation is invertible</li> <li>Rank deficient: Kernel is a line, Range is a line, dimension collapses</li> </ul> <p>Watch how vectors in the kernel (gray) all collapse to the origin, while vectors outside the kernel map to the range subspace.</p>"},{"location":"sims/kernel-range/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/kernel-range/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/kernel-range/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/kernel-range/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify the kernel and range of a transformation visually</li> <li>Verify the rank-nullity theorem for 2\u00d72 matrices</li> <li>Explain why rank-deficient transformations are not invertible</li> </ol>"},{"location":"sims/kernel-range/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify theorem: For each matrix, check that rank + nullity = 2</li> <li>Trace vectors: Follow a specific vector from domain to codomain</li> <li>Kernel test: Given a vector, predict if it's in the kernel</li> </ol>"},{"location":"sims/kernel-range/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If a 3\u00d74 matrix has rank 2, what is its nullity?</li> <li>Why can't a transformation with nontrivial kernel be inverted?</li> <li>How are the columns of a matrix related to its range?</li> </ol>"},{"location":"sims/kernel-range/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations - Kernel and Range section</li> <li>Rank-Nullity Theorem</li> </ul>"},{"location":"sims/kkt-conditions-visualizer/","title":"KKT Conditions Visualizer","text":"<p>Run the Visualization Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/kkt-conditions-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the Karush-Kuhn-Tucker (KKT) conditions for optimization with inequality constraints. The KKT conditions generalize Lagrange multipliers to handle inequality constraints.</p>"},{"location":"sims/kkt-conditions-visualizer/#problem-setup","title":"Problem Setup","text":"<ul> <li>Objective: Minimize f(x,y) = (x-2)\u00b2 + (y-2)\u00b2</li> <li>Constraints:<ul> <li>g\u2081: x \u2265 0</li> <li>g\u2082: y \u2265 0</li> <li>g\u2083: x + y \u2264 3</li> </ul> </li> </ul>"},{"location":"sims/kkt-conditions-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Toggle Constraints: Enable/disable constraints with checkboxes</li> <li>Click to Place Point: Click anywhere to set the current point</li> <li>Solve: Click to animate to the optimal point</li> <li>Observe: Watch which constraints become active (orange)</li> </ol>"},{"location":"sims/kkt-conditions-visualizer/#understanding-the-display","title":"Understanding the Display","text":"<ul> <li>Green Shaded Region: Feasible set (where all constraints are satisfied)</li> <li>Blue/Orange Lines: Constraint boundaries (orange = active at optimum)</li> <li>Gray Circles: Contours of the objective function</li> <li>Green Point: Optimal solution</li> <li>Blue/Red Point: Your current point (blue = feasible, red = infeasible)</li> </ul>"},{"location":"sims/kkt-conditions-visualizer/#the-kkt-conditions","title":"The KKT Conditions","text":"<p>For a constrained minimum x*, the KKT conditions state:</p> <ol> <li>Stationarity: \u2207f(x) + \u03a3\u03bc\u1d62\u2207g\u1d62(x) = 0</li> <li>Primal Feasibility: g\u1d62(x*) \u2264 0 for all i</li> <li>Dual Feasibility: \u03bc\u1d62 \u2265 0 for all i</li> <li>Complementary Slackness: \u03bc\u1d62g\u1d62(x*) = 0 for all i</li> </ol>"},{"location":"sims/kkt-conditions-visualizer/#complementary-slackness","title":"Complementary Slackness","text":"<p>The key insight is complementary slackness: for each constraint, either: - The constraint is inactive (g\u1d62 &lt; 0) and its multiplier is zero (\u03bc\u1d62 = 0), OR - The constraint is active (g\u1d62 = 0) and its multiplier may be positive</p> <p>Only constraints that the solution \"touches\" can affect the optimal point.</p>"},{"location":"sims/kkt-conditions-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/kkt-conditions-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand KKT conditions for inequality-constrained optimization</li> <li>Visualize active vs inactive constraints</li> <li>Interpret complementary slackness geometrically</li> </ul>"},{"location":"sims/kkt-conditions-visualizer/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>All Constraints: With all constraints enabled, find the optimal point</li> <li>Remove Constraints: Disable constraints one by one and observe how the optimum shifts</li> <li>Active vs Inactive: Notice which constraints are orange (active) at the solution</li> <li>Infeasible Points: Click outside the feasible region to see infeasibility</li> </ol>"},{"location":"sims/kkt-conditions-visualizer/#references","title":"References","text":"<ul> <li>Boyd &amp; Vandenberghe, Convex Optimization, Chapter 5</li> <li>Wikipedia: Karush-Kuhn-Tucker Conditions</li> </ul>"},{"location":"sims/lagrange-multiplier-geometry/","title":"Lagrange Multiplier Geometry","text":"<p>Run the Visualization Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/lagrange-multiplier-geometry/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the geometric interpretation of Lagrange multipliers. The key insight is that at a constrained optimum, the gradient of the objective function must be parallel to the gradient of the constraint (the constraint normal).</p>"},{"location":"sims/lagrange-multiplier-geometry/#problem-setup","title":"Problem Setup","text":"<ul> <li>Objective: Maximize f(x,y) = x + 2y</li> <li>Constraint: x\u00b2 + y\u00b2 = 4 (circle of radius 2)</li> </ul>"},{"location":"sims/lagrange-multiplier-geometry/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Red Point: Move it along the constraint circle</li> <li>Watch the Arrows: Blue is \u2207f (gradient of objective), Red is \u2207h (constraint normal)</li> <li>Find Optimum: Click the button to animate to the optimal point</li> <li>Toggle Fields: Show gradient and normal fields to see the global picture</li> </ol>"},{"location":"sims/lagrange-multiplier-geometry/#key-insight","title":"Key Insight","text":"<p>At the optimal point:</p> \\[\\nabla f(\\mathbf{x}^*) = \\lambda \\nabla h(\\mathbf{x}^*)\\] <p>The gradients are parallel! This means you cannot improve f while staying on the constraint surface.</p>"},{"location":"sims/lagrange-multiplier-geometry/#understanding-the-visualization","title":"Understanding the Visualization","text":"<ul> <li>Gray Lines: Level curves of f(x,y) = x + 2y (higher = upper-right)</li> <li>Bold Circle: The constraint h(x,y) = x\u00b2 + y\u00b2 - 4 = 0</li> <li>Blue Arrow: Direction of steepest increase of f (\u2207f)</li> <li>Red Arrow: Normal to the constraint (\u2207h)</li> <li>Green Point: At optimum, both gradients are parallel</li> </ul>"},{"location":"sims/lagrange-multiplier-geometry/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/lagrange-multiplier-geometry/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the geometric meaning of Lagrange multipliers</li> <li>Visualize why gradients must be parallel at constrained optima</li> <li>Connect the algebraic condition \u2207f = \u03bb\u2207h to geometry</li> </ul>"},{"location":"sims/lagrange-multiplier-geometry/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Exploration: Drag the point around the entire circle and observe when gradients align</li> <li>Two Optima: Notice there's both a maximum and minimum on the circle</li> <li>Field Visualization: Enable gradient field to see why \u2207f is constant for linear f</li> <li>Lambda Interpretation: Watch how \u03bb changes as you move the point</li> </ol>"},{"location":"sims/lagrange-multiplier-geometry/#references","title":"References","text":"<ul> <li>Boyd &amp; Vandenberghe, Convex Optimization, Chapter 5</li> <li>Wikipedia: Lagrange Multiplier</li> </ul>"},{"location":"sims/latent-space-interpolation/","title":"Latent Space Interpolation Visualizer","text":"<p>Run the Latent Space Interpolation Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/latent-space-interpolation/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates interpolation in latent space, a key technique in generative models. By smoothly moving between two points in latent space, we can generate intermediate samples that smoothly transition between the endpoints.</p>"},{"location":"sims/latent-space-interpolation/#how-to-use","title":"How to Use","text":"<ol> <li>Select Points: Click on shapes to select point A (red highlight) and point B (blue highlight)</li> <li>Adjust t: Use the slider to move along the interpolation path</li> <li>Change Steps: Adjust the number of intermediate samples</li> <li>Method: Switch between linear and spherical (SLERP) interpolation</li> </ol>"},{"location":"sims/latent-space-interpolation/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/latent-space-interpolation/#latent-space","title":"Latent Space","text":"<p>A latent space is a compressed representation where: - Each point corresponds to a potential generated sample - Nearby points produce similar outputs - The space is typically lower-dimensional than data space</p>"},{"location":"sims/latent-space-interpolation/#linear-interpolation","title":"Linear Interpolation","text":"<p>The simplest method walks in a straight line:</p> \\[\\mathbf{z}(t) = (1-t)\\mathbf{z}_1 + t\\mathbf{z}_2, \\quad t \\in [0, 1]\\]"},{"location":"sims/latent-space-interpolation/#spherical-interpolation-slerp","title":"Spherical Interpolation (SLERP)","text":"<p>For normalized latent vectors, SLERP maintains constant magnitude:</p> \\[\\mathbf{z}(t) = \\frac{\\sin((1-t)\\theta)}{\\sin\\theta}\\mathbf{z}_1 + \\frac{\\sin(t\\theta)}{\\sin\\theta}\\mathbf{z}_2\\] <p>where \\(\\theta = \\arccos(\\mathbf{z}_1 \\cdot \\mathbf{z}_2)\\)</p>"},{"location":"sims/latent-space-interpolation/#why-slerp","title":"Why SLERP?","text":"<ul> <li>Linear interpolation can pass through low-density regions</li> <li>SLERP stays on the \"surface\" of the latent manifold</li> <li>Often produces more realistic intermediate samples</li> </ul>"},{"location":"sims/latent-space-interpolation/#applications","title":"Applications","text":"<ul> <li>Image Morphing: Smooth transitions between faces</li> <li>Style Mixing: Blend attributes from different samples</li> <li>Data Augmentation: Generate novel training examples</li> <li>Exploration: Understand what the model has learned</li> </ul>"},{"location":"sims/latent-space-interpolation/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand the concept of latent space in generative models</li> <li>Compare linear vs spherical interpolation methods</li> <li>Predict how generated outputs change along interpolation paths</li> </ul> <p>Activities:</p> <ol> <li>Select two very different shapes and observe the transition</li> <li>Compare linear vs SLERP paths - when do they differ most?</li> <li>Find configurations where interpolation produces unexpected results</li> </ol> <p>Assessment:</p> <ul> <li>Why might linear interpolation produce unrealistic intermediate samples?</li> <li>When would you choose SLERP over linear interpolation?</li> <li>How does the number of interpolation steps affect perceived smoothness?</li> </ul>"},{"location":"sims/latent-space-interpolation/#references","title":"References","text":"<ul> <li>Understanding Latent Space</li> <li>Chapter 11: Generative AI and LLMs</li> <li>Spherical Linear Interpolation</li> </ul>"},{"location":"sims/learning-rate-effect/","title":"Learning Rate Effect on Convergence","text":"<p>Run the Learning Rate Effect MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/learning-rate-effect/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how the choice of learning rate dramatically affects gradient descent optimization. By running three optimizations simultaneously with different learning rates, students can directly observe and compare convergence behaviors.</p> <p>Learning Objective: Understand how learning rate choice affects optimization behavior through side-by-side comparison.</p>"},{"location":"sims/learning-rate-effect/#how-to-use","title":"How to Use","text":"<ol> <li>Click \"Run All\" to start the optimization on all three panels simultaneously</li> <li>Adjust individual learning rates using the sliders below each panel</li> <li>Use preset buttons to quickly set typical scenarios:</li> <li>Too Small: Very slow convergence</li> <li>Just Right: Efficient convergence</li> <li>Too Large: Oscillation or divergence</li> <li>Adjust animation speed to slow down or speed up the visualization</li> <li>Click \"Reset All\" to restart from the initial position</li> </ol>"},{"location":"sims/learning-rate-effect/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/learning-rate-effect/#the-learning-rate-tradeoff","title":"The Learning Rate Tradeoff","text":"<p>The learning rate \\(\\eta\\) (eta) controls the step size in gradient descent:</p> \\[\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\\] <ul> <li>Too small (\\(\\eta &lt; 0.01\\)): Safe but slow, may never reach optimum in reasonable time</li> <li>Just right (\\(\\eta \\approx 0.1\\)): Fast and stable convergence</li> <li>Too large (\\(\\eta &gt; 0.3\\)): May overshoot, oscillate, or diverge entirely</li> </ul>"},{"location":"sims/learning-rate-effect/#visual-indicators","title":"Visual Indicators","text":"<p>Each panel shows:</p> <ul> <li>Contour plot: Elliptical contours of the loss function</li> <li>Path trace: The trajectory taken by gradient descent</li> <li>Status indicator:</li> <li>Converging (blue): Making steady progress</li> <li>Converged (green): Reached the minimum</li> <li>Oscillating (yellow): Bouncing around the minimum</li> <li>Diverging (red): Moving away from the minimum</li> <li>Loss curve: Real-time plot of loss value over iterations</li> <li>Step count: Number of iterations taken</li> </ul>"},{"location":"sims/learning-rate-effect/#loss-function","title":"Loss Function","text":"<p>The visualization uses a quadratic loss function:</p> \\[L(x, y) = x^2 + 3y^2\\] <p>This creates elliptical contours where:</p> <ul> <li>The minimum is at the origin \\((0, 0)\\)</li> <li>The y-direction has a steeper gradient (factor of 3)</li> <li>Different eigenvalues create the classic \"elongated bowl\" optimization challenge</li> </ul>"},{"location":"sims/learning-rate-effect/#why-large-learning-rates-cause-problems","title":"Why Large Learning Rates Cause Problems","text":"<p>For quadratic functions, stability requires:</p> \\[\\eta &lt; \\frac{2}{\\lambda_{max}}\\] <p>where \\(\\lambda_{max}\\) is the largest eigenvalue of the Hessian. In our case:</p> <ul> <li>Hessian eigenvalues: 2 and 6</li> <li>Maximum stable learning rate: \\(\\eta &lt; 2/6 \\approx 0.33\\)</li> </ul> <p>Beyond this threshold, the optimizer overshoots and may diverge.</p>"},{"location":"sims/learning-rate-effect/#mathematical-details","title":"Mathematical Details","text":""},{"location":"sims/learning-rate-effect/#gradient-descent-update","title":"Gradient Descent Update","text":"<p>At each step, the algorithm computes:</p> \\[\\nabla L = \\begin{pmatrix} 2x \\\\ 6y \\end{pmatrix}\\] <p>And updates:</p> \\[\\begin{pmatrix} x_{t+1} \\\\ y_{t+1} \\end{pmatrix} = \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} - \\eta \\begin{pmatrix} 2x_t \\\\ 6y_t \\end{pmatrix}\\]"},{"location":"sims/learning-rate-effect/#convergence-rate","title":"Convergence Rate","text":"<p>For quadratic functions, the convergence rate is:</p> \\[\\|x_{t+1} - x^*\\| \\leq \\left(1 - \\frac{2\\eta\\lambda_{min}\\lambda_{max}}{(\\lambda_{min} + \\lambda_{max})^2}\\right) \\|x_t - x^*\\|\\] <p>The optimal learning rate that minimizes this bound is:</p> \\[\\eta^* = \\frac{2}{\\lambda_{min} + \\lambda_{max}}\\]"},{"location":"sims/learning-rate-effect/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/learning-rate-effect/main.html\"\n        height=\"520px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/learning-rate-effect/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/learning-rate-effect/#grade-level","title":"Grade Level","text":"<p>Undergraduate machine learning or optimization course</p>"},{"location":"sims/learning-rate-effect/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/learning-rate-effect/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of gradient descent</li> <li>Basic calculus (partial derivatives)</li> <li>Familiarity with loss functions</li> </ul>"},{"location":"sims/learning-rate-effect/#learning-activities","title":"Learning Activities","text":"<ol> <li>Initial Exploration (5 min):</li> <li>Run with default settings</li> <li>Observe the three different behaviors</li> <li> <p>Note which optimizer reaches the minimum first</p> </li> <li> <p>Learning Rate Sensitivity (5 min):</p> </li> <li>Use \"Too Large\" preset</li> <li>Watch oscillation and divergence</li> <li> <p>Identify the threshold where behavior changes</p> </li> <li> <p>Finding Optimal Rate (5 min):</p> </li> <li>Manually adjust sliders to find the fastest convergence</li> <li>Compare step counts to convergence</li> <li> <p>Discuss the tradeoff between speed and stability</p> </li> <li> <p>Analysis Discussion (5 min):</p> </li> <li>Why does the y-direction cause more oscillation?</li> <li>How do eigenvalues relate to the optimal learning rate?</li> <li> <p>What happens at the stability boundary?</p> </li> <li> <p>Real-World Connection (5 min):</p> </li> <li>How do modern optimizers (Adam, RMSprop) handle this?</li> <li>Why is learning rate scheduling important?</li> <li>Connection to neural network training</li> </ol>"},{"location":"sims/learning-rate-effect/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the optimizer oscillate more in the y-direction with large learning rates?</li> <li>What is the relationship between the loss function's curvature and the optimal learning rate?</li> <li>How would you design an adaptive learning rate algorithm based on these observations?</li> <li>Why might a learning rate that's \"just right\" for one problem be wrong for another?</li> <li>How do momentum-based optimizers help with oscillation?</li> </ol>"},{"location":"sims/learning-rate-effect/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Predict the behavior given a specific learning rate</li> <li>Calculate the maximum stable learning rate for a given Hessian</li> <li>Design an experiment to find the optimal learning rate empirically</li> <li>Explain the convergence/divergence criteria mathematically</li> </ul>"},{"location":"sims/learning-rate-effect/#connections-to-machine-learning","title":"Connections to Machine Learning","text":""},{"location":"sims/learning-rate-effect/#neural-network-training","title":"Neural Network Training","text":"<ul> <li>Learning rate is one of the most important hyperparameters</li> <li>Too small: training takes forever, may get stuck</li> <li>Too large: loss explodes, training fails</li> <li>Common strategy: start larger, decay over time</li> </ul>"},{"location":"sims/learning-rate-effect/#learning-rate-schedules","title":"Learning Rate Schedules","text":"<ul> <li>Step decay: Reduce by factor after fixed epochs</li> <li>Exponential decay: \\(\\eta_t = \\eta_0 e^{-kt}\\)</li> <li>Cosine annealing: Smooth decrease following cosine curve</li> <li>Warmup: Start small, increase, then decay</li> </ul>"},{"location":"sims/learning-rate-effect/#adaptive-methods","title":"Adaptive Methods","text":"<p>Modern optimizers adapt the learning rate per-parameter: - AdaGrad: Accumulates squared gradients - RMSprop: Exponential moving average of squared gradients - Adam: Combines momentum with adaptive rates</p>"},{"location":"sims/learning-rate-effect/#references","title":"References","text":"<ol> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press. Chapter 8.</li> <li>Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. COMPSTAT.</li> <li>Why Learning Rate is So Important - Jeremy Jordan</li> <li>Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv:1609.04747.</li> </ol>"},{"location":"sims/least-squares-visualizer/","title":"Least Squares Problem Visualizer","text":"<p>Run the Least Squares Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/least-squares-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the least squares problem from two complementary perspectives:</p> <ol> <li>Regression View: See how least squares finds the best-fit line through data points by minimizing the sum of squared residuals</li> <li>Geometric View: Understand least squares as finding the projection of vector b onto the column space of matrix A</li> </ol>"},{"location":"sims/least-squares-visualizer/#key-features","title":"Key Features","text":"<ul> <li>Dual View Modes: Switch between regression and geometric perspectives</li> <li>Interactive Data Points: Drag points in regression view to see solution update</li> <li>Residual Visualization: See vertical distances from points to fitted line</li> <li>3D Projection: Visualize Ax-hat as the closest point in Col(A) to b</li> <li>Error Vector: See e = b - Ax-hat perpendicular to the column space</li> <li>Method Comparison: Compare Normal Equations, QR, and SVD approaches</li> <li>Real-time Updates: Solution updates instantly as you modify inputs</li> </ul>"},{"location":"sims/least-squares-visualizer/#the-least-squares-problem","title":"The Least Squares Problem","text":"<p>Given an overdetermined system Ax = b (more equations than unknowns), we seek x-hat that minimizes:</p> \\[\\|b - Ax\\|^2 = \\sum_{i=1}^{m} (b_i - (Ax)_i)^2\\]"},{"location":"sims/least-squares-visualizer/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>The solution x-hat satisfies the normal equations:</p> \\[A^T A \\hat{x} = A^T b\\] <p>Geometrically:</p> <ul> <li>Ax-hat = projection of b onto Col(A)</li> <li>e = b - Ax-hat is perpendicular to Col(A)</li> <li>x-hat minimizes the length of the error vector</li> </ul>"},{"location":"sims/least-squares-visualizer/#for-linear-regression","title":"For Linear Regression","text":"<p>For fitting y = mx + c to data points (x_i, y_i):</p> \\[A = \\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix}, \\quad b = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad x = \\begin{bmatrix} c \\\\ m \\end{bmatrix}\\]"},{"location":"sims/least-squares-visualizer/#how-to-use","title":"How to Use","text":""},{"location":"sims/least-squares-visualizer/#regression-view","title":"Regression View","text":"<ol> <li>Drag data points to adjust their positions</li> <li>Watch the best-fit line update in real-time</li> <li>Toggle Show Residuals to see vertical error lines</li> <li>Observe the Sum of Squared Residuals (SSR) minimize</li> </ol>"},{"location":"sims/least-squares-visualizer/#geometric-view","title":"Geometric View","text":"<ol> <li>Adjust b vector using sliders</li> <li>Drag to rotate the 3D view</li> <li>See b (blue), Ax-hat (red), and e (green)</li> <li>Verify e is perpendicular to the column space plane</li> </ol>"},{"location":"sims/least-squares-visualizer/#method-comparison","title":"Method Comparison","text":"<p>Select different solution methods to understand their trade-offs:</p> Method Speed Stability Best For Normal Equations Fastest May be unstable Well-conditioned problems QR Decomposition Medium More stable General use SVD Method Slowest Most stable Rank-deficient problems"},{"location":"sims/least-squares-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain least squares as finding the projection onto column space</li> <li>Interpret the error vector as perpendicular to Col(A)</li> <li>Connect the normal equations to the projection formula</li> <li>Apply least squares to linear regression problems</li> <li>Recognize when different solution methods are appropriate</li> </ul>"},{"location":"sims/least-squares-visualizer/#visual-elements","title":"Visual Elements","text":"Element Color Meaning Data points Blue Observed data (regression view) Fitted line Red Least squares solution y-hat = mx + c Residuals Green dashed Vertical errors (y - y-hat) Vector b Blue Observation vector (geometric view) Vector Ax-hat Red Projection onto Col(A) Vector e Green Error = b - Ax-hat Plane Light blue Column space of A"},{"location":"sims/least-squares-visualizer/#mathematical-insights","title":"Mathematical Insights","text":""},{"location":"sims/least-squares-visualizer/#why-perpendicularity","title":"Why Perpendicularity?","text":"<p>The minimum occurs when e is perpendicular to Col(A) because:</p> <ol> <li>Moving Ax-hat in any direction within Col(A) would increase ||e||</li> <li>This is the same as minimizing distance from a point to a plane</li> </ol>"},{"location":"sims/least-squares-visualizer/#the-normal-equations","title":"The Normal Equations","text":"<p>From e perpendicular to Col(A): \\(\\(A^T e = 0\\)\\) \\(\\(A^T(b - A\\hat{x}) = 0\\)\\) \\(\\(A^T A \\hat{x} = A^T b\\)\\)</p>"},{"location":"sims/least-squares-visualizer/#condition-number-warning","title":"Condition Number Warning","text":"<p>If the condition number of A^T A is large (&gt; 100), the problem is ill-conditioned and:</p> <ul> <li>Small changes in data cause large changes in solution</li> <li>QR or SVD methods are more reliable than normal equations</li> </ul>"},{"location":"sims/least-squares-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/least-squares-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Motivate with a real example: \"Given noisy measurements, how do we find the best trend line?\"</p>"},{"location":"sims/least-squares-visualizer/#regression-view-exploration-10-minutes","title":"Regression View Exploration (10 minutes)","text":"<ol> <li>Start with default points</li> <li>Drag points to see line adjust</li> <li>Discuss what \"best fit\" means (minimizing squared errors)</li> <li>Show that residuals are vertical (why not perpendicular to line?)</li> </ol>"},{"location":"sims/least-squares-visualizer/#geometric-view-exploration-10-minutes","title":"Geometric View Exploration (10 minutes)","text":"<ol> <li>Switch to geometric view</li> <li>Explain Col(A) as the space of all possible Ax</li> <li>Show that b is usually not in Col(A)</li> <li>Identify Ax-hat as the closest point in Col(A)</li> <li>Verify e is perpendicular to the plane</li> </ol>"},{"location":"sims/least-squares-visualizer/#connecting-the-views-5-minutes","title":"Connecting the Views (5 minutes)","text":"<ul> <li>Regression: minimize sum of squared residuals</li> <li>Geometric: minimize ||b - Ax||</li> <li>Both are the same optimization problem!</li> </ul>"},{"location":"sims/least-squares-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we square the residuals instead of using absolute values?</li> <li>What would happen if all data points were collinear?</li> <li>When might QR be preferred over normal equations?</li> </ol>"},{"location":"sims/least-squares-visualizer/#references","title":"References","text":"<ul> <li>Chapter 9: Solving Linear Systems - Least Squares section</li> <li>Chapter 7: Matrix Decompositions - QR Decomposition</li> <li>3Blue1Brown: Least Squares</li> </ul>"},{"location":"sims/lidar-point-cloud/","title":"LIDAR Point Cloud Visualizer","text":"<p>Run the LIDAR Point Cloud Visualizer Fullscreen</p> <p>Edit the LIDAR Point Cloud Visualizer with the p5.js editor</p>"},{"location":"sims/lidar-point-cloud/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how LIDAR (Light Detection and Ranging) sensors capture 3D point cloud data for autonomous vehicles and robotics. LIDAR systems emit laser pulses and measure the time for reflections to return, creating a sparse but accurate 3D representation of the environment.</p>"},{"location":"sims/lidar-point-cloud/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/lidar-point-cloud/main.html\"\n        height=\"652px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/lidar-point-cloud/#features","title":"Features","text":"<ul> <li>3D Point Cloud Rendering: Visualize thousands of LIDAR points in an interactive 3D view</li> <li>Ground Segmentation: Toggle ground points (brown) vs object points</li> <li>Object Clustering: See how points are grouped into discrete objects with bounding boxes</li> <li>Distance Rings: Reference circles at 10m, 25m, and 50m from the ego vehicle</li> <li>Color Modes: View points colored by intensity, height, distance, or cluster assignment</li> <li>Interactive Controls: Drag to rotate, scroll to zoom</li> </ul>"},{"location":"sims/lidar-point-cloud/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/lidar-point-cloud/#lidar-data-structure","title":"LIDAR Data Structure","text":"<p>Each LIDAR point contains: - (x, y, z): 3D position in the sensor frame - Intensity: Reflection strength (0-255) - Cluster ID: Assignment to detected object (-1 for ground, -2 for noise)</p>"},{"location":"sims/lidar-point-cloud/#ground-segmentation","title":"Ground Segmentation","text":"<p>The simulation shows how points are classified as ground (low z-values) versus obstacles. Real systems use algorithms like RANSAC plane fitting or grid-based methods.</p>"},{"location":"sims/lidar-point-cloud/#object-clustering","title":"Object Clustering","text":"<p>Nearby points are grouped into clusters representing vehicles, pedestrians, or other obstacles. Each cluster has a bounding box showing its spatial extent.</p>"},{"location":"sims/lidar-point-cloud/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/lidar-point-cloud/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the structure of LIDAR point cloud data</li> <li>Recognize how ground segmentation separates drivable surface from obstacles</li> <li>Visualize how clustering algorithms group points into objects</li> </ul>"},{"location":"sims/lidar-point-cloud/#activities","title":"Activities","text":"<ol> <li>Explore Point Density: Observe how point density decreases with distance</li> <li>Compare Color Modes: Switch between intensity, height, and distance coloring to see different data properties</li> <li>Identify Clusters: Count the number of detected objects and estimate their sizes from bounding boxes</li> <li>Ground vs Objects: Toggle ground points to understand the two-class segmentation problem</li> </ol>"},{"location":"sims/lidar-point-cloud/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does LIDAR point density decrease with distance from the sensor?</li> <li>What information does point intensity provide about the detected surface?</li> <li>How do autonomous vehicles use clustered point clouds for navigation?</li> </ol>"},{"location":"sims/lidar-point-cloud/#references","title":"References","text":"<ul> <li>Velodyne LIDAR Documentation</li> <li>PCL (Point Cloud Library)</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/linear-combination-explorer/","title":"Linear Combination Explorer","text":"<p>Run the Linear Combination Explorer Fullscreen</p>"},{"location":"sims/linear-combination-explorer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand linear combinations by allowing them to adjust scalar coefficients and observe how the result vector changes. The challenge mode tests students' ability to find the right coefficients to reach a target point.</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p>"},{"location":"sims/linear-combination-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Coefficients: Use the c\u2081 and c\u2082 sliders to change the scalar multipliers</li> <li>Drag Basis Vectors: Click and drag the endpoints of v\u2081 (red) and v\u2082 (blue) to change their directions</li> <li>Observe the Result: The green arrow shows c\u2081v\u2081 + c\u2082v\u2082</li> <li>See Tip-to-Tail: Enable \"Show Components\" to see how the scaled vectors add tip-to-tail</li> <li>Challenge Mode: Click \"New Challenge\" to get a target point (yellow star), then find the coefficients to reach it</li> <li>Get Help: Click \"Show Solution\" to see the answer</li> </ol>"},{"location":"sims/linear-combination-explorer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/linear-combination-explorer/#linear-combination","title":"Linear Combination","text":"\\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\] <p>A linear combination is a sum of scalar multiples of vectors. Any vector in the plane can be written as a linear combination of two non-parallel vectors.</p>"},{"location":"sims/linear-combination-explorer/#span","title":"Span","text":"<p>The span of vectors is the set of all possible linear combinations. For two non-parallel vectors in 2D, the span is the entire plane.</p>"},{"location":"sims/linear-combination-explorer/#parallel-vectors","title":"Parallel Vectors","text":"<p>When v\u2081 and v\u2082 are parallel (one is a scalar multiple of the other), their span collapses to a line. The visualization shows this with a warning.</p>"},{"location":"sims/linear-combination-explorer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/linear-combination-explorer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/linear-combination-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/linear-combination-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/linear-combination-explorer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/linear-combination-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication</li> <li>Basic understanding of coordinate systems</li> </ul>"},{"location":"sims/linear-combination-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Set c\u2081 = 1, c\u2082 = 0 and observe: result equals v\u2081</li> <li>Set c\u2081 = 0, c\u2082 = 1 and observe: result equals v\u2082</li> <li> <p>Set c\u2081 = 1, c\u2082 = 1 and observe: tip-to-tail addition</p> </li> <li> <p>Coefficient Investigation (5 min):</p> </li> <li>What happens when c\u2081 = -1?</li> <li>Find coefficients that put the result at (0, 0)</li> <li> <p>Make the result point in the opposite direction of v\u2081</p> </li> <li> <p>Challenge Mode (10 min):</p> </li> <li>Click \"New Challenge\" to get a target</li> <li>Try to reach the target by adjusting only c\u2081 and c\u2082</li> <li>Record how many attempts it takes</li> <li> <p>After solving, verify by checking the math</p> </li> <li> <p>Span Investigation (5 min):</p> </li> <li>Drag v\u2082 to be parallel to v\u2081</li> <li>Notice the \"span is a line\" warning</li> <li>Try to reach targets outside the line - impossible!</li> </ol>"},{"location":"sims/linear-combination-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why can any point in the plane be reached with two non-parallel vectors?</li> <li>What happens to the span when the vectors become parallel?</li> <li>Is the linear combination c\u2081v\u2081 + c\u2082v\u2082 the same as c\u2082v\u2082 + c\u2081v\u2081?</li> <li>How would you find the coefficients algebraically?</li> </ol>"},{"location":"sims/linear-combination-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, calculate the coefficients by solving a system of equations</li> <li>Explain why two parallel vectors only span a line</li> <li>Predict whether a given point is reachable with given basis vectors</li> </ul>"},{"location":"sims/linear-combination-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors - Excellent visual introduction</li> <li>Khan Academy - Linear Combinations and Span</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.3.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 1.3.</li> </ol>"},{"location":"sims/linear-regression/","title":"Linear Regression Interactive Visualizer","text":"<p>Run the Linear Regression Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/linear-regression/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates linear regression as an optimization problem where we find the line that best fits the data by minimizing the sum of squared errors (residuals).</p>"},{"location":"sims/linear-regression/#key-features","title":"Key Features","text":"<ul> <li>Draggable Data Points: Click and drag any point to see how it affects the regression line</li> <li>Manual Parameter Control: Adjust the slope (w) and intercept (b) sliders to explore the loss landscape</li> <li>Fit OLS Button: Instantly compute the optimal parameters using Ordinary Least Squares</li> <li>Residual Visualization: See the vertical distances from each point to the fitted line</li> <li>Loss Surface Heatmap: Visualize the Mean Squared Error as a function of w and b</li> <li>Real-time Statistics: View current and optimal parameters, loss values, and R-squared</li> <li>Add Noise: Introduce random noise to see how it affects the fit</li> </ul>"},{"location":"sims/linear-regression/#the-linear-regression-problem","title":"The Linear Regression Problem","text":"<p>Given data points \\((x_i, y_i)\\) for \\(i = 1, \\ldots, n\\), we want to find the line:</p> \\[\\hat{y} = wx + b\\] <p>that minimizes the sum of squared residuals:</p> \\[L(w, b) = \\sum_{i=1}^{n} (y_i - (wx_i + b))^2\\]"},{"location":"sims/linear-regression/#the-normal-equations","title":"The Normal Equations","text":"<p>The optimal parameters satisfy the normal equations:</p> \\[(X^T X)\\beta = X^T y\\] <p>where:</p> \\[X = \\begin{bmatrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{bmatrix}, \\quad y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad \\beta = \\begin{bmatrix} b \\\\ w \\end{bmatrix}\\]"},{"location":"sims/linear-regression/#closed-form-solution","title":"Closed-Form Solution","text":"<p>The optimal slope and intercept are:</p> \\[w^* = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{n\\sum x_i^2 - (\\sum x_i)^2}\\] \\[b^* = \\bar{y} - w^*\\bar{x}\\]"},{"location":"sims/linear-regression/#how-to-use","title":"How to Use","text":""},{"location":"sims/linear-regression/#exploring-the-loss-surface","title":"Exploring the Loss Surface","text":"<ol> <li>Move the sliders for w (slope) and b (intercept)</li> <li>Watch the red dot move on the loss surface heatmap</li> <li>Notice how the loss increases as you move away from the optimal point (green dot)</li> <li>The bowl shape of the loss surface shows this is a convex optimization problem</li> </ol>"},{"location":"sims/linear-regression/#understanding-residuals","title":"Understanding Residuals","text":"<ol> <li>Enable Residuals checkbox to see vertical error lines</li> <li>Each residual shows the distance from a data point to the fitted line</li> <li>The small squares visualize the squared error being minimized</li> <li>Notice that residuals are vertical (not perpendicular to the line)</li> </ol>"},{"location":"sims/linear-regression/#interactive-exploration","title":"Interactive Exploration","text":"<ol> <li>Drag data points to change the dataset</li> <li>Watch the optimal line (green dashed) update instantly</li> <li>Use Fit OLS to snap to the optimal solution</li> <li>Compare your manual fit to the computed optimal</li> </ol>"},{"location":"sims/linear-regression/#visual-elements","title":"Visual Elements","text":"Element Color Meaning Data points Blue Observed data \\((x_i, y_i)\\) Fitted line Red Current line \\(\\hat{y} = wx + b\\) Optimal line Green dashed OLS solution Residuals Green dashed Vertical errors \\(y_i - \\hat{y}_i\\) Loss surface Blue-White-Red MSE as function of (w, b) Current position Red dot Current (w, b) on loss surface Optimal position Green dot Optimal \\((w^*, b^*)\\)"},{"location":"sims/linear-regression/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain why we minimize squared errors (not absolute errors)</li> <li>Interpret the loss surface as a function of model parameters</li> <li>Understand why the OLS solution is at the minimum of the loss surface</li> <li>Derive and apply the normal equations</li> <li>Calculate and interpret the R-squared goodness-of-fit measure</li> <li>Recognize that vertical residuals differ from perpendicular distances</li> </ul>"},{"location":"sims/linear-regression/#key-insights","title":"Key Insights","text":""},{"location":"sims/linear-regression/#why-squared-errors","title":"Why Squared Errors?","text":"<ol> <li>Differentiable: Allows calculus-based optimization</li> <li>Penalizes large errors: Outliers have significant impact</li> <li>Unique minimum: Convex loss surface guarantees global optimum</li> <li>Statistical properties: MLE under Gaussian noise assumption</li> </ol>"},{"location":"sims/linear-regression/#why-vertical-residuals","title":"Why Vertical Residuals?","text":"<p>Linear regression minimizes errors in y (the dependent variable), not perpendicular distance to the line. This makes sense when:</p> <ul> <li>x is known precisely (independent variable)</li> <li>y has measurement error (dependent variable)</li> </ul> <p>For errors in both variables, use Total Least Squares instead.</p>"},{"location":"sims/linear-regression/#the-r-squared-statistic","title":"The R-Squared Statistic","text":"\\[R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\] <ul> <li>\\(R^2 = 1\\): Perfect fit (all points on line)</li> <li>\\(R^2 = 0\\): Model no better than predicting the mean</li> <li>\\(R^2 &lt; 0\\): Model is worse than the mean (usually indicates error)</li> </ul>"},{"location":"sims/linear-regression/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/linear-regression/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Pose the question: \"Given scattered data, how do we find the 'best' line through it?\"</p>"},{"location":"sims/linear-regression/#exploration-phase-10-minutes","title":"Exploration Phase (10 minutes)","text":"<ol> <li>Let students drag sliders to find their best fit manually</li> <li>Discuss what makes a line \"good\" or \"bad\"</li> <li>Reveal the loss surface - where is your solution?</li> <li>Click \"Fit OLS\" to see the optimal solution</li> </ol>"},{"location":"sims/linear-regression/#mathematical-foundation-10-minutes","title":"Mathematical Foundation (10 minutes)","text":"<ol> <li>Introduce the loss function \\(L(w, b)\\)</li> <li>Show why the minimum is where partial derivatives equal zero</li> <li>Derive the normal equations</li> <li>Connect to matrix form \\(X^T X \\beta = X^T y\\)</li> </ol>"},{"location":"sims/linear-regression/#interactive-experiments-10-minutes","title":"Interactive Experiments (10 minutes)","text":"<ol> <li>Drag an outlier: How much does one point affect the fit?</li> <li>Add noise: How does noise affect R-squared?</li> <li>Change slope: What happens to the loss surface?</li> </ol>"},{"location":"sims/linear-regression/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we square the residuals instead of using absolute values?</li> <li>What would happen if we minimized perpendicular distance instead?</li> <li>How would you extend this to multiple independent variables?</li> <li>When might linear regression be inappropriate?</li> </ol>"},{"location":"sims/linear-regression/#connections-to-linear-algebra","title":"Connections to Linear Algebra","text":"<p>Linear regression connects to several key concepts:</p> <ul> <li>Least Squares: Finding the best approximation when Ax = b has no solution</li> <li>Projection: The fitted values \\(\\hat{y} = X\\beta\\) are the projection of y onto Col(X)</li> <li>Normal Equations: \\(X^T X \\beta = X^T y\\) ensures the residual is orthogonal to Col(X)</li> <li>Pseudoinverse: \\(\\beta = (X^T X)^{-1} X^T y = X^+ y\\)</li> </ul>"},{"location":"sims/linear-regression/#references","title":"References","text":"<ul> <li>Chapter 8: Vector Spaces and Subspaces - Least Squares section</li> <li>Chapter 9: Solving Linear Systems - Normal equations</li> <li>3Blue1Brown: Linear Regression</li> </ul>"},{"location":"sims/linear-transform-basics/","title":"Linear Transformation Fundamentals Visualizer","text":"<p>Run the Linear Transformation Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/linear-transform-basics/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization demonstrates the fundamental concepts of linear transformations:</p> <ol> <li>Basis vectors determine everything: The transformation is completely defined by where the standard basis vectors e\u2081 = (1,0) and e\u2082 = (0,1) map to</li> <li>Grid structure is preserved: Linear transformations map parallel lines to parallel lines (or points)</li> <li>The matrix columns are the transformed basis vectors: The transformation matrix A has columns T(e\u2081) and T(e\u2082)</li> </ol>"},{"location":"sims/linear-transform-basics/#how-to-use","title":"How to Use","text":"<ul> <li>Drag the handle points on T(e\u2081) and T(e\u2082) in the transformed space to define any linear transformation</li> <li>Use the Morph slider to animate between the identity transformation and your current transformation</li> <li>Select presets from the dropdown to see common transformations (rotation, scaling, shear, reflection)</li> <li>Toggle checkboxes to show/hide the grid and sample vector</li> </ul>"},{"location":"sims/linear-transform-basics/#embedding","title":"Embedding","text":"<p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/linear-transform-basics/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/linear-transform-basics/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/linear-transform-basics/#learning-objectives","title":"Learning Objectives","text":"<p>By using this simulation, students will be able to:</p> <ol> <li>Explain how the columns of a transformation matrix relate to the images of basis vectors</li> <li>Predict how a linear transformation will affect arbitrary vectors based on its matrix</li> <li>Identify common transformations (rotation, scaling, shear, reflection) by their matrix form</li> </ol>"},{"location":"sims/linear-transform-basics/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Discovery: Start with identity, then drag T(e\u2081) and T(e\u2082) to see how the grid deforms</li> <li>Prediction: Given a transformation matrix, predict where a sample vector will map before checking</li> <li>Recognition: Use presets to learn the characteristic matrix patterns for rotation, scaling, shear, and reflection</li> </ol>"},{"location":"sims/linear-transform-basics/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If T(e\u2081) = (2, 0) and T(e\u2082) = (0, 2), what type of transformation is this?</li> <li>What matrix represents a 90\u00b0 counterclockwise rotation?</li> <li>Why does a shear transformation preserve area but not angles?</li> </ol>"},{"location":"sims/linear-transform-basics/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations in Applied Linear Algebra for AI and Machine Learning</li> <li>3Blue1Brown: Linear transformations and matrices</li> </ul>"},{"location":"sims/lora-visualizer/","title":"LoRA Low-Rank Adaptation Visualizer","text":"<p>Run the LoRA Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/lora-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates LoRA (Low-Rank Adaptation), a technique for efficiently fine-tuning large language models by training only a small number of additional parameters.</p> <p>Instead of updating the full weight matrix \\(W\\), LoRA adds a low-rank decomposition:</p> \\[W' = W + \\Delta W = W + BA\\] <p>where: - \\(W\\) is frozen (not trained) - \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) are trainable - \\(r \\ll \\min(d, k)\\) is the rank (typically 4, 8, or 16)</p>"},{"location":"sims/lora-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Rank Slider: Adjust the LoRA rank (1-16) to see parameter savings</li> <li>Dimension Slider: Change matrix size to see scaling behavior</li> <li>Observe: Watch how parameter count changes with rank</li> </ol>"},{"location":"sims/lora-visualizer/#key-insights","title":"Key Insights","text":""},{"location":"sims/lora-visualizer/#parameter-efficiency","title":"Parameter Efficiency","text":"<p>For a \\(d \\times k\\) weight matrix:</p> Method Parameters Full fine-tuning \\(d \\times k\\) LoRA (rank \\(r\\)) \\(r(d + k)\\) <p>Example: With \\(d = k = 4096\\) and \\(r = 8\\): - Full: 16.7M parameters per matrix - LoRA: 65K parameters (0.4%)</p>"},{"location":"sims/lora-visualizer/#why-low-rank-works","title":"Why Low-Rank Works","text":"<p>Research suggests that model adaptation often lies in a low-dimensional subspace. The \"intrinsic dimension\" of fine-tuning is much smaller than the total parameter count.</p>"},{"location":"sims/lora-visualizer/#lora-benefits","title":"LoRA Benefits","text":"<ol> <li>Memory efficient: Train only 0.1-1% of original parameters</li> <li>No inference latency: Can merge \\(W' = W + BA\\) after training</li> <li>Modular: Swap different LoRA adapters for different tasks</li> <li>Stable: Original model weights remain frozen</li> </ol>"},{"location":"sims/lora-visualizer/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand why low-rank approximations enable efficient fine-tuning</li> <li>Calculate parameter savings for different rank values</li> <li>Explain the LoRA forward pass: \\(h = Wx + BAx\\)</li> </ul> <p>Activities:</p> <ol> <li>Find the rank that achieves 99% parameter savings for a 1024\u00d71024 matrix</li> <li>Compare parameter counts for different model sizes</li> <li>Discuss when LoRA might not work well (what if task requires full-rank updates?)</li> </ol> <p>Assessment:</p> <ul> <li>Why is \\(B\\) initialized to zeros and \\(A\\) to small random values?</li> <li>How does LoRA compare to other efficient fine-tuning methods?</li> <li>What's the computational overhead during training vs inference?</li> </ul>"},{"location":"sims/lora-visualizer/#references","title":"References","text":"<ul> <li>LoRA: Low-Rank Adaptation of Large Language Models - Original paper</li> <li>Chapter 11: Generative AI and LLMs</li> <li>Hugging Face PEFT Library</li> </ul>"},{"location":"sims/lu-decomposition/","title":"LU Decomposition Algorithm Visualizer","text":"<p>Run the LU Decomposition Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/lu-decomposition/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the LU Decomposition algorithm step-by-step, showing how Gaussian elimination transforms a matrix A into the product of:</p> <ul> <li>L - a lower triangular matrix containing the multipliers</li> <li>U - an upper triangular matrix (the row echelon form)</li> </ul>"},{"location":"sims/lu-decomposition/#key-features","title":"Key Features","text":"<ul> <li>Step-by-step execution: Watch each elimination step in detail</li> <li>Multiplier tracking: See how multipliers are stored in L</li> <li>Pivot highlighting: Current pivot shown in yellow</li> <li>Row highlighting: Row being eliminated shown in red</li> <li>Verification: Confirm that L \u00d7 U = A after completion</li> <li>Multiple sizes: Try with 3\u00d73 or 4\u00d74 matrices</li> </ul>"},{"location":"sims/lu-decomposition/#how-to-use","title":"How to Use","text":"<ol> <li>Click Next Step to advance through the algorithm</li> <li>Use Auto Run to automatically step through</li> <li>Adjust the Speed slider to control animation pace</li> <li>Click Reset to start over</li> <li>After completion, click Verify L\u00d7U=A to confirm</li> </ol>"},{"location":"sims/lu-decomposition/#the-algorithm","title":"The Algorithm","text":"<p>For each column k (from 1 to n-1):</p> <ol> <li>Select pivot: Use element A[k,k] as the pivot</li> <li>For each row below pivot (rows k+1 to n):<ul> <li>Calculate multiplier: <code>l[i,k] = A[i,k] / A[k,k]</code></li> <li>Store multiplier in L</li> <li>Subtract: <code>Row i = Row i - multiplier \u00d7 Row k</code></li> </ul> </li> <li>Continue until A becomes upper triangular (U)</li> </ol>"},{"location":"sims/lu-decomposition/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain how LU decomposition relates to Gaussian elimination</li> <li>Identify where multipliers are stored in the L matrix</li> <li>Understand why L is lower triangular with 1s on the diagonal</li> <li>Verify that A = LU holds after the decomposition</li> </ul>"},{"location":"sims/lu-decomposition/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/lu-decomposition/#warm-up-3-minutes","title":"Warm-up (3 minutes)","text":"<p>Ask students to recall Gaussian elimination and what information is \"lost\" during the process.</p>"},{"location":"sims/lu-decomposition/#demonstration-7-minutes","title":"Demonstration (7 minutes)","text":"<p>Walk through the 3\u00d73 example together:</p> <ol> <li>First pivot: A[1,1] = 2</li> <li>Eliminate A[2,1]: multiplier = 4/2 = 2</li> <li>Eliminate A[3,1]: multiplier = 8/2 = 4</li> <li>Continue with second pivot</li> </ol>"},{"location":"sims/lu-decomposition/#key-insight","title":"Key Insight","text":"<p>Emphasize that LU decomposition \"saves\" the multipliers that would otherwise be discarded in Gaussian elimination.</p>"},{"location":"sims/lu-decomposition/#practice-10-minutes","title":"Practice (10 minutes)","text":"<p>Have students:</p> <ol> <li>Predict the next multiplier before clicking</li> <li>Try the 4\u00d74 matrix</li> <li>Verify the decomposition</li> </ol>"},{"location":"sims/lu-decomposition/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is L lower triangular?</li> <li>Why are the diagonal elements of L all equal to 1?</li> <li>What would happen if a pivot were zero?</li> </ol>"},{"location":"sims/lu-decomposition/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - LU Decomposition section</li> <li>Strang, G. \"Introduction to Linear Algebra\" - Chapter on Elimination</li> <li>MIT OCW: LU Decomposition</li> </ul>"},{"location":"sims/matrix-basic-ops/","title":"Matrix Addition and Scalar Multiplication","text":"<p>Run the Matrix Basic Operations MicroSim Fullscreen</p> <p>Edit the Matrix Basic Operations MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-basic-ops/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-basic-ops/#description","title":"Description","text":"<p>This MicroSim provides hands-on practice with the two most fundamental matrix operations: addition and scalar multiplication. Both operations are element-wise, meaning they operate on corresponding entries independently.</p> <p>Key Features:</p> <ul> <li>Dual Operations: Switch between matrix addition (A + B = C) and scalar multiplication (k \u00d7 A = C)</li> <li>Visual Layout: Three matrices displayed with operation symbols for clear understanding</li> <li>Step-Through Mode: Click \"Step\" to highlight each calculation sequentially</li> <li>Adjustable Scalar: Slider controls the scalar value from -3 to 3 for multiplication</li> <li>Formula Display: Shows the mathematical formula and current calculation</li> </ul>"},{"location":"sims/matrix-basic-ops/#how-it-works","title":"How It Works","text":""},{"location":"sims/matrix-basic-ops/#matrix-addition","title":"Matrix Addition","text":"<p>For two matrices A and B of the same dimensions, their sum C is computed entry-by-entry:</p> \\[c_{ij} = a_{ij} + b_{ij}\\]"},{"location":"sims/matrix-basic-ops/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>For a scalar k and matrix A, the product C multiplies each entry by k:</p> \\[c_{ij} = k \\cdot a_{ij}\\]"},{"location":"sims/matrix-basic-ops/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-basic-ops/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Calculate the sum of two matrices by adding corresponding entries</li> <li>Multiply a matrix by a scalar by multiplying each entry</li> <li>Recognize that both operations preserve matrix dimensions</li> <li>Understand the element-wise nature of these operations</li> </ol>"},{"location":"sims/matrix-basic-ops/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Start with Addition: Observe how each entry in C equals the sum of corresponding entries in A and B</li> <li>Use Step Mode: Click \"Step\" repeatedly to see each calculation highlighted in sequence</li> <li>Switch to Scalar Multiply: Change to scalar multiplication and adjust the slider</li> <li>Explore Edge Cases: What happens when k = 0? When k = -1?</li> </ol>"},{"location":"sims/matrix-basic-ops/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Dimension Requirement: For addition, matrices must have the same dimensions</li> <li>Commutativity: Matrix addition is commutative (A + B = B + A)</li> <li>Scalar Distribution: k(A + B) = kA + kB</li> </ul>"},{"location":"sims/matrix-basic-ops/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If A[2,3] = 4 and B[2,3] = -2, what is C[2,3] in A + B?</li> <li>If k = -2 and A[1,1] = 5, what is C[1,1] in kA?</li> <li>Does the order of matrices matter for addition? Why or why not?</li> </ol>"},{"location":"sims/matrix-basic-ops/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix addition and scalar multiplication in context</li> <li>Linear Algebra and Its Applications - Lay, Lay, and McDonald</li> </ul>"},{"location":"sims/matrix-inverse/","title":"Matrix Inverse Explorer","text":"<p>Run the Matrix Inverse Explorer MicroSim Fullscreen</p> <p>Edit the Matrix Inverse Explorer MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-inverse/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-inverse/#description","title":"Description","text":"<p>The matrix inverse generalizes division to matrices. For a square matrix A, its inverse A\u207b\u00b9 (if it exists) satisfies AA\u207b\u00b9 = A\u207b\u00b9A = I, where I is the identity matrix. This MicroSim lets you explore matrix inversion interactively.</p> <p>Key Features:</p> <ul> <li>Real-Time Computation: See the inverse update instantly for random matrices</li> <li>Verification Display: Watch AA\u207b\u00b9 = I computed live</li> <li>Determinant Indicator: Color-coded display shows invertibility status</li> <li>Singularity Exploration: Make matrices singular or approach singularity smoothly</li> <li>Formula Display: See the 2\u00d72 inverse formula applied</li> </ul>"},{"location":"sims/matrix-inverse/#the-22-inverse-formula","title":"The 2\u00d72 Inverse Formula","text":"<p>For a 2\u00d72 matrix:</p> \\[A = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\] <p>The inverse is:</p> \\[A^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\] <p>Key insight: The inverse exists if and only if det(A) = ad - bc \u2260 0.</p>"},{"location":"sims/matrix-inverse/#singular-matrices","title":"Singular Matrices","text":"<p>A matrix is singular (not invertible) when:</p> <ul> <li>det(A) = 0</li> <li>The columns are linearly dependent</li> <li>The matrix maps some non-zero vector to zero</li> </ul> <p>The MicroSim shows this by turning red when you click \"Make Singular\" or slide toward singularity.</p>"},{"location":"sims/matrix-inverse/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-inverse/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Apply the 2\u00d72 inverse formula to compute A\u207b\u00b9</li> <li>Verify that AA\u207b\u00b9 = I for invertible matrices</li> <li>Identify singular matrices by their zero determinant</li> <li>Explain why singular matrices have no inverse</li> </ol>"},{"location":"sims/matrix-inverse/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Start with Default: Observe the invertible matrix and verify AA\u207b\u00b9 = I</li> <li>Click Randomize: Generate new matrices and check determinants</li> <li>Make Singular: Click the button and observe the warning</li> <li>Approach Singularity: Use the slider to see determinant approach zero</li> </ol>"},{"location":"sims/matrix-inverse/#discussion-points","title":"Discussion Points","text":"<ul> <li>What happens to the inverse entries as det(A) approaches zero?</li> <li>Why does \"Make Singular\" make row 2 proportional to row 1?</li> <li>How does this connect to solving systems of equations?</li> </ul>"},{"location":"sims/matrix-inverse/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For A = [[3, 1], [2, 1]], compute det(A) and A\u207b\u00b9.</li> <li>Why can't you divide by a matrix the way you divide by a number?</li> <li>If det(A) = 0.001, is A technically invertible? Is it practically invertible?</li> </ol>"},{"location":"sims/matrix-inverse/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix inverse in context</li> <li>Khan Academy: Inverse Matrices - Video explanation</li> </ul>"},{"location":"sims/matrix-multiplication/","title":"Matrix Multiplication Visualizer","text":"<p>Run the Matrix Multiplication MicroSim Fullscreen</p> <p>Edit the Matrix Multiplication MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-multiplication/main.html\" height=\"407px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-multiplication/#about-this-microsim","title":"About This MicroSim","text":"<p>Matrix multiplication is the most important and nuanced matrix operation in linear algebra\u2014and arguably the most misunderstood! This interactive MicroSim transforms an abstract algorithm into a visual, step-by-step experience that makes the row-by-column dot product process crystal clear. Watch as matrices come alive with color-coded highlighting, animated calculations, and real-time feedback that turns confusion into understanding.</p> <p>Key Features:</p> <ul> <li>Color-Coded Highlighting: The current row of A glows blue while the matching column of B glows green, showing exactly which vectors combine to form each result</li> <li>Step-by-Step Animation: Watch each element-wise multiplication happen in real-time with the current operation highlighted in yellow</li> <li>Live Running Sum: See the dot product accumulate as each term is added, building intuition for how entries are computed</li> <li>Operations Counter: Displays the total number of multiplications and additions required, reinforcing computational complexity concepts</li> <li>Auto-Play Mode: Sit back and watch the entire multiplication unfold automatically at adjustable speeds</li> <li>Flexible Dimensions: Experiment with 2\u00d72, 2\u00d73, 3\u00d72, and 3\u00d73 matrices to understand dimension compatibility rules</li> </ul>"},{"location":"sims/matrix-multiplication/#how-to-use-this-microsim","title":"How to Use This MicroSim","text":""},{"location":"sims/matrix-multiplication/#controls","title":"Controls","text":"Control Function A Dimension Selector Choose the dimensions of matrix A (2\u00d72, 2\u00d73, 3\u00d72, or 3\u00d73) B Dimension Selector Choose the dimensions of matrix B (options depend on A's columns) Reset Generate new random matrices and restart the visualization First/Next Multiplication Step through the calculation one multiplication at a time Auto/Stop Toggle automatic playback of the entire multiplication Animation Speed Slider Adjust playback speed (Slower on left, Faster on right)"},{"location":"sims/matrix-multiplication/#understanding-the-calculation-display","title":"Understanding the Calculation Display","text":"<p>The MicroSim displays the dot product calculation using special notation to show progress:</p> Notation Meaning Description \\(3 \\times 2\\) Completed This multiplication has been computed and added to the running sum \\([4 \\times 5]\\) Current This is the multiplication about to be performed (square brackets) \\((2 \\times 1)\\) Pending This multiplication is waiting to be computed (parentheses) <p>For example, when computing \\(c_{12}\\) with a 3-element dot product:</p> \\[3 \\times 2 + [4 \\times 5] + (2 \\times 1)\\] \\[\\underbrace{3 \\times 2}_{\\text{Done}} + \\underbrace{[4 \\times 5]}_{\\text{Current}} + \\underbrace{(2 \\times 1)}_{\\text{Future}}\\]"},{"location":"sims/matrix-multiplication/#step-by-step-walkthrough","title":"Step-by-Step Walkthrough","text":"<ol> <li>Start Fresh: Click Reset to generate new random matrices</li> <li>Observe the Setup: Notice how matrix A (blue-tinted) and B (green-tinted) are displayed with their dimensions shown below</li> <li>Click First Multiplication: The first row of A and first column of B light up, showing which elements combine</li> <li>Watch the Calculation: The equation area shows the dot product formula with the current term in brackets</li> <li>See the Running Sum: After each multiplication, the running sum updates in red</li> <li>Continue Stepping: Click Next Multiplication to process each term until the entry is complete</li> <li>Move to Next Entry: Once \\(c_{11}\\) is computed (shown in gold), the visualization moves to \\(c_{12}\\)</li> <li>Try Auto Mode: Click Auto to watch the entire process animate automatically</li> </ol>"},{"location":"sims/matrix-multiplication/#tips-for-learning","title":"Tips for Learning","text":"<ul> <li>Predict Before Clicking: Before each step, mentally calculate what the running sum will be</li> <li>Slow Down: Use the Animation Speed slider to slow down auto-play for careful observation</li> <li>Change Dimensions: Try different matrix sizes to see how the number of operations changes</li> <li>Count Operations: Notice how the \"Number of Operations\" line updates when you change dimensions</li> </ul>"},{"location":"sims/matrix-multiplication/#how-matrix-multiplication-works","title":"How Matrix Multiplication Works","text":"<p>For matrices A (m\u00d7n) and B (n\u00d7p), the result C is an (m\u00d7p) matrix where:</p> \\[c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}\\] <p>Each entry \\(c_{ij}\\) is the dot product of row \\(i\\) of A with column \\(j\\) of B.</p>"},{"location":"sims/matrix-multiplication/#dimension-compatibility-rule","title":"Dimension Compatibility Rule","text":"<p>The number of columns in A must equal the number of rows in B:</p> Matrix A Matrix B Result C Valid? 2\u00d73 3\u00d72 2\u00d72 Yes 3\u00d72 3\u00d74 \u2014 No 2\u00d72 2\u00d72 2\u00d72 Yes"},{"location":"sims/matrix-multiplication/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-multiplication/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain the row-by-column dot product process for computing each entry</li> <li>Determine the dimensions of the result matrix given input dimensions</li> <li>Identify when two matrices can be multiplied (dimension compatibility)</li> <li>Calculate individual entries of a matrix product</li> </ol>"},{"location":"sims/matrix-multiplication/#guided-exploration-7-10-minutes","title":"Guided Exploration (7-10 minutes)","text":"<ol> <li>Watch One Entry: Click \"First Multiplication\" then \"Next Multiplication\" repeatedly to see \\(c_{11}\\) computed step by step</li> <li>Observe the Notation: Notice how completed terms have no brackets, the current term has square brackets, and future terms have parentheses</li> <li>Use Auto-Play: Click \"Auto\" to watch the entire multiplication animate automatically</li> <li>Change Dimensions: Try 3\u00d73 matrices to see more computation steps and higher operation counts</li> <li>Predict Before Clicking: Before each step, predict what the running sum will become</li> </ol>"},{"location":"sims/matrix-multiplication/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Not Commutative: Unlike regular multiplication, AB \u2260 BA in general</li> <li>Dimension Flow: (m\u00d7n) \u00d7 (n\u00d7p) \u2192 (m\u00d7p) \u2014 the inner dimensions must match</li> <li>Computational Cost: Each entry requires n multiplications and n-1 additions</li> </ul>"},{"location":"sims/matrix-multiplication/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For A (2\u00d73) and B (3\u00d74), what are the dimensions of C = AB?</li> <li>How many multiplication operations are needed to compute one entry of C?</li> <li>If A[1,2] = 3 and B[2,1] = 4, what is their contribution to C[1,1]?</li> </ol>"},{"location":"sims/matrix-multiplication/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix multiplication in context</li> <li>3Blue1Brown: Matrix Multiplication - Visual intuition for matrix multiplication</li> </ul>"},{"location":"sims/matrix-power-calculator/","title":"Matrix Power Calculator","text":"<p>Run the Matrix Power Calculator Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/matrix-power-calculator/#about-this-microsim","title":"About This MicroSim","text":"<p>This calculator demonstrates one of the most practical applications of diagonalization: efficiently computing high powers of a matrix. Instead of multiplying A by itself k times, we use the decomposition A = PDP\u207b\u00b9 to compute A^k = PD^kP\u207b\u00b9.</p> <p>Key Features:</p> <ul> <li>Editable matrix: Click cells to enter custom values</li> <li>Power slider: Compute A^k for k from 1 to 20</li> <li>Diagonalization display: See P, D, and P\u207b\u00b9 matrices</li> <li>Step-by-step: Watch how D^k is trivially computed</li> <li>Defective detection: Shows when diagonalization fails</li> </ul>"},{"location":"sims/matrix-power-calculator/#how-to-use","title":"How to Use","text":"<ol> <li>Click matrix cells to enter custom values</li> <li>Adjust the power slider to compute different powers</li> <li>Toggle \"Show Steps\" to see the diagonalization process</li> <li>Use preset buttons for quick examples:</li> <li>Random: Generate a random matrix</li> <li>Diagonalizable: A simple diagonalizable example</li> <li>Defective: A matrix that cannot be diagonalized</li> </ol>"},{"location":"sims/matrix-power-calculator/#why-diagonalization-is-efficient","title":"Why Diagonalization is Efficient","text":"<p>Direct Method: To compute A^20, you need 19 matrix multiplications.</p> <p>Diagonalization Method: 1. Compute P, D, P\u207b\u00b9 once (3 operations) 2. Compute D^20 = diag(\u03bb\u2081^20, \u03bb\u2082^20) (trivial - just raise scalars to powers) 3. Compute PD^20P\u207b\u00b9 (2 matrix multiplications)</p> <p>For large k, diagonalization is dramatically faster!</p>"},{"location":"sims/matrix-power-calculator/#mathematical-formula","title":"Mathematical Formula","text":"<p>For a diagonalizable matrix A = PDP\u207b\u00b9:</p> <p>A^k = PD^kP\u207b\u00b9</p> <p>where D^k = diag(\u03bb\u2081^k, \u03bb\u2082^k, ..., \u03bb\u2099^k)</p> <p>This works because: - A\u00b2 = (PDP\u207b\u00b9)(PDP\u207b\u00b9) = PD(P\u207b\u00b9P)DP\u207b\u00b9 = PD\u00b2P\u207b\u00b9 - By induction: A^k = PD^kP\u207b\u00b9</p>"},{"location":"sims/matrix-power-calculator/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/matrix-power-calculator/main.html\" height=\"532px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/matrix-power-calculator/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-power-calculator/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Compute matrix powers using diagonalization</li> <li>Explain why D^k is easy to compute (diagonal matrices)</li> <li>Identify when diagonalization cannot be used (defective matrices)</li> </ol>"},{"location":"sims/matrix-power-calculator/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Manual verification: For A = [[2, 1], [0, 3]], verify that PD\u00b2P\u207b\u00b9 = A\u00b2</li> <li>Efficiency comparison: Count operations for A^10 using both methods</li> <li>Large powers: Use the calculator for A^20 and notice how eigenvalues dominate</li> </ol>"},{"location":"sims/matrix-power-calculator/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why is D^k so easy to compute?</li> <li>If A has eigenvalues 0.5 and 2, what happens to A^k as k \u2192 \u221e?</li> <li>Can you use this method for complex eigenvalues?</li> </ol>"},{"location":"sims/matrix-power-calculator/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/matrix-rank-visualizer/","title":"Matrix Rank Visualizer","text":"<p>Run the Matrix Rank Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/matrix-rank-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates matrix rank by showing the geometric relationship between a matrix's column vectors and its column space.</p> <p>Key Concepts:</p> <ul> <li>Column vectors are displayed as colored arrows in 3D space</li> <li>Column space is the span of all column vectors, shown as:<ul> <li>A point (rank 0) - zero matrix</li> <li>A line (rank 1) - all columns are parallel</li> <li>A plane (rank 2) - columns span a 2D subspace</li> <li>All of R\u00b3 (rank 3) - full rank, columns span 3D space</li> </ul> </li> <li>Row echelon form shows pivot positions (highlighted in yellow)</li> <li>Pivot columns indicate which columns are linearly independent</li> </ul>"},{"location":"sims/matrix-rank-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Select a preset from the dropdown to see different rank scenarios</li> <li>Toggle checkboxes to show/hide column vectors and column space</li> <li>Drag to rotate the 3D view for different perspectives</li> <li>Observe how the row echelon form reveals the rank</li> </ol>"},{"location":"sims/matrix-rank-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain the geometric meaning of matrix rank</li> <li>Identify rank-deficient matrices visually</li> <li>Connect row echelon form to linear independence of columns</li> <li>Distinguish between full rank and rank-deficient cases</li> </ul>"},{"location":"sims/matrix-rank-visualizer/#presets","title":"Presets","text":"Preset Matrix Rank Column Space Rank 2 (Default) [[1,2,3],[4,5,6],[7,8,9]] 2 Plane Full Rank (3) Identity matrix 3 All of R\u00b3 Rank 1 [[1,2,3],[2,4,6],[3,6,9]] 1 Line Rank 0 (Zero) Zero matrix 0 Point"},{"location":"sims/matrix-rank-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/matrix-rank-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Start by asking students: \"What does it mean for a matrix to be 'full rank'?\" Introduce the concept that rank measures the dimension of the column space.</p>"},{"location":"sims/matrix-rank-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<p>Have students work through the presets:</p> <ol> <li>Start with the Full Rank preset - observe that all three column vectors point in different directions</li> <li>Switch to Rank 2 - notice how the third column lies in the plane spanned by the first two</li> <li>Try Rank 1 - see how all columns are parallel (scalar multiples of each other)</li> <li>Finally, Rank 0 - the trivial case of the zero matrix</li> </ol>"},{"location":"sims/matrix-rank-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the rank of [[1,2,3],[4,5,6],[7,8,9]] equal to 2, not 3?</li> <li>What happens to the column space when we have linearly dependent columns?</li> <li>How does the row echelon form reveal which columns are pivot columns?</li> </ol>"},{"location":"sims/matrix-rank-visualizer/#assessment","title":"Assessment","text":"<p>Ask students to predict the rank of a new matrix before computing it, based on visual inspection of the column vectors.</p>"},{"location":"sims/matrix-rank-visualizer/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Matrix Rank section</li> <li>3Blue1Brown: Column Space</li> <li>Strang, G. \"Linear Algebra and Its Applications\" - Chapter on Vector Spaces</li> </ul>"},{"location":"sims/ml-pipeline/","title":"Machine Learning Pipeline Workflow","text":"<p>Run the ML Pipeline Workflow Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/ml-pipeline/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive flowchart visualizes the complete machine learning pipeline from raw data to a trained model. Each stage is color-coded by category and includes both hover explanations and Python code examples.</p> <p>Pipeline Stages:</p> <ol> <li>Raw Data - Original features, possibly different scales and units</li> <li>Standardization - Transform to zero mean, unit variance</li> <li>PCA (optional) - Reduce dimensionality while preserving variance</li> <li>Train/Test Split - Hold out data for evaluation</li> <li>Model Selection - Choose algorithm (OLS, Ridge, or Lasso)</li> <li>Optimization - Find optimal parameters</li> <li>Evaluation - Assess on test set (MSE, R-squared)</li> <li>Trained Model - Ready for deployment</li> </ol>"},{"location":"sims/ml-pipeline/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over nodes to see detailed explanations of each stage</li> <li>Click on nodes to view Python code examples for that stage</li> <li>Click \"Clear Selection\" or click elsewhere to dismiss the code panel</li> <li>Follow the arrows to understand the data flow through the pipeline</li> </ol>"},{"location":"sims/ml-pipeline/#color-coding","title":"Color Coding","text":"Color Category Stages Blue Data Processing Raw Data, Standardization, PCA, Train/Test Split Green Modeling Model Selection, OLS, Ridge, Lasso Orange Optimization Optimization Purple Evaluation Evaluation, Trained Model"},{"location":"sims/ml-pipeline/#the-ml-pipeline-in-linear-algebra-terms","title":"The ML Pipeline in Linear Algebra Terms","text":"<p>The machine learning pipeline heavily relies on linear algebra operations:</p>"},{"location":"sims/ml-pipeline/#standardization","title":"Standardization","text":"\\[z = \\frac{x - \\mu}{\\sigma}\\] <p>This transforms the data matrix \\(X\\) so each column has mean 0 and variance 1.</p>"},{"location":"sims/ml-pipeline/#pca-principal-component-analysis","title":"PCA (Principal Component Analysis)","text":"<p>Uses SVD to find the directions of maximum variance: \\(\\(X = U\\Sigma V^T\\)\\)</p> <p>The principal components are the columns of \\(V\\).</p>"},{"location":"sims/ml-pipeline/#linear-regression-models","title":"Linear Regression Models","text":"<p>OLS (Ordinary Least Squares): \\(\\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\)\\)</p> <p>Ridge Regression (L2 regularization): \\(\\(\\hat{\\beta} = (X^TX + \\lambda I)^{-1}X^Ty\\)\\)</p> <p>Lasso Regression (L1 regularization): Minimizes \\(\\|y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1\\)</p>"},{"location":"sims/ml-pipeline/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/ml-pipeline/main.html\" height=\"552px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/ml-pipeline/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/ml-pipeline/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Describe the complete workflow of a machine learning pipeline</li> <li>Explain the purpose of each preprocessing and modeling stage</li> <li>Compare and contrast OLS, Ridge, and Lasso regression approaches</li> <li>Identify where linear algebra operations occur in the pipeline</li> </ol>"},{"location":"sims/ml-pipeline/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Trace the pipeline: Follow the data flow and explain what happens at each stage</li> <li>Code walkthrough: Use the code examples to implement a complete pipeline in Python</li> <li>Model comparison: Run all three regression types on the same dataset and compare results</li> </ol>"},{"location":"sims/ml-pipeline/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why is standardization important before applying PCA?</li> <li>What is the key difference between Ridge and Lasso regularization?</li> <li>Why do we need a separate test set for evaluation?</li> <li>At which stage does the closed-form solution \\((X^TX)^{-1}X^Ty\\) get computed?</li> </ol>"},{"location":"sims/ml-pipeline/#references","title":"References","text":"<ul> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 7: Matrix Decompositions</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/momentum-dynamics-visualizer/","title":"Momentum Dynamics Visualizer","text":"<p>Run the Momentum Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/momentum-dynamics-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization compares three optimization methods on an ill-conditioned quadratic function:</p> <ul> <li>SGD (blue): Standard gradient descent with characteristic zig-zag oscillation</li> <li>Momentum (green): Accumulates velocity to smooth the path</li> <li>Nesterov (orange): \"Looks ahead\" before computing the gradient</li> </ul>"},{"location":"sims/momentum-dynamics-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Momentum (\u03b2): Control velocity decay (0 = no momentum, 0.99 = high momentum)</li> <li>Adjust Learning Rate: Control step size</li> <li>Step/Run: Execute optimization steps manually or automatically</li> <li>Velocity Vectors: Toggle arrows showing accumulated velocity direction and magnitude</li> <li>Click: Click anywhere on the plot to set a new starting point</li> </ol>"},{"location":"sims/momentum-dynamics-visualizer/#key-observations","title":"Key Observations","text":"<ul> <li>SGD shows characteristic zig-zag pattern on elongated contours</li> <li>Momentum builds up speed in consistent directions, reducing oscillation</li> <li>Nesterov typically converges slightly faster than classical momentum</li> </ul>"},{"location":"sims/momentum-dynamics-visualizer/#the-momentum-update","title":"The Momentum Update","text":"<p>Classical momentum maintains a velocity vector:</p> \\[\\mathbf{v}_{k+1} = \\beta \\mathbf{v}_k + \\nabla f(\\mathbf{x}_k)$$ $$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\mathbf{v}_{k+1}\\] <p>The velocity accumulates in directions where the gradient is consistent, while damping oscillations.</p>"},{"location":"sims/momentum-dynamics-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/momentum-dynamics-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand how momentum accumulates and dampens oscillations</li> <li>Compare SGD, Momentum, and Nesterov acceleration</li> <li>Visualize velocity vectors during optimization</li> </ul>"},{"location":"sims/momentum-dynamics-visualizer/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>No Momentum: Set \u03b2=0 and observe pure SGD behavior</li> <li>High Momentum: Set \u03b2=0.95 and watch smooth acceleration</li> <li>Compare Methods: Run all three simultaneously and count iterations to convergence</li> <li>Velocity Vectors: Enable velocity display to see how momentum builds</li> </ol>"},{"location":"sims/momentum-dynamics-visualizer/#references","title":"References","text":"<ul> <li>Sutskever et al., On the importance of initialization and momentum in deep learning, 2013</li> <li>Wikipedia: Momentum (gradient descent)</li> </ul>"},{"location":"sims/multi-head-attention/","title":"Multi-Head Attention Visualizer","text":"<p>Run the Multi-Head Attention Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/multi-head-attention/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how multi-head attention captures diverse relationship patterns by running multiple attention operations in parallel.</p> <p>Each attention head can learn to focus on different types of relationships:</p> <ul> <li>Position proximity: Nearby tokens attend to each other</li> <li>Semantic similarity: Words with similar meanings connect</li> <li>Syntactic structure: Subject-verb, modifier-noun relationships</li> <li>Long-range dependencies: Connections across the sequence</li> </ul>"},{"location":"sims/multi-head-attention/#how-to-use","title":"How to Use","text":"<ol> <li>Number of Heads: Adjust the slider to see 1-8 attention heads</li> <li>Hover: Move over any head to see what pattern type it has learned</li> <li>Show Concatenation: Toggle to see how head outputs combine</li> </ol>"},{"location":"sims/multi-head-attention/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/multi-head-attention/#why-multiple-heads","title":"Why Multiple Heads?","text":"<p>A single attention head might focus on only one type of relationship. Multiple heads allow the model to:</p> <ul> <li>Capture syntactic AND semantic relationships</li> <li>Attend to both local and global context</li> <li>Learn diverse, complementary patterns</li> </ul>"},{"location":"sims/multi-head-attention/#the-multi-head-formula","title":"The Multi-Head Formula","text":"\\[\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O\\] <p>where each head is:</p> \\[\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\\]"},{"location":"sims/multi-head-attention/#dimension-management","title":"Dimension Management","text":"<p>With \\(d_{model} = 512\\) and \\(h = 8\\) heads:</p> <ul> <li>Each head uses \\(d_k = d_v = d_{model}/h = 64\\)</li> <li>Total computation is similar to single full-dimension head</li> <li>But captures 8\u00d7 richer patterns</li> </ul>"},{"location":"sims/multi-head-attention/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand why multiple attention heads improve model expressiveness</li> <li>Visualize how different heads learn different patterns</li> <li>Trace the concatenation and projection flow</li> </ul> <p>Activities:</p> <ol> <li>Compare attention patterns across all 8 heads</li> <li>Identify which heads capture local vs. global patterns</li> <li>Explain why the output projection (W_O) is necessary</li> </ol> <p>Assessment:</p> <ul> <li>Why not just use one head with larger dimension?</li> <li>How does head count affect model capacity vs. computation?</li> <li>What would happen if all heads learned the same pattern?</li> </ul>"},{"location":"sims/multi-head-attention/#references","title":"References","text":"<ul> <li>Attention Is All You Need - Original transformer paper</li> <li>Chapter 11: Generative AI and LLMs</li> <li>BertViz: Attention Head Visualization</li> </ul>"},{"location":"sims/multiplicity-comparison/","title":"Multiplicity Comparison Chart","text":"<p>Run the Multiplicity Comparison Chart Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/multiplicity-comparison/#about-this-microsim","title":"About This MicroSim","text":"<p>This comparison chart displays three canonical cases that illustrate the relationship between algebraic and geometric multiplicity, and how this relationship determines whether a matrix is diagonalizable.</p> <p>The Three Cases:</p> <ol> <li>Distinct Eigenvalues (Green): Each eigenvalue has multiplicity 1 \u2192 Always diagonalizable</li> <li>Repeated with Full Eigenspace (Blue): Repeated eigenvalue but m_g = m_a \u2192 Diagonalizable (scalar matrix example)</li> <li>Defective Matrix (Red): Repeated eigenvalue with m_g &lt; m_a \u2192 NOT diagonalizable</li> </ol>"},{"location":"sims/multiplicity-comparison/#key-concepts","title":"Key Concepts","text":"Term Definition Algebraic Multiplicity (m_a) How many times \u03bb appears as a root of the characteristic polynomial Geometric Multiplicity (m_g) Dimension of the eigenspace, i.e., number of linearly independent eigenvectors Defective Matrix A matrix where m_g &lt; m_a for some eigenvalue"},{"location":"sims/multiplicity-comparison/#the-multiplicity-inequality","title":"The Multiplicity Inequality","text":"<p>For any eigenvalue \u03bb:</p> <p>1 \u2264 geometric multiplicity \u2264 algebraic multiplicity</p> <p>A matrix is diagonalizable if and only if geometric multiplicity equals algebraic multiplicity for ALL eigenvalues.</p>"},{"location":"sims/multiplicity-comparison/#how-to-use","title":"How to Use","text":"<ol> <li>Compare the three cards side by side</li> <li>Examine the ratio bar showing m_g/m_a</li> <li>Hover over cards to highlight them</li> <li>Note the status indicator showing diagonalizability</li> </ol>"},{"location":"sims/multiplicity-comparison/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/multiplicity-comparison/main.html\" height=\"532px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/multiplicity-comparison/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/multiplicity-comparison/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Distinguish between algebraic and geometric multiplicity</li> <li>Identify when a matrix is diagonalizable based on multiplicities</li> <li>Recognize defective matrices and explain why they cannot be diagonalized</li> </ol>"},{"location":"sims/multiplicity-comparison/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verification: Compute the eigenspaces for each example matrix and verify the geometric multiplicities</li> <li>Create examples: Find a 3\u00d73 defective matrix with a different structure</li> <li>Borderline cases: Why is [[2, 0], [0, 2]] diagonalizable but [[2, 1], [0, 2]] is not?</li> </ol>"},{"location":"sims/multiplicity-comparison/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If a 4\u00d74 matrix has characteristic polynomial (\u03bb-3)\u2074, what are the possible geometric multiplicities? Which would make it diagonalizable?</li> <li>Can a matrix with distinct eigenvalues ever be defective? Explain.</li> <li>What is special about the eigenvectors of a defective matrix?</li> </ol>"},{"location":"sims/multiplicity-comparison/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/neural-network-architecture/","title":"Neural Network Architecture","text":"<p>Run the Neural Network Architecture Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/neural-network-architecture/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps you understand how neural networks are structured, showing the connections between layers and the dimensions of weight matrices that make learning possible.</p>"},{"location":"sims/neural-network-architecture/#key-concepts","title":"Key Concepts","text":"<ul> <li>Layers: Input, hidden, and output layers each serve different purposes</li> <li>Weight Matrix: For a layer with \\(n_{in}\\) inputs and \\(n_{out}\\) outputs, \\(W \\in \\mathbb{R}^{n_{out} \\times n_{in}}\\)</li> <li>Bias Vector: Each layer has a bias \\(\\mathbf{b} \\in \\mathbb{R}^{n_{out}}\\)</li> <li>Parameters: Total trainable values = weights + biases</li> </ul>"},{"location":"sims/neural-network-architecture/#interactive-features","title":"Interactive Features","text":"<ul> <li>Input Neurons: Adjust the input layer size (feature dimension)</li> <li>Hidden Layers: Change the number of hidden layers (1-5)</li> <li>Hidden Neurons: Set the width of hidden layers</li> <li>Output Neurons: Set the output dimension (e.g., number of classes)</li> <li>Show Dims: Toggle weight matrix dimension labels</li> </ul>"},{"location":"sims/neural-network-architecture/#understanding-the-display","title":"Understanding the Display","text":"<ul> <li>Green nodes: Input layer (receives data)</li> <li>Blue nodes: Hidden layers (learn features)</li> <li>Red nodes: Output layer (produces predictions)</li> <li>W labels: Weight matrix dimensions (rows \u00d7 columns)</li> <li>b labels: Bias vector dimensions</li> <li>\u03c3 labels: Activation function at each layer</li> </ul>"},{"location":"sims/neural-network-architecture/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/neural-network-architecture/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Describe the role of input, hidden, and output layers</li> <li>Calculate the dimensions of weight matrices between layers</li> <li>Compute the total number of parameters in a network</li> <li>Explain why hidden layer width and depth affect capacity</li> </ol>"},{"location":"sims/neural-network-architecture/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Parameter Counting: Create a 784\u2192128\u219264\u219210 network (like MNIST) and verify the parameter count</li> <li>Dimension Matching: Explain why W must have shape (output_size \u00d7 input_size)</li> <li>Depth vs Width: Compare 4\u219216\u219216\u21922 vs 4\u219232\u21922. Which has more parameters?</li> <li>Scaling Analysis: How does doubling hidden neurons affect parameter count?</li> </ol>"},{"location":"sims/neural-network-architecture/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why must the weight matrix dimensions be \\(n_{out} \\times n_{in}\\) and not the reverse?</li> <li>How do biases differ from weights in terms of what they learn?</li> <li>What's the tradeoff between deeper networks and wider networks?</li> <li>Why might a 4\u21928\u21928\u21922 network be preferred over 4\u219216\u21922?</li> </ol>"},{"location":"sims/neural-network-architecture/#references","title":"References","text":"<ul> <li>Goodfellow et al. (2016). Deep Learning, Chapter 6: Deep Feedforward Networks</li> <li>He et al. (2015). Delving Deep into Rectifiers (weight initialization)</li> </ul>"},{"location":"sims/neural-network-layer/","title":"Neural Network Layer","text":"<p>Run the Neural Network Layer MicroSim Fullscreen</p> <p>Edit the Neural Network Layer MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/neural-network-layer/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/neural-network-layer/#description","title":"Description","text":"<p>This MicroSim visualizes how a fully connected neural network layer is implemented as matrix-vector multiplication. Each layer computes:</p> \\[h = \\sigma(Wx + b)\\] <p>where:</p> <ul> <li>x is the input vector (left neurons)</li> <li>W is the weight matrix (connection lines)</li> <li>b is the bias vector (optional, shown as nodes above outputs)</li> <li>\u03c3 is the activation function (ReLU, sigmoid, tanh, or none)</li> <li>h is the output vector (right neurons)</li> </ul> <p>Key Features:</p> <ul> <li>Visual Weights: Connection thickness shows weight magnitude; blue = positive, red = negative</li> <li>Activation Functions: Compare ReLU, sigmoid, tanh, or linear (none)</li> <li>Adjustable Architecture: Change the number of inputs and outputs</li> <li>Bias Toggle: Show or hide bias terms</li> <li>Random Initialization: Generate new weights or inputs</li> </ul>"},{"location":"sims/neural-network-layer/#the-matrix-view","title":"The Matrix View","text":"<p>The weight matrix W has dimensions (outputs \u00d7 inputs). Each row of W corresponds to one output neuron and contains the weights for all connections to that neuron.</p> <p>For 3 inputs and 2 outputs:</p> \\[W = \\begin{bmatrix} w_{11} &amp; w_{12} &amp; w_{13} \\\\ w_{21} &amp; w_{22} &amp; w_{23} \\end{bmatrix}\\] <p>The output is computed as:</p> \\[h_i = \\sigma\\left(\\sum_{j=1}^{n} w_{ij} x_j + b_i\\right)\\]"},{"location":"sims/neural-network-layer/#activation-functions","title":"Activation Functions","text":"Function Formula Range Properties None \u03c3(z) = z (-\u221e, \u221e) Linear, no nonlinearity ReLU \u03c3(z) = max(0, z) [0, \u221e) Sparse activation, fast Sigmoid \u03c3(z) = 1/(1+e^(-z)) (0, 1) Smooth, probability interpretation Tanh \u03c3(z) = tanh(z) (-1, 1) Zero-centered, smooth"},{"location":"sims/neural-network-layer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/neural-network-layer/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain how a neural network layer implements matrix-vector multiplication</li> <li>Describe the role of weights, biases, and activation functions</li> <li>Calculate the output of a simple layer by hand</li> <li>Connect linear algebra concepts to deep learning</li> </ol>"},{"location":"sims/neural-network-layer/#guided-exploration-7-10-minutes","title":"Guided Exploration (7-10 minutes)","text":"<ol> <li>Start Simple: Set inputs=2, outputs=2, activation=none, no bias</li> <li>Observe Weights: Click \"Random W\" and watch connection changes</li> <li>Add Nonlinearity: Switch to ReLU and note how negative pre-activations become 0</li> <li>Enable Bias: Toggle bias on and observe the bias nodes appear</li> </ol>"},{"location":"sims/neural-network-layer/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Why do we need activation functions? (Without them, the whole network is just one big linear transformation)</li> <li>What does a large positive weight vs large negative weight mean visually?</li> <li>How many parameters (weights + biases) does this layer have?</li> </ul>"},{"location":"sims/neural-network-layer/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>For a layer with 4 inputs and 3 outputs, what are the dimensions of W?</li> <li>If all weights are positive, can an output ever be negative (with ReLU)?</li> <li>How many total parameters does a 10-input, 5-output layer have (with bias)?</li> </ol>"},{"location":"sims/neural-network-layer/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Neural network layers as matrix operations</li> <li>Deep Learning Book - Goodfellow, Bengio, and Courville</li> </ul>"},{"location":"sims/newton-vs-gradient-descent/","title":"Newton vs Gradient Descent Comparison","text":"<p>Run the Comparison Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/newton-vs-gradient-descent/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization compares the convergence behavior of Gradient Descent (blue) and Newton's Method (orange) on quadratic functions with varying condition numbers. The condition number controls how \"elongated\" the contour ellipses are.</p>"},{"location":"sims/newton-vs-gradient-descent/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Condition Number: Use the slider to change from 1 (circular contours) to 100 (highly elongated)</li> <li>Adjust Learning Rate: Control the step size for gradient descent</li> <li>Step/Run: Execute optimization steps manually or automatically</li> <li>Click: Click anywhere on the plot to set a new starting point</li> <li>Reset: Return to initial state</li> </ol>"},{"location":"sims/newton-vs-gradient-descent/#key-observations","title":"Key Observations","text":"<ul> <li>Newton's Method converges in exactly 1 step for quadratic functions regardless of condition number</li> <li>Gradient Descent shows characteristic zig-zag oscillation on ill-conditioned problems</li> <li>Higher condition numbers require smaller learning rates for gradient descent stability</li> <li>Newton's method is condition-number invariant</li> </ul>"},{"location":"sims/newton-vs-gradient-descent/#why-newton-converges-faster","title":"Why Newton Converges Faster","text":"<p>Newton's method uses the inverse Hessian to rescale the gradient:</p> \\[\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\mathbf{H}^{-1} \\nabla f(\\mathbf{x}_k)\\] <p>This effectively transforms the problem into one with circular contours, eliminating the zig-zag behavior of gradient descent on elongated landscapes.</p>"},{"location":"sims/newton-vs-gradient-descent/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/newton-vs-gradient-descent/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Compare convergence rates of first and second-order methods</li> <li>Understand the impact of condition number on optimization</li> <li>Visualize why Newton's method is condition-number invariant</li> </ul>"},{"location":"sims/newton-vs-gradient-descent/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Low Condition Number: Set condition number to 1 and observe both methods</li> <li>High Condition Number: Increase to 50-100 and watch gradient descent struggle</li> <li>Learning Rate Tuning: Find the largest stable learning rate for each condition number</li> <li>Iteration Counting: Compare how many iterations each method needs</li> </ol>"},{"location":"sims/newton-vs-gradient-descent/#references","title":"References","text":"<ul> <li>Boyd &amp; Vandenberghe, Convex Optimization, Chapter 9</li> <li>Wikipedia: Newton's Method in Optimization</li> </ul>"},{"location":"sims/norm-comparison-visualizer/","title":"Norm Comparison Visualizer","text":"<p>Run the Norm Comparison Visualizer Fullscreen</p>"},{"location":"sims/norm-comparison-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand different ways to measure vector \"length\" or distance. By showing the unit \"circles\" for L1, L2, and L-infinity norms simultaneously, students can see how each norm defines what it means for a vector to have \"length 1.\"</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p>"},{"location":"sims/norm-comparison-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Point: Click and drag the black point to any location on the grid</li> <li>Observe Norm Values: The info panel shows all three norm values for the current point</li> <li>Compare Unit Shapes:</li> <li>Blue Circle: L2 norm (Euclidean) - points at distance 1 from origin</li> <li>Green Diamond: L1 norm (Manhattan) - points with L1 distance 1 from origin</li> <li>Orange Square: L\u221e norm (Maximum) - points with L\u221e distance 1 from origin</li> <li>Toggle Norms: Use checkboxes to show/hide each norm's unit shape</li> <li>Adjust Radius: Use the slider to see how the shapes scale</li> <li>Animate: Watch the point move around the L2 unit circle</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/norm-comparison-visualizer/#l2-norm-euclidean","title":"L2 Norm (Euclidean)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_2 = \\sqrt{x^2 + y^2}\\)\\) - Standard \"straight-line\" distance - Unit shape is a circle - Used in: Least squares regression, ridge regularization</p>"},{"location":"sims/norm-comparison-visualizer/#l1-norm-manhattan","title":"L1 Norm (Manhattan)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_1 = |x| + |y|\\)\\) - Distance measured along axes (like city blocks) - Unit shape is a diamond (rotated square) - Used in: Lasso regularization, sparse solutions</p>"},{"location":"sims/norm-comparison-visualizer/#l-norm-maximum","title":"L\u221e Norm (Maximum)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_\\infty = \\max(|x|, |y|)\\)\\) - Maximum absolute component value - Unit shape is a square - Used in: Constraining maximum deviation</p>"},{"location":"sims/norm-comparison-visualizer/#why-different-norms-matter","title":"Why Different Norms Matter","text":"<ul> <li>For (3, 4): L1 = 7, L2 = 5, L\u221e = 4</li> <li>The same point can have very different \"distances\" depending on which norm you use</li> <li>In machine learning, L1 promotes sparse solutions while L2 distributes weight evenly</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/norm-comparison-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/norm-comparison-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/norm-comparison-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or machine learning</p>"},{"location":"sims/norm-comparison-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/norm-comparison-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of vectors and distance</li> <li>Familiarity with absolute value</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Drag the point to various locations</li> <li>Observe how the three norm values change differently</li> <li> <p>Find points where two norms give the same value</p> </li> <li> <p>Unit Shape Analysis (5 min):</p> </li> <li>Turn off two norms and examine one at a time</li> <li>Drag the point onto each boundary</li> <li> <p>Notice the indicator when point is \"on boundary\"</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Find a point where L1 &gt; L2 &gt; L\u221e</li> <li>Find a point where all three norms are equal (hint: on an axis)</li> <li> <p>Predict which norm will be largest for a given point</p> </li> <li> <p>Application Discussion (5 min):</p> </li> <li>Why might we use L1 in machine learning?</li> <li>What does it mean geometrically for L1 to \"promote sparsity\"?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the L1 unit \"circle\" a diamond shape?</li> <li>For what points are all three norms equal?</li> <li>Which norm gives the smallest value for most points? Why?</li> <li>How does the relationship between norms change as points move farther from the origin?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Calculate all three norms for a given vector</li> <li>Predict which shape a point is inside/outside of</li> <li>Explain why L1 regularization promotes sparse solutions</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#references","title":"References","text":"<ol> <li>Understanding Different Norms - Wolfram MathWorld</li> <li>L1 vs L2 Regularization - StatQuest</li> <li>Why L1 promotes sparsity - Cross Validated discussion</li> <li>Boyd, S. &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.</li> </ol>"},{"location":"sims/normalization-comparison/","title":"Normalization Comparison","text":"<p>Run the Normalization Comparison Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/normalization-comparison/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization clarifies the difference between batch normalization and layer normalization by showing exactly which dimensions of a tensor each technique normalizes over.</p>"},{"location":"sims/normalization-comparison/#batch-normalization","title":"Batch Normalization","text":"<p>Normalizes across the batch dimension for each channel:</p> <p>\\(\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\\)</p> <p>where \\(\\mu_B\\) and \\(\\sigma_B^2\\) are computed over all samples in the batch, but separately for each channel.</p>"},{"location":"sims/normalization-comparison/#layer-normalization","title":"Layer Normalization","text":"<p>Normalizes across the feature dimension for each sample:</p> <p>\\(\\hat{x} = \\frac{x - \\mu_L}{\\sqrt{\\sigma_L^2 + \\epsilon}}\\)</p> <p>where \\(\\mu_L\\) and \\(\\sigma_L^2\\) are computed over all features in a single sample.</p>"},{"location":"sims/normalization-comparison/#key-differences","title":"Key Differences","text":"Aspect Batch Norm Layer Norm Normalizes Per channel Per sample Batch dependency Yes No Train/Test Different Same Best for CNNs, large batches Transformers, RNNs Batch size 1 Problems Works fine"},{"location":"sims/normalization-comparison/#interactive-features","title":"Interactive Features","text":"<ul> <li>View Selector: Compare both techniques or focus on one</li> <li>Animated Highlight: Shows which cells are normalized together</li> <li>Properties Panel: Details about each technique</li> </ul>"},{"location":"sims/normalization-comparison/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/normalization-comparison/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Distinguish which tensor dimensions batch norm and layer norm operate over</li> <li>Explain why batch normalization needs different behavior during training vs inference</li> <li>Choose the appropriate normalization for different architectures</li> <li>Describe the role of learnable parameters \u03b3 and \u03b2</li> </ol>"},{"location":"sims/normalization-comparison/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Dimension Tracing: For a tensor of shape (B, C, H, W), identify which values batch norm averages together</li> <li>Architecture Matching: Why do transformers prefer layer norm while CNNs use batch norm?</li> <li>Small Batch Problem: What happens to batch norm statistics with batch size 1?</li> <li>Running Statistics: Why does batch norm track running mean/variance during training?</li> </ol>"},{"location":"sims/normalization-comparison/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why can't batch normalization be used effectively with batch size 1?</li> <li>How do the learnable parameters \u03b3 and \u03b2 help after normalization?</li> <li>Why is layer normalization independent of batch size?</li> <li>When might you use instance normalization or group normalization?</li> </ol>"},{"location":"sims/normalization-comparison/#references","title":"References","text":"<ul> <li>Ioffe &amp; Szegedy (2015). Batch Normalization: Accelerating Deep Network Training</li> <li>Ba, Kiros &amp; Hinton (2016). Layer Normalization</li> <li>Wu &amp; He (2018). Group Normalization</li> </ul>"},{"location":"sims/numerical-stability/","title":"Numerical Stability Demonstration","text":"<p>Run the Numerical Stability MicroSim Fullscreen</p> <p>Edit the MicroSim Using the p5.js Editor</p>"},{"location":"sims/numerical-stability/#about-this-microsim","title":"About This MicroSim","text":"<p>When solving systems of linear equations on a computer, small errors in the input can sometimes cause large errors in the output. This phenomenon is governed by the condition number of the matrix\u2014a fundamental concept in numerical linear algebra that every practitioner must understand.</p> <p>This MicroSim allows you to experiment with different types of systems and see firsthand how tiny perturbations in the matrix coefficients or right-hand side vector can dramatically change the solution.</p>"},{"location":"sims/numerical-stability/#what-youll-see","title":"What You'll See","text":"<p>The simulation displays:</p> <ul> <li>Original System (left): The unperturbed augmented matrix \\([A | b]\\) with its exact solution</li> <li>Perturbed System (right): The same system with small random changes applied, showing the new solution</li> <li>Error Analysis Panel: Quantitative measures including condition number, input perturbation size, output error, and error magnification factor</li> <li>Geometric View: For 2\u00d72 systems, a visualization showing how the lines (and their intersection point) shift with perturbation</li> </ul>"},{"location":"sims/numerical-stability/#key-insight","title":"Key Insight","text":"<p>The condition number \\(\\kappa(A)\\) bounds how much errors can be amplified:</p> \\[\\frac{\\|\\Delta x\\| / \\|x\\|}{\\|\\Delta A\\| / \\|A\\|} \\leq \\kappa(A)\\] <p>This means if your input has a relative error of \\(10^{-6}\\) and \\(\\kappa(A) = 10^{6}\\), your solution could have a relative error as large as \\(10^{0} = 1\\) (100% error!).</p>"},{"location":"sims/numerical-stability/#understanding-the-condition-number","title":"Understanding the Condition Number","text":"Condition Number Interpretation Digits of Accuracy Lost \\(\\kappa \\approx 1\\) Well-conditioned, highly stable ~0 \\(\\kappa \\approx 10^2\\) Moderate sensitivity ~2 \\(\\kappa \\approx 10^6\\) Ill-conditioned ~6 \\(\\kappa \\to \\infty\\) Singular (no unique solution) All <p>Rule of Thumb: If you compute with \\(d\\) digits of precision and \\(\\kappa(A) \\approx 10^k\\), expect only about \\(d - k\\) accurate digits in your solution.</p>"},{"location":"sims/numerical-stability/#why-does-this-matter","title":"Why Does This Matter?","text":"<p>In real-world computations, numerical stability is critical:</p> <ol> <li>Floating-point arithmetic introduces rounding errors at every operation (typically \\(10^{-16}\\) relative error in double precision)</li> <li>Measurement errors in experimental data are unavoidable (sensors, instruments, sampling)</li> <li>Ill-conditioned systems amplify these small errors, potentially making solutions meaningless</li> <li>AI and Machine Learning systems rely on solving linear systems\u2014ill-conditioning can cause training instability</li> </ol>"},{"location":"sims/numerical-stability/#real-world-examples","title":"Real-World Examples","text":"<ul> <li>Structural engineering: Stiffness matrices for finite element analysis can become ill-conditioned for thin structures</li> <li>Computer graphics: Transformation matrices for nearly-degenerate geometries</li> <li>Machine learning: Covariance matrices with highly correlated features</li> <li>Signal processing: Vandermonde matrices for polynomial interpolation with many points</li> </ul>"},{"location":"sims/numerical-stability/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>For 2\u00d72 systems, each equation represents a line. The solution is where the lines intersect.</p> <ul> <li>Nearly perpendicular lines \u2192 Well-conditioned: The intersection point is clearly defined and stable</li> <li>Nearly parallel lines \u2192 Ill-conditioned: A tiny rotation of one line moves the intersection dramatically</li> </ul> <p>The Geometric View panel visualizes this: solid blue lines show the original system, dashed red lines show the perturbed system. Watch how the green (original) and red (perturbed) solution points diverge as the condition number increases.</p>"},{"location":"sims/numerical-stability/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/numerical-stability/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Define the condition number and explain what it measures</li> <li>Distinguish between well-conditioned and ill-conditioned systems visually and numerically</li> <li>Predict the magnitude of solution errors given input perturbations and condition number</li> <li>Explain why the Hilbert matrix is notoriously ill-conditioned</li> <li>Connect geometric intuition (angle between lines) to numerical stability</li> <li>Recognize when computed solutions may be unreliable</li> </ol>"},{"location":"sims/numerical-stability/#step-by-step-activities","title":"Step-by-Step Activities","text":""},{"location":"sims/numerical-stability/#activity-1-exploring-well-conditioned-systems-5-minutes","title":"Activity 1: Exploring Well-Conditioned Systems (5 minutes)","text":"<ol> <li>Select \"Well-Conditioned\" from the Example dropdown</li> <li>Observe the condition number in the Error Analysis panel (should be small, around 2-3)</li> <li>Click \"New Random Perturbation\" several times</li> <li>Notice that even though the matrix entries change slightly (highlighted in pink), the solution points (green and red dots) stay close together</li> <li>Question to consider: What angle do the two lines make in the Geometric View?</li> </ol>"},{"location":"sims/numerical-stability/#activity-2-understanding-error-magnification-10-minutes","title":"Activity 2: Understanding Error Magnification (10 minutes)","text":"<ol> <li>Stay on the \"Well-Conditioned\" example</li> <li>Set the Perturbation slider to \\(10^{-4}\\) (the leftmost position)</li> <li>Record the Error Magnification value</li> <li>Move the slider to \\(10^{-3}\\), then \\(10^{-2}\\), then \\(10^{-1}\\)</li> <li>Observe: Does the error magnification stay roughly constant regardless of perturbation size?</li> <li>Key insight: The error magnification should stay bounded by the condition number</li> </ol>"},{"location":"sims/numerical-stability/#activity-3-witnessing-catastrophic-error-amplification-10-minutes","title":"Activity 3: Witnessing Catastrophic Error Amplification (10 minutes)","text":"<ol> <li>Switch to \"Near-Singular\" from the dropdown</li> <li>Observe the very high condition number (typically \\(10^5\\) or more)</li> <li>Set perturbation to \\(10^{-4}\\) and click \"New Random Perturbation\" several times</li> <li>Notice: Even with tiny input changes (\\(10^{-4}\\)), the solution can jump by large amounts</li> <li>Look at the Geometric View: the two lines are nearly parallel!</li> <li>Discussion: Why would solving this system on a computer be problematic?</li> </ol>"},{"location":"sims/numerical-stability/#activity-4-the-infamous-hilbert-matrix-10-minutes","title":"Activity 4: The Infamous Hilbert Matrix (10 minutes)","text":"<ol> <li>Select \"Severely Ill-Conditioned (Hilbert)\" from the dropdown</li> <li>This is a 3\u00d73 Hilbert matrix where \\(H_{ij} = \\frac{1}{i+j-1}\\)</li> <li>Observe the extremely high condition number</li> <li>Set perturbation to \\(10^{-4}\\) and generate several perturbations</li> <li>Notice: The x, y, z solution values can change dramatically</li> <li>Research extension: Look up why Hilbert matrices arise in polynomial fitting and why they're problematic</li> </ol>"},{"location":"sims/numerical-stability/#activity-5-connecting-geometry-to-algebra-10-minutes","title":"Activity 5: Connecting Geometry to Algebra (10 minutes)","text":"<ol> <li>Select \"Moderately Ill-Conditioned\"</li> <li>In the Geometric View, observe that the lines are close to parallel but not quite</li> <li>Switch to \"Well-Conditioned\" and compare the angle between lines</li> <li>Switch to \"Near-Singular\" and observe the lines are nearly identical</li> <li>Complete this statement: \"The condition number is large when the lines are ______\"</li> <li>Challenge: Can you explain why nearly parallel lines lead to sensitive solutions?</li> </ol>"},{"location":"sims/numerical-stability/#activity-6-quantitative-analysis-15-minutes","title":"Activity 6: Quantitative Analysis (15 minutes)","text":"<ol> <li>For each of the four presets, record:<ul> <li>The condition number \\(\\kappa\\)</li> <li>The input perturbation size (from slider)</li> <li>The output error (solution change)</li> <li>The error magnification</li> </ul> </li> <li>Create a table comparing these values</li> <li>Verify that: Error Magnification \u2264 Condition Number (approximately)</li> <li>Discuss: Why is the actual error magnification often less than the theoretical maximum?</li> </ol>"},{"location":"sims/numerical-stability/#self-assessment-quiz","title":"Self-Assessment Quiz","text":"Question 1: What does a large condition number indicate? <p>A) The matrix has a large determinant</p> <p>B) The system has many solutions</p> <p>C) Small input errors may cause large output errors</p> <p>D) The matrix is symmetric</p> Answer <p>C) Small input errors may cause large output errors</p> <p>The condition number measures the sensitivity of the solution to perturbations in the input. A large condition number means the system is ill-conditioned and small errors can be amplified significantly.</p> Question 2: A matrix has condition number \u03ba = 10\u2078. If your input data has relative error 10\u207b\u00b9\u2070, what is the worst-case relative error in your solution? <p>A) 10\u207b\u00b9\u2078</p> <p>B) 10\u207b\u00b9\u2070</p> <p>C) 10\u207b\u00b2</p> <p>D) 10\u2078</p> Answer <p>C) 10\u207b\u00b2</p> <p>The condition number bounds the error amplification: relative output error \u2264 \u03ba \u00d7 relative input error = 10\u2078 \u00d7 10\u207b\u00b9\u2070 = 10\u207b\u00b2.</p> Question 3: In the geometric view of a 2\u00d72 system, what visual characteristic indicates an ill-conditioned system? <p>A) The lines are perpendicular</p> <p>B) The lines are nearly parallel</p> <p>C) The lines pass through the origin</p> <p>D) The lines have positive slopes</p> Answer <p>B) The lines are nearly parallel</p> <p>When lines are nearly parallel, a small rotation of one line causes the intersection point to move dramatically. This geometric instability corresponds to algebraic ill-conditioning.</p> Question 4: Why is the Hilbert matrix particularly problematic for numerical computation? <p>A) It is always singular</p> <p>B) It has complex eigenvalues</p> <p>C) Its condition number grows rapidly with matrix size</p> <p>D) It cannot be stored in computer memory</p> Answer <p>C) Its condition number grows rapidly with matrix size</p> <p>The condition number of an n\u00d7n Hilbert matrix grows exponentially with n. Even a 10\u00d710 Hilbert matrix has a condition number around 10\u00b9\u00b3, making accurate numerical solution nearly impossible with standard double precision.</p> Question 5: You solve a linear system and get a solution, but you suspect the system may be ill-conditioned. What should you do to verify your result? <p>A) Check if the determinant is zero</p> <p>B) Compute the condition number and compare to your working precision</p> <p>C) Verify the matrix is square</p> <p>D) Check if all entries are positive</p> Answer <p>B) Compute the condition number and compare to your working precision</p> <p>The condition number tells you how many digits of accuracy you might lose. If \u03ba \u2248 10\u1d4f and you work with d digits of precision, you can trust only about d\u2212k digits in your solution. If k \u2265 d, your solution may be meaningless.</p>"},{"location":"sims/object-tracking/","title":"Object Tracking Visualizer","text":"<p>Run the Object Tracking Visualizer Fullscreen</p> <p>Edit the Object Tracking Visualizer with the p5.js editor</p>"},{"location":"sims/object-tracking/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates multi-object tracking (MOT) - the problem of maintaining consistent identities for multiple moving objects across frames. This is essential for autonomous vehicles to track pedestrians, vehicles, and cyclists.</p>"},{"location":"sims/object-tracking/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/object-tracking/main.html\"\n        height=\"652px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/object-tracking/#features","title":"Features","text":"<ul> <li>Multiple Objects: Colored bounding boxes with unique track IDs</li> <li>Predictions: Dashed boxes show where tracks are expected to appear</li> <li>Detections: Red boxes show noisy sensor measurements</li> <li>Associations: Yellow lines connect predictions to matched detections</li> <li>Track Trails: Historical positions showing object paths</li> <li>Track Management: New tracks created, lost tracks deleted</li> </ul>"},{"location":"sims/object-tracking/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/object-tracking/#the-tracking-pipeline","title":"The Tracking Pipeline","text":"<ol> <li>Prediction: Use motion model to predict where each track will be</li> <li>Detection: Generate (noisy) measurements of objects</li> <li>Association: Match predictions to detections</li> <li>Update: Refine track states with matched detections</li> <li>Track Management: Create new tracks, delete lost tracks</li> </ol>"},{"location":"sims/object-tracking/#data-association","title":"Data Association","text":"<p>The key challenge is matching detections to tracks: - Gating: Only consider detections within a distance threshold - Cost Matrix: Compute distance between all prediction-detection pairs - Hungarian Algorithm: Find optimal one-to-one assignment</p>"},{"location":"sims/object-tracking/#association-metrics","title":"Association Metrics","text":"Metric Description IoU Intersection over Union of bounding boxes Mahalanobis Distance accounting for uncertainty Euclidean Simple distance between centers"},{"location":"sims/object-tracking/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/object-tracking/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the multi-object tracking pipeline</li> <li>Recognize the role of prediction in data association</li> <li>Apply tracking concepts to handle missed detections</li> </ul>"},{"location":"sims/object-tracking/#activities","title":"Activities","text":"<ol> <li>Step Through: Click \"Step\" to see each predict-associate-update cycle</li> <li>High Noise: Increase detection noise, observe association errors</li> <li>High Miss Rate: Increase miss probability, see tracks use predictions</li> <li>Track Creation/Deletion: Observe new tracks spawn, lost tracks disappear</li> </ol>"},{"location":"sims/object-tracking/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why do we predict before associating detections?</li> <li>What happens when a detection is missed for several frames?</li> <li>How would you handle objects that cross paths?</li> </ol>"},{"location":"sims/object-tracking/#references","title":"References","text":"<ul> <li>SORT: Simple Online Realtime Tracking</li> <li>DeepSORT</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/optimizer-comparison-arena/","title":"Optimizer Comparison Arena","text":"<p>Run the Arena Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/optimizer-comparison-arena/#about-this-microsim","title":"About This MicroSim","text":"<p>This arena lets you race four popular optimization algorithms against each other on different loss landscapes. Watch as they navigate toward the minimum (green dot) with varying strategies.</p>"},{"location":"sims/optimizer-comparison-arena/#the-optimizers","title":"The Optimizers","text":"Optimizer Color Key Feature SGD Blue Basic gradient descent Momentum Green Accumulates velocity RMSprop Purple Adapts per-parameter rates Adam Orange Combines momentum + adaptive rates"},{"location":"sims/optimizer-comparison-arena/#how-to-use","title":"How to Use","text":"<ol> <li>Select Landscape: Choose from Quadratic, Rosenbrock, Beale, or Saddle</li> <li>Enable/Disable Optimizers: Check which optimizers to include</li> <li>Race!: Click to start the competition</li> <li>Observe: Watch which optimizer reaches the minimum first</li> </ol>"},{"location":"sims/optimizer-comparison-arena/#loss-landscapes","title":"Loss Landscapes","text":"<ul> <li>Quadratic: Elongated ellipses testing condition number handling</li> <li>Rosenbrock: Famous banana-shaped valley with global minimum at (1,1)</li> <li>Beale: Multiple local valleys with tricky gradients</li> <li>Saddle Point: Tests optimizer behavior near saddle points</li> </ul>"},{"location":"sims/optimizer-comparison-arena/#key-observations","title":"Key Observations","text":"<ul> <li>SGD may oscillate wildly on ill-conditioned problems</li> <li>Momentum accelerates in consistent directions</li> <li>RMSprop handles varying gradient scales well</li> <li>Adam often provides good all-around performance</li> </ul>"},{"location":"sims/optimizer-comparison-arena/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/optimizer-comparison-arena/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Compare convergence behavior of different optimizers</li> <li>Understand when different optimizers excel</li> <li>Visualize adaptive learning rate mechanisms</li> </ul>"},{"location":"sims/optimizer-comparison-arena/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Quadratic Landscape: Which optimizer handles ill-conditioning best?</li> <li>Rosenbrock Challenge: Watch optimizers navigate the narrow valley</li> <li>Saddle Point: Observe behavior when there's no local minimum</li> <li>Selective Racing: Disable optimizers one by one to see their individual paths</li> </ol>"},{"location":"sims/optimizer-comparison-arena/#references","title":"References","text":"<ul> <li>Ruder, S., An overview of gradient descent optimization algorithms, 2016</li> <li>Kingma &amp; Ba, Adam: A Method for Stochastic Optimization, 2014</li> </ul>"},{"location":"sims/orthogonal-projection/","title":"Orthogonal Projection Interactive Visualizer","text":"<p>Run the Orthogonal Projection Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/orthogonal-projection/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates orthogonal projection of vectors onto lines:</p> <ul> <li>Projection formula: proj_\u00fb(v) = (v\u00b7\u00fb)\u00fb where \u00fb is the unit direction vector</li> <li>Error vector: The component perpendicular to the line (v - proj(v))</li> <li>Orthogonality: The error vector is always perpendicular to the projection line</li> </ul>"},{"location":"sims/orthogonal-projection/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the vector v (blue) to change its position</li> <li>Adjust the line angle to rotate the projection line</li> <li>Toggle checkboxes to show/hide error vector, right angle indicator, and formulas</li> <li>Click Animate to see the vector oscillate between original and projected positions</li> </ol>"},{"location":"sims/orthogonal-projection/#key-concepts","title":"Key Concepts","text":"<p>The projection of v onto unit vector \u00fb is:</p> \\[\\text{proj}_{\\hat{\\mathbf{u}}}(\\mathbf{v}) = (\\mathbf{v} \\cdot \\hat{\\mathbf{u}}) \\hat{\\mathbf{u}}\\] <p>Properties:</p> <ul> <li>The error vector is perpendicular to the projection line</li> <li>Projection is idempotent: projecting twice gives the same result</li> <li>The error vector has minimum length among all vectors from v to the line</li> </ul>"},{"location":"sims/orthogonal-projection/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/orthogonal-projection/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/orthogonal-projection/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/orthogonal-projection/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Calculate the projection of a vector onto a line using the dot product formula</li> <li>Identify the error vector and verify its orthogonality</li> <li>Explain why projection minimizes distance to the subspace</li> </ol>"},{"location":"sims/orthogonal-projection/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify orthogonality: Check that error \u00b7 projection = 0 for various vectors</li> <li>Minimize distance: Show that the projection gives the closest point on the line</li> <li>Special cases: What happens when v is parallel or perpendicular to the line?</li> </ol>"},{"location":"sims/orthogonal-projection/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If v = (4, 3) and the line is the x-axis, what is proj(v)?</li> <li>When is the error vector zero?</li> <li>How does the projection change as the line rotates?</li> </ol>"},{"location":"sims/orthogonal-projection/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations - Projection section</li> <li>Chapter 8: Orthogonality (upcoming)</li> </ul>"},{"location":"sims/orthogonal-transform/","title":"Orthogonal Matrix Transformation","text":"<p>Run the Orthogonal Transform MicroSim Fullscreen</p> <p>Edit the Orthogonal Transform MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/orthogonal-transform/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/orthogonal-transform/#description","title":"Description","text":"<p>An orthogonal matrix Q satisfies Q^T Q = I, meaning its transpose equals its inverse. This special property makes orthogonal matrices preserve lengths and angles\u2014they represent rotations and reflections that transform shapes without distortion.</p> <p>Key Features:</p> <ul> <li>Real-Time Rotation: Drag the angle slider to see the unit square rotate smoothly</li> <li>Length Preservation: Toggle \"Lengths\" to verify that |Qv| = |v| for all vectors</li> <li>Angle Preservation: Toggle \"Angles\" to see that angles between vectors are maintained</li> <li>Reflection Toggle: Click \"Reflect\" to add a reflection (changes det(Q) from +1 to -1)</li> <li>Live Matrix Display: Watch the rotation matrix entries update as cos(\u03b8) and sin(\u03b8)</li> </ul>"},{"location":"sims/orthogonal-transform/#the-rotation-matrix","title":"The Rotation Matrix","text":"<p>A 2D rotation by angle \u03b8 is represented by:</p> \\[Q = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\] <p>Properties:</p> <ul> <li>Orthogonal: Q^T Q = Q Q^T = I</li> <li>Determinant: det(Q) = 1 (rotation) or -1 (reflection)</li> <li>Preserves Lengths: ||Qx|| = ||x|| for all vectors x</li> <li>Preserves Angles: The angle between Qx and Qy equals the angle between x and y</li> </ul>"},{"location":"sims/orthogonal-transform/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/orthogonal-transform/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain why orthogonal matrices preserve lengths and angles</li> <li>Write the 2D rotation matrix for any angle \u03b8</li> <li>Distinguish between rotation (det = +1) and reflection (det = -1)</li> <li>Verify that Q^T Q = I for rotation matrices</li> </ol>"},{"location":"sims/orthogonal-transform/#guided-exploration-5-7-minutes","title":"Guided Exploration (5-7 minutes)","text":"<ol> <li>Rotate the Square: Move the slider from 0\u00b0 to 90\u00b0 and observe the shape transformation</li> <li>Check Lengths: Enable \"Lengths\" and verify the sample vectors maintain their magnitude</li> <li>Check Angles: Enable \"Angles\" and verify the angle between vectors is preserved</li> <li>Add Reflection: Click \"Reflect\" and observe how the square flips</li> </ol>"},{"location":"sims/orthogonal-transform/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Why does the unit square maintain its shape during rotation?</li> <li>What happens to the determinant when reflection is added?</li> <li>How does this relate to preserving area?</li> </ul>"},{"location":"sims/orthogonal-transform/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the rotation matrix for \u03b8 = 90\u00b0?</li> <li>If a rotation matrix has det = -1, is it a pure rotation?</li> <li>Why is Q^(-1) = Q^T computationally advantageous?</li> </ol>"},{"location":"sims/orthogonal-transform/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Orthogonal matrices in context</li> <li>3Blue1Brown: Linear Transformations - Visual understanding of transformations</li> </ul>"},{"location":"sims/orthonormal-basis-finder/","title":"Orthonormal Basis Coordinate Finder","text":"<p>Run the Orthonormal Basis Coordinate Finder Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/orthonormal-basis-finder/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the remarkable simplicity of finding coordinates when using an orthonormal basis. Instead of solving a system of equations, coordinates are simply inner products!</p> <p>Learning Objective: Demonstrate how orthonormal bases simplify finding coordinates via inner products: \\(c_i = \\langle v, q_i \\rangle\\)</p>"},{"location":"sims/orthonormal-basis-finder/#key-features","title":"Key Features","text":"<ul> <li>Draggable Vector: Move the target vector v to see coordinates update instantly</li> <li>Adjustable Basis: Rotate the orthonormal basis to any angle</li> <li>Projection Visualization: See how projections give the coordinates</li> <li>Parseval's Identity: Verify that energy (norm squared) is preserved</li> <li>Standard Basis Comparison: Toggle to compare with standard coordinates</li> </ul>"},{"location":"sims/orthonormal-basis-finder/#the-mathematics","title":"The Mathematics","text":""},{"location":"sims/orthonormal-basis-finder/#coordinates-as-inner-products","title":"Coordinates as Inner Products","text":"<p>For an orthonormal basis \\(\\{q_1, q_2\\}\\), the coordinates of any vector \\(v\\) are:</p> \\[c_1 = \\langle v, q_1 \\rangle = v \\cdot q_1$$ $$c_2 = \\langle v, q_2 \\rangle = v \\cdot q_2\\] <p>This works because: \\(\\(v = c_1 q_1 + c_2 q_2\\)\\)</p> <p>Taking the inner product with \\(q_1\\): \\(\\(\\langle v, q_1 \\rangle = c_1 \\langle q_1, q_1 \\rangle + c_2 \\langle q_2, q_1 \\rangle = c_1 \\cdot 1 + c_2 \\cdot 0 = c_1\\)\\)</p>"},{"location":"sims/orthonormal-basis-finder/#parsevals-identity","title":"Parseval's Identity","text":"<p>For orthonormal bases, the norm is preserved: \\(\\(\\|v\\|^2 = c_1^2 + c_2^2\\)\\)</p> <p>This means the \"energy\" of the vector equals the sum of squared coordinates.</p>"},{"location":"sims/orthonormal-basis-finder/#computational-advantage","title":"Computational Advantage","text":"Method For Orthonormal Basis For General Basis Find coordinates 2 dot products Solve 2x2 system Complexity O(n) O(n^2) to O(n^3) Numerical stability Excellent Depends on condition"},{"location":"sims/orthonormal-basis-finder/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the vector v (green) to change the target vector</li> <li>Drag the q1 endpoint (red) to rotate the orthonormal basis</li> <li>Use the angle slider for precise basis angles</li> <li>Toggle Show Projections to see projection lines</li> <li>Toggle Compare to Standard Basis to see both coordinate systems</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#visual-elements","title":"Visual Elements","text":"Element Color Meaning q1, q2 Red, Blue Orthonormal basis vectors v Green Target vector Projection lines Orange (dashed) Perpendicular projections c1, c2 labels Orange Coordinate values e1, e2 Gray (dashed) Standard basis (when enabled)"},{"location":"sims/orthonormal-basis-finder/#learning-activities","title":"Learning Activities","text":""},{"location":"sims/orthonormal-basis-finder/#activity-1-verify-the-formula-5-minutes","title":"Activity 1: Verify the Formula (5 minutes)","text":"<ol> <li>Set v = (3, 2) and basis angle = 45 degrees</li> <li>Manually calculate: \\(c_1 = 3 \\cdot \\cos(45) + 2 \\cdot \\sin(45)\\)</li> <li>Compare with the displayed c1 value</li> <li>Verify the reconstruction: \\(c_1 q_1 + c_2 q_2 = v\\)</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#activity-2-explore-parsevals-identity-5-minutes","title":"Activity 2: Explore Parseval's Identity (5 minutes)","text":"<ol> <li>Note that \\(\\|v\\|^2 = v_x^2 + v_y^2\\) in standard coordinates</li> <li>Rotate the basis to different angles</li> <li>Observe that \\(c_1^2 + c_2^2\\) always equals \\(\\|v\\|^2\\)</li> <li>This works because orthonormal bases preserve length!</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#activity-3-compare-with-standard-basis-5-minutes","title":"Activity 3: Compare with Standard Basis (5 minutes)","text":"<ol> <li>Enable \"Compare to Standard Basis\"</li> <li>At 0 degrees: orthonormal and standard bases align</li> <li>Notice coordinates match when bases align</li> <li>At other angles: different coordinates, same vector!</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#activity-4-special-angles-10-minutes","title":"Activity 4: Special Angles (10 minutes)","text":"<p>Find vectors where coordinates become nice numbers:</p> <ol> <li>At 45 degrees, find a vector with c1 = c2</li> <li>Find a vector where c2 = 0 (lies along q1)</li> <li>Find a vector where c1 = 0 (lies along q2)</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the formula \\(c_i = \\langle v, q_i \\rangle\\) only work for orthonormal bases?</li> <li>What would happen if we tried this formula with a non-orthonormal basis?</li> <li>Why is computational efficiency important for high-dimensional spaces?</li> <li>How does this relate to Fourier analysis (representing signals as sums of sines and cosines)?</li> </ol>"},{"location":"sims/orthonormal-basis-finder/#connection-to-other-topics","title":"Connection to Other Topics","text":"<ul> <li>Fourier Transform: Sines and cosines form an orthonormal basis for functions</li> <li>Principal Component Analysis: Uses orthonormal eigenvectors</li> <li>Quantum Mechanics: States expressed in orthonormal bases of observables</li> <li>Signal Processing: Orthogonal wavelets for compression</li> </ul>"},{"location":"sims/orthonormal-basis-finder/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of basis and coordinates</li> <li>Inner product (dot product)</li> <li>Projection of vectors</li> </ul>"},{"location":"sims/orthonormal-basis-finder/#references","title":"References","text":"<ul> <li>Chapter 8: Vector Spaces and Inner Products - Orthonormal Bases section</li> <li>Chapter 7: Matrix Decompositions - QR Decomposition</li> <li>3Blue1Brown: Change of basis</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 4.4.</li> </ul>"},{"location":"sims/path-planning/","title":"Path Planning Visualizer","text":"<p>Run the Path Planning Visualizer Fullscreen</p> <p>Edit the Path Planning Visualizer with the p5.js editor</p>"},{"location":"sims/path-planning/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization compares path planning algorithms used in robotics and autonomous systems to find collision-free paths from start to goal through obstacle-filled environments.</p>"},{"location":"sims/path-planning/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/path-planning/main.html\"\n        height=\"702px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/path-planning/#features","title":"Features","text":"<ul> <li>Multiple Algorithms: Compare A*, Dijkstra, and RRT</li> <li>Custom Obstacles: Draw obstacles by dragging</li> <li>Exploration Visualization: See which cells/nodes are explored</li> <li>Performance Metrics: Path length and computation time</li> <li>Interactive Start/Goal: Click to reposition</li> </ul>"},{"location":"sims/path-planning/#algorithms-compared","title":"Algorithms Compared","text":""},{"location":"sims/path-planning/#a-algorithm","title":"A* Algorithm","text":"\\[f(n) = g(n) + h(n)\\] <ul> <li>g(n): Cost from start to node n</li> <li>h(n): Heuristic estimate to goal</li> <li>Optimal: Yes (with admissible heuristic)</li> <li>Complete: Yes</li> </ul>"},{"location":"sims/path-planning/#dijkstras-algorithm","title":"Dijkstra's Algorithm","text":"<ul> <li>Same as A* but with h(n) = 0 (no heuristic)</li> <li>Explores more cells but guarantees optimality</li> <li>Good for multiple goals or when heuristic is hard to define</li> </ul>"},{"location":"sims/path-planning/#rrt-rapidly-exploring-random-trees","title":"RRT (Rapidly-exploring Random Trees)","text":"<ul> <li>Sampling-based, builds random tree toward goal</li> <li>Works well in high-dimensional spaces</li> <li>Probabilistically complete but not optimal</li> <li>RRT* variant achieves asymptotic optimality</li> </ul>"},{"location":"sims/path-planning/#key-observations","title":"Key Observations","text":"Algorithm Explored Cells Path Quality Speed A* Fewer (focused) Optimal Fast Dijkstra More (uniform) Optimal Slower RRT Variable Suboptimal Varies"},{"location":"sims/path-planning/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/path-planning/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Compare search-based vs sampling-based planning</li> <li>Evaluate algorithm tradeoffs (optimality, completeness, speed)</li> <li>Apply appropriate algorithm for different scenarios</li> </ul>"},{"location":"sims/path-planning/#activities","title":"Activities","text":"<ol> <li>A* vs Dijkstra: Run both on same environment, compare explored cells</li> <li>RRT Randomness: Run RRT multiple times, observe different paths</li> <li>Dense Obstacles: Add many obstacles, see which algorithm struggles</li> <li>Narrow Passages: Create a maze, test RRT vs A*</li> </ol>"},{"location":"sims/path-planning/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does A* explore fewer cells than Dijkstra?</li> <li>When might RRT be preferred over A*?</li> <li>What makes a heuristic \"admissible\"?</li> </ol>"},{"location":"sims/path-planning/#references","title":"References","text":"<ul> <li>A* Algorithm Wikipedia</li> <li>RRT Paper (LaValle, 1998)</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/pca-explorer/","title":"PCA Step-by-Step Visualizer","text":"<p>Run the PCA Explorer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/pca-explorer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates Principal Component Analysis (PCA) step by step, showing how data is transformed from its original representation to a lower-dimensional projection.</p> <p>PCA finds the directions of maximum variance in the data and projects onto those directions:</p> \\[\\text{PCA: } X \\rightarrow X_{centered} \\rightarrow \\text{eigenvectors of } X^TX \\rightarrow \\text{projection}\\]"},{"location":"sims/pca-explorer/#key-features","title":"Key Features","text":"<ul> <li>Four-Panel View: See each transformation stage simultaneously</li> <li>Animated Transitions: Watch the data transform step by step</li> <li>Interactive Data Generation: Control cluster shape, spread, rotation, and elongation</li> <li>Scree Plot: Visualize eigenvalue magnitudes</li> <li>Variance Explained: See how much information each component captures</li> <li>Reconstruction Toggle: Visualize the error from dimensionality reduction</li> </ul>"},{"location":"sims/pca-explorer/#the-pca-steps","title":"The PCA Steps","text":"Step Operation Geometric Effect 0 Original Data Raw data with mean point shown 1 Center Translate data so mean is at origin 2 Find PCs Compute eigenvectors of covariance matrix 3 Project Project onto first k principal components"},{"location":"sims/pca-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Click \"1. Center\" to see the data shifted to have zero mean</li> <li>Click \"2. Find PCs\" to compute and display principal component vectors</li> <li>Click \"3. Project\" to see data projected onto the first PC</li> <li>Adjust k slider to keep 1 or 2 components</li> <li>Toggle \"Show Reconstruction\" to see projection error</li> <li>Use data controls to change cluster shape and regenerate</li> </ol>"},{"location":"sims/pca-explorer/#understanding-the-visualization","title":"Understanding the Visualization","text":""},{"location":"sims/pca-explorer/#panel-1-original-data","title":"Panel 1: Original Data","text":"<ul> <li>Raw data points colored by index</li> <li>Red dot shows the mean (center of mass)</li> <li>Data may be offset from origin</li> </ul>"},{"location":"sims/pca-explorer/#panel-2-centered-data","title":"Panel 2: Centered Data","text":"<ul> <li>Data shifted so mean is at origin</li> <li>Same shape, just translated</li> <li>This is essential before computing covariance</li> </ul>"},{"location":"sims/pca-explorer/#panel-3-principal-components","title":"Panel 3: Principal Components","text":"<ul> <li>Red arrow (PC1): Direction of maximum variance</li> <li>Green arrow (PC2): Direction of second-most variance (orthogonal to PC1)</li> <li>Arrow lengths proportional to square root of eigenvalues</li> </ul>"},{"location":"sims/pca-explorer/#panel-4-projected-data","title":"Panel 4: Projected Data","text":"<ul> <li>Points projected onto PC1 (when k=1)</li> <li>Red line shows the projection subspace</li> <li>Orange dashed lines show reconstruction error (when enabled)</li> </ul>"},{"location":"sims/pca-explorer/#scree-plot","title":"Scree Plot","text":"<ul> <li>Bar chart showing eigenvalue magnitudes</li> <li>Helps decide how many components to keep</li> <li>Large drop between bars suggests natural dimensionality</li> </ul>"},{"location":"sims/pca-explorer/#variance-explained","title":"Variance Explained","text":"<ul> <li>Percentage of total variance captured by each PC</li> <li>Sum of kept components shows information retained</li> <li>Higher is better for reconstruction quality</li> </ul>"},{"location":"sims/pca-explorer/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain why centering data is the first step in PCA</li> <li>Identify principal components as eigenvectors of the covariance matrix</li> <li>Understand that eigenvalues indicate variance along each PC direction</li> <li>Demonstrate how projection reduces dimensionality while preserving variance</li> <li>Interpret scree plots to choose the number of components</li> </ul>"},{"location":"sims/pca-explorer/#observations-to-make","title":"Observations to Make","text":""},{"location":"sims/pca-explorer/#elongated-cluster","title":"Elongated Cluster","text":"<p>When the elongation is high: - PC1 captures most of the variance (large first eigenvalue) - Projection onto PC1 loses little information - Scree plot shows large first bar, small second bar</p>"},{"location":"sims/pca-explorer/#circular-cluster-elongation-1","title":"Circular Cluster (elongation \u2248 1)","text":"<p>When data is roughly circular: - Both eigenvalues are similar - Neither PC direction is clearly \"best\" - Dimensionality reduction loses significant information</p>"},{"location":"sims/pca-explorer/#effect-of-rotation","title":"Effect of Rotation","text":"<ul> <li>Changing rotation changes PC directions</li> <li>But variance explained remains the same</li> <li>PCA finds the natural axes regardless of original orientation</li> </ul>"},{"location":"sims/pca-explorer/#mathematical-background","title":"Mathematical Background","text":"<p>For data matrix \\(X\\) (centered):</p> <ol> <li> <p>Covariance Matrix: \\(C = \\frac{1}{n}X^TX\\)</p> </li> <li> <p>Eigendecomposition: \\(C = V\\Lambda V^T\\)</p> </li> <li>\\(V\\): matrix of eigenvectors (principal components)</li> <li> <p>\\(\\Lambda\\): diagonal matrix of eigenvalues</p> </li> <li> <p>Projection: \\(X_{reduced} = XV_k\\)</p> </li> <li> <p>\\(V_k\\): first k columns of V</p> </li> <li> <p>Reconstruction: \\(\\hat{X} = X_{reduced}V_k^T\\)</p> </li> <li> <p>Reconstruction Error: \\(\\|X - \\hat{X}\\|^2 = \\sum_{i=k+1}^{d} \\lambda_i\\)</p> </li> </ol>"},{"location":"sims/pca-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/pca-explorer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask: \"How can we represent high-dimensional data with fewer dimensions while keeping the important structure?\"</p> <p>Introduce PCA as finding the directions where data varies most.</p>"},{"location":"sims/pca-explorer/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<ol> <li>Generate an elongated cluster and step through all phases</li> <li>Point out:</li> <li>Mean computation and centering</li> <li>PC1 aligns with the elongated direction</li> <li> <p>Projection loses the \"thickness\" but keeps the \"length\"</p> </li> <li> <p>Toggle reconstruction to show what information is lost</p> </li> </ol>"},{"location":"sims/pca-explorer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<p>Have students:</p> <ol> <li>Adjust elongation and observe scree plot changes</li> <li>Find settings where 90% variance is captured by PC1</li> <li>Find settings where both PCs capture equal variance</li> </ol>"},{"location":"sims/pca-explorer/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why must we center the data before computing covariance?</li> <li>What does a large gap in the scree plot indicate?</li> <li>When would reducing to 1 dimension be a bad choice?</li> <li>How does the rotation of the original data affect the variance explained?</li> </ol>"},{"location":"sims/pca-explorer/#applications","title":"Applications","text":"<ul> <li>Image Compression: Reduce dimensionality of face images (eigenfaces)</li> <li>Feature Extraction: Find most informative features for machine learning</li> <li>Data Visualization: Project high-dimensional data to 2D or 3D for plotting</li> <li>Noise Reduction: Remove low-variance components that may be noise</li> </ul>"},{"location":"sims/pca-explorer/#references","title":"References","text":"<ul> <li>Chapter 9: Dimensionality Reduction (PCA section)</li> <li>3Blue1Brown: PCA</li> <li>Shlens, J. \"A Tutorial on Principal Component Analysis\"</li> </ul>"},{"location":"sims/perceptron-decision-boundary/","title":"Perceptron Decision Boundary","text":"<p>Run the Perceptron Decision Boundary Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/perceptron-decision-boundary/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization demonstrates how a perceptron\u2014the simplest neural network\u2014creates a linear decision boundary to classify data points into two classes.</p>"},{"location":"sims/perceptron-decision-boundary/#key-concepts","title":"Key Concepts","text":"<ul> <li>Decision Boundary: The line where \\(\\mathbf{w}^T\\mathbf{x} + b = 0\\)</li> <li>Weight Vector: Perpendicular to the decision boundary, determines its orientation</li> <li>Bias: Shifts the boundary away from the origin</li> <li>Linear Separability: Some datasets (like XOR) cannot be separated by a single line</li> </ul>"},{"location":"sims/perceptron-decision-boundary/#interactive-features","title":"Interactive Features","text":"<ol> <li>Drag the Weight Vector: Click and drag the purple arrow to rotate the decision boundary</li> <li>Adjust Bias: Use the slider to shift the boundary parallel to itself</li> <li>Add Custom Points: Click \"Add Points\" then click on the plot to add data points</li> <li>Switch Classes: Press SPACE while in add-point mode to toggle between blue (+1) and red (-1)</li> <li>Run Learning: Watch the perceptron learning algorithm find a solution</li> <li>Preset Datasets: Compare linearly separable data with the XOR pattern</li> </ol>"},{"location":"sims/perceptron-decision-boundary/#visual-indicators","title":"Visual Indicators","text":"<ul> <li>Blue region: Points classified as +1</li> <li>Red region: Points classified as -1</li> <li>Yellow outline: Misclassified points</li> <li>Accuracy: Shown as percentage in the control area</li> </ul>"},{"location":"sims/perceptron-decision-boundary/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/perceptron-decision-boundary/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Explain how weight vectors and bias define a linear decision boundary</li> <li>Identify whether a dataset is linearly separable</li> <li>Describe the perceptron learning algorithm</li> <li>Recognize why the XOR problem motivated multilayer networks</li> </ol>"},{"location":"sims/perceptron-decision-boundary/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Explore Linear Separability: Load different datasets and observe which can achieve 100% accuracy</li> <li>Manual Classification: Try to manually position the boundary to classify all points correctly</li> <li>XOR Challenge: Attempt to classify XOR data and discover why it fails</li> <li>Learning Animation: Run the learning algorithm and observe how weights update</li> </ol>"},{"location":"sims/perceptron-decision-boundary/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the weight vector always point perpendicular to the decision boundary?</li> <li>What happens when you increase the bias? What about when you make it negative?</li> <li>Why can't a single perceptron solve the XOR problem?</li> <li>How could you modify the network to handle non-linear boundaries?</li> </ol>"},{"location":"sims/perceptron-decision-boundary/#references","title":"References","text":"<ul> <li>Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</li> <li>Minsky &amp; Papert (1969). Perceptrons: An Introduction to Computational Geometry</li> </ul>"},{"location":"sims/point-cloud-visualizer/","title":"Point Cloud Visualizer","text":"<p>Run the Point Cloud Visualizer Fullscreen</p> <p>Edit the Point Cloud Visualizer with the p5.js editor</p>"},{"location":"sims/point-cloud-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates point cloud representation and processing. Explore different datasets, color modes, downsampling, and surface normal visualization.</p>"},{"location":"sims/point-cloud-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Select Dataset: Choose terrain, building, sphere, or random point clouds</li> <li>Color Mode: Visualize by height, intensity, normal direction, or RGB</li> <li>Point Size: Adjust visualization size</li> <li>Downsample: Apply voxel grid downsampling</li> <li>Show Normals: Display surface normal vectors</li> <li>Drag to Rotate: Change the 3D view angle</li> <li>Scroll to Zoom: Adjust scale</li> </ol>"},{"location":"sims/point-cloud-visualizer/#key-concepts","title":"Key Concepts","text":"Attribute Type Description Position (x, y, z) 3D coordinates Color (r, g, b) RGB values Normal (nx, ny, nz) Surface orientation Intensity scalar Reflection strength <p>Voxel Grid Downsampling: Divides space into voxels (3D cells) and replaces points within each voxel with their centroid.</p>"},{"location":"sims/point-cloud-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand point cloud data representation - Apply voxel grid downsampling - Interpret surface normals - Visualize 3D spatial data</p>"},{"location":"sims/point-cloud-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/point-cloud-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Point clouds are the raw output of 3D sensors like lidar and depth cameras. Each point has coordinates and optional attributes.</p>"},{"location":"sims/point-cloud-visualizer/#exploration-15-minutes","title":"Exploration (15 minutes)","text":"<ol> <li>Load the terrain dataset - notice how height varies</li> <li>Switch color mode to see different attributes</li> <li>Increase downsampling - observe point reduction</li> <li>Enable normals - see surface orientation</li> <li>Compare building (structured) vs random (unstructured)</li> </ol>"},{"location":"sims/point-cloud-visualizer/#key-insight","title":"Key Insight","text":"<p>Point clouds can represent any 3D surface but require processing (normal estimation, segmentation, registration) for analysis.</p>"},{"location":"sims/point-cloud-visualizer/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Point Cloud Library (PCL)</li> </ul>"},{"location":"sims/positive-definiteness/","title":"Positive Definiteness Visualizer","text":"<p>Run the Positive Definiteness Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/positive-definiteness/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates positive definiteness for 2\u00d72 symmetric matrices by plotting the quadratic form:</p> \\[f(x, y) = \\mathbf{x}^T A \\mathbf{x} = a_{11}x^2 + 2a_{12}xy + a_{22}y^2\\] <p>The 3D surface shows how the function value changes with input, revealing the geometric meaning of:</p> <ul> <li>Positive Definite: Bowl shape (minimum at origin), all eigenvalues positive</li> <li>Negative Definite: Inverted bowl (maximum at origin), all eigenvalues negative</li> <li>Indefinite: Saddle shape, eigenvalues have opposite signs</li> <li>Semi-Definite: Trough shape, one eigenvalue is zero</li> </ul>"},{"location":"sims/positive-definiteness/#key-features","title":"Key Features","text":"<ul> <li>3D Surface Plot: Visualize the quadratic form as a surface</li> <li>Eigenvalue Display: See eigenvalues with color coding (green=positive, red=negative)</li> <li>Contour Lines: Level curves showing where f(x,y) = constant</li> <li>Eigenvector Directions: Principal axes shown on the base plane</li> <li>Interactive Matrix: Adjust matrix entries with sliders</li> <li>Presets: Common examples of each classification</li> </ul>"},{"location":"sims/positive-definiteness/#how-to-use","title":"How to Use","text":"<ol> <li>Select a preset to see classic examples of each type</li> <li>Adjust sliders to modify individual matrix entries</li> <li>Drag to rotate the 3D view</li> <li>Toggle contours to see level curves</li> <li>Observe how eigenvalues change with matrix entries</li> </ol>"},{"location":"sims/positive-definiteness/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Connect eigenvalue signs to the shape of quadratic forms</li> <li>Identify positive definite matrices visually</li> <li>Understand why positive definite matrices arise in optimization</li> <li>Recognize the relationship between contour shapes and eigenvalues</li> </ul>"},{"location":"sims/positive-definiteness/#mathematical-background","title":"Mathematical Background","text":"<p>A symmetric matrix A is:</p> Classification Condition Surface Shape Positive Definite All \u03bb &gt; 0 Bowl (upward) Negative Definite All \u03bb &lt; 0 Bowl (downward) Indefinite Mixed signs Saddle Positive Semi-Def All \u03bb \u2265 0, some = 0 Trough"},{"location":"sims/positive-definiteness/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/positive-definiteness/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask students: \"What does it mean for a function to have a minimum at the origin?\"</p> <p>Connect this to the quadratic form: f(x,y) &gt; 0 for all (x,y) \u2260 (0,0) means the surface is a bowl opening upward.</p>"},{"location":"sims/positive-definiteness/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Positive Definite: Start with preset, note bowl shape and green eigenvalues</li> <li>Negative Definite: Switch preset, observe inverted bowl and red eigenvalues</li> <li>Indefinite: See the saddle point and mixed-color eigenvalues</li> <li>Semi-Definite: Watch how the surface flattens when one eigenvalue is zero</li> </ol>"},{"location":"sims/positive-definiteness/#key-insight","title":"Key Insight","text":"<p>The eigenvectors are the principal axes of the contour ellipses. The eigenvalues determine how \"stretched\" the ellipses are along each axis.</p>"},{"location":"sims/positive-definiteness/#hands-on-activity","title":"Hands-on Activity","text":"<p>Have students predict what happens when:</p> <ol> <li>a\u2081\u2081 and a\u2082\u2082 are both positive but a\u2081\u2082 is large</li> <li>a\u2081\u2082 = 0 (diagonal matrix)</li> <li>a\u2081\u2081 = a\u2082\u2082 and a\u2081\u2082 = 0 (scalar matrix)</li> </ol>"},{"location":"sims/positive-definiteness/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the geometric meaning of x\u1d40Ax &gt; 0?</li> <li>Why must all eigenvalues be positive for positive definiteness?</li> <li>What shape do the contour lines form?</li> </ol>"},{"location":"sims/positive-definiteness/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Cholesky Decomposition section</li> <li>Chapter 6: Eigenvalues and Eigenvectors - Symmetric matrices</li> <li>3Blue1Brown: Eigenvectors and eigenvalues</li> </ul>"},{"location":"sims/power-iteration/","title":"Power Iteration Algorithm","text":"<p>Run Power Iteration Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/power-iteration/#about-this-microsim","title":"About This MicroSim","text":"<p>Power iteration is a simple yet powerful algorithm for finding the dominant eigenvalue (largest in absolute value) and its corresponding eigenvector. It works by repeatedly multiplying a vector by the matrix and normalizing.</p> <p>Key Features:</p> <ul> <li>Step-by-step iteration: Watch each multiply-and-normalize step</li> <li>Convergence visualization: Plot of angle error over iterations</li> <li>Rayleigh quotient: Real-time eigenvalue estimate</li> <li>Theoretical comparison: Green line shows expected convergence rate</li> <li>Editable matrix: Try different matrices</li> </ul>"},{"location":"sims/power-iteration/#the-algorithm","title":"The Algorithm","text":"<pre><code>1. Start with random unit vector x\u2080\n2. Repeat:\n   a. y = Ax\n   b. x = y / ||y||\n   c. \u03bb \u2248 x\u1d40Ax / x\u1d40x (Rayleigh quotient)\n3. Until converged\n</code></pre>"},{"location":"sims/power-iteration/#convergence-rate","title":"Convergence Rate","text":"<p>The error decreases proportionally to |\u03bb\u2082/\u03bb\u2081|^k where: - \u03bb\u2081 is the dominant eigenvalue - \u03bb\u2082 is the second largest eigenvalue - k is the iteration number</p> <p>Faster convergence when |\u03bb\u2082/\u03bb\u2081| is small!</p>"},{"location":"sims/power-iteration/#how-to-use","title":"How to Use","text":"<ol> <li>Click \"Step\" to perform one iteration</li> <li>Click \"Run\" for continuous iteration</li> <li>Adjust speed slider to control animation speed</li> <li>Click \"Reset\" to start with a new random vector</li> <li>Edit matrix cells to try different matrices</li> </ol>"},{"location":"sims/power-iteration/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/power-iteration/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/power-iteration/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/power-iteration/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Explain how power iteration finds the dominant eigenvector</li> <li>Connect the eigenvalue ratio |\u03bb\u2082/\u03bb\u2081| to convergence speed</li> <li>Use the Rayleigh quotient to estimate eigenvalues</li> </ol>"},{"location":"sims/power-iteration/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Predict convergence: Before running, predict if convergence will be fast or slow based on eigenvalue ratio</li> <li>Worst case: Find a matrix where power iteration converges very slowly</li> <li>Best case: Find a matrix where it converges in one step</li> <li>Complex eigenvalues: What happens with [[0, -1], [1, 0]]?</li> </ol>"},{"location":"sims/power-iteration/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does power iteration find the dominant eigenvalue specifically?</li> <li>If |\u03bb\u2081| = |\u03bb\u2082|, what goes wrong with power iteration?</li> <li>How does the Rayleigh quotient provide a better eigenvalue estimate than just looking at vector scaling?</li> </ol>"},{"location":"sims/power-iteration/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/projection-subspace/","title":"Projection onto Subspace Visualizer","text":"<p>Run the Projection onto Subspace Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/projection-subspace/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how vectors project onto subspaces (planes or lines) in 3D space. Key concepts include:</p> <ul> <li>Projection: Finding the closest point in a subspace to a given vector</li> <li>Error vector: The perpendicular component from the projection to the original vector</li> <li>Orthogonality: The error vector is always perpendicular to the subspace</li> </ul>"},{"location":"sims/projection-subspace/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust vector v using the sliders (x, y, z components) to change the vector being projected</li> <li>Modify basis vectors u1 and u2 to change the orientation of the subspace</li> <li>Toggle subspace type between 2D plane and 1D line</li> <li>Enable/disable displays for error vector and projection matrix information</li> <li>Drag in the 3D view to rotate and explore different viewing angles</li> <li>Click Reset to return to default values</li> </ol>"},{"location":"sims/projection-subspace/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/projection-subspace/#projection-onto-a-line-1d-subspace","title":"Projection onto a Line (1D Subspace)","text":"<p>The projection of vector v onto line spanned by u is:</p> \\[\\text{proj}_W(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{u}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}\\]"},{"location":"sims/projection-subspace/#projection-onto-a-plane-2d-subspace","title":"Projection onto a Plane (2D Subspace)","text":"<p>For a plane W spanned by vectors u1 and u2, the projection is:</p> \\[\\mathbf{p} = A(A^T A)^{-1} A^T \\mathbf{v}\\] <p>where \\(A = [\\mathbf{u}_1 | \\mathbf{u}_2]\\) is the matrix with basis vectors as columns.</p>"},{"location":"sims/projection-subspace/#key-properties","title":"Key Properties","text":"<ol> <li>Closest point: The projection p is the closest point in the subspace to v</li> <li>Orthogonal error: The error e = v - p is perpendicular to the subspace</li> <li>Minimum distance: ||e|| is the minimum distance from v to any point in the subspace</li> </ol>"},{"location":"sims/projection-subspace/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/projection-subspace/main.html\" height=\"532px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/projection-subspace/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/projection-subspace/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Calculate the projection of a vector onto a subspace</li> <li>Verify that the error vector is orthogonal to the subspace</li> <li>Explain why projection gives the closest point in the subspace</li> <li>Connect projection to least squares and orthogonal decomposition</li> </ol>"},{"location":"sims/projection-subspace/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify orthogonality: Check that e . u1 = 0 and e . u2 = 0 for various configurations</li> <li>Minimize distance: Show that moving v closer to the projection decreases ||e||</li> <li>Special cases: What happens when v is already in the subspace?</li> <li>Compare subspace dimensions: How does projection onto a line differ from projection onto a plane?</li> </ol>"},{"location":"sims/projection-subspace/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If v = (2, 3, 4) and W is the xy-plane, what is proj_W(v)?</li> <li>When is the error vector zero?</li> <li>How does the projection change as the subspace rotates?</li> <li>What is the relationship between projection and orthogonal complement?</li> </ol>"},{"location":"sims/projection-subspace/#applications","title":"Applications","text":"<ul> <li>Least squares regression: Finding the best-fit line/plane by projecting data onto a subspace</li> <li>Principal Component Analysis: Projecting high-dimensional data onto lower-dimensional subspaces</li> <li>Computer graphics: Shadow casting and 3D-to-2D projection</li> <li>Signal processing: Extracting signal components by projecting onto signal subspaces</li> </ul>"},{"location":"sims/projection-subspace/#references","title":"References","text":"<ul> <li>Chapter 8: Orthogonality and Projections</li> <li>Chapter 9: Least Squares Applications</li> </ul>"},{"location":"sims/pseudoinverse-solver/","title":"Pseudoinverse Solver MicroSim","text":"<p>Run the Pseudoinverse Solver Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/pseudoinverse-solver/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the Moore-Penrose pseudoinverse and how it solves least squares problems. The pseudoinverse \\(A^+\\) provides a generalized solution to systems \\(Ax = b\\) even when:</p> <ul> <li>The system is overdetermined (more equations than unknowns)</li> <li>The system is underdetermined (more unknowns than equations)</li> <li>The matrix is rank-deficient (linearly dependent rows or columns)</li> </ul>"},{"location":"sims/pseudoinverse-solver/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/pseudoinverse-solver/#the-pseudoinverse-solution","title":"The Pseudoinverse Solution","text":"<p>For any matrix \\(A\\), the pseudoinverse \\(A^+\\) provides:</p> \\[x = A^+ b\\] <p>This solution has two important properties:</p> <ol> <li>Least squares: Minimizes \\(\\|Ax - b\\|\\) when no exact solution exists</li> <li>Minimum norm: Among all solutions, gives the one with smallest \\(\\|x\\|\\)</li> </ol>"},{"location":"sims/pseudoinverse-solver/#svd-connection","title":"SVD Connection","text":"<p>The pseudoinverse is computed from the SVD:</p> \\[A = U \\Sigma V^T\\] \\[A^+ = V \\Sigma^+ U^T\\] <p>where \\(\\Sigma^+\\) has reciprocals of non-zero singular values.</p>"},{"location":"sims/pseudoinverse-solver/#preset-examples","title":"Preset Examples","text":"Example Type What It Shows Overdetermined (3x2) More rows than columns Least squares solution minimizing residual Rank Deficient (3x2) Linearly dependent columns Pseudoinverse handles rank deficiency Underdetermined (2x3) More columns than rows Minimum-norm solution among infinitely many Full Rank (2x2) Square invertible Pseudoinverse equals regular inverse"},{"location":"sims/pseudoinverse-solver/#how-to-use","title":"How to Use","text":"<ol> <li>Select a preset to see different system types</li> <li>Modify matrix entries directly in the input fields</li> <li>Click Compute Pseudoinverse to update results</li> <li>Toggle Show SVD to see the underlying decomposition</li> <li>Observe the residual norm to verify solution quality</li> </ol>"},{"location":"sims/pseudoinverse-solver/#understanding-the-display","title":"Understanding the Display","text":"<ul> <li>Matrix A: The input system matrix with rank indicator</li> <li>Pseudoinverse A+: The computed generalized inverse</li> <li>Vector b: The right-hand side target</li> <li>Solution x: The computed solution \\(x = A^+ b\\)</li> <li>Residual r: The error vector \\(r = Ax - b\\)</li> <li>Residual Norm: \\(\\|r\\|\\) - should be minimal for least squares</li> </ul>"},{"location":"sims/pseudoinverse-solver/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Apply the pseudoinverse to solve overdetermined systems</li> <li>Explain why the pseudoinverse gives the least squares solution</li> <li>Identify rank-deficient systems and understand their solutions</li> <li>Compare pseudoinverse with normal equation approach</li> <li>Connect the pseudoinverse to SVD computation</li> </ul>"},{"location":"sims/pseudoinverse-solver/#mathematical-background","title":"Mathematical Background","text":""},{"location":"sims/pseudoinverse-solver/#normal-equations-vs-pseudoinverse","title":"Normal Equations vs Pseudoinverse","text":"<p>For full column rank matrices, the normal equation gives:</p> \\[x = (A^T A)^{-1} A^T b\\] <p>The pseudoinverse approach:</p> \\[x = A^+ b = V \\Sigma^+ U^T b\\] <p>Both give the same result for full rank, but the pseudoinverse:</p> <ul> <li>Works for rank-deficient matrices</li> <li>Is numerically more stable</li> <li>Gives minimum-norm solution for underdetermined systems</li> </ul>"},{"location":"sims/pseudoinverse-solver/#geometric-interpretation","title":"Geometric Interpretation","text":"<ul> <li>Overdetermined: Projects \\(b\\) onto column space of \\(A\\)</li> <li>Underdetermined: Finds the smallest \\(x\\) in row space of \\(A\\)</li> <li>Residual: Always perpendicular to column space</li> </ul>"},{"location":"sims/pseudoinverse-solver/#observations-to-make","title":"Observations to Make","text":""},{"location":"sims/pseudoinverse-solver/#overdetermined-system-default","title":"Overdetermined System (Default)","text":"<p>With the 3x2 system, notice: - The residual is generally non-zero (no exact solution) - The solution minimizes the squared error - Adding the residual to \\(Ax\\) gives exactly \\(b\\)</p>"},{"location":"sims/pseudoinverse-solver/#rank-deficient-system","title":"Rank-Deficient System","text":"<p>When columns are linearly dependent: - Rank is less than the number of columns - Some singular values are zero - Pseudoinverse still works correctly</p>"},{"location":"sims/pseudoinverse-solver/#underdetermined-system","title":"Underdetermined System","text":"<p>When there are more unknowns than equations: - Infinitely many solutions exist - Pseudoinverse gives the one with minimum \\(\\|x\\|\\) - Residual is typically zero (exact solution exists)</p>"},{"location":"sims/pseudoinverse-solver/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/pseudoinverse-solver/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask: \"What do we do when a system has no exact solution?\"</p> <p>Introduce least squares as finding the closest approximation.</p>"},{"location":"sims/pseudoinverse-solver/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<ol> <li>Start with the overdetermined preset</li> <li>Show that the residual is non-zero but minimal</li> <li>Try adjusting \\(b\\) to make the system consistent (residual = 0)</li> <li>Switch to underdetermined and observe minimum-norm property</li> </ol>"},{"location":"sims/pseudoinverse-solver/#key-insight","title":"Key Insight","text":"<p>The pseudoinverse unifies: - Solving overdetermined systems (least squares) - Solving underdetermined systems (minimum norm) - Handling rank deficiency gracefully</p>"},{"location":"sims/pseudoinverse-solver/#practice-10-minutes","title":"Practice (10 minutes)","text":"<p>Have students: 1. Predict if a system will have zero or non-zero residual 2. Verify the least squares property manually 3. Compare with the normal equation when applicable</p>"},{"location":"sims/pseudoinverse-solver/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why is \\(\\|Ax - b\\| = 0\\) not always achievable?</li> <li>What does \"minimum norm\" mean geometrically?</li> <li>When does \\(A^+ = A^{-1}\\)?</li> </ol>"},{"location":"sims/pseudoinverse-solver/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Pseudoinverse section</li> <li>Chapter 9: Solving Linear Systems - Least Squares</li> <li>Strang, G. \"Linear Algebra and Learning from Data\"</li> </ul>"},{"location":"sims/quaternion-rotation/","title":"Quaternion Rotation Visualizer","text":"<p>Run the Quaternion Rotation Visualizer Fullscreen</p> <p>Edit the Quaternion Rotation Visualizer with the p5.js editor</p>"},{"location":"sims/quaternion-rotation/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates quaternion rotation representation. Define a rotation axis and angle, see the quaternion components, apply rotations, and compose multiple rotations without gimbal lock.</p>"},{"location":"sims/quaternion-rotation/#how-to-use","title":"How to Use","text":"<ol> <li>Set Rotation Axis: Adjust X, Y, Z sliders to define the rotation axis</li> <li>Set Angle: Choose the rotation angle (0\u00b0 to 360\u00b0)</li> <li>Apply Rotation: Set the object to this orientation</li> <li>Compose: Multiply this rotation with the current orientation</li> <li>Show Euler Angles: Toggle to see equivalent Euler angles</li> <li>Drag to Rotate: Change the view angle</li> </ol>"},{"location":"sims/quaternion-rotation/#key-concepts","title":"Key Concepts","text":"<p>The quaternion formula for rotation by angle \u03b8 about unit axis n\u0302:</p> \\[\\mathbf{q} = \\left(\\cos\\frac{\\theta}{2}, \\sin\\frac{\\theta}{2} \\cdot n_x, \\sin\\frac{\\theta}{2} \\cdot n_y, \\sin\\frac{\\theta}{2} \\cdot n_z\\right)\\] Component Symbol Range Scalar w -1 to 1 Vector (x, y, z) unit sphere"},{"location":"sims/quaternion-rotation/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Convert between axis-angle and quaternion representations - Apply quaternion rotation formula q\u00b7p\u00b7q* - Compose rotations using quaternion multiplication - Compare quaternion and Euler angle representations</p>"},{"location":"sims/quaternion-rotation/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/quaternion-rotation/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Quaternions extend complex numbers to 4D and provide a singularity-free rotation representation.</p>"},{"location":"sims/quaternion-rotation/#exploration-15-minutes","title":"Exploration (15 minutes)","text":"<ol> <li>Set axis to (0, 1, 0) and angle to 90\u00b0 - observe the quaternion</li> <li>Apply the rotation and see the object orient</li> <li>Compose multiple rotations without gimbal lock</li> <li>Compare with equivalent Euler angles</li> </ol>"},{"location":"sims/quaternion-rotation/#key-insight","title":"Key Insight","text":"<p>Notice the half-angle in the quaternion: 90\u00b0 rotation uses 45\u00b0 in the formula because the q\u00b7p\u00b7q* operation effectively applies the rotation twice.</p>"},{"location":"sims/quaternion-rotation/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Quaternions - Wikipedia</li> </ul>"},{"location":"sims/ref-vs-rref/","title":"REF vs RREF Comparison","text":"<p>Run the REF vs RREF MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/ref-vs-rref/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows the same matrix in three forms side by side: the original matrix, its Row Echelon Form (REF), and its Reduced Row Echelon Form (RREF). Key features like pivot positions and created zeros are highlighted.</p>"},{"location":"sims/ref-vs-rref/#key-differences","title":"Key Differences","text":"Property REF RREF Leading entries Any nonzero Must be 1 Zeros below pivots Yes Yes Zeros above pivots No Yes Solution method Back substitution Direct reading Uniqueness Not unique Unique Computation cost Fewer operations More operations"},{"location":"sims/ref-vs-rref/#visual-legend","title":"Visual Legend","text":"<ul> <li>Gold circles: Pivot positions (leading entries)</li> <li>Green cells: Zeros created by elimination</li> </ul>"},{"location":"sims/ref-vs-rref/#when-to-use-each-form","title":"When to Use Each Form","text":"<p>Row Echelon Form (REF):</p> <ul> <li>More efficient for solving a single system</li> <li>Sufficient for determining solution existence</li> <li>Useful when computational cost matters</li> </ul> <p>Reduced Row Echelon Form (RREF):</p> <ul> <li>Best for reading solutions directly</li> <li>Required for finding null space basis</li> <li>Unique representation of the matrix</li> <li>Better for multiple right-hand sides</li> </ul>"},{"location":"sims/ref-vs-rref/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/ref-vs-rref/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Distinguish between REF and RREF visually</li> <li>Identify the additional operations needed to reach RREF from REF</li> <li>Understand when each form is preferable</li> <li>Recognize that RREF is unique while REF is not</li> </ol>"},{"location":"sims/ref-vs-rref/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Count the Difference: Generate several random systems and compare the operation counts</li> <li>Identify Properties: For each generated matrix, verify that REF and RREF satisfy their defining properties</li> <li>Solution Extraction: Practice reading solutions from RREF and using back substitution on REF</li> <li>Predict RREF: Given the REF, predict what additional operations are needed to reach RREF</li> </ol>"},{"location":"sims/regularization-geometry/","title":"Regularization Geometry Visualizer","text":"<p>Run the Regularization Geometry Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/regularization-geometry/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the geometric interpretation of regularization in machine learning. By showing how constraint regions (L1 diamond or L2 circle) intersect with loss function contours, students can understand why L1 regularization produces sparse solutions while L2 produces smooth shrinkage.</p> <p>Learning Objective: Understand how L1 and L2 regularization constrain weights geometrically and why L1 produces sparsity.</p>"},{"location":"sims/regularization-geometry/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the OLS Point: Click and drag the red OLS (Ordinary Least Squares) solution point to different locations</li> <li>Adjust Regularization Strength: Use the alpha slider to change the constraint region size</li> <li>Toggle L1/L2: Click the buttons to switch between Lasso (L1) and Ridge (L2) regularization</li> <li>Show Regularization Path: Enable to see how the solution moves as alpha changes</li> <li>Animate: Watch the regularized solution evolve as constraint strength increases</li> </ol>"},{"location":"sims/regularization-geometry/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/regularization-geometry/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"\\[\\min_\\theta \\|y - X\\theta\\|_2^2 + \\alpha \\|\\theta\\|_2^2\\] <ul> <li>Constraint region is a circle</li> <li>Solution is found where loss contour is tangent to circle</li> <li>All weights shrink proportionally toward zero</li> <li>Weights become small but rarely exactly zero</li> </ul>"},{"location":"sims/regularization-geometry/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"\\[\\min_\\theta \\|y - X\\theta\\|_2^2 + \\alpha \\|\\theta\\|_1\\] <ul> <li>Constraint region is a diamond (rotated square)</li> <li>Solution often hits corners of the diamond</li> <li>Corners lie on the axes where one or more weights equal zero</li> <li>This produces sparse solutions with exact zeros</li> </ul>"},{"location":"sims/regularization-geometry/#why-l1-produces-sparsity","title":"Why L1 Produces Sparsity","text":"<p>The key insight is geometric:</p> <ul> <li>Elliptical loss contours typically first touch the diamond constraint at a corner</li> <li>Corners of the diamond correspond to sparse solutions (weights on axes)</li> <li>For L2, the smooth circle has no corners, so solutions rarely hit the axes exactly</li> </ul>"},{"location":"sims/regularization-geometry/#mathematical-interpretation","title":"Mathematical Interpretation","text":""},{"location":"sims/regularization-geometry/#constrained-vs-penalized-form","title":"Constrained vs Penalized Form","text":"<p>The visualization shows the constrained form:</p> <ul> <li>L2: \\(\\|\\theta\\|_2 \\leq t\\) (inside circle)</li> <li>L1: \\(\\|\\theta\\|_1 \\leq t\\) (inside diamond)</li> </ul> <p>This is equivalent to the penalized form used in practice, where \\(\\alpha\\) controls the tradeoff.</p>"},{"location":"sims/regularization-geometry/#the-regularization-path","title":"The Regularization Path","text":"<p>As \\(\\alpha\\) increases (constraint tightens):</p> <ul> <li>L2: Solution moves smoothly toward origin along a curved path</li> <li>L1: Solution moves toward axes, with weights becoming exactly zero</li> </ul>"},{"location":"sims/regularization-geometry/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/regularization-geometry/main.html\"\n        height=\"532px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/regularization-geometry/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/regularization-geometry/#grade-level","title":"Grade Level","text":"<p>Undergraduate machine learning or statistics course</p>"},{"location":"sims/regularization-geometry/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/regularization-geometry/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of least squares regression</li> <li>Familiarity with norms (L1 and L2)</li> <li>Basic optimization concepts</li> </ul>"},{"location":"sims/regularization-geometry/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Start with L2 regularization</li> <li>Drag the OLS solution to various positions</li> <li>Observe how the regularized solution moves along the circle boundary</li> <li> <p>Note that both weights shrink but neither becomes exactly zero</p> </li> <li> <p>L1 Discovery (5 min):</p> </li> <li>Switch to L1 regularization</li> <li>Observe that the constraint region is now a diamond</li> <li>Drag the OLS solution and watch the regularized solution</li> <li> <p>Notice when the solution \"snaps\" to an axis (sparse solution)</p> </li> <li> <p>Comparative Analysis (5 min):</p> </li> <li>Enable \"Show Regularization Path\"</li> <li>Click \"Animate\" to watch the full path</li> <li>Compare paths for L1 vs L2</li> <li> <p>Identify when L1 produces exact zeros</p> </li> <li> <p>Critical Thinking (5 min):</p> </li> <li>Why do the corners of the diamond lie on the axes?</li> <li>What OLS positions produce sparse solutions most easily?</li> <li> <p>How does ellipse orientation affect the result?</p> </li> <li> <p>Application Discussion (5 min):</p> </li> <li>When would you prefer L1 over L2?</li> <li>Feature selection implications</li> <li>Real-world examples (gene selection, image compression)</li> </ol>"},{"location":"sims/regularization-geometry/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the L1 constraint region diamond-shaped?</li> <li>For what OLS solution positions will L1 definitely produce sparsity?</li> <li>How does the regularization path differ between L1 and L2?</li> <li>Why might L2 be preferred when all features are believed to be relevant?</li> <li>What happens when the OLS solution is already close to the origin?</li> </ol>"},{"location":"sims/regularization-geometry/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Predict whether L1 or L2 will produce a sparse solution for a given OLS position</li> <li>Explain geometrically why L1 promotes sparsity</li> <li>Calculate the regularized solution for simple cases</li> <li>Compare elastic net (combination of L1 and L2) behavior</li> </ul>"},{"location":"sims/regularization-geometry/#connections-to-machine-learning","title":"Connections to Machine Learning","text":""},{"location":"sims/regularization-geometry/#ridge-regression-l2","title":"Ridge Regression (L2)","text":"<ul> <li>Used when multicollinearity is present</li> <li>Keeps all features with reduced magnitudes</li> <li>Closed-form solution exists</li> </ul>"},{"location":"sims/regularization-geometry/#lasso-regression-l1","title":"Lasso Regression (L1)","text":"<ul> <li>Automatic feature selection</li> <li>Produces interpretable sparse models</li> <li>No closed-form solution (requires iterative methods)</li> </ul>"},{"location":"sims/regularization-geometry/#elastic-net","title":"Elastic Net","text":"<ul> <li>Combines L1 and L2 penalties</li> <li>Benefits of both sparsity and grouped selection</li> <li>Useful when features are correlated</li> </ul>"},{"location":"sims/regularization-geometry/#references","title":"References","text":"<ol> <li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The Elements of Statistical Learning. Springer. Chapter 3.</li> <li>Visual explanation of regularization - Terence Parr</li> <li>Why L1 norm for sparse models - Cross Validated</li> <li>Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, 58(1), 267-288.</li> </ol>"},{"location":"sims/rgb-channel-decomposition/","title":"RGB Channel Decomposition","text":"<p>Run the RGB Channel Decomposition Fullscreen</p> <p>Edit the RGB Channel Decomposition with the p5.js editor</p>"},{"location":"sims/rgb-channel-decomposition/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates how color images are composed of three separate channels: Red, Green, and Blue. Each channel is a grayscale matrix that, when combined, produces the full-color image.</p>"},{"location":"sims/rgb-channel-decomposition/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Choose from Rainbow, Sunset, Nature, or Random patterns</p> </li> <li> <p>Channel Intensity Sliders: Adjust the R, G, B sliders (0-100%) to see how reducing a channel's contribution affects the final image</p> </li> <li> <p>Channel Toggles: Enable/disable individual channels to see their contribution</p> </li> <li> <p>Color Tint Toggle: Switch between color-tinted channel views and grayscale views</p> </li> <li> <p>Hover: Move mouse over any pixel to see its RGB values</p> </li> </ol>"},{"location":"sims/rgb-channel-decomposition/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand that color images consist of three separate channels - Analyze how each channel contributes to the final color - Recognize the additive nature of RGB color mixing - Connect matrix operations to color image manipulation</p>"},{"location":"sims/rgb-channel-decomposition/#key-concepts","title":"Key Concepts","text":"<ul> <li>Additive Color Model: RGB colors add together - mixing all three at full intensity produces white</li> <li>Channel Independence: Each channel is a separate matrix that can be processed independently</li> <li>Color Space: RGB is the standard for displays; other spaces (HSV, YCbCr) are used for different purposes</li> </ul>"},{"location":"sims/rgb-channel-decomposition/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/rgb-channel-decomposition/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Explain that every color on a screen is created by mixing red, green, and blue light at different intensities. This is fundamentally different from mixing paints (subtractive color).</p>"},{"location":"sims/rgb-channel-decomposition/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with Rainbow - observe how pure hues use different channel combinations</li> <li>Disable channels one at a time to see their contribution</li> <li>Reduce channel intensities to see how colors shift</li> </ol>"},{"location":"sims/rgb-channel-decomposition/#interactive-exercise-10-minutes","title":"Interactive Exercise (10 minutes)","text":"<ol> <li>Try to predict: What color results from R=255, G=255, B=0? (Yellow)</li> <li>What about R=255, G=0, B=255? (Magenta)</li> <li>Why does the sky appear mostly in the Blue channel?</li> </ol>"},{"location":"sims/rgb-channel-decomposition/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why do we use three channels? (Human vision has three cone types)</li> <li>What would happen with four channels? (Some cameras use RGBW)</li> <li>How does JPEG compression use this? (YCbCr separates brightness from color)</li> </ul>"},{"location":"sims/rgb-channel-decomposition/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>RGB Color Model - Wikipedia</li> </ul>"},{"location":"sims/rigid-body-transform/","title":"Rigid Body Transform Chain","text":"<p>Run the Rigid Body Transform Chain Fullscreen</p> <p>Edit the Rigid Body Transform Chain with the p5.js editor</p>"},{"location":"sims/rigid-body-transform/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates how rigid body transforms compose in a kinematic chain, like a robot arm. See how each joint's rotation and link's translation combine to position the end effector.</p>"},{"location":"sims/rigid-body-transform/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Joint Angles: Use sliders to rotate each joint</li> <li>Show Coordinate Frames: Toggle to see local coordinate systems</li> <li>Show Transforms: View the transform composition</li> <li>Drag to Rotate: Change the 3D view angle</li> <li>Track End Effector: Watch the computed position update</li> </ol>"},{"location":"sims/rigid-body-transform/#key-concepts","title":"Key Concepts","text":"<p>Each transform in SE(3) combines rotation R and translation t:</p> \\[\\mathbf{T} = \\begin{bmatrix} \\mathbf{R} &amp; \\mathbf{t} \\\\ \\mathbf{0}^T &amp; 1 \\end{bmatrix}\\] <p>Transform composition: T_total = T\u2081 \u00b7 T\u2082 \u00b7 T\u2083</p> Joint Rotation Axis Link Length 1 (Base) Y (yaw) 120 2 (Shoulder) X (pitch) 100 3 (Elbow) X (pitch) 80"},{"location":"sims/rigid-body-transform/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand SE(3) rigid body transforms - Compose multiple transforms via matrix multiplication - Compute forward kinematics for serial chains - Visualize how local transforms affect global position</p>"},{"location":"sims/rigid-body-transform/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/rigid-body-transform/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Robot arms use chains of transforms where each joint adds rotation and each link adds translation. The composition order matters!</p>"},{"location":"sims/rigid-body-transform/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with all joints at zero</li> <li>Rotate joint 1 - notice the entire arm rotates</li> <li>Rotate joint 2 - only links 2 and 3 move</li> <li>Observe how the end effector position updates</li> </ol>"},{"location":"sims/rigid-body-transform/#forward-kinematics","title":"Forward Kinematics","text":"<p>The end effector position is computed by multiplying all transform matrices from base to tip. This is called forward kinematics.</p>"},{"location":"sims/rigid-body-transform/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>SE(3) - Wikipedia</li> </ul>"},{"location":"sims/row-column-vectors/","title":"Row and Column Vectors","text":"<p>Run the Row and Column Vectors MicroSim Fullscreen</p> <p>Edit the Row and Column Vectors MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/row-column-vectors/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/row-column-vectors/#description","title":"Description","text":"<p>This MicroSim provides a side-by-side comparison of row vectors and column vectors, the two fundamental vector orientations in linear algebra. Understanding the distinction between these vector types is essential for matrix operations, as the orientation determines how vectors interact with matrices during multiplication.</p> <p>Key Features:</p> <ul> <li>Visual Comparison: Row vectors are displayed horizontally (1\u00d7n), while column vectors are displayed vertically (m\u00d71)</li> <li>Color Coding: Blue for row vectors, green for column vectors helps reinforce the distinction</li> <li>Dynamic Dimensions: Adjust the number of elements (2-6) to see how both vector types scale</li> <li>Dimension Annotations: Toggle display of dimension labels (1\u00d7n and m\u00d71) to reinforce notation</li> </ul>"},{"location":"sims/row-column-vectors/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/row-column-vectors/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Visually distinguish between row vectors and column vectors</li> <li>Correctly identify the dimensions of row vectors (1\u00d7n) and column vectors (m\u00d71)</li> <li>Understand that the same values can be arranged as either a row or column vector</li> <li>Recognize how vector orientation affects matrix compatibility in multiplication</li> </ol>"},{"location":"sims/row-column-vectors/#warm-up-activity-2-minutes","title":"Warm-up Activity (2 minutes)","text":"<p>Ask students: \"If I have 4 numbers, how many different ways can I arrange them in a rectangle?\" Let them discover that a 1\u00d74 row and 4\u00d71 column are two valid arrangements.</p>"},{"location":"sims/row-column-vectors/#guided-exploration-5-minutes","title":"Guided Exploration (5 minutes)","text":"<ol> <li>Start with the default 4 elements</li> <li>Point out that both vectors contain the same number of values</li> <li>Toggle the dimension annotations to show 1\u00d74 for the row and 4\u00d71 for the column</li> <li>Click \"Randomize\" several times to show that values change but structure remains</li> </ol>"},{"location":"sims/row-column-vectors/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Notation: Row vectors are written horizontally: \\([r_1, r_2, r_3, r_4]\\)</li> <li>Transpose Relationship: A row vector transposed becomes a column vector</li> <li>Matrix Multiplication: A row vector (1\u00d7n) can multiply a column vector (n\u00d71) to produce a scalar (dot product)</li> </ul>"},{"location":"sims/row-column-vectors/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What are the dimensions of a row vector with 5 elements?</li> <li>If you transpose a 1\u00d76 row vector, what are the resulting dimensions?</li> <li>Can you multiply a 1\u00d73 row vector by a 3\u00d71 column vector? What is the result's dimension?</li> </ol>"},{"location":"sims/row-column-vectors/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Row and column vectors in context</li> <li>Linear Algebra and Its Applications - Lay, Lay, and McDonald</li> </ul>"},{"location":"sims/row-operations/","title":"Row Operations Practice","text":"<p>Run the Row Operations MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/row-operations/#about-this-microsim","title":"About This MicroSim","text":"<p>Practice applying the three elementary row operations on an augmented matrix. This tool helps build fluency with the operations used in Gaussian elimination.</p> <p>The Three Row Operations:</p> Operation Notation Description Row Swap R\u2081 \u2194 R\u2082 Exchange two rows Row Scaling k \u00d7 R\u1d62 \u2192 R\u1d62 Multiply a row by a nonzero constant Row Addition R\u1d62 + k\u00d7R\u2c7c \u2192 R\u1d62 Add a multiple of one row to another"},{"location":"sims/row-operations/#how-to-use","title":"How to Use","text":"<ol> <li>Select Operation Type: Choose Swap, Scale, or Add Multiple from the dropdown</li> <li>Choose Rows: Select which row(s) the operation affects</li> <li>Set Scalar: For scaling or adding, enter the multiplier value</li> <li>Apply: Click \"Apply Operation\" to execute</li> <li>Track Progress: View your operation history on the right</li> <li>Undo/Reset: Use Undo to step back or Reset to start over</li> </ol>"},{"location":"sims/row-operations/#default-system","title":"Default System","text":"<p>The default matrix represents this system:</p> \\[2x + y - z = 8$$ $$-3x - y + 2z = -11$$ $$-2x + y + 2z = -3\\] <p>The solution is \\(x = 2\\), \\(y = 3\\), \\(z = -1\\).</p>"},{"location":"sims/row-operations/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/row-operations/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Execute the three elementary row operations correctly</li> <li>Understand how row operations preserve the solution set</li> <li>Develop intuition for which operations help reach row echelon form</li> </ol>"},{"location":"sims/row-operations/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Reduce to REF: Use row operations to reduce the matrix to row echelon form</li> <li>Minimal Operations Challenge: Find the shortest sequence of operations to reach REF</li> <li>Reverse Engineering: Given an REF matrix, work backward to find an original system</li> <li>Create Zeros: Practice creating zeros below pivots using row addition</li> </ol>"},{"location":"sims/row-operations/#tips-for-efficient-reduction","title":"Tips for Efficient Reduction","text":"<ul> <li>Start with the leftmost column</li> <li>Use row swaps to place a convenient pivot at the top</li> <li>Scale to get a leading 1 (optional but often helpful)</li> <li>Use row addition to create zeros below each pivot</li> <li>Move right and repeat</li> </ul>"},{"location":"sims/sarrus-rule/","title":"Rule of Sarrus Interactive Visualizer","text":"<p>Run the Rule of Sarrus Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/sarrus-rule/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the Rule of Sarrus for computing 3\u00d73 determinants through step-by-step animation.</p> <p>The method:</p> <ol> <li>Extend the matrix by repeating the first two columns</li> <li>Add products along the three downward diagonals (green)</li> <li>Subtract products along the three upward diagonals (red)</li> </ol> <p>Important</p> <p>The Rule of Sarrus only works for 3\u00d73 matrices. For larger matrices, use cofactor expansion.</p>"},{"location":"sims/sarrus-rule/#how-to-use","title":"How to Use","text":"<ol> <li>Step through: Click \"Step\" to advance one diagonal at a time</li> <li>Auto-play: Click \"Play\" to animate automatically</li> <li>Adjust speed: Use the slider to control animation speed</li> <li>Try examples: Select Identity, Rotation, or Singular matrix presets</li> </ol>"},{"location":"sims/sarrus-rule/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/sarrus-rule/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/sarrus-rule/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/sarrus-rule/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Apply the Rule of Sarrus to compute 3\u00d73 determinants</li> <li>Identify positive and negative diagonal products</li> <li>Understand why this method only works for 3\u00d73 matrices</li> </ol>"},{"location":"sims/sarrus-rule/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Practice computation: Step through each example and verify the calculation</li> <li>Compare methods: Use Sarrus on a matrix, then verify using cofactor expansion</li> <li>Predict results: Before stepping through, predict which diagonals will contribute most</li> </ol>"},{"location":"sims/sarrus-rule/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What is the determinant of the identity matrix? Why does this make sense geometrically?</li> <li>The example matrix [[1,2,3],[4,5,6],[7,8,9]] has determinant 0. What does this tell you?</li> <li>How many multiplications does Sarrus require compared to cofactor expansion?</li> </ol>"},{"location":"sims/sarrus-rule/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - The 3\u00d73 Determinant section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/scree-plot/","title":"Scree Plot and Component Selection","text":"<p>Run the Scree Plot MicroSim Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/scree-plot/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization teaches the critical skill of selecting the optimal number of principal components in PCA. Choosing the right number of components balances dimensionality reduction against information preservation.</p> <p>The key decision in PCA is: How many components \\(k\\) should we keep?</p> \\[\\text{Keep } k \\text{ components if } \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i} \\geq \\text{threshold}\\]"},{"location":"sims/scree-plot/#key-features","title":"Key Features","text":"<ul> <li>Dual Panel View: Scree plot (left) and cumulative variance (right) side by side</li> <li>Multiple Datasets: Compare different eigenvalue decay patterns</li> <li>Three Selection Methods: Elbow method, variance threshold, and Kaiser criterion</li> <li>Interactive Threshold: Drag to adjust target variance level</li> <li>Reconstruction Error Display: Visualize information loss at different k values</li> </ul>"},{"location":"sims/scree-plot/#selection-methods","title":"Selection Methods","text":"Method Rule Best For Elbow Method Find sharp bend in scree plot Data with clear structure Variance Threshold Keep k to explain 95% variance When error tolerance is known Kaiser Criterion Keep components with eigenvalue &gt; 1 Standardized data"},{"location":"sims/scree-plot/#how-to-use","title":"How to Use","text":"<ol> <li>Select a Dataset: Different patterns show when each method works best</li> <li>Observe the Scree Plot: Look for the \"elbow\" where eigenvalues drop sharply</li> <li>Check Cumulative Variance: See how much variance each component adds</li> <li>Drag the Threshold Line: Interactively adjust your target variance</li> <li>Compare Methods: The summary box shows suggested k from each method</li> <li>Toggle Reconstruction Error: Visualize the trade-off between k and information loss</li> </ol>"},{"location":"sims/scree-plot/#understanding-the-visualization","title":"Understanding the Visualization","text":""},{"location":"sims/scree-plot/#left-panel-scree-plot","title":"Left Panel: Scree Plot","text":"<ul> <li>Blue bars: Eigenvalues of selected components</li> <li>Gray bars: Eigenvalues of discarded components</li> <li>Orange circle: Detected elbow point</li> <li>Red dashed line: Kaiser criterion (eigenvalue = 1)</li> <li>Connecting line: Helps visualize the \"elbow\"</li> </ul>"},{"location":"sims/scree-plot/#right-panel-cumulative-variance","title":"Right Panel: Cumulative Variance","text":"<ul> <li>Blue line: Running sum of variance explained</li> <li>Blue shading: Variance captured by selected components</li> <li>Green dashed line: Target variance threshold</li> <li>Green dot: Point where threshold is achieved</li> <li>Draggable handle: Adjust threshold interactively</li> </ul>"},{"location":"sims/scree-plot/#dataset-patterns","title":"Dataset Patterns","text":""},{"location":"sims/scree-plot/#synthetic-elbow-at-k3","title":"Synthetic (Elbow at k=3)","text":"<ul> <li>Clear elbow makes selection straightforward</li> <li>All three methods roughly agree</li> <li>Ideal case for the elbow method</li> </ul>"},{"location":"sims/scree-plot/#gradual-decay","title":"Gradual Decay","text":"<ul> <li>No sharp elbow visible</li> <li>Variance threshold method works better</li> <li>Common in real-world data</li> </ul>"},{"location":"sims/scree-plot/#uniform","title":"Uniform","text":"<ul> <li>No clear structure in eigenvalues</li> <li>All components roughly equal importance</li> <li>Dimensionality reduction may not be appropriate</li> </ul>"},{"location":"sims/scree-plot/#two-groups","title":"Two Groups","text":"<ul> <li>Clear separation between important and noise components</li> <li>Strong agreement between methods</li> <li>Common pattern in data with distinct signal and noise</li> </ul>"},{"location":"sims/scree-plot/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Interpret scree plots to identify natural dimensionality</li> <li>Apply the elbow method to select number of components</li> <li>Set and justify variance thresholds for component selection</li> <li>Understand Kaiser criterion and when to apply it</li> <li>Recognize when dimensionality reduction is appropriate</li> <li>Evaluate trade-offs between compression and reconstruction quality</li> </ul>"},{"location":"sims/scree-plot/#mathematical-background","title":"Mathematical Background","text":""},{"location":"sims/scree-plot/#variance-explained","title":"Variance Explained","text":"<p>For eigenvalues \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_d\\):</p> <p>Individual variance: \\(\\frac{\\lambda_i}{\\sum_j \\lambda_j}\\)</p> <p>Cumulative variance: \\(\\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{j=1}^{d} \\lambda_j}\\)</p>"},{"location":"sims/scree-plot/#reconstruction-error","title":"Reconstruction Error","text":"<p>The Frobenius norm reconstruction error equals the sum of discarded eigenvalues:</p> \\[\\|X - X_k\\|_F^2 = \\sum_{i=k+1}^{d} \\lambda_i\\]"},{"location":"sims/scree-plot/#kaiser-criterion","title":"Kaiser Criterion","text":"<p>For standardized data (correlation matrix), keep components where:</p> \\[\\lambda_i \\geq 1\\] <p>This threshold represents average variance per original variable.</p>"},{"location":"sims/scree-plot/#selection-guidelines","title":"Selection Guidelines","text":""},{"location":"sims/scree-plot/#use-elbow-method-when","title":"Use Elbow Method When:","text":"<ul> <li>Scree plot shows clear bend</li> <li>Data has distinct signal vs. noise components</li> <li>You want a data-driven selection</li> </ul>"},{"location":"sims/scree-plot/#use-variance-threshold-when","title":"Use Variance Threshold When:","text":"<ul> <li>You have a specific accuracy requirement</li> <li>Domain knowledge suggests acceptable error level</li> <li>Elbow is not clearly visible</li> </ul>"},{"location":"sims/scree-plot/#use-kaiser-criterion-when","title":"Use Kaiser Criterion When:","text":"<ul> <li>Data is standardized (mean 0, variance 1)</li> <li>Working with correlation matrix</li> <li>You want components that explain more than one original variable</li> </ul>"},{"location":"sims/scree-plot/#common-patterns-and-interpretations","title":"Common Patterns and Interpretations","text":""},{"location":"sims/scree-plot/#sharp-initial-drop","title":"Sharp Initial Drop","text":"<ul> <li>First few components capture most variance</li> <li>Good candidate for significant dimensionality reduction</li> <li>Clear separation between signal and noise</li> </ul>"},{"location":"sims/scree-plot/#gradual-decay-exponential","title":"Gradual Decay (Exponential)","text":"<ul> <li>Information distributed across many components</li> <li>May need more components to preserve structure</li> <li>Consider domain-specific thresholds</li> </ul>"},{"location":"sims/scree-plot/#nearly-flat","title":"Nearly Flat","text":"<ul> <li>No dominant directions in data</li> <li>PCA may not be the right technique</li> <li>Consider other approaches or full dimensionality</li> </ul>"},{"location":"sims/scree-plot/#practical-tips","title":"Practical Tips","text":"<ol> <li>Always visualize: Don't rely solely on numbers</li> <li>Compare methods: Agreement suggests robust choice</li> <li>Consider the task: Classification may need fewer components than reconstruction</li> <li>Cross-validate: Test downstream performance at different k values</li> <li>Be conservative: When uncertain, keep more components</li> </ol>"},{"location":"sims/scree-plot/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/scree-plot/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<ul> <li>Ask: \"After computing PCA, how do we decide how many components to use?\"</li> <li>Introduce the bias-variance trade-off in choosing k</li> </ul>"},{"location":"sims/scree-plot/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<ol> <li>Start with Synthetic dataset showing clear elbow</li> <li>Walk through each selection method</li> <li>Show how the summary box compares suggestions</li> <li>Demonstrate dragging the threshold line</li> </ol>"},{"location":"sims/scree-plot/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<p>Have students:</p> <ol> <li>Switch to Gradual Decay - note elbow is less clear</li> <li>Find what k achieves 90% variance for each dataset</li> <li>Identify which dataset is hardest to choose k for</li> <li>Predict reconstruction quality at different k values</li> </ol>"},{"location":"sims/scree-plot/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why might different selection methods suggest different k values?</li> <li>When would you choose a higher k than the elbow suggests?</li> <li>What does it mean if eigenvalues are nearly uniform?</li> <li>How does the variance threshold relate to reconstruction error?</li> </ol>"},{"location":"sims/scree-plot/#applications","title":"Applications","text":"<ul> <li>Image Processing: Choose k for face recognition (eigenfaces)</li> <li>Genomics: Select significant gene expression components</li> <li>Finance: Identify market factors from many correlated assets</li> <li>Signal Processing: Separate signal from noise components</li> <li>Machine Learning: Feature selection for model training</li> </ul>"},{"location":"sims/scree-plot/#references","title":"References","text":"<ul> <li>Chapter 9: Dimensionality Reduction (Component Selection section)</li> <li>Cattell, R.B. \"The Scree Test for the Number of Factors\"</li> <li>Kaiser, H.F. \"The Application of Electronic Computers to Factor Analysis\"</li> </ul>"},{"location":"sims/sensor-fusion/","title":"Sensor Fusion Visualizer","text":"<p>Run the Sensor Fusion Visualizer Fullscreen</p> <p>Edit the Sensor Fusion Visualizer with the p5.js editor</p>"},{"location":"sims/sensor-fusion/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates sensor fusion - combining data from multiple sensors to achieve better accuracy and robustness than any single sensor alone. It shows GPS (noisy but absolute) and IMU (smooth but drifting) sensors being fused via Kalman filtering.</p>"},{"location":"sims/sensor-fusion/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/sensor-fusion/main.html\"\n        height=\"652px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/sensor-fusion/#features","title":"Features","text":"<ul> <li>True Position: Blue circle follows a circular path with variations</li> <li>GPS Measurements: Cyan markers appear periodically (low rate, noisy, absolute)</li> <li>IMU Dead Reckoning: Orange trail showing integration drift</li> <li>Fused Estimate: Green trail with uncertainty ellipse (best of both)</li> <li>RMSE Comparison: Real-time error statistics for each method</li> </ul>"},{"location":"sims/sensor-fusion/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/sensor-fusion/#why-fuse-sensors","title":"Why Fuse Sensors?","text":"Sensor Strengths Weaknesses GPS Absolute position, global Low rate (~10Hz), poor in tunnels/canyons IMU High rate (~400Hz), works everywhere Drift accumulates over time"},{"location":"sims/sensor-fusion/#the-fusion-principle","title":"The Fusion Principle","text":"<p>Kalman filter fusion exploits complementary characteristics: - GPS corrects IMU drift with absolute measurements - IMU fills gaps between GPS updates with smooth predictions - Result: High-rate, accurate position with bounded error</p>"},{"location":"sims/sensor-fusion/#error-growth","title":"Error Growth","text":"<ul> <li>GPS Only: Error bounded but noisy (no smooth trajectory)</li> <li>IMU Only: Error grows unboundedly over time (drift)</li> <li>Fused: Error bounded AND smooth (best of both worlds)</li> </ul>"},{"location":"sims/sensor-fusion/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/sensor-fusion/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand why multiple sensors outperform single sensors</li> <li>Recognize drift vs noise characteristics</li> <li>Analyze fusion improvement quantitatively via RMSE</li> </ul>"},{"location":"sims/sensor-fusion/#activities","title":"Activities","text":"<ol> <li>GPS Only: Disable IMU, observe jumpy but bounded tracking</li> <li>IMU Only: Disable GPS, watch trajectory drift away</li> <li>Fusion Benefit: Enable both, compare RMSE values</li> <li>Noise Tradeoffs: Increase GPS noise, see fusion degrade gracefully</li> </ol>"},{"location":"sims/sensor-fusion/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why does IMU-only tracking drift while GPS-only stays bounded?</li> <li>How does the fused estimate achieve lower RMSE than either sensor alone?</li> <li>What happens to fusion quality when GPS updates become less frequent?</li> </ol>"},{"location":"sims/sensor-fusion/#references","title":"References","text":"<ul> <li>Sensor Fusion and Tracking (MATLAB)</li> <li>GPS/IMU Integration</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/sgd-trajectory-visualizer/","title":"SGD Trajectory Visualizer","text":"<p>Run the SGD Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/sgd-trajectory-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates how batch size affects the behavior of Stochastic Gradient Descent (SGD). Small batch sizes lead to noisy, erratic optimization paths, while larger batch sizes produce smoother convergence.</p>"},{"location":"sims/sgd-trajectory-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Batch Size: Slide from 1 (pure SGD) to 128 (full batch)</li> <li>Adjust Learning Rate: Control step size</li> <li>Step/Run: Execute optimization steps manually or automatically</li> <li>Show Noise: Toggle visualization of gradient variance</li> <li>Click: Click anywhere on the plot to set a new starting point</li> </ol>"},{"location":"sims/sgd-trajectory-visualizer/#key-observations","title":"Key Observations","text":"Batch Size Behavior 1 (Pure SGD) Very noisy, erratic path 8-32 Moderate noise, faster per-step 128 (Full Batch) Smooth path, slower per-step"},{"location":"sims/sgd-trajectory-visualizer/#understanding-the-visualization","title":"Understanding the Visualization","text":"<ul> <li>Blue Path: The optimization trajectory</li> <li>Orange Cloud: Represents gradient variance (uncertainty)</li> <li>Gray Arrow: True gradient direction</li> <li>Orange Arrows: Sample stochastic gradients</li> </ul>"},{"location":"sims/sgd-trajectory-visualizer/#why-batch-size-matters","title":"Why Batch Size Matters","text":"<p>The variance of the stochastic gradient estimate decreases with batch size:</p> \\[\\text{Var}[\\nabla f_B] = \\frac{\\text{Var}[\\nabla f_i]}{|B|}\\] <p>Larger batches give more accurate gradient estimates but require more computation per step.</p>"},{"location":"sims/sgd-trajectory-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/sgd-trajectory-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the trade-off between gradient accuracy and computation</li> <li>Visualize how noise affects optimization paths</li> <li>Compare pure SGD, mini-batch, and full-batch gradient descent</li> </ul>"},{"location":"sims/sgd-trajectory-visualizer/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Pure SGD: Set batch size to 1 and observe the noisy trajectory</li> <li>Mini-Batch: Try batch sizes of 8, 16, 32 and compare smoothness</li> <li>Full Batch: Set to 128 and see the deterministic path</li> <li>Learning Rate Interaction: High learning rate + small batch = unstable</li> </ol>"},{"location":"sims/sgd-trajectory-visualizer/#references","title":"References","text":"<ul> <li>Goodfellow et al., Deep Learning, Chapter 8</li> <li>Wikipedia: Stochastic Gradient Descent</li> </ul>"},{"location":"sims/signed-area/","title":"Signed Area Interactive Visualizer","text":"<p>Run the Signed Area Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/signed-area/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the geometric meaning of the 2\u00d72 determinant as the signed area of the parallelogram formed by two vectors.</p> <p>Key concepts illustrated:</p> <ul> <li>Positive area: When vector v is counterclockwise from vector u</li> <li>Negative area: When vector v is clockwise from vector u</li> <li>Zero area: When vectors are parallel (collinear)</li> </ul> <p>The formula shown is: Area = ad - bc, where u = (a, c) and v = (b, d).</p>"},{"location":"sims/signed-area/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the vector endpoints (red for u, blue for v) to change the vectors</li> <li>Observe the signed area value update in real-time</li> <li>Watch the color change: green for positive, red for negative, gray for zero</li> <li>Toggle options: Show/hide the parallelogram and formula display</li> </ol>"},{"location":"sims/signed-area/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/signed-area/main.html\" height=\"452px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/signed-area/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/signed-area/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Interpret the determinant geometrically as signed area</li> <li>Explain how vector orientation affects the sign</li> <li>Predict when the determinant equals zero</li> </ol>"},{"location":"sims/signed-area/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Predict the sign: Before dragging, predict whether the area will be positive or negative</li> <li>Find zero area: Move vectors to make the area exactly zero - what do you notice?</li> <li>Maximize area: For vectors of fixed length, what orientation maximizes area?</li> </ol>"},{"location":"sims/signed-area/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If u = (3, 0) and v = (0, 2), what is the signed area? Is it positive or negative?</li> <li>What happens to the signed area if you swap the two vectors?</li> <li>Why does the signed area equal zero when vectors are parallel?</li> </ol>"},{"location":"sims/signed-area/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Geometric Motivation section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/singular-matrix/","title":"Singular vs Non-Singular Matrix Visualizer","text":"<p>Run the Singular Matrix Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/singular-matrix/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows the fundamental geometric difference between singular and non-singular matrices.</p> <p>Key Insights:</p> <ul> <li>Non-singular (det \u2260 0): Transformation is reversible, area preserved (scaled)</li> <li>Singular (det = 0): Transformation collapses dimension, not reversible</li> </ul> <p>Watch how the unit square transforms:</p> <ul> <li>Green: Non-singular, positive determinant (orientation preserved)</li> <li>Purple: Non-singular, negative determinant (orientation flipped)</li> <li>Red line: Singular - the square collapses to a line!</li> </ul>"},{"location":"sims/singular-matrix/#how-to-use","title":"How to Use","text":"<ol> <li>Click preset buttons: Singular, Non-singular, or Random matrix</li> <li>Use morph slider: Smoothly animate from identity to target matrix</li> <li>Toggle grid: Show/hide the transformed coordinate grid</li> <li>Watch the collapse: See how singular matrices flatten the plane</li> </ol>"},{"location":"sims/singular-matrix/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/singular-matrix/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/singular-matrix/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/singular-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Recognize singular matrices visually and algebraically</li> <li>Explain why singular matrices are not invertible</li> <li>Connect det = 0 to dimension collapse</li> </ol>"},{"location":"sims/singular-matrix/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Morph slowly: Watch the exact moment area becomes zero</li> <li>Column relationship: When singular, what's the relationship between columns?</li> <li>Predict singularity: Before clicking, guess if a random matrix will be singular</li> </ol>"},{"location":"sims/singular-matrix/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why can't you \"undo\" a singular transformation?</li> <li>What happens to a circle under a singular transformation?</li> <li>If two columns are parallel, what is the determinant? Why?</li> </ol>"},{"location":"sims/singular-matrix/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Singular Matrices section</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/slam-visualizer/","title":"SLAM Visualizer","text":"<p>Run the SLAM Visualizer Fullscreen</p> <p>Edit the SLAM Visualizer with the p5.js editor</p>"},{"location":"sims/slam-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates SLAM (Simultaneous Localization and Mapping) - the problem of building a map while simultaneously localizing within it. SLAM solves the chicken-and-egg problem: you need a map to localize, but you need to know your location to build a map.</p>"},{"location":"sims/slam-visualizer/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/slam-visualizer/main.html\"\n        height=\"702px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/slam-visualizer/#features","title":"Features","text":"<ul> <li>Robot Trajectory: Move the robot to build a path (green poses)</li> <li>Landmark Observation: Detect and track landmarks (blue triangles)</li> <li>Odometry Drift: Watch accumulated error grow without loop closure</li> <li>Loop Closure: Detect when you revisit a place (red arc)</li> <li>Graph Optimization: Correct entire trajectory with loop closure constraints</li> <li>Uncertainty Ellipses: Visualize growing and shrinking uncertainty</li> </ul>"},{"location":"sims/slam-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Move Robot: Click repeatedly to build a trajectory (notice drift accumulates)</li> <li>Observe Landmarks: Create landmarks to constrain the map</li> <li>Complete a Loop: Keep moving until you return near the start</li> <li>Loop Closure: Click when near a previous position to detect revisit</li> <li>Optimize: Apply graph optimization to correct the trajectory</li> </ol>"},{"location":"sims/slam-visualizer/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/slam-visualizer/#the-slam-problem","title":"The SLAM Problem","text":"<p>SLAM jointly estimates: - Localization: Robot pose over time \\(\\mathbf{x}_R\\) - Mapping: Landmark positions \\(\\mathbf{m}_1, \\mathbf{m}_2, ..., \\mathbf{m}_n\\)</p>"},{"location":"sims/slam-visualizer/#drift-accumulation","title":"Drift Accumulation","text":"<p>Without loop closure, odometry errors accumulate: - Each motion adds noise - Uncertainty ellipses grow over time - Map becomes inconsistent</p>"},{"location":"sims/slam-visualizer/#loop-closure","title":"Loop Closure","text":"<p>When the robot recognizes a previously visited place: 1. Creates a constraint between distant poses 2. Graph optimization distributes the correction 3. Entire trajectory and map improve</p>"},{"location":"sims/slam-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/slam-visualizer/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand the simultaneous localization and mapping problem</li> <li>Recognize how drift accumulates in dead-reckoning</li> <li>Analyze how loop closure corrects the entire trajectory</li> </ul>"},{"location":"sims/slam-visualizer/#activities","title":"Activities","text":"<ol> <li>Build Without Loop Closure: Move 10+ times without closing the loop, observe drift</li> <li>Return to Start: Navigate back to starting area, perform loop closure</li> <li>Before/After: Compare trajectory before and after optimization</li> <li>Multiple Loops: Add multiple loop closures to see cumulative improvement</li> </ol>"},{"location":"sims/slam-visualizer/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why can't you build an accurate map with pure odometry alone?</li> <li>How does a single loop closure help correct poses far from the closure point?</li> <li>What would happen with very noisy odometry but frequent loop closures?</li> </ol>"},{"location":"sims/slam-visualizer/#references","title":"References","text":"<ul> <li>GraphSLAM Tutorial</li> <li>ORB-SLAM</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/solution-sets/","title":"Solution Set Visualizer","text":"<p>Run the Solution Set MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/solution-sets/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps you understand the three possible outcomes when solving a system of linear equations:</p> <ol> <li>Unique Solution: Exactly one answer exists</li> <li>Infinite Solutions: A line, plane, or higher-dimensional subspace of answers</li> <li>No Solution: The system is inconsistent</li> </ol>"},{"location":"sims/solution-sets/#solution-types-explained","title":"Solution Types Explained","text":"Type RREF Indicator Geometric Meaning Unique All variable columns are pivot columns Lines/planes meet at exactly one point Infinite Some columns are not pivot columns (free variables) Lines/planes overlap along a line or plane None Row of form [0 0 ... 0 | c] where c \u2260 0 Lines/planes are parallel, never meeting"},{"location":"sims/solution-sets/#variables-classification","title":"Variables Classification","text":"<p>Basic Variables (pivot columns - highlighted yellow):</p> <ul> <li>Correspond to pivot columns in RREF</li> <li>Their values are determined by the free variables</li> <li>Appear in the parametric solution</li> </ul> <p>Free Variables (non-pivot columns - highlighted blue):</p> <ul> <li>Correspond to non-pivot columns</li> <li>Can take any real value</li> <li>The number of free variables equals the dimension of the solution set</li> </ul>"},{"location":"sims/solution-sets/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/solution-sets/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Classify systems as having unique, infinite, or no solutions</li> <li>Identify basic and free variables from the RREF</li> <li>Understand the geometric meaning of each solution type</li> <li>Connect the algebraic analysis to geometric visualization</li> </ol>"},{"location":"sims/solution-sets/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Explore Each Type: Select each preset and observe how the RREF reveals the solution type</li> <li>Count Free Variables: For infinite solutions, count the free variables and relate to the solution geometry</li> <li>Find Inconsistency: For the \"No Solution\" case, identify the inconsistent row</li> <li>Predict Before Viewing: Before checking RREF, predict the solution type from the original matrix</li> </ol>"},{"location":"sims/sparse-dense-matrices/","title":"Sparse vs Dense Matrices","text":"<p>Run the Sparse vs Dense MicroSim Fullscreen</p> <p>Edit the Sparse vs Dense MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/sparse-dense-matrices/main.html\" height=\"402px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/sparse-dense-matrices/#description","title":"Description","text":"<p>Matrices are classified by their distribution of zero entries. This MicroSim provides a visual comparison of dense matrices (mostly non-zero) and sparse matrices (mostly zero), helping students understand why sparsity matters for computation.</p> <p>Key Features:</p> <ul> <li>Visual Comparison: Side-by-side matrix visualization with color intensity showing values</li> <li>Sparsity Patterns: Choose from random, diagonal, banded, or block patterns</li> <li>Storage Statistics: Real-time comparison of memory requirements</li> <li>Adjustable Parameters: Control matrix size and sparsity level</li> </ul>"},{"location":"sims/sparse-dense-matrices/#dense-vs-sparse","title":"Dense vs Sparse","text":"Property Dense Matrix Sparse Matrix Zero entries Few Many (typically &gt;90%) Storage O(n\u00b2) O(nnz) Storage format 2D array CSR, CSC, COO Example Covariance matrices Graph adjacency <p>where nnz = number of non-zero entries.</p>"},{"location":"sims/sparse-dense-matrices/#why-sparsity-matters","title":"Why Sparsity Matters","text":""},{"location":"sims/sparse-dense-matrices/#storage-efficiency","title":"Storage Efficiency","text":"<p>A 10,000 \u00d7 10,000 dense matrix requires 800 MB (100M entries \u00d7 8 bytes). The same matrix with 99% sparsity needs only ~16 MB in sparse format.</p>"},{"location":"sims/sparse-dense-matrices/#computational-speed","title":"Computational Speed","text":"<p>Operations that skip zeros are much faster:</p> <ul> <li>Matrix-vector multiply: O(nnz) vs O(n\u00b2)</li> <li>Solving linear systems: specialized algorithms exist</li> </ul>"},{"location":"sims/sparse-dense-matrices/#real-world-examples","title":"Real-World Examples","text":"<ul> <li>Graph adjacency: Most nodes connect to few others</li> <li>Document-term matrices: Each document uses few of all possible words</li> <li>Finite elements: Local interactions create banded structures</li> </ul>"},{"location":"sims/sparse-dense-matrices/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/sparse-dense-matrices/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Distinguish sparse and dense matrices visually</li> <li>Estimate storage requirements for both types</li> <li>Recognize common sparsity patterns in applications</li> <li>Explain why sparse storage formats save memory</li> </ol>"},{"location":"sims/sparse-dense-matrices/#exploration-activity-5-minutes","title":"Exploration Activity (5 minutes)","text":"<ol> <li>Compare Visually: Note the color distribution in both panels</li> <li>Try Patterns: Select diagonal, banded, and block patterns</li> <li>Increase Size: Slide to larger matrices and observe storage savings</li> <li>Adjust Sparsity: See how storage ratio changes with sparsity level</li> </ol>"},{"location":"sims/sparse-dense-matrices/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>At what sparsity level does sparse storage become advantageous?</li> <li>Why do graph adjacency matrices tend to be sparse?</li> <li>What real-world data would produce a banded matrix?</li> </ul>"},{"location":"sims/sparse-dense-matrices/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Sparse and dense matrices in context</li> <li>SciPy Sparse Matrix Documentation - Python sparse matrix formats</li> </ul>"},{"location":"sims/special-matrices/","title":"Special Matrix Types Gallery","text":"<p>Run the Special Matrix Types MicroSim Fullscreen</p> <p>Edit the Special Matrix Types MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/special-matrices/main.html\" height=\"542px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/special-matrices/#description","title":"Description","text":"<p>Many matrices have special structures that simplify computation or carry geometric meaning. This gallery displays four fundamental matrix types side-by-side, helping students recognize their distinctive patterns.</p> <p>Featured Matrix Types:</p> Type Pattern Key Property Identity 1s on diagonal, 0s elsewhere AI = IA = A Diagonal Non-zeros only on diagonal Easy powers: D^k has d_i^k Upper Triangular Zeros below diagonal Back substitution Lower Triangular Zeros above diagonal Forward substitution <p>Interactive Features:</p> <ul> <li>Adjustable Size: Change matrix dimensions from 3\u00d73 to 6\u00d76</li> <li>Toggle Zeros: Show or hide zero entries to focus on structure</li> <li>Click to Randomize: Click any matrix card to generate new random values</li> </ul>"},{"location":"sims/special-matrices/#why-these-matrices-matter","title":"Why These Matrices Matter","text":""},{"location":"sims/special-matrices/#identity-matrix-i","title":"Identity Matrix (I)","text":"<p>The multiplicative identity for matrices. Multiplying any matrix by I leaves it unchanged\u2014like multiplying a number by 1.</p>"},{"location":"sims/special-matrices/#diagonal-matrices","title":"Diagonal Matrices","text":"<p>Store information efficiently (only n values for an n\u00d7n matrix). Powers, inverses, and eigenvalues are trivial to compute.</p>"},{"location":"sims/special-matrices/#triangular-matrices","title":"Triangular Matrices","text":"<p>Enable efficient equation solving. LU decomposition factors any matrix into L (lower) and U (upper) triangular components.</p>"},{"location":"sims/special-matrices/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/special-matrices/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify the visual pattern of each special matrix type</li> <li>State the defining property of each type</li> <li>Recognize these patterns when they appear in larger problems</li> </ol>"},{"location":"sims/special-matrices/#quick-recognition-drill-3-minutes","title":"Quick Recognition Drill (3 minutes)","text":"<ol> <li>Display the gallery at different sizes</li> <li>Toggle zeros off and ask students to identify each type by structure alone</li> <li>Click to randomize and verify the pattern holds regardless of specific values</li> </ol>"},{"location":"sims/special-matrices/#discussion-points","title":"Discussion Points","text":"<ul> <li>Why is the identity matrix always the same regardless of random values?</li> <li>How does triangular structure help in solving equations?</li> <li>What's the relationship between diagonal and identity matrices?</li> </ul>"},{"location":"sims/special-matrices/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Special matrix types in context</li> <li>MIT OCW 18.06: Linear Algebra - Gilbert Strang's course</li> </ul>"},{"location":"sims/spectral-theorem/","title":"Spectral Theorem Demonstration","text":"<p>Run the Spectral Theorem Demo Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/spectral-theorem/#about-this-microsim","title":"About This MicroSim","text":"<p>The Spectral Theorem is one of the most beautiful results in linear algebra. It states that every symmetric matrix can be orthogonally diagonalized: A = Q\u039bQ\u1d40, where Q is orthogonal and \u039b is diagonal.</p> <p>Key Features:</p> <ul> <li>Symmetric matrix input: Automatically enforces a\u2081\u2082 = a\u2082\u2081</li> <li>Orthogonal decomposition: See Q, \u039b, and Q\u1d40 matrices</li> <li>Geometric visualization: Orthogonal eigenvectors shown at right angles</li> <li>Orthogonality verification: Confirms q\u2081 \u00b7 q\u2082 = 0 and Q\u1d40Q = I</li> </ul>"},{"location":"sims/spectral-theorem/#the-spectral-theorem","title":"The Spectral Theorem","text":"<p>For any real symmetric matrix A (where A = A\u1d40):</p> <p>A = Q\u039bQ\u1d40</p> <p>Where: - Q is an orthogonal matrix (Q\u1d40 = Q\u207b\u00b9) - Columns of Q are orthonormal eigenvectors - \u039b is a diagonal matrix of eigenvalues - All eigenvalues are real (not complex)</p>"},{"location":"sims/spectral-theorem/#why-symmetric-matrices-are-special","title":"Why Symmetric Matrices Are Special","text":"Property General Matrix Symmetric Matrix Eigenvalues May be complex Always real Eigenvectors Not orthogonal Orthogonal for distinct \u03bb Diagonalizable Not guaranteed Always P\u207b\u00b9 computation Matrix inversion Just P\u1d40 (transpose)"},{"location":"sims/spectral-theorem/#how-to-use","title":"How to Use","text":"<ol> <li>Click matrix cells to edit values (symmetry is maintained)</li> <li>Use \"Random Symmetric\" for new examples</li> <li>Observe the decomposition A = Q\u039bQ\u1d40</li> <li>Check orthogonality: q\u2081 \u00b7 q\u2082 should equal 0</li> </ol>"},{"location":"sims/spectral-theorem/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/spectral-theorem/main.html\" height=\"532px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/spectral-theorem/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/spectral-theorem/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>State the spectral theorem for real symmetric matrices</li> <li>Verify that eigenvectors of symmetric matrices are orthogonal</li> <li>Explain why Q\u207b\u00b9 = Q\u1d40 for orthogonal matrices</li> <li>Apply the spectral theorem to decompose symmetric matrices</li> </ol>"},{"location":"sims/spectral-theorem/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Verify decomposition: Multiply Q\u039bQ\u1d40 and confirm it equals A</li> <li>Check orthogonality: Compute q\u2081 \u00b7 q\u2082 for several examples</li> <li>Non-symmetric: What happens if you try a non-symmetric matrix?</li> <li>Repeated eigenvalues: Test A = [[2, 0], [0, 2]]</li> </ol>"},{"location":"sims/spectral-theorem/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why must eigenvalues of symmetric matrices be real?</li> <li>If A is symmetric with eigenvalues 3 and 7, what are the eigenvalues of A\u00b2?</li> <li>What is the computational advantage of Q\u1d40Q = I?</li> </ol>"},{"location":"sims/spectral-theorem/#references","title":"References","text":"<ul> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Linear Algebra Learning Graph</li> </ul>"},{"location":"sims/subspace-tester/","title":"Subspace Tester","text":"<p>Run the Subspace Tester Fullscreen</p> <p>Edit the Subspace Tester with the p5.js editor</p>"},{"location":"sims/subspace-tester/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand what makes a set a subspace by testing the closure property. Students can select different sets, drag test vectors within those sets, and observe whether linear combinations stay within the set.</p> <p>Learning Objective: Test whether sets are subspaces by checking closure under linear combinations.</p>"},{"location":"sims/subspace-tester/#how-to-use","title":"How to Use","text":"<ol> <li>Select a Set: Use the dropdown to choose different sets (line through origin, line not through origin, first quadrant, circle, or entire plane)</li> <li>Drag Test Vectors: Click and drag the endpoints of vectors u (red) and v (blue). Vectors are constrained to stay within the selected set</li> <li>Adjust Scalars: Use the c and d sliders to change the scalar multipliers for the linear combination cu + dv</li> <li>Observe the Result: The result vector is shown in green if it stays in the set, or orange if it leaves the set</li> <li>Check Subspace: Click \"Check if Subspace\" to see an explanation of why the set is or is not a subspace</li> </ol>"},{"location":"sims/subspace-tester/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/subspace-tester/#subspace-definition","title":"Subspace Definition","text":"<p>A subset H of a vector space V is a subspace if:</p> <ol> <li>Zero vector: The zero vector is in H</li> <li>Closure under addition: For all u, v in H, u + v is in H</li> <li>Closure under scalar multiplication: For all u in H and scalar c, cu is in H</li> </ol> <p>Equivalently, H is a subspace if it is closed under linear combinations: for all u, v in H and scalars c, d, the vector cu + dv is also in H.</p>"},{"location":"sims/subspace-tester/#examples-explored","title":"Examples Explored","text":"Set Is Subspace? Reason Line through origin Yes Contains zero, closed under linear combinations Line NOT through origin No Does not contain zero vector First quadrant No Not closed under scalar multiplication (try c = -1) Circle No Does not contain zero, not closed under addition Entire plane R^2 Yes The whole space is always a subspace"},{"location":"sims/subspace-tester/#finding-counter-examples","title":"Finding Counter-Examples","text":"<p>For non-subspaces, try these strategies to find counter-examples:</p> <ul> <li>Line not through origin: Set c = 0, d = 0 to see that (0, 0) is not on the line</li> <li>First quadrant: Set c = -1, d = 0 to see that -u has negative components</li> <li>Circle: Set c = 1, d = 1 to see that u + v is not on the circle</li> </ul>"},{"location":"sims/subspace-tester/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/subspace-tester/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/subspace-tester/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/subspace-tester/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/subspace-tester/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/subspace-tester/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication</li> <li>Linear combinations</li> <li>Basic set theory concepts</li> </ul>"},{"location":"sims/subspace-tester/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration with a True Subspace (5 min):</li> <li>Select \"Line through origin (y = 2x)\"</li> <li>Drag vectors u and v along the line</li> <li>Adjust c and d sliders and observe that cu + dv always stays on the line</li> <li> <p>Click \"Check if Subspace\" to confirm</p> </li> <li> <p>Finding Counter-Examples (10 min):</p> </li> <li>Select \"Line not through origin (y = 2x + 1)\"</li> <li>Note that vectors can exist on this line</li> <li>Set c = 0, d = 0. Where is the origin relative to the line?</li> <li> <p>Explain why this fails the subspace test</p> </li> <li> <p>First Quadrant Investigation (5 min):</p> </li> <li>Select \"First quadrant\"</li> <li>Place u at (1, 2)</li> <li>Set c = -1, d = 0. Where does the result end up?</li> <li> <p>Discuss: even though (1, 2) is in the first quadrant, (-1, -2) is not</p> </li> <li> <p>Circle Failure (5 min):</p> </li> <li>Select \"Circle\"</li> <li>Place u at (2, 0) and v at (0, 2)</li> <li>With c = 1, d = 1, where is u + v?</li> <li>Verify it's not on the circle</li> </ol>"},{"location":"sims/subspace-tester/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why must every subspace contain the zero vector?</li> <li>If a set is closed under addition, is it necessarily closed under scalar multiplication?</li> <li>Can you think of a set that contains zero but is not a subspace?</li> <li>What is the smallest possible subspace of R^2?</li> </ol>"},{"location":"sims/subspace-tester/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a description of a set, predict whether it's a subspace</li> <li>For non-subspaces, provide specific counter-examples</li> <li>Prove that the intersection of two subspaces is also a subspace</li> </ul>"},{"location":"sims/subspace-tester/#mathematical-background","title":"Mathematical Background","text":""},{"location":"sims/subspace-tester/#the-subspace-test","title":"The Subspace Test","text":"<p>To verify H is a subspace, we can use the combined test: H is a subspace if and only if for all u, v in H and all scalars c, d:</p> \\[c\\mathbf{u} + d\\mathbf{v} \\in H\\] <p>This single condition implies all three properties (zero, addition closure, scalar multiplication closure).</p>"},{"location":"sims/subspace-tester/#why-lines-through-origin-are-subspaces","title":"Why Lines Through Origin Are Subspaces","text":"<p>A line through the origin can be written as:</p> \\[L = \\{t\\mathbf{v} : t \\in \\mathbb{R}\\}\\] <p>for some direction vector v. For any \\(t_1\\mathbf{v}\\) and \\(t_2\\mathbf{v}\\) in L:</p> \\[c(t_1\\mathbf{v}) + d(t_2\\mathbf{v}) = (ct_1 + dt_2)\\mathbf{v} \\in L\\]"},{"location":"sims/subspace-tester/#why-the-first-quadrant-fails","title":"Why the First Quadrant Fails","text":"<p>The first quadrant Q = {(x, y) : x &gt;= 0, y &gt;= 0} contains (1, 1). However:</p> \\[(-1) \\cdot (1, 1) = (-1, -1) \\notin Q\\] <p>So Q is not closed under scalar multiplication.</p>"},{"location":"sims/subspace-tester/#references","title":"References","text":"<ol> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 3.1 - Spaces of Vectors.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 4.1 - Vector Spaces and Subspaces.</li> <li>3Blue1Brown - Span and Subspaces - Visual introduction to subspaces.</li> <li>Khan Academy - Subspaces</li> </ol>"},{"location":"sims/svd-compression-visualizer/","title":"SVD Compression Visualizer","text":"<p>Run the SVD Compression Visualizer Fullscreen</p> <p>Edit the SVD Compression Visualizer with the p5.js editor</p>"},{"location":"sims/svd-compression-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates how Singular Value Decomposition enables image compression by keeping only the most significant components. Watch quality degrade gracefully as you reduce the number of singular values.</p>"},{"location":"sims/svd-compression-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li> <p>Select an Image Pattern: Different patterns have different rank characteristics</p> </li> <li> <p>Adjust Rank k: Control how many singular values to keep (1 to 16)</p> </li> <li> <p>Show Error: Toggle to see the difference between original and reconstructed images</p> </li> </ol>"},{"location":"sims/svd-compression-visualizer/#key-insights","title":"Key Insights","text":"<ul> <li>Singular values are ordered by importance - first few capture most information</li> <li>Low rank images (gradients, simple shapes) compress well</li> <li>Complex/noisy images need more components for good reconstruction</li> <li>Compression ratio improves as k decreases, but quality suffers</li> </ul>"},{"location":"sims/svd-compression-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand SVD as a sum of rank-1 matrices - Apply truncated SVD for lossy compression - Evaluate the quality-compression tradeoff - Connect singular value magnitude to image features</p>"},{"location":"sims/svd-compression-visualizer/#the-mathematics","title":"The Mathematics","text":"<p>The SVD decomposes an image matrix as:</p> \\[\\mathbf{I} = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\] <p>The truncated SVD keeps only k terms:</p> \\[\\mathbf{I}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T\\] <p>Storage comparison: - Original: H \u00d7 W values - Compressed: k \u00d7 (H + W + 1) values</p>"},{"location":"sims/svd-compression-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/svd-compression-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>SVD reveals that images can be approximated as sums of simpler \"building block\" images. Each building block is the outer product of two vectors.</p>"},{"location":"sims/svd-compression-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with Gradient image - observe it reconstructs perfectly with few components (it's nearly rank-1!)</li> <li>Try Low Rank image - designed to show clear rank structure</li> <li>Use Complex image - needs many components</li> </ol>"},{"location":"sims/svd-compression-visualizer/#compression-analysis-10-minutes","title":"Compression Analysis (10 minutes)","text":"<ol> <li>Note the \"knee\" in the singular value plot</li> <li>Calculate compression ratios at different k values</li> <li>Observe PSNR and energy retained metrics</li> </ol>"},{"location":"sims/svd-compression-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>Why does the gradient compress so well?</li> <li>What does a large first singular value indicate?</li> <li>How does JPEG differ from SVD compression?</li> </ul>"},{"location":"sims/svd-compression-visualizer/#references","title":"References","text":"<ul> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Chapter 7: Matrix Decompositions</li> <li>SVD - Wikipedia</li> </ul>"},{"location":"sims/svd-forms-comparison/","title":"SVD Forms Comparison","text":"<p>Run the SVD Forms Comparison Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/svd-forms-comparison/#about-this-microsim","title":"About This MicroSim","text":"<p>This infographic compares the three main forms of the Singular Value Decomposition:</p> <ol> <li>Full SVD: Complete decomposition with all singular vectors</li> <li>Compact SVD: Only keeps non-zero singular values</li> <li>Truncated SVD: Keeps only the top k singular values</li> </ol>"},{"location":"sims/svd-forms-comparison/#svd-forms-summary","title":"SVD Forms Summary","text":"Form Matrices Exact? Use Case Full U(m\u00d7m), \u03a3(m\u00d7n), V\u1d40(n\u00d7n) Yes Complete subspace analysis Compact U(m\u00d7r), \u03a3(r\u00d7r), V\u1d40(r\u00d7n) Yes Efficient exact storage Truncated U(m\u00d7k), \u03a3(k\u00d7k), V\u1d40(k\u00d7n) No Low-rank approximation"},{"location":"sims/svd-forms-comparison/#visual-legend","title":"Visual Legend","text":"<ul> <li>Blue blocks: Kept components (contain information)</li> <li>Gray blocks: Discarded components (zeros or truncated)</li> <li>Light gray: Structural zeros</li> </ul>"},{"location":"sims/svd-forms-comparison/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust m and n to change matrix dimensions</li> <li>Set rank r to see how many singular values are non-zero</li> <li>Set k to control truncation level</li> <li>Compare storage requirements between forms</li> </ol>"},{"location":"sims/svd-forms-comparison/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Distinguish between Full, Compact, and Truncated SVD</li> <li>Calculate storage requirements for each form</li> <li>Choose the appropriate SVD form for a given application</li> <li>Understand the tradeoff between accuracy and storage</li> </ul>"},{"location":"sims/svd-forms-comparison/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - SVD section</li> <li>NumPy documentation: <code>np.linalg.svd(full_matrices=False)</code></li> <li>scikit-learn: <code>TruncatedSVD</code></li> </ul>"},{"location":"sims/svd-geometry/","title":"SVD Geometric Interpretation","text":"<p>Run the SVD Geometry Visualizer Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/svd-geometry/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates the geometric interpretation of SVD by showing how a matrix transformation can be decomposed into three steps:</p> \\[A = U \\Sigma V^T\\] <ul> <li>V\u1d40: Rotate to align with principal axes</li> <li>\u03a3: Scale along each axis by singular values</li> <li>U: Rotate to final orientation</li> </ul>"},{"location":"sims/svd-geometry/#key-features","title":"Key Features","text":"<ul> <li>Four-panel View: See each transformation stage simultaneously</li> <li>Animated Transition: Watch the unit circle transform step by step</li> <li>Interactive Matrix: Modify the 2\u00d72 matrix entries</li> <li>Singular Vectors: Visualize v\u2081, v\u2082 and u\u2081, u\u2082 directions</li> <li>SVD Computation: See the full decomposition A = U\u03a3V\u1d40</li> </ul>"},{"location":"sims/svd-geometry/#the-svd-geometry","title":"The SVD Geometry","text":"<p>The SVD reveals that any linear transformation can be understood as:</p> Step Operation Geometric Effect 1 V\u1d40 Rotate to align with right singular vectors 2 \u03a3 Stretch/compress by \u03c3\u2081 and \u03c3\u2082 3 U Rotate to align with left singular vectors <p>The unit circle becomes an ellipse with: - Semi-axes of length \u03c3\u2081 and \u03c3\u2082 - Axes directions along columns of U</p>"},{"location":"sims/svd-geometry/#how-to-use","title":"How to Use","text":"<ol> <li>Click Animate to watch the transformation unfold</li> <li>Use the Phase slider to manually control the animation</li> <li>Adjust matrix entries to see different transformations</li> <li>Toggle Show Singular Vectors to see v and u directions</li> <li>Click Reset to return to the beginning</li> </ol>"},{"location":"sims/svd-geometry/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Explain SVD as rotation-scaling-rotation</li> <li>Identify singular values as ellipse semi-axis lengths</li> <li>Connect singular vectors to input/output principal directions</li> <li>Understand why any matrix can be decomposed this way</li> </ul>"},{"location":"sims/svd-geometry/#observations-to-make","title":"Observations to Make","text":""},{"location":"sims/svd-geometry/#symmetric-matrix-try-a12-a21","title":"Symmetric Matrix (try a\u2081\u2082 = a\u2082\u2081)","text":"<p>When A is symmetric, U = V (or U = -V), so the input and output rotations align.</p>"},{"location":"sims/svd-geometry/#diagonal-matrix-try-a12-a21-0","title":"Diagonal Matrix (try a\u2081\u2082 = a\u2082\u2081 = 0)","text":"<p>No rotation needed\u2014the circle just stretches along the coordinate axes.</p>"},{"location":"sims/svd-geometry/#rotation-matrix-try-a11-a22-cos-a12-a21-sin","title":"Rotation Matrix (try a\u2081\u2081 = a\u2082\u2082 = cos(\u03b8), a\u2081\u2082 = -a\u2082\u2081 = sin(\u03b8))","text":"<p>Both singular values are 1, so no scaling\u2014just rotation.</p>"},{"location":"sims/svd-geometry/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/svd-geometry/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Ask: \"What's the simplest way to understand what any matrix transformation does?\"</p> <p>Introduce the idea that any matrix can be broken into rotation, scaling, and rotation.</p>"},{"location":"sims/svd-geometry/#demonstration-10-minutes","title":"Demonstration (10 minutes)","text":"<ol> <li>Start with the default matrix and animate</li> <li>Point out:</li> <li>The unit circle becomes an ellipse</li> <li>The ellipse axes are \u03c3\u2081 and \u03c3\u2082</li> <li> <p>Red and green vectors show principal directions</p> </li> <li> <p>Try different matrices:</p> </li> <li>Symmetric: a\u2081\u2082 = a\u2082\u2081</li> <li>Diagonal: off-diagonal = 0</li> <li>Singular: det(A) \u2248 0</li> </ol>"},{"location":"sims/svd-geometry/#key-insight","title":"Key Insight","text":"<p>The SVD finds the directions where the transformation acts most simply\u2014just stretching, no rotation.</p>"},{"location":"sims/svd-geometry/#practice-10-minutes","title":"Practice (10 minutes)","text":"<p>Have students:</p> <ol> <li>Predict the shape of the transformed ellipse</li> <li>Identify which singular value is larger</li> <li>Draw the principal directions before/after</li> </ol>"},{"location":"sims/svd-geometry/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>What does \u03c3\u2081 &gt; \u03c3\u2082 mean geometrically?</li> <li>Why does the unit circle become an ellipse?</li> <li>When would the ellipse be a circle?</li> </ol>"},{"location":"sims/svd-geometry/#mathematical-notes","title":"Mathematical Notes","text":"<p>For a 2\u00d72 matrix:</p> <ul> <li>Singular values: \u03c3\u1d62 = \u221a(eigenvalues of A\u1d40A)</li> <li>Right singular vectors (V): eigenvectors of A\u1d40A</li> <li>Left singular vectors (U): eigenvectors of AA\u1d40</li> </ul> <p>The decomposition exists for any matrix\u2014not just square or symmetric ones.</p>"},{"location":"sims/svd-geometry/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - SVD section</li> <li>3Blue1Brown: SVD</li> <li>Strang, G. \"Linear Algebra and Learning from Data\"</li> </ul>"},{"location":"sims/svd-image-compression/","title":"SVD Image Compression","text":"<p>Run the SVD Image Compression Fullscreen</p> <p>Edit the MicroSim in the p5.js Editor</p>"},{"location":"sims/svd-image-compression/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates image compression using SVD by showing:</p> <ul> <li>Original image as a grayscale matrix</li> <li>Rank-k approximation using truncated SVD</li> <li>Error image (difference between original and compressed)</li> <li>Singular value spectrum showing which values are kept</li> </ul>"},{"location":"sims/svd-image-compression/#key-concepts","title":"Key Concepts","text":"<ul> <li>Images can be represented as matrices (pixel values)</li> <li>SVD finds the \"most important\" directions in the image</li> <li>Keeping only top k singular values compresses the image</li> <li>Larger singular values capture more of the image structure</li> </ul>"},{"location":"sims/svd-image-compression/#how-to-use","title":"How to Use","text":"<ol> <li>Select a pattern to generate different test images</li> <li>Adjust rank k using the slider to control compression</li> <li>Observe how image quality changes with k</li> <li>Watch the singular value spectrum to see truncation point</li> </ol>"},{"location":"sims/svd-image-compression/#statistics-explained","title":"Statistics Explained","text":"Metric Meaning Compression How many times smaller the storage Error Frobenius norm error as percentage Variance Percentage of total variance captured Storage Actual elements stored"},{"location":"sims/svd-image-compression/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ul> <li>Apply SVD for image compression</li> <li>Understand the quality-storage tradeoff</li> <li>Interpret singular value spectra</li> <li>Choose appropriate truncation level</li> </ul>"},{"location":"sims/svd-image-compression/#references","title":"References","text":"<ul> <li>Chapter 7: Matrix Decompositions - Low-Rank Approximation</li> <li>Eckart-Young Theorem</li> </ul>"},{"location":"sims/symmetric-matrix/","title":"Symmetric Matrix","text":"<p>Run the Symmetric Matrix MicroSim Fullscreen</p> <p>Edit the Symmetric Matrix MicroSim with the p5.js editor</p> <p>You can include this MicroSim on your website using the following <code>iframe</code>:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/symmetric-matrix/main.html\" height=\"520px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/symmetric-matrix/#description","title":"Description","text":"<p>This MicroSim provides an interactive visualization of symmetric matrices, a fundamental concept in linear algebra. A matrix is symmetric when it equals its own transpose, meaning the element at position (i,j) equals the element at position (j,i) for all indices.</p> <p>Key Features:</p> <ul> <li>Visual Symmetry: Color coding highlights the relationship between the upper triangle (blue), lower triangle (green), and diagonal elements (tan)</li> <li>Dynamic Size: Adjust the matrix dimensions from 2\u00d72 to 10\u00d710 using the slider</li> <li>Random Generation: The Regenerate button creates new random symmetric matrices with values 0-9</li> <li>Index Labels: Row and column indices help identify element positions</li> </ul>"},{"location":"sims/symmetric-matrix/#symmetry-property","title":"Symmetry Property","text":"<p>A matrix \\(A\\) is symmetric if and only if:</p> \\[A = A^T\\] <p>Which means for all valid indices \\(i\\) and \\(j\\):</p> \\[a_{ij} = a_{ji}\\]"},{"location":"sims/symmetric-matrix/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/symmetric-matrix/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Identify whether a given matrix is symmetric by visual inspection</li> <li>Understand that symmetric matrices are equal to their transpose</li> <li>Recognize that only square matrices can be symmetric</li> <li>Observe that the diagonal elements have no symmetry constraint</li> </ol>"},{"location":"sims/symmetric-matrix/#warm-up-activity-2-minutes","title":"Warm-up Activity (2 minutes)","text":"<p>Ask students: \"If you fold a square matrix along its main diagonal, which elements would overlap?\" Let them discover that \\(a_{ij}\\) overlaps with \\(a_{ji}\\).</p>"},{"location":"sims/symmetric-matrix/#guided-exploration-5-minutes","title":"Guided Exploration (5 minutes)","text":"<ol> <li>Start with a small 3\u00d73 matrix</li> <li>Point out that blue cells in the upper triangle have matching green cells in the lower triangle</li> <li>Click \"Regenerate\" several times to see that the symmetry property always holds</li> <li>Increase the size to 6\u00d76 and observe the same pattern</li> </ol>"},{"location":"sims/symmetric-matrix/#key-discussion-points","title":"Key Discussion Points","text":"<ul> <li>Transpose Relationship: \\(A^T\\) is obtained by swapping rows and columns, so \\(A = A^T\\) means this swap leaves the matrix unchanged</li> <li>Degrees of Freedom: An \\(n\u00d7n\\) symmetric matrix has only \\(\\frac{n(n+1)}{2}\\) independent values (upper triangle + diagonal)</li> <li>Applications: Covariance matrices, distance matrices, and adjacency matrices of undirected graphs are always symmetric</li> </ul>"},{"location":"sims/symmetric-matrix/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>How many unique values determine a 5\u00d75 symmetric matrix?</li> <li>If \\(a_{23} = 7\\) in a symmetric matrix, what is \\(a_{32}\\)?</li> <li>Can a 3\u00d74 matrix be symmetric? Why or why not?</li> </ol>"},{"location":"sims/symmetric-matrix/#references","title":"References","text":"<ul> <li>Chapter 2: Matrices and Matrix Operations - Matrix properties</li> <li>Special Matrices - Other important matrix types</li> </ul>"},{"location":"sims/system-geometry/","title":"System of Equations Geometry","text":"<p>Run the System of Equations Geometry MicroSim Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/system-geometry/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization helps students understand how the solution to a system of linear equations corresponds to the geometric intersection of lines on a coordinate plane.</p> <p>Key Features:</p> <ul> <li>Line Visualization: See two linear equations displayed as colored lines on a coordinate grid</li> <li>Real-time Updates: Adjust coefficients with sliders and watch the geometry change instantly</li> <li>Solution Detection: Automatically identifies unique solutions, infinite solutions (coincident lines), or no solution (parallel lines)</li> <li>Random Generator: Create random systems that always have solutions within the visible grid</li> </ul>"},{"location":"sims/system-geometry/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Coefficients: Use the sliders to change the values of a, b, and c for each equation in the form ax + by = c</li> <li>View Solution: The green point marks the intersection (solution) when one exists</li> <li>Generate Random: Click \"Random\" to create a new system with a guaranteed solution</li> <li>Explore Cases: Try creating parallel lines (no solution) or coincident lines (infinite solutions)</li> </ol>"},{"location":"sims/system-geometry/#solution-types","title":"Solution Types","text":"Configuration Geometric Interpretation Algebraic Meaning Lines intersect at one point Unique solution det(A) \u2260 0 Lines are parallel No solution det(A) = 0, inconsistent Lines are coincident Infinite solutions det(A) = 0, consistent"},{"location":"sims/system-geometry/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/system-geometry/#learning-objectives","title":"Learning Objectives","text":"<p>After using this MicroSim, students will be able to:</p> <ol> <li>Interpret the solution of a system of linear equations geometrically</li> <li>Identify when systems have unique, infinite, or no solutions</li> <li>Connect the algebraic determinant condition to geometric configuration</li> </ol>"},{"location":"sims/system-geometry/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Exploration: Start with the default system (x + y = 3, x - y = 1) and verify the solution (2, 1) by substitution</li> <li>Create Parallel Lines: Adjust coefficients to make the lines parallel. What pattern do you notice in the coefficients?</li> <li>Create Coincident Lines: Make both equations represent the same line. How are the coefficients related?</li> <li>Predict Before Moving: Before adjusting a slider, predict how the line will change</li> </ol>"},{"location":"sims/system-geometry/#discussion-questions","title":"Discussion Questions","text":"<ul> <li>How does changing the constant term c affect the line's position?</li> <li>What coefficient patterns lead to parallel lines?</li> <li>Why does the determinant being zero correspond to parallel or coincident lines?</li> </ul>"},{"location":"sims/tensor-operations/","title":"Tensor Operations","text":"<p>Run the Tensor Operations Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/tensor-operations/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps you understand how tensor shapes transform under common operations. Tensors are the fundamental data structures in deep learning, and manipulating their shapes is a crucial skill.</p>"},{"location":"sims/tensor-operations/#operations-covered","title":"Operations Covered","text":"Operation Description Example Reshape Changes shape, preserves elements (2,3,4) \u2192 (6,4) Transpose Permutes axes (2,3,4) \u2192 (2,4,3) Flatten Collapses to 1D (2,3,4) \u2192 (24,) Squeeze Removes size-1 dims (1,3,1) \u2192 (3,) Unsqueeze Adds size-1 dim (3,4) \u2192 (1,3,4)"},{"location":"sims/tensor-operations/#key-principle","title":"Key Principle","text":"<p>Total elements must be preserved in reshape operations:</p> <p>\\(\\prod_i \\text{input\\_dim}_i = \\prod_j \\text{output\\_dim}_j\\)</p> <p>For a (2,3,4) tensor: \\(2 \\times 3 \\times 4 = 24\\) elements</p>"},{"location":"sims/tensor-operations/#interactive-features","title":"Interactive Features","text":"<ul> <li>Dimension Sliders: Adjust input tensor dimensions</li> <li>Operation Selector: Choose which transformation to apply</li> <li>Apply Button: Animate the transformation</li> <li>Shape Annotations: See input and output shapes with element counts</li> </ul>"},{"location":"sims/tensor-operations/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/tensor-operations/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Predict output tensor shapes for common operations</li> <li>Verify that element counts are preserved in reshapes</li> <li>Choose the appropriate operation for a given shape transformation</li> <li>Debug shape mismatch errors in neural network code</li> </ol>"},{"location":"sims/tensor-operations/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Reshape Chain: Starting from (2,3,4), reshape to (6,4), then (24,), then back to (2,3,4)</li> <li>Transpose Exploration: What happens when you transpose twice?</li> <li>Squeeze/Unsqueeze: Create a tensor (1,5,1,3), squeeze it, then unsqueeze back</li> <li>Invalid Reshape: Try to reshape (2,3,4) to (5,5). Why does it fail?</li> </ol>"},{"location":"sims/tensor-operations/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why might you need to flatten a tensor before a fully-connected layer?</li> <li>When would you use unsqueeze vs reshape to add a dimension?</li> <li>How does transpose relate to matrix transpose from linear algebra?</li> <li>What does \"contiguous\" mean for tensors after transpose?</li> </ol>"},{"location":"sims/tensor-operations/#common-patterns-in-deep-learning","title":"Common Patterns in Deep Learning","text":"<pre><code># Batch dimension: (batch, features) \u2192 (batch, 1, features)\nx.unsqueeze(1)\n\n# Flatten for FC layer: (batch, C, H, W) \u2192 (batch, C*H*W)\nx.flatten(start_dim=1)\n\n# Swap height/width: (B, C, H, W) \u2192 (B, C, W, H)\nx.transpose(2, 3)\n</code></pre>"},{"location":"sims/tensor-operations/#references","title":"References","text":"<ul> <li>PyTorch Documentation: Tensor Views and Reshaping</li> <li>NumPy Documentation: Array Manipulation Routines</li> </ul>"},{"location":"sims/trajectory-optimization/","title":"Trajectory Optimization Visualizer","text":"<p>Run the Trajectory Optimization Visualizer Fullscreen</p> <p>Edit the Trajectory Optimization Visualizer with the p5.js editor</p>"},{"location":"sims/trajectory-optimization/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates trajectory optimization - finding smooth, safe paths that respect velocity limits and obstacle constraints. Unlike path planning, trajectory optimization produces dynamically feasible motions with time profiles.</p>"},{"location":"sims/trajectory-optimization/#embedding","title":"Embedding","text":"<p>You can embed this MicroSim in your website using:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/trajectory-optimization/main.html\"\n        height=\"702px\" width=\"100%\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/trajectory-optimization/#features","title":"Features","text":"<ul> <li>Draggable Waypoints: Define initial path by moving waypoints</li> <li>Gradient-Based Optimization: Watch cost decrease as path improves</li> <li>Velocity Coloring: Green=slow, Red=fast along the path</li> <li>Velocity/Acceleration Profiles: See dynamics constraints</li> <li>Obstacle Clearance: Configurable safety margin around obstacles</li> <li>Cost Convergence Plot: Visualize optimization progress</li> </ul>"},{"location":"sims/trajectory-optimization/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/trajectory-optimization/#trajectory-cost-function","title":"Trajectory Cost Function","text":"<p>The optimization minimizes:</p> \\[J = \\underbrace{w_1 \\sum_i \\|\\ddot{\\mathbf{x}}_i\\|^2}_{\\text{smoothness}} + \\underbrace{w_2 \\sum_i c_{obs}(\\mathbf{x}_i)}_{\\text{obstacle}} + \\underbrace{w_3 \\sum_i [\\|v_i\\| - v_{max}]_+^2}_{\\text{velocity}}\\] <p>Where: - Smoothness term: Penalizes curvature/acceleration - Obstacle term: Penalizes proximity to obstacles - Velocity term: Penalizes exceeding speed limit</p>"},{"location":"sims/trajectory-optimization/#gradient-descent","title":"Gradient Descent","text":"<p>The optimizer iteratively adjusts each path point in the direction that reduces total cost:</p> \\[\\mathbf{x}_i^{new} = \\mathbf{x}_i^{old} - \\alpha \\nabla_{\\mathbf{x}_i} J\\]"},{"location":"sims/trajectory-optimization/#tradeoffs","title":"Tradeoffs","text":"Parameter Low Value High Value Smoothness Sharp turns Gentle curves Speed Limit Slow, safe Fast, aggressive Clearance Close to obstacles Wide margin"},{"location":"sims/trajectory-optimization/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/trajectory-optimization/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Understand multi-objective trajectory optimization</li> <li>Recognize tradeoffs between smoothness, speed, and safety</li> <li>Apply gradient descent to motion planning problems</li> </ul>"},{"location":"sims/trajectory-optimization/#activities","title":"Activities","text":"<ol> <li>Smooth vs Short: Increase smoothness weight, observe longer but smoother paths</li> <li>Speed Constraint: Lower speed limit, see velocity profile flatten</li> <li>Tight Passages: Move waypoints through narrow gaps, adjust clearance</li> <li>Before/After: Compare initial waypoint path vs optimized trajectory</li> </ol>"},{"location":"sims/trajectory-optimization/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>Why might a longer path have lower total cost?</li> <li>How does the obstacle cost term create a \"force field\" effect?</li> <li>What happens if smoothness weight is zero?</li> </ol>"},{"location":"sims/trajectory-optimization/#references","title":"References","text":"<ul> <li>CHOMP: Covariant Hamiltonian Optimization for Motion Planning</li> <li>TrajOpt: Trajectory Optimization</li> <li>Chapter 15: Autonomous Systems and Sensor Fusion</li> </ul>"},{"location":"sims/transform-composition/","title":"Transformation Composition Visualizer","text":"<p>Run the Composition Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/transform-composition/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates one of the most important properties of matrix multiplication: order matters!</p> <p>When composing two transformations T and S:</p> <ul> <li>T then S means apply T first, then S \u2192 Matrix product is S\u00b7T</li> <li>S then T means apply S first, then T \u2192 Matrix product is T\u00b7S</li> </ul> <p>In general, S\u00b7T \u2260 T\u00b7S (matrix multiplication is not commutative).</p>"},{"location":"sims/transform-composition/#how-to-use","title":"How to Use","text":"<ol> <li>Select transformation types for T and S (rotation, scaling, or shear)</li> <li>Adjust parameters using the sliders</li> <li>Compare the results: Left shows T then S, right shows S then T</li> <li>Toggle \"Show Steps\" to see intermediate states</li> <li>Click Animate to watch the transformations apply sequentially</li> </ol>"},{"location":"sims/transform-composition/#when-do-they-commute","title":"When Do They Commute?","text":"<p>Some pairs of transformations give the same result regardless of order:</p> <ul> <li>Two rotations: R(\u03b1)\u00b7R(\u03b2) = R(\u03b2)\u00b7R(\u03b1) = R(\u03b1+\u03b2)</li> <li>Two uniform scalings: Same result either way</li> <li>Rotation by 180\u00b0 and reflection</li> </ul> <p>Try different combinations to discover which ones commute!</p>"},{"location":"sims/transform-composition/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/transform-composition/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/transform-composition/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/transform-composition/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Demonstrate that matrix multiplication order affects the result</li> <li>Identify cases where transformations commute</li> <li>Interpret matrix products as composed transformations</li> </ol>"},{"location":"sims/transform-composition/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Compare orders: Try rotation then scaling vs scaling then rotation</li> <li>Find commutative pairs: Which transformation pairs give the same result?</li> <li>Predict results: Given T and S, predict if TS = ST before checking</li> </ol>"},{"location":"sims/transform-composition/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If T is rotation by 90\u00b0 and S is scaling by 2, is TS = ST?</li> <li>When does rotation commute with shear?</li> <li>Why does the order \"T then S\" correspond to the product S\u00b7T?</li> </ol>"},{"location":"sims/transform-composition/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations - Composition section</li> <li>Matrix multiplication order</li> </ul>"},{"location":"sims/transform-gallery/","title":"Geometric Transformations Interactive Gallery","text":"<p>Run the Transform Gallery Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/transform-gallery/#about-this-microsim","title":"About This MicroSim","text":"<p>This gallery allows you to compare and contrast the four fundamental geometric transformations:</p> Transformation Preserves Determinant Invertible Rotation Lengths, angles, area 1 Always Scaling Angles (if uniform) sx \u00d7 sy If no zero scale Shear Area, parallel lines 1 Always Reflection Lengths, angles -1 Always"},{"location":"sims/transform-gallery/#how-to-use","title":"How to Use","text":"<ol> <li>Select a transformation type using the buttons (Rotation, Scaling, Shear, Reflection)</li> <li>Adjust parameters with the sliders to see how the transformation changes</li> <li>Change the shape to see how different geometries are affected</li> <li>Toggle the grid to see how the entire space deforms</li> <li>Click Animate to see smooth transitions from identity to the transformation</li> </ol>"},{"location":"sims/transform-gallery/#key-observations","title":"Key Observations","text":"<ul> <li>Rotation: The only transformation that preserves both lengths and angles</li> <li>Scaling: Changes area by |sx \u00d7 sy|; negative values also reflect</li> <li>Shear: Turns rectangles into parallelograms while preserving area</li> <li>Reflection: Flips orientation (determinant = -1)</li> </ul>"},{"location":"sims/transform-gallery/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/transform-gallery/main.html\" height=\"522px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/transform-gallery/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/transform-gallery/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify transformation type from its matrix structure</li> <li>Predict how transformations affect area (using determinant)</li> <li>Compare properties preserved by different transformations</li> </ol>"},{"location":"sims/transform-gallery/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Classification: Given a matrix, identify which transformation type it represents</li> <li>Prediction: Before applying a transformation, predict the resulting shape</li> <li>Composition: What happens when you combine shear followed by rotation?</li> </ol>"},{"location":"sims/transform-gallery/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does shear preserve area but not angles?</li> <li>When is a scaling transformation equivalent to a rotation?</li> <li>How can you tell if a matrix represents an invertible transformation?</li> </ol>"},{"location":"sims/transform-gallery/#references","title":"References","text":"<ul> <li>Chapter 4: Linear Transformations</li> <li>Essence of Linear Algebra - 3Blue1Brown</li> </ul>"},{"location":"sims/transformer-block/","title":"Transformer Block Visualizer","text":"<p>Run the Transformer Block Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/transformer-block/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization shows the architecture of a transformer neural network, the foundation of modern large language models like GPT and BERT.</p> <p>Each transformer block contains:</p> <ol> <li>Layer Normalization - Stabilizes training</li> <li>Multi-Head Attention - Models relationships between positions</li> <li>Feed Forward Network - Applies position-wise transformations</li> <li>Residual Connections - Enable gradient flow through deep networks</li> </ol>"},{"location":"sims/transformer-block/#how-to-use","title":"How to Use","text":"<ol> <li>Number of Blocks: Adjust to see 1-6 stacked transformer blocks</li> <li>Show Dimensions: Toggle to see tensor shapes at each stage</li> <li>Highlight Residuals: Toggle to emphasize residual connection paths</li> </ol>"},{"location":"sims/transformer-block/#key-concepts","title":"Key Concepts","text":""},{"location":"sims/transformer-block/#residual-connections","title":"Residual Connections","text":"<p>Residual connections add the input directly to the output:</p> \\[\\text{output} = x + \\text{SubLayer}(x)\\] <p>Benefits: - Enable gradient flow through very deep networks - Allow layers to learn \"refinements\" rather than complete transformations - Stabilize training</p>"},{"location":"sims/transformer-block/#layer-normalization","title":"Layer Normalization","text":"<p>Normalizes across features (not batch):</p> \\[\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma} + \\beta\\]"},{"location":"sims/transformer-block/#transformer-block-structure","title":"Transformer Block Structure","text":"<pre><code>Input x\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nMultiHeadAttn          \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u25bc                  \u2502\nLayerNorm              \u2502\n    \u2502                  \u2502\nFeedForward            \u2502\n    \u2502                  \u2502\n    \u25bc                  \u2502\n    + \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  (residual)\n    \u2502\nOutput\n</code></pre>"},{"location":"sims/transformer-block/#lesson-plan","title":"Lesson Plan","text":"<p>Learning Objectives:</p> <ul> <li>Understand the structure of a transformer block</li> <li>Explain the role of residual connections</li> <li>Trace data flow through stacked transformer layers</li> </ul> <p>Activities:</p> <ol> <li>Count total operations per block and multiply by block count</li> <li>Explain why residual connections help with deep networks</li> <li>Compare pre-norm vs post-norm architectures (this shows pre-norm)</li> </ol> <p>Assessment:</p> <ul> <li>What would happen without residual connections in a 12-layer model?</li> <li>Why use layer norm instead of batch norm in transformers?</li> <li>How do dimensions change (or not change) through a transformer block?</li> </ul>"},{"location":"sims/transformer-block/#references","title":"References","text":"<ul> <li>Attention Is All You Need - Original transformer paper</li> <li>Chapter 11: Generative AI and LLMs</li> <li>The Annotated Transformer</li> </ul>"},{"location":"sims/triangulation-visualizer/","title":"Triangulation Visualizer","text":"<p>Run the Triangulation Visualizer Fullscreen</p> <p>Edit the Triangulation Visualizer with the p5.js editor</p>"},{"location":"sims/triangulation-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This MicroSim demonstrates triangulation - the process of recovering 3D points from stereo correspondences. See how observation noise affects 3D reconstruction accuracy.</p>"},{"location":"sims/triangulation-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Baseline: Change the stereo camera separation</li> <li>Add Noise: Introduce measurement noise to observe error effects</li> <li>Drag Left Point: Manually adjust the left image observation</li> <li>Compare: See true point (green) vs triangulated point (red)</li> <li>Drag to Rotate: Change the 3D view angle</li> </ol>"},{"location":"sims/triangulation-visualizer/#key-concepts","title":"Key Concepts","text":"<p>Triangulation finds the 3D point P where rays from two cameras intersect.</p> <p>Given: - Camera matrices P_L, P_R - Corresponding image points p_L, p_R</p> <p>Solve: Find P such that p_L ~ P_L \u00b7 P and p_R ~ P_R \u00b7 P</p> Method Accuracy Speed Mid-point Moderate Fast Linear (DLT) Good Fast Optimal Best Iterative"},{"location":"sims/triangulation-visualizer/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to: - Understand how triangulation recovers 3D structure - Observe the effect of noise on reconstruction accuracy - Relate baseline to depth precision - Apply linear algebra to solve the triangulation problem</p>"},{"location":"sims/triangulation-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/triangulation-visualizer/#introduction-5-minutes","title":"Introduction (5 minutes)","text":"<p>Triangulation is the core algorithm for 3D reconstruction from stereo. Given known camera geometry and corresponding points, we can recover depth.</p>"},{"location":"sims/triangulation-visualizer/#exploration-10-minutes","title":"Exploration (10 minutes)","text":"<ol> <li>Start with zero noise - observe perfect reconstruction</li> <li>Add noise - see how the triangulated point deviates</li> <li>Increase baseline - observe improved depth precision</li> <li>Manually drag the left point to simulate matching errors</li> </ol>"},{"location":"sims/triangulation-visualizer/#key-insight","title":"Key Insight","text":"<p>Longer baseline improves depth precision (reduces the uncertainty cone) but makes finding correspondences harder.</p>"},{"location":"sims/triangulation-visualizer/#references","title":"References","text":"<ul> <li>Chapter 14: 3D Geometry and Transformations</li> <li>Triangulation - Wikipedia</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/","title":"2D and 3D Vector Visualization","text":"<p>Run the 2D and 3D Vector Visualization Fullscreen</p> <p>You can embed this MicroSim in your own webpage using the following iframe code:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-2d-3d-visualizer/main.html\"\n        height=\"562px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-2d-3d-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization helps students understand how vector components determine position and direction in both 2D and 3D coordinate systems. Students can manipulate the x, y, and z components using sliders and observe how the vector changes in real-time.</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p>"},{"location":"sims/vector-2d-3d-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Components: Use the X, Y, and Z sliders to change the vector components (range: -5 to 5)</li> <li>Switch Views: Click \"Switch to 3D\" to toggle between 2D and 3D visualization modes</li> <li>Toggle Projections: Enable/disable dashed projection lines that show how the vector projects onto each axis</li> <li>Toggle Labels: Show or hide component labels and axis labels</li> <li>Rotate 3D View: In 3D mode, click and drag on the canvas to rotate the view</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Components: How x, y (and z in 3D) values determine the vector endpoint</li> <li>Vector Magnitude: The length of the vector, calculated as \\(\\|v\\| = \\sqrt{x^2 + y^2}\\) in 2D or \\(\\|v\\| = \\sqrt{x^2 + y^2 + z^2}\\) in 3D</li> <li>Coordinate Axes: The standard basis vectors along x, y, and z directions</li> <li>Projection: How a vector projects onto coordinate planes and axes</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-2d-3d-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-2d-3d-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-2d-3d-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of Cartesian coordinate systems</li> <li>Basic knowledge of what vectors represent (magnitude and direction)</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Have students explore the 2D view first, adjusting x and y sliders to see how vectors change</li> <li>Pattern Recognition (5 min): Ask students to find vectors with the same magnitude but different directions</li> <li>3D Extension (5 min): Switch to 3D view and explore how the z-component adds a third dimension</li> <li>Projection Analysis (5 min): Enable projections and discuss how vectors decompose into components</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What happens to the magnitude when you double all components?</li> <li>Can two different vectors have the same magnitude? Give examples.</li> <li>How do the projection lines help you understand vector components?</li> <li>What is the geometric interpretation when x=0 or y=0?</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Ask students to create a vector with a specific magnitude</li> <li>Have students predict the direction before adjusting sliders</li> <li>Quiz on calculating magnitudes from given components</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Vectors, what even are they? - Excellent visual introduction to vectors</li> <li>Khan Academy - Introduction to Vectors</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.</li> </ol>"},{"location":"sims/vector-operations-playground/","title":"Vector Operations Playground","text":"<p>Run the Vector Operations Playground Fullscreen</p>"},{"location":"sims/vector-operations-playground/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive playground allows students to explore vector operations by directly manipulating vectors and observing the results in real-time. Students can perform vector addition, subtraction, and scalar multiplication while seeing both the geometric and numerical representations.</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p>"},{"location":"sims/vector-operations-playground/#how-to-use","title":"How to Use","text":"<ol> <li>Drag Vectors: Click and drag the circular endpoints of vectors u (blue) and v (red) to position them anywhere on the grid</li> <li>Select Operation: Use the radio buttons to choose between:</li> <li>Add: Shows u + v (green result vector)</li> <li>Subtract: Shows u - v (green result vector)</li> <li>Scalar \u00d7: Shows c\u00b7u where c is controlled by the slider</li> <li>Adjust Scalar: When in scalar multiplication mode, use the slider to change the scalar value from -3 to 3</li> <li>Toggle Visualizations:</li> <li>Parallelogram: Shows the parallelogram construction for addition</li> <li>Components: Shows projection lines to the axes</li> <li>Animate: Click to see a step-by-step animation of the operation</li> <li>Reset: Return all vectors to their default positions</li> </ol>"},{"location":"sims/vector-operations-playground/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Addition: The parallelogram rule and tip-to-tail method</li> <li>Vector Subtraction: Finding the difference vector u - v</li> <li>Scalar Multiplication: How scalars stretch, shrink, or reverse vectors</li> <li>Component Operations: How operations work on individual components</li> </ul>"},{"location":"sims/vector-operations-playground/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Addition: \\(\\mathbf{u} + \\mathbf{v} = (u_x + v_x, u_y + v_y)\\)</p> <p>Subtraction: \\(\\mathbf{u} - \\mathbf{v} = (u_x - v_x, u_y - v_y)\\)</p> <p>Scalar Multiplication: \\(c\\mathbf{u} = (c \\cdot u_x, c \\cdot u_y)\\)</p>"},{"location":"sims/vector-operations-playground/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-operations-playground/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-operations-playground/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-operations-playground/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-operations-playground/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/vector-operations-playground/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of 2D coordinate systems</li> <li>Basic knowledge of vectors as arrows with magnitude and direction</li> </ul>"},{"location":"sims/vector-operations-playground/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Let students freely drag vectors and observe how operations change</li> <li>Addition Investigation (5 min):</li> <li>Enable parallelogram view</li> <li>Ask students to verify the parallelogram rule geometrically</li> <li>Have them predict the sum before moving vectors</li> <li>Subtraction Investigation (5 min):</li> <li>Switch to subtraction mode</li> <li>Explore how u - v relates to the vector from v's tip to u's tip</li> <li>Scalar Multiplication (5 min):</li> <li>Vary the scalar from -3 to 3</li> <li>Observe what happens at c = 0, c = 1, c = -1</li> <li>Synthesis (5 min): Combine concepts to solve problems</li> </ol>"},{"location":"sims/vector-operations-playground/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What is the geometric meaning of the parallelogram in vector addition?</li> <li>If u + v = w, what is u - v geometrically?</li> <li>What happens to the direction of a vector when multiplied by a negative scalar?</li> <li>Can two different pairs of vectors have the same sum?</li> </ol>"},{"location":"sims/vector-operations-playground/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, find u and v that add to reach it</li> <li>Predict the result of an operation before seeing it</li> <li>Find a scalar that makes cu equal to a specific vector</li> </ul>"},{"location":"sims/vector-operations-playground/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors</li> <li>Khan Academy - Vector Addition</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press. Chapter 1.</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/","title":"Vector Space Axiom Explorer","text":"<p>Run the Vector Space Axiom Explorer Fullscreen</p> <p>Edit the Vector Space Axiom Explorer Using the p5.js Editor</p>"},{"location":"sims/vector-space-axiom-explorer/#about-this-infographic","title":"About This Infographic","text":"<p>This interactive infographic helps students learn and remember the ten vector space axioms. The axioms are organized into two groups: five for vector addition and five for scalar multiplication. Click on each axiom card to see its definition and a concrete example.</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p>"},{"location":"sims/vector-space-axiom-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over the central hub to see what a vector space is and examples of vector spaces and fields</li> <li>Click on axiom cards to expand and see:</li> <li>Full definition of the axiom</li> <li>A concrete numerical example in R\u00b2</li> <li>Track your progress with the counter at the bottom showing how many axioms you've viewed</li> <li>A checkmark appears on viewed axioms</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#the-ten-vector-space-axioms","title":"The Ten Vector Space Axioms","text":"<p>For a set V to be a vector space over a field F, it must satisfy:</p>"},{"location":"sims/vector-space-axiom-explorer/#addition-axioms-1-5","title":"Addition Axioms (1-5)","text":"<ol> <li>Closure: u + v \u2208 V</li> <li>Commutativity: u + v = v + u</li> <li>Associativity: (u + v) + w = u + (v + w)</li> <li>Identity: v + 0 = v</li> <li>Inverse: v + (-v) = 0</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#scalar-multiplication-axioms-6-10","title":"Scalar Multiplication Axioms (6-10)","text":"<ol> <li>Closure: cv \u2208 V</li> <li>Distributivity (vectors): c(u + v) = cu + cv</li> <li>Distributivity (scalars): (c + d)v = cv + dv</li> <li>Associativity: c(dv) = (cd)v</li> <li>Identity: 1\u00b7v = v</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#why-these-axioms-matter","title":"Why These Axioms Matter","text":"<p>These ten axioms are the foundation of linear algebra. Any set that satisfies all ten axioms is a vector space, which means all theorems about vector spaces apply to it. This includes:</p> <ul> <li>R^n (n-dimensional real space)</li> <li>Polynomials of degree \u2264 n</li> <li>Matrices of a given size</li> <li>Continuous functions on an interval</li> <li>Solutions to homogeneous differential equations</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#embedding-this-infographic","title":"Embedding This Infographic","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-space-axiom-explorer/main.html\"\n        height=\"750px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-space-axiom-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-space-axiom-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/vector-space-axiom-explorer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-space-axiom-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication concepts</li> <li>Familiarity with R\u00b2 as an example vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Introduction (3 min):</li> <li>Hover over the central hub to understand what a vector space is</li> <li> <p>Note the two types of operations: addition and scalar multiplication</p> </li> <li> <p>Addition Axioms (5 min):</p> </li> <li>Click through all five addition axioms</li> <li>Work through each example mentally or on paper</li> <li> <p>Note how they formalize intuitive properties</p> </li> <li> <p>Scalar Multiplication Axioms (5 min):</p> </li> <li>Click through all five scalar multiplication axioms</li> <li>Compare distributivity axioms 7 and 8</li> <li> <p>Understand why the identity axiom uses 1</p> </li> <li> <p>Verification Exercise (5 min):</p> </li> <li>Given a candidate vector space (e.g., 2\u00d72 matrices)</li> <li>Check that all ten axioms hold</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we need all ten axioms? What would go wrong if one was missing?</li> <li>Can you think of a set with addition that violates one of these axioms?</li> <li>Why is the additive identity (zero vector) unique?</li> <li>What's the difference between the two distributivity axioms?</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>List all ten axioms from memory</li> <li>Given a set and operations, verify which axioms hold</li> <li>Explain why a given set is NOT a vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Abstract vector spaces</li> <li>Khan Academy - Vector Spaces</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.1.</li> <li>Axler, S. (2015). Linear Algebra Done Right (3rd ed.). Chapter 1.</li> </ol>"},{"location":"sims/vector-space-gallery/","title":"Vector Space Examples Gallery","text":"<p>Run the Vector Space Gallery Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/vector-space-gallery/#about-this-microsim","title":"About This MicroSim","text":"<p>This gallery presents six diverse examples of vector spaces, demonstrating that the vector space structure appears throughout mathematics. Each card shows:</p> <ul> <li>The vector space name and dimension</li> <li>Visual representation with animated icons</li> <li>The zero vector for that space</li> <li>A concrete example of an element</li> </ul>"},{"location":"sims/vector-space-gallery/#the-six-vector-spaces","title":"The Six Vector Spaces","text":"Vector Space Dimension Zero Vector Key Property \\(\\mathbb{R}^2\\) 2 Origin \\((0,0)\\) Standard 2D plane \\(\\mathbb{R}^3\\) 3 Origin \\((0,0,0)\\) Physical 3D space \\(\\mathcal{P}_2\\) 3 \\(p(x) = 0\\) Polynomials up to degree 2 \\(C[a,b]\\) \\(\\infty\\) \\(f(x) = 0\\) All continuous functions \\(\\mathbb{R}^{2\\times 2}\\) 4 Zero matrix Square matrices \\(\\text{Null}(A)\\) varies Origin Solution space of \\(A\\mathbf{x} = \\mathbf{0}\\)"},{"location":"sims/vector-space-gallery/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over any card to see vector addition and scalar multiplication examples</li> <li>Notice the colors: blue for finite dimensional, purple for infinite dimensional</li> <li>Observe the animations that show elements \"living\" in each space</li> <li>Click a card to highlight it for discussion</li> </ol>"},{"location":"sims/vector-space-gallery/#key-insights","title":"Key Insights","text":""},{"location":"sims/vector-space-gallery/#what-makes-these-vector-spaces","title":"What Makes These Vector Spaces?","text":"<p>All six examples satisfy the same ten axioms:</p> <ol> <li>Closure under addition: Adding two elements stays in the space</li> <li>Closure under scalar multiplication: Scaling stays in the space</li> <li>Contains a zero vector: Every space has an additive identity</li> <li>Additive inverses exist: Every element has a negative</li> </ol>"},{"location":"sims/vector-space-gallery/#finite-vs-infinite-dimensional","title":"Finite vs. Infinite Dimensional","text":"<ul> <li>Finite: Can be described by finitely many basis vectors (\\(\\mathbb{R}^n\\), \\(\\mathcal{P}_n\\), matrices)</li> <li>Infinite: Requires infinitely many basis vectors (continuous functions, sequences)</li> </ul>"},{"location":"sims/vector-space-gallery/#why-this-matters","title":"Why This Matters","text":"<p>Understanding that polynomials, functions, and matrices are vector spaces allows us to apply all linear algebra tools (basis, dimension, linear transformations) to these objects.</p>"},{"location":"sims/vector-space-gallery/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-space-gallery/main.html\" height=\"502px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-space-gallery/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-space-gallery/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Identify six different examples of vector spaces</li> <li>Recognize the zero vector in each vector space</li> <li>Distinguish finite from infinite dimensional spaces</li> <li>Verify that addition and scalar multiplication work in each space</li> </ol>"},{"location":"sims/vector-space-gallery/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Identification: For each card, verify closure under addition using the hover examples</li> <li>Zero Vector: Explain why each \"zero\" really acts as the additive identity</li> <li>Dimension Count: For finite spaces, identify a basis and verify the dimension</li> </ol>"},{"location":"sims/vector-space-gallery/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is \\(\\mathcal{P}_2\\) (polynomials of degree at most 2) three-dimensional, not two-dimensional?</li> <li>Why can't we just count the number of functions to find the dimension of \\(C[a,b]\\)?</li> <li>If \\(A\\) is a \\(3 \\times 3\\) matrix with rank 2, what is the dimension of its null space?</li> </ol>"},{"location":"sims/vector-space-gallery/#references","title":"References","text":"<ul> <li>Chapter 2: Vector Spaces and Subspaces</li> <li>MIT 18.06 - Introduction to Vector Spaces</li> </ul>"},{"location":"sims/volume-scaling-3d/","title":"3D Volume Scaling Interactive Visualizer","text":"<p>Run the 3D Volume Scaling Visualizer Fullscreen</p> <p>Edit the MicroSim with the p5.js editor</p>"},{"location":"sims/volume-scaling-3d/#about-this-microsim","title":"About This MicroSim","text":"<p>This 3D visualization extends the concept of signed area to signed volume in three dimensions.</p> <p>Key Concepts:</p> <ul> <li>The determinant of a 3\u00d73 matrix equals the signed volume of the parallelepiped formed by its column vectors</li> <li>\\(|\\det(A)|\\) = volume scaling factor</li> <li>\\(\\det(A) &gt; 0\\): orientation preserved (green)</li> <li>\\(\\det(A) &lt; 0\\): orientation reversed (red)</li> <li>\\(\\det(A) = 0\\): volume collapses (singular)</li> </ul>"},{"location":"sims/volume-scaling-3d/#how-to-use","title":"How to Use","text":"<ol> <li>Click preset buttons: Scaling, Rotation, or Singular matrix</li> <li>Drag to rotate: Click and drag in the 3D view to rotate camera</li> <li>Use morph slider: Animate from identity to target matrix</li> <li>Toggle unit cube: Show/hide the reference unit cube</li> </ol>"},{"location":"sims/volume-scaling-3d/#embedding","title":"Embedding","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/volume-scaling-3d/main.html\" height=\"482px\" scrolling=\"no\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/volume-scaling-3d/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/volume-scaling-3d/#learning-objectives","title":"Learning Objectives","text":"<p>Students will be able to:</p> <ol> <li>Visualize the determinant as a volume scaling factor in 3D</li> <li>Distinguish between orientation-preserving and orientation-reversing transformations</li> <li>Recognize when a 3D transformation is singular</li> </ol>"},{"location":"sims/volume-scaling-3d/#suggested-activities","title":"Suggested Activities","text":"<ol> <li>Compare 2D and 3D: How does area scaling in 2D relate to volume scaling in 3D?</li> <li>Rotation exploration: Why do rotation matrices have det = 1?</li> <li>Singular detection: What happens to the parallelepiped when columns become coplanar?</li> </ol>"},{"location":"sims/volume-scaling-3d/#assessment-questions","title":"Assessment Questions","text":"<ol> <li>If det(A) = 8, how does a 1-cubic-unit region transform?</li> <li>What is the determinant of a 90\u00b0 rotation matrix in 3D?</li> <li>Why does a scaling matrix [[2,0,0],[0,3,0],[0,0,4]] have det = 24?</li> </ol>"},{"location":"sims/volume-scaling-3d/#references","title":"References","text":"<ul> <li>Chapter 5: Determinants and Matrix Properties - Volume Scaling section</li> <li>Linear Algebra Learning Graph</li> </ul>"}]}