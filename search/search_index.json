{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Linear Algebra","text":"<p>Course Description</p> <p>In Linear Algebra, you won\u2019t just learn matrices and vectors\u2014you'll experience them in action. This course empowers undergraduate students in Computer Science and Artificial Intelligence to develop a deep, functional understanding of linear algebra\u2019s essential role in modern technology.</p>"},{"location":"about/","title":"About the Linear Algebra Book","text":"<p>In January of 2025 I was invited to lead a group of four students in a senior design project.  These were all student in their senior year at the University of Minnesota Department of Electrical Engineering and Computer Design.  My team all were assigned to create an intelligent textbook for either a class they took or a subject they were interested in.  One of these students selected Linear Algebra.  This course is a fork of that textbook, but it has been redesigned significantly to focus on the application of linear algebra on machine learning.</p> <p>Our tools have also matured since January 2025.  This version of our intelligent textbook was generated using Claude Code Skills.  We also put a much stronger focus on creating high-quality MicroSims.</p> <ul> <li>Dan McCreary</li> <li>January 17th, 2025</li> </ul>"},{"location":"course-description/","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"course-description/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"course-description/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"course-description/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"course-description/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"course-description/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"course-description/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"course-description/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"course-description/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"course-description/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"course-description/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"course-description/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"course-description/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"course-description/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"course-description/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"course-description/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"course-description/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"course-description/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"course-description/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"course-description/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"course-description/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"course-description/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"course-description/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"course-description/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"course-description/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"course-description/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"course-description/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"course-description/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"course-description/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"course-description/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"course-description/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"course-description/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"course-description/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 15 chapters covering 300 concepts across four major parts.</p>"},{"location":"chapters/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":"<ol> <li> <p>Vectors and Vector Spaces - Introduces vectors as fundamental building blocks, covering operations, norms, and vector space theory.</p> </li> <li> <p>Matrices and Matrix Operations - Covers matrix notation, special types, and core operations including multiplication and inverse.</p> </li> <li> <p>Systems of Linear Equations - Methods for representing and solving linear systems using elimination and matrix techniques.</p> </li> <li> <p>Linear Transformations - How matrices represent geometric transformations including rotation, scaling, and projection.</p> </li> </ol>"},{"location":"chapters/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":"<ol> <li> <p>Determinants and Matrix Properties - Determinant computation, properties, and geometric interpretation.</p> </li> <li> <p>Eigenvalues and Eigenvectors - Eigentheory fundamentals including characteristic polynomials and diagonalization.</p> </li> <li> <p>Matrix Decompositions - Matrix factorization methods including LU, QR, Cholesky, and SVD.</p> </li> <li> <p>Vector Spaces and Inner Products - Abstract inner product spaces, orthogonality, and least squares.</p> </li> </ol>"},{"location":"chapters/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":"<ol> <li> <p>Machine Learning Foundations - Data representation, PCA, linear regression, and gradient descent.</p> </li> <li> <p>Neural Networks and Deep Learning - Deep learning architecture, weight matrices, and backpropagation.</p> </li> <li> <p>Generative AI and Large Language Models - Embeddings, attention mechanisms, and transformer architecture.</p> </li> <li> <p>Optimization and Learning Algorithms - Optimization algorithms for training machine learning models.</p> </li> </ol>"},{"location":"chapters/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":"<ol> <li> <p>Image Processing and Computer Vision - Image representation, convolution, filtering, and feature detection.</p> </li> <li> <p>3D Geometry and Transformations - Three-dimensional geometry, quaternions, and camera models.</p> </li> <li> <p>Autonomous Systems and Sensor Fusion - Kalman filtering, SLAM, and autonomous navigation.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>This textbook is designed for sequential learning - each chapter builds on concepts from previous chapters. The dependency structure ensures that prerequisite knowledge is always introduced before it's needed. While you can jump ahead to specific topics of interest, completing earlier chapters first will provide the strongest foundation.</p> <p>Each chapter includes a list of concepts covered, allowing you to track your progress through the learning graph. Interactive microsimulations throughout the book help reinforce abstract mathematical concepts with visual, hands-on exploration.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/","title":"Vectors and Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#summary","title":"Summary","text":"<p>This chapter introduces vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces. By the end of this chapter, you will understand how to perform vector operations, compute norms and distances, and work with the abstract concepts of span, linear independence, and basis vectors that form the foundation for all subsequent topics.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description: College Algebra and familiarity with basic calculus concepts.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#introduction","title":"Introduction","text":"<p>Linear algebra provides the mathematical foundation for modern artificial intelligence, machine learning, and computer vision. At the heart of this mathematical framework lies the vector\u2014an elegant structure that represents both magnitude and direction. Whether you're representing a data point in a machine learning model, describing the velocity of an autonomous vehicle, or encoding the semantic meaning of a word, vectors serve as your fundamental tool.</p> <p>This chapter establishes the foundational concepts you'll build upon throughout this course. We begin with the simple distinction between scalars and vectors, then progressively develop your understanding through vector operations, norms, and the powerful abstractions of vector spaces.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalars-and-vectors","title":"Scalars and Vectors","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-scalars","title":"Understanding Scalars","text":"<p>A scalar is a single numerical value that represents a quantity with magnitude only. Scalars are the numbers you've worked with throughout your mathematical education\u2014they can be positive, negative, or zero. In the context of linear algebra, scalars typically come from a field (usually the real numbers \\(\\mathbb{R}\\) or complex numbers \\(\\mathbb{C}\\)).</p> <p>Examples of scalars include:</p> <ul> <li>Temperature: 72\u00b0F</li> <li>Mass: 5.2 kg</li> <li>Speed: 60 mph</li> <li>Count: 42 items</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#what-is-a-vector","title":"What is a Vector?","text":"<p>A vector is an ordered collection of scalars that represents both magnitude and direction. While scalars tell us \"how much,\" vectors tell us \"how much and in which direction.\" Geometrically, we can visualize a vector as an arrow in space\u2014the length represents magnitude, and the arrowhead indicates direction.</p> <p>More formally, a vector in \\(n\\)-dimensional space is an ordered \\(n\\)-tuple of real numbers:</p> \\[\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\] <p>where each \\(v_i\\) is a scalar component of the vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-notation","title":"Vector Notation","text":"<p>Mathematical texts use several conventions for denoting vectors. Throughout this course, we use the following:</p> Notation Description Example Bold lowercase Standard textbook notation \\(\\mathbf{v}\\), \\(\\mathbf{u}\\), \\(\\mathbf{w}\\) Arrow above Handwritten notation \\(\\vec{v}\\), \\(\\vec{u}\\) Column matrix Computational notation \\(\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}\\) Row matrix Alternative notation \\([v_1 \\quad v_2]\\) Component form Explicit values \\((3, 4)\\) or \\(\\langle 3, 4 \\rangle\\) <p>Notation in Code</p> <p>In programming contexts like NumPy, vectors are typically represented as one-dimensional arrays: <code>v = np.array([3, 4, 5])</code>.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vectors-in-different-dimensions","title":"Vectors in Different Dimensions","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#2d-vectors","title":"2D Vectors","text":"<p>A 2D vector exists in a two-dimensional plane and consists of two components. We write a 2D vector as:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\end{bmatrix}\\] <p>where \\(v_x\\) is the horizontal component and \\(v_y\\) is the vertical component. Geometrically, this represents an arrow from the origin to the point \\((v_x, v_y)\\).</p> <p>Common applications of 2D vectors include:</p> <ul> <li>Position coordinates on a screen</li> <li>Velocity in a plane</li> <li>Force components in 2D physics simulations</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#3d-vectors","title":"3D Vectors","text":"<p>A 3D vector extends into three-dimensional space with three components:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_x \\\\ v_y \\\\ v_z \\end{bmatrix}\\] <p>The additional \\(z\\)-component allows us to represent quantities in our three-dimensional world, such as the position of a drone or the direction of a camera in virtual reality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-2d-and-3d-vector-visualization","title":"Diagram: 2D and 3D Vector Visualization","text":"2D and 3D Vector Visualization MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, visualize</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p> <p>Canvas layout: - Left panel (500px): 2D vector visualization with coordinate grid - Right panel (400px): 3D vector visualization with rotatable axes - Bottom controls: Component sliders and toggle switches</p> <p>Visual elements: - 2D coordinate grid with x and y axes, grid lines at unit intervals - 3D coordinate system with x, y, z axes (perspective view) - Vector arrow from origin to point, with distinct arrowhead - Dashed projection lines from vector tip to each axis - Component labels showing current values - Origin point clearly marked</p> <p>Interactive controls: - Slider: x-component (-5 to 5, default 3) - Slider: y-component (-5 to 5, default 4) - Slider: z-component (-5 to 5, default 2) for 3D view - Toggle: Show/hide projection lines - Toggle: Show/hide component labels - Button: Switch between 2D and 3D views - For 3D: Mouse drag to rotate view</p> <p>Default parameters: - 2D vector: (3, 4) - 3D vector: (3, 4, 2) - Projection lines: visible - Component labels: visible</p> <p>Behavior: - As sliders change, vector arrow updates in real-time - Projection lines dynamically connect to axis intersections - Magnitude value updates and displays - 3D view allows full rotation with mouse drag</p> <p>Implementation: p5.js with WEBGL for 3D rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#n-dimensional-vectors","title":"N-Dimensional Vectors","text":"<p>An N-dimensional vector generalizes to any number of dimensions:</p> \\[\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\] <p>While we cannot visualize vectors beyond three dimensions geometrically, the mathematical operations extend naturally. In machine learning, it's common to work with vectors having hundreds or thousands of dimensions:</p> <ul> <li>Word embeddings: 300-dimensional vectors representing word meanings</li> <li>Image features: 2048-dimensional vectors from convolutional neural networks</li> <li>User preference vectors: 100+ dimensions in recommendation systems</li> </ul> <p>The power of linear algebra is that the same operations and theorems apply regardless of dimensionality.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#basic-vector-operations","title":"Basic Vector Operations","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-addition","title":"Vector Addition","text":"<p>Vector addition combines two vectors of the same dimension by adding their corresponding components:</p> \\[\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_n \\end{bmatrix} + \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\\] <p>Geometrically, vector addition follows the parallelogram rule or the tip-to-tail method. If you place the tail of \\(\\mathbf{v}\\) at the tip of \\(\\mathbf{u}\\), the sum \\(\\mathbf{u} + \\mathbf{v}\\) points from the tail of \\(\\mathbf{u}\\) to the tip of \\(\\mathbf{v}\\).</p> <p>Vector addition has the following properties:</p> <ul> <li>Commutative: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associative: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Scalar multiplication multiplies each component of a vector by a scalar value:</p> \\[c \\cdot \\mathbf{v} = c \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} c \\cdot v_1 \\\\ c \\cdot v_2 \\\\ \\vdots \\\\ c \\cdot v_n \\end{bmatrix}\\] <p>The geometric effect of scalar multiplication:</p> <ul> <li>\\(c &gt; 1\\): Stretches the vector (increases magnitude)</li> <li>\\(0 &lt; c &lt; 1\\): Shrinks the vector (decreases magnitude)</li> <li>\\(c = 0\\): Produces the zero vector</li> <li>\\(c &lt; 0\\): Reverses direction and scales magnitude</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-subtraction","title":"Vector Subtraction","text":"<p>Vector subtraction is defined in terms of addition and scalar multiplication:</p> \\[\\mathbf{u} - \\mathbf{v} = \\mathbf{u} + (-1)\\mathbf{v}\\] <p>This yields component-wise subtraction:</p> \\[\\mathbf{u} - \\mathbf{v} = \\begin{bmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\\\ \\vdots \\\\ u_n - v_n \\end{bmatrix}\\]"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-operations-playground","title":"Diagram: Vector Operations Playground","text":"Vector Operations Playground MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, practice, demonstrate</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p> <p>Canvas layout: - Main area (600px): Coordinate grid showing vectors - Right panel (200px): Controls and result display - Bottom: Operation selector and calculation display</p> <p>Visual elements: - Coordinate grid with axes from -10 to 10 - Vector u (blue arrow) with adjustable endpoint - Vector v (red arrow) with adjustable endpoint - Result vector (green arrow) showing operation result - Parallelogram outline when showing addition (dashed lines) - Component labels on each vector - Magnitude display for each vector</p> <p>Interactive controls: - Draggable endpoints for vectors u and v - Radio buttons: Addition, Subtraction, Scalar Multiply - Slider: Scalar value c (-3 to 3, default 2) for scalar multiplication - Checkbox: Show parallelogram construction - Checkbox: Show component breakdown - Button: Animate operation step-by-step - Button: Reset to defaults</p> <p>Default parameters: - Vector u: (3, 2) - Vector v: (1, 4) - Operation: Addition - Scalar: 2 - Show parallelogram: true</p> <p>Behavior: - Dragging vector endpoints updates all calculations in real-time - Operation result vector updates immediately - Step-by-step animation shows geometric construction - Component breakdown displays numerical computation - Parallelogram appears for addition to show geometric interpretation</p> <p>Implementation: p5.js with drag-and-drop interaction</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-products","title":"Vector Products","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#the-dot-product","title":"The Dot Product","text":"<p>The dot product (also called the inner product or scalar product) takes two vectors of the same dimension and returns a scalar. For vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\):</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i = u_1 v_1 + u_2 v_2 + \\cdots + u_n v_n\\] <p>The geometric interpretation reveals the dot product's significance:</p> \\[\\mathbf{u} \\cdot \\mathbf{v} = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\cos\\theta\\] <p>where \\(\\theta\\) is the angle between the vectors and \\(\\|\\cdot\\|\\) denotes magnitude. This relationship connects algebraic computation with geometric meaning.</p> <p>Key properties and insights:</p> Condition Geometric Meaning Dot Product Value \\(\\theta = 0\u00b0\\) Vectors point same direction Maximum positive \\(\\theta = 90\u00b0\\) Vectors are perpendicular Zero \\(\\theta = 180\u00b0\\) Vectors point opposite directions Maximum negative <p>The dot product has important applications:</p> <ul> <li>Computing angles between vectors</li> <li>Projecting one vector onto another</li> <li>Calculating work in physics (force \u00b7 displacement)</li> <li>Measuring similarity between feature vectors</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-cross-product","title":"The Cross Product","text":"<p>The cross product is defined only for 3D vectors and produces a vector perpendicular to both input vectors:</p> \\[\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_2 v_3 - u_3 v_2 \\\\ u_3 v_1 - u_1 v_3 \\\\ u_1 v_2 - u_2 v_1 \\end{bmatrix}\\] <p>The magnitude of the cross product equals the area of the parallelogram formed by the two vectors:</p> \\[\\|\\mathbf{u} \\times \\mathbf{v}\\| = \\|\\mathbf{u}\\| \\|\\mathbf{v}\\| \\sin\\theta\\] <p>The direction follows the right-hand rule: if you curl your fingers from \\(\\mathbf{u}\\) toward \\(\\mathbf{v}\\), your thumb points in the direction of \\(\\mathbf{u} \\times \\mathbf{v}\\).</p> <p>Important properties of the cross product:</p> <ul> <li>Anti-commutative: \\(\\mathbf{u} \\times \\mathbf{v} = -(\\mathbf{v} \\times \\mathbf{u})\\)</li> <li>Not associative: \\((\\mathbf{u} \\times \\mathbf{v}) \\times \\mathbf{w} \\neq \\mathbf{u} \\times (\\mathbf{v} \\times \\mathbf{w})\\)</li> <li>Self-cross is zero: \\(\\mathbf{v} \\times \\mathbf{v} = \\mathbf{0}\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-dot-product-and-cross-product-visualizer","title":"Diagram: Dot Product and Cross Product Visualizer","text":"Dot Product and Cross Product Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare, differentiate</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p> <p>Canvas layout: - Left panel (400px): 2D view for dot product with projection visualization - Right panel (400px): 3D view for cross product with parallelogram and result vector - Bottom panel: Numerical results and formulas</p> <p>Visual elements: - 2D panel: Two vectors, projection line, angle arc, shaded projection component - 3D panel: Two vectors, cross product result vector (perpendicular), parallelogram surface - Angle measurement display with theta symbol - Area calculation display for parallelogram - Formula display showing computation steps</p> <p>Interactive controls: - Draggable vector endpoints in both panels - Slider: Angle between vectors (0\u00b0 to 180\u00b0) - Toggle: Show projection in 2D - Toggle: Show parallelogram in 3D - Toggle: Show right-hand rule animation - Button: Animate angle sweep from 0\u00b0 to 180\u00b0 - 3D rotation via mouse drag</p> <p>Default parameters: - Vector u: (3, 2) in 2D, (3, 2, 0) in 3D - Vector v: (4, 1) in 2D, (1, 4, 0) in 3D - Show projection: true - Show parallelogram: true</p> <p>Behavior: - Dot product value updates with cos(theta) relationship visible - When vectors perpendicular, dot product display highlights zero - Cross product vector length changes with parallelogram area - Right-hand rule animation shows finger curl and thumb direction - Numerical display shows step-by-step calculation</p> <p>Implementation: p5.js with WEBGL for 3D panel</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#magnitude-and-norms","title":"Magnitude and Norms","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#vector-magnitude","title":"Vector Magnitude","text":"<p>The magnitude (or length) of a vector measures how far the vector extends from the origin. For a vector \\(\\mathbf{v} = (v_1, v_2, \\ldots, v_n)\\), the magnitude is:</p> \\[\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This formula is the generalization of the Pythagorean theorem to \\(n\\) dimensions. For a 2D vector \\((3, 4)\\), the magnitude is \\(\\sqrt{9 + 16} = 5\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#understanding-norms","title":"Understanding Norms","text":"<p>A norm is a function that assigns a non-negative length to each vector in a vector space. Different norms measure \"length\" differently, and each has applications in machine learning and optimization.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#l2-norm-euclidean-norm","title":"L2 Norm (Euclidean Norm)","text":"<p>The L2 norm is the standard Euclidean distance from the origin:</p> \\[\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\] <p>This is the magnitude we computed above. It's the \"straight-line\" distance and is used extensively in:</p> <ul> <li>Least squares regression</li> <li>Euclidean distance calculations</li> <li>Ridge regularization (L2 regularization)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l1-norm-manhattan-norm","title":"L1 Norm (Manhattan Norm)","text":"<p>The L1 norm sums the absolute values of components:</p> \\[\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^{n} |v_i|\\] <p>Named after the grid-like street layout of Manhattan, this norm measures distance as if you could only travel along axes. It's used in:</p> <ul> <li>Lasso regularization (L1 regularization)</li> <li>Robust statistics</li> <li>Sparse signal recovery</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#l-infinity-norm-maximum-norm","title":"L-Infinity Norm (Maximum Norm)","text":"<p>The L-infinity norm returns the maximum absolute component value:</p> \\[\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\\] <p>This norm is useful when you need to constrain the maximum deviation in any single dimension.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-norm-comparison-visualizer","title":"Diagram: Norm Comparison Visualizer","text":"Norm Comparison Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p> <p>Canvas layout: - Main area (600px): Coordinate grid with unit \"circles\" for each norm - Right panel (200px): Numerical values and controls</p> <p>Visual elements: - Coordinate grid from -2 to 2 on both axes - Unit circle for L2 norm (actual circle, blue) - Unit \"circle\" for L1 norm (diamond/rhombus shape, green) - Unit \"circle\" for L-infinity norm (square, orange) - Point showing selected vector with lines to origin - Distance measurements for each norm displayed</p> <p>Interactive controls: - Draggable point to select vector - Checkboxes: Show L1, L2, L-infinity unit shapes (all on by default) - Slider: Adjust norm \"radius\" to see scaled shapes - Toggle: Animate point around each unit shape - Display: Current coordinates and all three norm values</p> <p>Default parameters: - Selected point: (0.6, 0.8) - All three norm shapes visible - Radius: 1</p> <p>Behavior: - As point is dragged, all three norm values update - Unit shapes clearly show different \"distance = 1\" definitions - Points on each unit shape highlight when norm equals 1 - Animation moves point around each shape showing equal norm path</p> <p>Implementation: p5.js with parametric curve drawing</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#euclidean-distance","title":"Euclidean Distance","text":"<p>The Euclidean distance between two vectors measures the straight-line separation:</p> \\[d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} (u_i - v_i)^2}\\] <p>This fundamental metric appears throughout machine learning:</p> <ul> <li>k-Nearest Neighbors classification</li> <li>K-means clustering</li> <li>Distance-based similarity measures</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#unit-vectors-and-normalization","title":"Unit Vectors and Normalization","text":"<p>A unit vector has magnitude exactly 1. Any non-zero vector can be converted to a unit vector through normalization:</p> \\[\\hat{\\mathbf{v}} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\\] <p>The resulting vector \\(\\hat{\\mathbf{v}}\\) (pronounced \"v-hat\") points in the same direction as \\(\\mathbf{v}\\) but has unit length. Normalization is essential for:</p> <ul> <li>Comparing vector directions without magnitude bias</li> <li>Creating orthonormal bases</li> <li>Preparing features for machine learning algorithms</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations-and-span","title":"Linear Combinations and Span","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-combinations","title":"Linear Combinations","text":"<p>A linear combination of vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\) is a sum of scalar multiples:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k\\] <p>where \\(c_1, c_2, \\ldots, c_k\\) are scalars. Linear combinations allow us to create new vectors from existing ones and form the foundation for understanding vector spaces.</p> <p>Example: Given \\(\\mathbf{v}_1 = (1, 0)\\) and \\(\\mathbf{v}_2 = (0, 1)\\), the linear combination \\(3\\mathbf{v}_1 + 2\\mathbf{v}_2 = (3, 2)\\).</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-span-of-vectors","title":"The Span of Vectors","text":"<p>The span of a set of vectors is the collection of all possible linear combinations of those vectors:</p> \\[\\text{span}\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\} = \\{c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k : c_i \\in \\mathbb{R}\\}\\] <p>Geometrically, the span represents all points reachable by combining the vectors:</p> <ul> <li>The span of one non-zero vector is a line through the origin</li> <li>The span of two non-parallel vectors in 3D is a plane through the origin</li> <li>The span of three non-coplanar vectors in 3D is all of \\(\\mathbb{R}^3\\)</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-linear-combination-explorer","title":"Diagram: Linear Combination Explorer","text":"Linear Combination Explorer MicroSim <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: use, demonstrate, calculate</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p> <p>Canvas layout: - Main area (550px): 2D coordinate grid with vectors and span visualization - Right panel (250px): Controls for coefficients and target challenges</p> <p>Visual elements: - Coordinate grid from -6 to 6 - Two basis vectors v1 (red) and v2 (blue) with adjustable directions - Scaled versions c1v1 (light red) and c2v2 (light blue) - Result vector as sum (green, thick arrow) - Shaded region showing span (when two vectors span a plane) - Target point for challenges (yellow star) - Trail showing path as coefficients change</p> <p>Interactive controls: - Slider: Coefficient c1 (-3 to 3, step 0.1) - Slider: Coefficient c2 (-3 to 3, step 0.1) - Draggable endpoints to adjust v1 and v2 directions - Toggle: Show span region (shaded area) - Toggle: Show component arrows tip-to-tail - Button: Random target challenge - Button: Show solution for current target - Display: Current linear combination equation</p> <p>Default parameters: - v1: (2, 1) - v2: (1, 2) - c1: 1, c2: 1 - Show span: true - Show components: true</p> <p>Behavior: - Adjusting c1/c2 sliders moves result vector smoothly - Target challenges require finding correct coefficients - When vectors are parallel, span collapses to a line (visual feedback) - Component arrows show tip-to-tail construction of sum</p> <p>Implementation: p5.js with slider controls</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence-and-dependence","title":"Linear Independence and Dependence","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#linear-independence","title":"Linear Independence","text":"<p>A set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k\\}\\) is linearly independent if the only solution to:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>is \\(c_1 = c_2 = \\cdots = c_k = 0\\). In other words, no vector in the set can be written as a linear combination of the others.</p> <p>Geometric interpretation:</p> <ul> <li>Two vectors are linearly independent if they are not parallel</li> <li>Three vectors in 3D are linearly independent if they don't all lie in the same plane</li> <li>Linearly independent vectors point in \"genuinely different\" directions</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#linear-dependence","title":"Linear Dependence","text":"<p>Vectors are linearly dependent if at least one can be expressed as a linear combination of the others. This occurs when there exist scalars, not all zero, such that:</p> \\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\cdots + c_k\\mathbf{v}_k = \\mathbf{0}\\] <p>Example of dependence: The vectors \\(\\mathbf{v}_1 = (1, 2)\\), \\(\\mathbf{v}_2 = (2, 4)\\), and \\(\\mathbf{v}_3 = (3, 6)\\) are linearly dependent because \\(\\mathbf{v}_2 = 2\\mathbf{v}_1\\) and \\(\\mathbf{v}_3 = 3\\mathbf{v}_1\\). They all lie on the same line.</p> Property Linearly Independent Linearly Dependent Unique representation Each point in span has unique coefficients Multiple ways to reach same point Redundancy No redundant vectors At least one vector is redundant Span efficiency Minimal set to span a subspace Could remove vectors without reducing span"},{"location":"chapters/01-vectors-and-vector-spaces/#basis-and-coordinate-systems","title":"Basis and Coordinate Systems","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#basis-vectors","title":"Basis Vectors","text":"<p>A basis for a vector space is a set of linearly independent vectors that span the entire space. Every vector in the space can be written uniquely as a linear combination of basis vectors.</p> <p>A basis provides:</p> <ul> <li>A coordinate system for the space</li> <li>A way to represent any vector as a list of coefficients</li> <li>The minimum number of vectors needed to describe the space</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#the-standard-basis","title":"The Standard Basis","text":"<p>The standard basis for \\(\\mathbb{R}^n\\) consists of vectors with a single 1 and all other components 0:</p> <p>For \\(\\mathbb{R}^2\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>For \\(\\mathbb{R}^3\\): \\(\\(\\mathbf{e}_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{e}_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\\)\\)</p> <p>The standard basis vectors align with the coordinate axes, making them intuitive for visualization and computation.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#coordinate-systems","title":"Coordinate Systems","text":"<p>A coordinate system assigns a unique tuple of numbers to each point in space. When we choose a basis \\(\\{\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n\\}\\), any vector \\(\\mathbf{v}\\) can be written as:</p> \\[\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2 + \\cdots + c_n\\mathbf{b}_n\\] <p>The coefficients \\((c_1, c_2, \\ldots, c_n)\\) are the coordinates of \\(\\mathbf{v}\\) with respect to this basis. Different bases give different coordinate representations of the same vector.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-basis-and-coordinate-system-visualizer","title":"Diagram: Basis and Coordinate System Visualizer","text":"Basis and Coordinate System Visualizer MicroSim <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret, compare</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p> <p>Canvas layout: - Left panel (400px): Standard basis grid with vector - Right panel (400px): Custom basis grid with same vector - Bottom: Coordinate comparison display</p> <p>Visual elements: - Standard basis: Traditional x-y grid with e1, e2 as unit vectors - Custom basis: Skewed grid based on chosen basis vectors b1, b2 - Same geometric vector shown in both coordinate systems - Grid lines parallel to basis vectors in each panel - Coordinate labels showing (x,y) in standard and (c1,c2) in custom - Dashed lines from vector tip to axes showing coordinates</p> <p>Interactive controls: - Draggable point to select vector (synced between panels) - Draggable endpoints for custom basis vectors b1 and b2 - Preset buttons: Standard, Rotated 45\u00b0, Skewed, Stretched - Toggle: Show grid lines - Toggle: Show coordinate projections - Animation: Morph from standard to custom basis</p> <p>Default parameters: - Vector: (3, 2) in standard basis - Custom basis: b1 = (2, 0), b2 = (1, 1) - Grid lines: visible - Projections: visible</p> <p>Behavior: - Moving point updates coordinates in both systems - Changing custom basis redraws right panel grid - Coordinates update in real-time showing transformation - Morph animation shows how grid deforms between bases</p> <p>Implementation: p5.js with matrix transformation for grid rendering</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#vector-spaces","title":"Vector Spaces","text":""},{"location":"chapters/01-vectors-and-vector-spaces/#definition-of-a-vector-space","title":"Definition of a Vector Space","text":"<p>A vector space is a collection of objects called vectors that satisfies ten axioms regarding addition and scalar multiplication. For a set \\(V\\) to be a vector space over a field \\(F\\) (typically \\(\\mathbb{R}\\)), it must satisfy:</p> <p>Addition axioms:</p> <ol> <li>Closure: \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</li> <li>Commutativity: \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</li> <li>Associativity: \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</li> <li>Identity: There exists \\(\\mathbf{0}\\) such that \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</li> <li>Inverse: For each \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ol> <p>Scalar multiplication axioms:</p> <ol> <li>Closure: \\(c\\mathbf{v} \\in V\\)</li> <li>Distributivity (vectors): \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> <li>Distributivity (scalars): \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</li> <li>Associativity: \\(c(d\\mathbf{v}) = (cd)\\mathbf{v}\\)</li> <li>Identity: \\(1\\mathbf{v} = \\mathbf{v}\\)</li> </ol>"},{"location":"chapters/01-vectors-and-vector-spaces/#examples-of-vector-spaces","title":"Examples of Vector Spaces","text":"<p>The most familiar vector space is \\(\\mathbb{R}^n\\), but vector spaces are more general:</p> Vector Space Elements Operations \\(\\mathbb{R}^n\\) n-tuples of real numbers Component-wise addition/scaling Polynomials of degree \u2264 n Polynomials \\(a_0 + a_1x + \\cdots + a_nx^n\\) Add coefficients, scale coefficients Continuous functions on \\([a,b]\\) Functions \\(f: [a,b] \\to \\mathbb{R}\\) \\((f+g)(x) = f(x) + g(x)\\) \\(m \\times n\\) matrices Matrices with \\(m\\) rows, \\(n\\) columns Matrix addition, scalar multiplication"},{"location":"chapters/01-vectors-and-vector-spaces/#dimension-of-a-vector-space","title":"Dimension of a Vector Space","text":"<p>The dimension of a vector space is the number of vectors in any basis. This count is always the same regardless of which basis you choose\u2014a fundamental theorem of linear algebra.</p> <ul> <li>\\(\\mathbb{R}^2\\) has dimension 2</li> <li>\\(\\mathbb{R}^3\\) has dimension 3</li> <li>The space of polynomials of degree \u2264 3 has dimension 4 (basis: \\(\\{1, x, x^2, x^3\\}\\))</li> </ul> <p>Dimension tells us the \"degrees of freedom\" in a vector space\u2014how many independent directions exist.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#diagram-vector-space-axiom-explorer","title":"Diagram: Vector Space Axiom Explorer","text":"Vector Space Axiom Explorer Infographic <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize, list</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p> <p>Layout: Central hub showing \"Vector Space\" with two branches for Addition and Scalar Multiplication axioms</p> <p>Visual elements: - Central node: \"Vector Space V over field F\" - Left branch (blue theme): Five addition axiom nodes - Right branch (green theme): Five scalar multiplication axiom nodes - Each axiom node shows: name, symbolic form, geometric mini-visualization - Connection lines showing axiom groupings</p> <p>Interactive elements: - Hover over axiom node: Tooltip shows full definition and concrete example - Click axiom node: Animates a 2D example demonstrating the axiom - Hover over \"V\": Shows examples of vector spaces - Hover over \"F\": Shows examples of fields (R, C) - Progress tracker: Checkmarks for viewed axioms</p> <p>Hover text content: - Closure (add): \"For any u, v in V, the sum u + v is also in V\" - Commutativity: \"Order doesn't matter: u + v = v + u\" - Associativity (add): \"Grouping doesn't matter: (u + v) + w = u + (v + w)\" - Additive identity: \"The zero vector 0 exists: v + 0 = v\" - Additive inverse: \"Every vector has an opposite: v + (-v) = 0\" - Closure (scalar): \"For any scalar c and v in V, cv is also in V\" - Distributivity (vectors): \"c(u + v) = cu + cv\" - Distributivity (scalars): \"(c + d)v = cv + dv\" - Associativity (scalar): \"c(dv) = (cd)v\" - Scalar identity: \"1v = v\"</p> <p>Visual style: Modern node-and-edge layout with pastel colors</p> <p>Implementation: HTML/CSS/JavaScript with SVG nodes and CSS transitions</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#applications-in-ai-and-machine-learning","title":"Applications in AI and Machine Learning","text":"<p>The concepts introduced in this chapter form the foundation for understanding machine learning algorithms:</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#feature-vectors","title":"Feature Vectors","text":"<p>Machine learning represents data points as vectors. A sample with features \\((x_1, x_2, \\ldots, x_n)\\) becomes a vector in \\(\\mathbb{R}^n\\). Vector operations enable:</p> <ul> <li>Distance-based classification: k-NN uses Euclidean distance between feature vectors</li> <li>Similarity measures: Cosine similarity uses the dot product to compare vectors</li> <li>Centering data: Subtracting the mean vector centers the dataset at the origin</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#word-embeddings","title":"Word Embeddings","text":"<p>Natural language processing represents words as dense vectors (typically 100-300 dimensions). These embedding vectors capture semantic meaning:</p> <ul> <li>Similar words have nearby vectors (small Euclidean distance)</li> <li>Analogies appear as vector arithmetic: \\(\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}\\)</li> <li>The dot product measures semantic similarity</li> </ul>"},{"location":"chapters/01-vectors-and-vector-spaces/#neural-network-parameters","title":"Neural Network Parameters","text":"<p>Neural networks store learned knowledge in weight matrices and bias vectors. Forward propagation consists of:</p> <ol> <li>Linear combination: \\(\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}\\)</li> <li>Nonlinear activation: \\(\\mathbf{a} = \\sigma(\\mathbf{z})\\)</li> </ol> <p>Understanding vector spaces helps interpret what neural networks learn and how they transform data.</p>"},{"location":"chapters/01-vectors-and-vector-spaces/#summary_1","title":"Summary","text":"<p>This chapter established the fundamental concepts of linear algebra that underpin all subsequent topics:</p> <p>Core Concepts:</p> <ul> <li>Scalars are single numbers; vectors are ordered collections with magnitude and direction</li> <li>Vectors exist in spaces of any dimension, from 2D and 3D to high-dimensional feature spaces</li> <li>Basic operations include addition, scalar multiplication, and subtraction</li> </ul> <p>Products and Measurements:</p> <ul> <li>The dot product returns a scalar measuring projection and angle</li> <li>The cross product (3D only) returns a perpendicular vector</li> <li>Norms (L1, L2, L-infinity) measure vector length in different ways</li> <li>Normalization creates unit vectors for direction comparison</li> </ul> <p>Abstract Structures:</p> <ul> <li>Linear combinations create new vectors from scaled sums</li> <li>The span is all reachable points through linear combinations</li> <li>Linear independence means no vector is redundant</li> <li>A basis provides a coordinate system for a vector space</li> <li>The dimension counts basis vectors (degrees of freedom)</li> </ul> <p>These concepts provide the vocabulary and tools for the matrix operations, transformations, and decompositions explored in subsequent chapters.</p> Self-Check: Can you answer these questions? <ol> <li>What is the geometric interpretation of the dot product \\(\\mathbf{u} \\cdot \\mathbf{v} = 0\\)?</li> <li>If vectors \\(\\mathbf{v}_1\\), \\(\\mathbf{v}_2\\), \\(\\mathbf{v}_3\\) are linearly dependent, what does this mean geometrically in 3D?</li> <li>Why must a basis for \\(\\mathbb{R}^3\\) contain exactly three vectors?</li> <li>How does the L1 norm differ from the L2 norm when measuring the vector \\((3, 4)\\)?</li> </ol>"},{"location":"chapters/02-matrices-and-matrix-operations/","title":"Matrices and Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#summary","title":"Summary","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations. You will learn about matrix notation, various special matrix types including diagonal, triangular, symmetric, and orthogonal matrices, and master core operations like multiplication, transpose, and inverse. These concepts form the computational backbone of all linear algebra applications.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"chapters/02-matrices-and-matrix-operations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#introduction-to-matrices","title":"Introduction to Matrices","text":"<p>In the previous chapter, we explored vectors as the fundamental objects of linear algebra\u2014ordered lists of numbers representing points or directions in space. Now we extend this foundation to matrices, rectangular arrays of numbers that organize multiple vectors into a single mathematical object. Matrices are ubiquitous in modern computing: they represent images as pixel grids, encode neural network weights, store graph adjacency relationships, and transform coordinates in computer graphics.</p> <p>The matrix perspective transforms our understanding of linear systems. Rather than thinking of equations individually, we can represent entire systems compactly and manipulate them using matrix operations. This algebraic framework enables efficient computation on modern hardware, where matrix operations are highly optimized through libraries like NumPy, TensorFlow, and PyTorch.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#what-is-a-matrix","title":"What is a Matrix?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns. We denote matrices with bold uppercase letters such as \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), or \\(\\mathbf{M}\\). The numbers within a matrix are called entries or elements.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-definition","title":"Matrix Definition","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(a_{ij}\\) is the entry in row \\(i\\) and column \\(j\\)</li> <li>\\(m\\) is the number of rows</li> <li>\\(n\\) is the number of columns</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-notation-and-dimensions","title":"Matrix Notation and Dimensions","text":"<p>The dimensions of a matrix describe its shape. An \\(m \\times n\\) matrix (read \"m by n\") has \\(m\\) rows and \\(n\\) columns. The first number always indicates rows, and the second indicates columns.</p> Dimensions Description Example Use Case \\(3 \\times 3\\) Square matrix Rotation in 3D \\(28 \\times 28\\) Square matrix MNIST digit image \\(m \\times n\\) Rectangular Data matrix with \\(m\\) samples, \\(n\\) features \\(1 \\times n\\) Row vector Single observation \\(m \\times 1\\) Column vector Single feature across samples <p>Using standard matrix notation, we refer to individual entries with subscripts. The entry \\(a_{ij}\\) (or \\(A_{ij}\\)) denotes the element in row \\(i\\) and column \\(j\\). This indexing convention\u2014row first, column second\u2014is consistent across mathematics and most programming languages.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#row-and-column-vectors","title":"Row and Column Vectors","text":"<p>Matrices with only one row or one column receive special names. A row vector is a \\(1 \\times n\\) matrix containing \\(n\\) elements arranged horizontally:</p> <p>\\(\\mathbf{r} = \\begin{bmatrix} r_1 &amp; r_2 &amp; \\cdots &amp; r_n \\end{bmatrix}\\)</p> <p>A column vector is an \\(m \\times 1\\) matrix containing \\(m\\) elements arranged vertically:</p> <p>\\(\\mathbf{c} = \\begin{bmatrix} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{bmatrix}\\)</p> <p>In machine learning contexts, column vectors typically represent individual data points or feature vectors, while row vectors represent observations in a data matrix. This distinction matters because matrix multiplication requires compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-row-and-column-vector-visualization","title":"Diagram: Row and Column Vector Visualization","text":"Row and Column Vector Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: compare, contrast</p> <p>Learning Objective: Help students visually distinguish between row vectors (horizontal) and column vectors (vertical), understanding how their orientation affects matrix operations.</p> <p>Canvas layout: - Main drawing area showing both vector types side by side - Controls below for adjusting vector dimensions</p> <p>Visual elements: - Left side: A row vector displayed horizontally with labeled entries - Right side: A column vector displayed vertically with labeled entries - Color coding: row vector in blue, column vector in green - Grid background showing the row/column structure - Dimension labels showing \"1 \u00d7 n\" for row and \"m \u00d7 1\" for column</p> <p>Interactive controls: - Slider: Number of elements (2-6) - Toggle: Show/hide dimension annotations - Button: Randomize values</p> <p>Default parameters: - Elements: 4 - Values: random integers 1-9</p> <p>Behavior: - Adjusting element count updates both vectors simultaneously - Dimension labels update dynamically - Values displayed inside each cell</p> <p>Implementation: p5.js</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-entries-and-indexing","title":"Matrix Entries and Indexing","text":"<p>Each matrix entry \\(a_{ij}\\) occupies a specific position determined by its row index \\(i\\) and column index \\(j\\). Understanding this indexing system is essential for implementing matrix algorithms and interpreting matrix operations.</p> <p>Consider a \\(3 \\times 4\\) matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\end{bmatrix}\\)</p> <p>In this example:</p> <ul> <li>\\(a_{11} = 1\\) (first row, first column)</li> <li>\\(a_{23} = 7\\) (second row, third column)</li> <li>\\(a_{32} = 10\\) (third row, second column)</li> </ul> <p>Zero-Based vs One-Based Indexing</p> <p>Mathematical notation uses one-based indexing (starting from 1), while programming languages like Python and JavaScript use zero-based indexing (starting from 0). In NumPy, accessing element \\(a_{23}\\) requires <code>A[1, 2]</code>.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#viewing-matrices-as-collections-of-vectors","title":"Viewing Matrices as Collections of Vectors","text":"<p>A powerful perspective views matrices as collections of vectors. An \\(m \\times n\\) matrix can be interpreted as:</p> <ul> <li>\\(n\\) column vectors of dimension \\(m\\), or</li> <li>\\(m\\) row vectors of dimension \\(n\\)</li> </ul> <p>This dual interpretation underlies many matrix operations. When we multiply a matrix by a vector, we're computing a linear combination of the matrix's column vectors. When we compute the transpose, we're swapping between row and column interpretations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#basic-matrix-operations","title":"Basic Matrix Operations","text":""},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition","title":"Matrix Addition","text":"<p>Matrix addition combines two matrices of identical dimensions by adding corresponding entries. If \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are both \\(m \\times n\\) matrices, their sum \\(\\mathbf{C} = \\mathbf{A} + \\mathbf{B}\\) is also an \\(m \\times n\\) matrix where each entry is:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-addition-formula","title":"Matrix Addition Formula","text":"<p>\\(c_{ij} = a_{ij} + b_{ij}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in the sum matrix</li> <li>\\(a_{ij}\\) is the corresponding entry in \\(\\mathbf{A}\\)</li> <li>\\(b_{ij}\\) is the corresponding entry in \\(\\mathbf{B}\\)</li> </ul> <p>Matrix addition is both commutative (\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)) and associative (\\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\)).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Matrix scalar multiplication multiplies every entry of a matrix by a single number (scalar). If \\(k\\) is a scalar and \\(\\mathbf{A}\\) is a matrix, then \\(k\\mathbf{A}\\) has entries:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#scalar-multiplication-formula","title":"Scalar Multiplication Formula","text":"<p>\\((k\\mathbf{A})_{ij} = k \\cdot a_{ij}\\)</p> <p>where:</p> <ul> <li>\\(k\\) is the scalar multiplier</li> <li>\\(a_{ij}\\) is the original matrix entry</li> </ul> <p>Scalar multiplication scales all entries uniformly. In neural networks, this operation appears when applying learning rates to gradient matrices during backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-addition-and-scalar-multiplication","title":"Diagram: Matrix Addition and Scalar Multiplication","text":"Matrix Addition and Scalar Multiplication Interactive <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to practice matrix addition and scalar multiplication interactively, reinforcing the element-wise nature of these operations.</p> <p>Canvas layout: - Top section: Two input matrices A and B displayed side by side - Middle: Result matrix C with operation indicator - Bottom: Control panel</p> <p>Visual elements: - 3\u00d73 matrices displayed as grids with editable cells - Color highlighting showing corresponding entries during addition - Animation showing values flowing into result matrix - Operation symbol (+, \u00d7) displayed between matrices</p> <p>Interactive controls: - Radio buttons: Select operation (Addition / Scalar Multiply) - Slider: Scalar value k (for scalar multiplication, range -3 to 3) - Button: Randomize matrices - Button: Step through calculation - Toggle: Show/hide calculation details</p> <p>Default parameters: - Matrix size: 3\u00d73 - Operation: Addition - Scalar: 2 - Values: small integers (-5 to 5)</p> <p>Behavior: - Clicking a cell allows value editing - Result updates in real-time as inputs change - Step mode highlights each entry calculation sequentially - Animation shows entry-by-entry computation</p> <p>Implementation: p5.js with editable input fields</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>Matrix multiplication is the most important and nuanced matrix operation. Unlike addition, matrix multiplication has specific dimension requirements and is not commutative.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product","title":"Matrix-Vector Product","text":"<p>The matrix-vector product multiplies an \\(m \\times n\\) matrix by an \\(n \\times 1\\) column vector, producing an \\(m \\times 1\\) column vector. This operation represents applying a linear transformation to a vector.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-vector-product-formula","title":"Matrix-Vector Product Formula","text":"<p>\\(\\mathbf{y} = \\mathbf{A}\\mathbf{x}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix</li> <li>\\(\\mathbf{x}\\) is an \\(n \\times 1\\) column vector</li> <li>\\(\\mathbf{y}\\) is the resulting \\(m \\times 1\\) column vector</li> </ul> <p>Each entry of the result is the dot product of a row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\):</p> <p>\\(y_i = \\sum_{j=1}^{n} a_{ij} x_j = a_{i1}x_1 + a_{i2}x_2 + \\cdots + a_{in}x_n\\)</p> <p>Alternatively, the matrix-vector product can be viewed as a linear combination of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\mathbf{A}\\mathbf{x} = x_1 \\mathbf{a}_1 + x_2 \\mathbf{a}_2 + \\cdots + x_n \\mathbf{a}_n\\)</p> <p>where \\(\\mathbf{a}_j\\) denotes the \\(j\\)-th column of \\(\\mathbf{A}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-matrix-multiplication","title":"Matrix-Matrix Multiplication","text":"<p>Matrix multiplication extends the matrix-vector product. The product of an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) and an \\(n \\times p\\) matrix \\(\\mathbf{B}\\) is an \\(m \\times p\\) matrix \\(\\mathbf{C}\\).</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-multiplication-formula","title":"Matrix Multiplication Formula","text":"<p>\\(c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\\)</p> <p>where:</p> <ul> <li>\\(c_{ij}\\) is the entry in row \\(i\\), column \\(j\\) of the product</li> <li>The sum runs over the shared dimension \\(n\\)</li> <li>Each entry requires \\(n\\) multiplications and \\(n-1\\) additions</li> </ul> <p>The dimension compatibility rule states: the number of columns in \\(\\mathbf{A}\\) must equal the number of rows in \\(\\mathbf{B}\\).</p> Matrix A Matrix B Product C Valid? \\(2 \\times 3\\) \\(3 \\times 4\\) \\(2 \\times 4\\) Yes \\(3 \\times 2\\) \\(3 \\times 4\\) \u2014 No \\(4 \\times 4\\) \\(4 \\times 4\\) \\(4 \\times 4\\) Yes \\(1 \\times 5\\) \\(5 \\times 1\\) \\(1 \\times 1\\) Yes (scalar) <p>Matrix Multiplication is Not Commutative</p> <p>In general, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). In fact, if \\(\\mathbf{A}\\mathbf{B}\\) exists, \\(\\mathbf{B}\\mathbf{A}\\) may not even be defined (different dimensions). Even for square matrices where both products exist, they typically differ.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-multiplication-visualizer","title":"Diagram: Matrix Multiplication Visualizer","text":"Matrix Multiplication Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand the row-by-column computation process in matrix multiplication by animating each entry calculation.</p> <p>Canvas layout: - Left: Matrix A (highlighted rows) - Center: Matrix B (highlighted columns) - Right: Result matrix C (entries fill in as computed) - Bottom: Control panel and calculation display</p> <p>Visual elements: - Matrix A with current row highlighted in blue - Matrix B with current column highlighted in green - Result matrix C with current entry position highlighted in yellow - Dot product calculation shown step by step below matrices - Running sum displayed during computation</p> <p>Interactive controls: - Dropdown: Matrix A dimensions (2\u00d72, 2\u00d73, 3\u00d72, 3\u00d73) - Dropdown: Matrix B dimensions (matching first dimension of result) - Button: \"Next Entry\" - advance to next calculation - Button: \"Auto Play\" - animate all calculations - Slider: Animation speed (200ms - 2000ms per step) - Button: Reset - Toggle: Show column interpretation (linear combination view)</p> <p>Default parameters: - Matrix A: 2\u00d73 - Matrix B: 3\u00d72 - Animation speed: 800ms - Values: small integers (1-5)</p> <p>Behavior: - Highlight corresponding row of A and column of B - Show element-wise multiplication with + signs - Display running sum as computation proceeds - Fill in result entry when complete - Move to next entry automatically or on button click - Column interpretation mode shows result as linear combination</p> <p>Implementation: p5.js with animation states</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-multiplication","title":"Properties of Matrix Multiplication","text":"<p>Matrix multiplication satisfies several important algebraic properties:</p> <ul> <li>Associativity: \\((\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\)</li> <li>Distributivity: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\)</li> <li>Scalar compatibility: \\(k(\\mathbf{A}\\mathbf{B}) = (k\\mathbf{A})\\mathbf{B} = \\mathbf{A}(k\\mathbf{B})\\)</li> <li>Non-commutativity: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\) in general</li> </ul> <p>These properties enable powerful algebraic manipulations while requiring careful attention to the order of operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#the-matrix-transpose","title":"The Matrix Transpose","text":"<p>The matrix transpose operation flips a matrix over its diagonal, converting rows to columns and vice versa. For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\), its transpose \\(\\mathbf{A}^T\\) is an \\(n \\times m\\) matrix.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-definition","title":"Transpose Definition","text":"<p>\\((\\mathbf{A}^T)_{ij} = a_{ji}\\)</p> <p>where:</p> <ul> <li>The entry in row \\(i\\), column \\(j\\) of \\(\\mathbf{A}^T\\) equals the entry in row \\(j\\), column \\(i\\) of \\(\\mathbf{A}\\)</li> </ul> <p>For example:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\implies \\mathbf{A}^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix}\\)</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#transpose-properties","title":"Transpose Properties","text":"<p>The transpose operation satisfies these properties:</p> <ul> <li>\\((\\mathbf{A}^T)^T = \\mathbf{A}\\) (double transpose returns original)</li> <li>\\((\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\) (transpose distributes over addition)</li> <li>\\((k\\mathbf{A})^T = k\\mathbf{A}^T\\) (scalars pass through)</li> <li>\\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) (order reverses for products!)</li> </ul> <p>The last property is particularly important: when transposing a product, the order of the matrices reverses. This \"reverse order law\" appears frequently in derivations involving neural network backpropagation.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#special-matrix-types","title":"Special Matrix Types","text":"<p>Many matrices have special structures that simplify computation or carry geometric meaning. Recognizing these types enables algorithmic optimizations and deeper understanding.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#identity-matrix","title":"Identity Matrix","text":"<p>The identity matrix \\(\\mathbf{I}_n\\) is the multiplicative identity for \\(n \\times n\\) matrices. It has ones on the diagonal and zeros elsewhere:</p> <p>\\(\\mathbf{I}_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>For any matrix \\(\\mathbf{A}\\) with compatible dimensions:</p> <p>\\(\\mathbf{I}_m \\mathbf{A} = \\mathbf{A} \\mathbf{I}_n = \\mathbf{A}\\)</p> <p>The identity matrix represents the \"do nothing\" transformation\u2014it maps every vector to itself.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>A diagonal matrix has nonzero entries only on its main diagonal. All off-diagonal entries are zero:</p> <p>\\(\\mathbf{D} = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 \\\\ 0 &amp; d_2 &amp; 0 \\\\ 0 &amp; 0 &amp; d_3 \\end{bmatrix}\\)</p> <p>Diagonal matrices have special computational properties:</p> <ul> <li>Multiplication: Multiplying by a diagonal matrix scales rows or columns</li> <li>Powers: \\(\\mathbf{D}^k\\) has entries \\(d_i^k\\)</li> <li>Inverse: \\(\\mathbf{D}^{-1}\\) has entries \\(1/d_i\\) (if all \\(d_i \\neq 0\\))</li> </ul> <p>In machine learning, diagonal matrices appear in batch normalization and as covariance matrices for independent features.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#triangular-matrices","title":"Triangular Matrices","text":"<p>A triangular matrix has zeros on one side of the diagonal.</p> <p>An upper triangular matrix has zeros below the diagonal:</p> <p>\\(\\mathbf{U} = \\begin{bmatrix} u_{11} &amp; u_{12} &amp; u_{13} \\\\ 0 &amp; u_{22} &amp; u_{23} \\\\ 0 &amp; 0 &amp; u_{33} \\end{bmatrix}\\)</p> <p>A lower triangular matrix has zeros above the diagonal:</p> <p>\\(\\mathbf{L} = \\begin{bmatrix} l_{11} &amp; 0 &amp; 0 \\\\ l_{21} &amp; l_{22} &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; l_{33} \\end{bmatrix}\\)</p> <p>Triangular matrices are computationally advantageous:</p> <ul> <li>Solving \\(\\mathbf{L}\\mathbf{x} = \\mathbf{b}\\) uses forward substitution (start from top)</li> <li>Solving \\(\\mathbf{U}\\mathbf{x} = \\mathbf{b}\\) uses back substitution (start from bottom)</li> <li>Both require only \\(O(n^2)\\) operations instead of \\(O(n^3)\\)</li> </ul> <p>LU decomposition factors any matrix into lower and upper triangular components, enabling efficient system solving.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-special-matrix-types-gallery","title":"Diagram: Special Matrix Types Gallery","text":"Special Matrix Types Interactive Gallery <p>Type: infographic</p> <p>Bloom Level: Remember (L1) Bloom Verb: identify, recognize</p> <p>Learning Objective: Help students visually identify and distinguish between different special matrix types (identity, diagonal, upper/lower triangular) by their structural patterns.</p> <p>Layout: Grid of matrix visualizations with interactive selection</p> <p>Visual elements: - 4\u00d74 grid showing four matrix types simultaneously - Each matrix displayed as a colored grid:   - Identity: ones on diagonal (gold), zeros elsewhere (light gray)   - Diagonal: colored diagonal entries, zeros elsewhere   - Upper triangular: colored upper triangle including diagonal, zeros below   - Lower triangular: colored lower triangle including diagonal, zeros above - Structural pattern highlighted with shading - Labels below each matrix type</p> <p>Interactive features: - Click on a matrix type to enlarge and see detailed properties - Hover over entries to see position (i,j) and value - Toggle: Show/hide zero entries - Slider: Matrix size (3\u00d73 to 6\u00d76) - Button: Show random example values</p> <p>Information panels (on click): - Definition of the matrix type - Key properties - Computational advantages - Common applications</p> <p>Color scheme: - Non-zero entries: blue gradient based on value - Zero entries: light gray or hidden - Diagonal: highlighted in gold - Structural regions: subtle background shading</p> <p>Implementation: HTML/CSS/JavaScript with SVG or Canvas</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A symmetric matrix equals its own transpose: \\(\\mathbf{A} = \\mathbf{A}^T\\). This means \\(a_{ij} = a_{ji}\\) for all entries\u2014the matrix is mirror-symmetric across the diagonal.</p> <p>\\(\\mathbf{S} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 6 \\end{bmatrix}\\)</p> <p>Symmetric matrices arise naturally in many contexts:</p> <ul> <li>Covariance matrices in statistics</li> <li>Adjacency matrices for undirected graphs</li> <li>Hessian matrices (second derivatives) in optimization</li> <li>Gram matrices \\(\\mathbf{A}^T\\mathbf{A}\\) from any matrix \\(\\mathbf{A}\\)</li> </ul> <p>Symmetric matrices have remarkable spectral properties: they always have real eigenvalues and orthogonal eigenvectors, enabling powerful decomposition methods.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#orthogonal-matrix","title":"Orthogonal Matrix","text":"<p>An orthogonal matrix \\(\\mathbf{Q}\\) satisfies \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{Q}\\mathbf{Q}^T = \\mathbf{I}\\). Equivalently, its inverse equals its transpose: \\(\\mathbf{Q}^{-1} = \\mathbf{Q}^T\\).</p> <p>The columns of an orthogonal matrix form an orthonormal set\u2014they are mutually perpendicular unit vectors. Orthogonal matrices represent rotations and reflections that preserve lengths and angles.</p> <p>Properties of orthogonal matrices:</p> <ul> <li>Length preservation: \\(\\|\\mathbf{Q}\\mathbf{x}\\| = \\|\\mathbf{x}\\|\\) for all vectors \\(\\mathbf{x}\\)</li> <li>Angle preservation: The angle between \\(\\mathbf{Q}\\mathbf{x}\\) and \\(\\mathbf{Q}\\mathbf{y}\\) equals the angle between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)</li> <li>Determinant: \\(\\det(\\mathbf{Q}) = \\pm 1\\) (rotation if \\(+1\\), reflection if \\(-1\\))</li> <li>Easy inversion: Computing \\(\\mathbf{Q}^{-1}\\) requires only a transpose</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-orthogonal-matrix-transformation","title":"Diagram: Orthogonal Matrix Transformation","text":"Orthogonal Matrix Transformation Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Demonstrate that orthogonal matrices preserve lengths and angles by visualizing how rotation matrices transform shapes without distortion.</p> <p>Canvas layout: - Main area: 2D coordinate plane with original and transformed shapes - Right panel: Matrix display and controls</p> <p>Visual elements: - Coordinate grid with x and y axes - Original shape (unit square or set of vectors) in blue - Transformed shape in red (semi-transparent overlay) - Length indicators showing preservation - Angle arc showing angle preservation between vectors - 2\u00d72 rotation matrix displayed with current angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (0\u00b0 to 360\u00b0) - Checkbox: Show length comparison - Checkbox: Show angle comparison - Dropdown: Shape to transform (square, triangle, circle of vectors) - Button: Add reflection (multiply by reflection matrix) - Display: Matrix values updating with angle</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: unit square - Show lengths: true - Show angles: false</p> <p>Behavior: - Shape smoothly rotates as angle slider changes - Length indicators show |Qx| = |x| - Angle arcs update to show angle preservation - Matrix entries update: cos(\u03b8), -sin(\u03b8), sin(\u03b8), cos(\u03b8) - Reflection button flips across an axis</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse","title":"Matrix Inverse","text":"<p>The matrix inverse generalizes division to matrices. For a square matrix \\(\\mathbf{A}\\), its inverse \\(\\mathbf{A}^{-1}\\) (if it exists) satisfies:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#matrix-inverse-definition","title":"Matrix Inverse Definition","text":"<p>\\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is an \\(n \\times n\\) square matrix</li> <li>\\(\\mathbf{A}^{-1}\\) is the inverse matrix</li> <li>\\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#invertible-matrices","title":"Invertible Matrices","text":"<p>A matrix is invertible (also called nonsingular or non-degenerate) if its inverse exists. Not all matrices are invertible\u2014those without inverses are called singular.</p> <p>Conditions for invertibility (all equivalent):</p> <ul> <li>\\(\\mathbf{A}^{-1}\\) exists</li> <li>\\(\\det(\\mathbf{A}) \\neq 0\\)</li> <li>The columns of \\(\\mathbf{A}\\) are linearly independent</li> <li>The rows of \\(\\mathbf{A}\\) are linearly independent</li> <li>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has only the trivial solution \\(\\mathbf{x} = \\mathbf{0}\\)</li> <li>\\(\\mathbf{A}\\) has full rank</li> </ul> <p>For a 2\u00d72 matrix, the inverse has a closed form:</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#22-matrix-inverse-formula","title":"2\u00d72 Matrix Inverse Formula","text":"<p>\\(\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\implies \\mathbf{A}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(ad - bc\\) is the determinant of \\(\\mathbf{A}\\)</li> <li>The inverse exists only if \\(ad - bc \\neq 0\\)</li> </ul>"},{"location":"chapters/02-matrices-and-matrix-operations/#properties-of-matrix-inverse","title":"Properties of Matrix Inverse","text":"<p>When inverses exist, they satisfy these properties:</p> <ul> <li>\\((\\mathbf{A}^{-1})^{-1} = \\mathbf{A}\\)</li> <li>\\((\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T\\)</li> <li>\\((k\\mathbf{A})^{-1} = \\frac{1}{k}\\mathbf{A}^{-1}\\) for \\(k \\neq 0\\)</li> <li>\\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\) (order reverses!)</li> </ul> <p>The inverse enables solving linear systems: if \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), then \\(\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}\\).</p> <p>Computational Practice</p> <p>While mathematically elegant, computing \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly is usually avoided in practice. Instead, we solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) directly using methods like LU decomposition, which are more numerically stable and efficient.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-matrix-inverse-explorer","title":"Diagram: Matrix Inverse Explorer","text":"Matrix Inverse Interactive Explorer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: calculate, demonstrate</p> <p>Learning Objective: Enable students to explore matrix inversion for 2\u00d72 and 3\u00d73 matrices, verify the inverse property AA\u207b\u00b9 = I, and understand when matrices are singular.</p> <p>Canvas layout: - Left: Input matrix A (editable) - Center: Computed inverse A\u207b\u00b9 (or \"Singular\" warning) - Right: Verification showing AA\u207b\u00b9 = I - Bottom: Controls and determinant display</p> <p>Visual elements: - Editable matrix cells with color-coded entries - Determinant value prominently displayed - Color indicator: green for invertible, red for singular/near-singular - Verification matrix showing identity (or near-identity for numerical precision) - Warning icon when determinant is near zero</p> <p>Interactive controls: - Matrix size toggle: 2\u00d72 / 3\u00d73 - Editable cells for matrix entries - Button: Randomize matrix - Button: Make singular (set det = 0) - Slider: Approach singularity (smoothly varies toward singular) - Toggle: Show calculation steps</p> <p>Default parameters: - Size: 2\u00d72 - Initial matrix: [[2, 1], [1, 1]] (invertible)</p> <p>Behavior: - Real-time inverse computation as entries change - Determinant updates continuously - Verification matrix computes A \u00d7 A\u207b\u00b9 - Near-singular matrices show numerical instability - Step-by-step mode shows cofactor/adjugate method for 2\u00d72</p> <p>Implementation: p5.js with matrix computation library</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#sparse-and-dense-matrices","title":"Sparse and Dense Matrices","text":"<p>Matrices are classified by their distribution of zero and nonzero entries.</p> <p>A dense matrix has few zero entries relative to its total size. Most entries contain meaningful nonzero values. Dense matrices require \\(O(mn)\\) storage and \\(O(mn)\\) operations for most computations.</p> <p>A sparse matrix has many zero entries\u2014typically the number of nonzero entries is \\(O(n)\\) or \\(O(n \\log n)\\) rather than \\(O(n^2)\\) for an \\(n \\times n\\) matrix. Sparse matrices arise naturally in:</p> <ul> <li>Graph adjacency matrices: Most nodes connect to few others</li> <li>Document-term matrices: Each document uses few of all possible words</li> <li>Finite element methods: Local interactions produce banded structures</li> </ul> Property Dense Matrix Sparse Matrix Storage \\(O(mn)\\) \\(O(\\text{nnz})\\) Zero entries Few Many (typically &gt;90%) Storage format 2D array CSR, CSC, COO Multiplication Standard algorithms Specialized sparse algorithms Examples Covariance matrices Adjacency matrices <p>where \\(\\text{nnz}\\) denotes the number of nonzero entries.</p> <p>Sparse matrix formats like Compressed Sparse Row (CSR) store only nonzero values along with their positions, dramatically reducing memory requirements and enabling faster operations by skipping zero computations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-sparse-vs-dense-matrix-visualization","title":"Diagram: Sparse vs Dense Matrix Visualization","text":"Sparse vs Dense Matrix Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, contrast, differentiate</p> <p>Learning Objective: Help students understand the structural difference between sparse and dense matrices and appreciate why sparsity enables computational efficiency.</p> <p>Layout: Side-by-side matrix visualizations with statistics</p> <p>Visual elements: - Left panel: Dense matrix (most cells filled with color) - Right panel: Sparse matrix (few colored cells, most white/gray) - Color intensity indicates value magnitude - Zero entries shown as white or light gray - Statistics overlay:   - Total entries   - Nonzero entries   - Sparsity percentage   - Memory comparison (dense vs sparse storage)</p> <p>Interactive features: - Slider: Matrix size (10\u00d710 to 100\u00d7100) - Slider: Sparsity level (10% to 99% zeros) - Dropdown: Sparse pattern (random, diagonal, banded, block) - Toggle: Show storage comparison bar chart - Hover: Show value at position</p> <p>Example data: - Dense: Random values in most cells - Sparse: Graph adjacency pattern or banded structure</p> <p>Statistics display: - Memory ratio: \"Sparse uses X% of dense storage\" - Multiplication speedup estimate</p> <p>Implementation: HTML Canvas or p5.js with efficient rendering</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrices","title":"Block Matrices","text":"<p>A block matrix (or partitioned matrix) is a matrix viewed as an array of smaller matrices called blocks or submatrices. Block structure often reflects natural problem decomposition.</p> <p>\\(\\mathbf{M} = \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\), \\(\\mathbf{B}\\), \\(\\mathbf{C}\\), \\(\\mathbf{D}\\) are matrices of compatible dimensions.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#block-matrix-operations","title":"Block Matrix Operations","text":"<p>Block matrices support \"block-wise\" operations that mirror scalar operations:</p> <p>Block Addition: If two matrices have the same block structure:</p> <p>\\(\\begin{bmatrix} \\mathbf{A}_1 &amp; \\mathbf{B}_1 \\\\ \\mathbf{C}_1 &amp; \\mathbf{D}_1 \\end{bmatrix} + \\begin{bmatrix} \\mathbf{A}_2 &amp; \\mathbf{B}_2 \\\\ \\mathbf{C}_2 &amp; \\mathbf{D}_2 \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}_1 + \\mathbf{A}_2 &amp; \\mathbf{B}_1 + \\mathbf{B}_2 \\\\ \\mathbf{C}_1 + \\mathbf{C}_2 &amp; \\mathbf{D}_1 + \\mathbf{D}_2 \\end{bmatrix}\\)</p> <p>Block Multiplication: With compatible block dimensions:</p> <p>\\(\\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{B} \\\\ \\mathbf{C} &amp; \\mathbf{D} \\end{bmatrix} \\begin{bmatrix} \\mathbf{E} \\\\ \\mathbf{F} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{A}\\mathbf{E} + \\mathbf{B}\\mathbf{F} \\\\ \\mathbf{C}\\mathbf{E} + \\mathbf{D}\\mathbf{F} \\end{bmatrix}\\)</p> <p>Block structure enables:</p> <ul> <li>Parallel computation on independent blocks</li> <li>Efficient algorithms exploiting structure</li> <li>Conceptual clarity in complex systems</li> <li>Memory-efficient storage when blocks have special properties</li> </ul> <p>In deep learning, weight matrices are often organized in blocks corresponding to different layers or attention heads.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-block-matrix-structure","title":"Diagram: Block Matrix Structure","text":"Block Matrix Partitioning Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, classify</p> <p>Learning Objective: Help students understand how matrices can be partitioned into blocks and how block operations mirror element-wise operations at a higher level.</p> <p>Canvas layout: - Main area: Large matrix with adjustable partition lines - Right panel: Block extraction and operation display</p> <p>Visual elements: - 8\u00d78 matrix displayed as a grid - Draggable horizontal and vertical partition lines - Distinct colors for each block region - Block labels (A, B, C, D, etc.) overlaid on regions - Extracted blocks shown separately on right</p> <p>Interactive controls: - Drag partition lines to resize blocks - Dropdown: Example partitioning (2\u00d72 blocks, row partition, column partition) - Toggle: Show block dimensions - Button: Demonstrate block multiplication - Checkbox: Show compatibility requirements</p> <p>Default parameters: - Matrix size: 8\u00d78 - Initial partition: 4\u00d74 blocks (2\u00d72 block structure) - Values: integers 1-9</p> <p>Behavior: - Partition lines snap to integer positions - Block colors update as partitioning changes - Dimension labels update dynamically - Block multiplication demo shows step-by-step block operations</p> <p>Implementation: p5.js with draggable elements</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#applications-in-machine-learning-and-ai","title":"Applications in Machine Learning and AI","text":"<p>The matrix concepts covered in this chapter form the computational foundation of modern machine learning systems.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#data-representation","title":"Data Representation","text":"<p>Machine learning datasets are naturally represented as matrices:</p> <ul> <li>Design matrix \\(\\mathbf{X}\\): Each row is a sample, each column is a feature</li> <li>Weight matrix \\(\\mathbf{W}\\): Neural network parameters connecting layers</li> <li>Embedding matrix \\(\\mathbf{E}\\): Word or token embeddings in NLP</li> </ul> <p>For a dataset with \\(m\\) samples and \\(n\\) features, the design matrix is \\(m \\times n\\). Feature scaling, normalization, and transformation all use matrix operations.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#neural-network-layers","title":"Neural Network Layers","text":"<p>A fully connected neural network layer performs:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{W}\\) is the weight matrix</li> <li>\\(\\mathbf{x}\\) is the input vector</li> <li>\\(\\mathbf{b}\\) is the bias vector</li> <li>\\(\\sigma\\) is a nonlinear activation function</li> </ul> <p>Batch processing extends this to:</p> <p>\\(\\mathbf{H} = \\sigma(\\mathbf{X}\\mathbf{W}^T + \\mathbf{b})\\)</p> <p>where \\(\\mathbf{X}\\) contains multiple input vectors as rows, enabling parallel computation of many samples.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#attention-mechanisms","title":"Attention Mechanisms","text":"<p>Transformer models compute attention using three matrices:</p> <ul> <li>Query matrix \\(\\mathbf{Q}\\): What we're looking for</li> <li>Key matrix \\(\\mathbf{K}\\): What's available to match</li> <li>Value matrix \\(\\mathbf{V}\\): What we retrieve</li> </ul> <p>The attention computation:</p> <p>\\(\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\\)</p> <p>relies entirely on matrix multiplications and transposes covered in this chapter.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#diagram-neural-network-layer-matrix-operations","title":"Diagram: Neural Network Layer Matrix Operations","text":"Neural Network Layer Forward Pass Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: implement, demonstrate</p> <p>Learning Objective: Show how matrix-vector multiplication implements a neural network layer, connecting abstract matrix operations to concrete deep learning computations.</p> <p>Canvas layout: - Left: Input vector visualization - Center: Weight matrix with connection lines - Right: Output vector (pre and post activation) - Bottom: Controls and formula display</p> <p>Visual elements: - Input neurons (circles) with values - Weight matrix displayed as a heatmap - Connection lines from inputs to outputs (thickness = weight magnitude) - Output neurons showing weighted sums - Activation function visualization (ReLU, sigmoid curve) - Formula: h = \u03c3(Wx + b) displayed with current values</p> <p>Interactive controls: - Slider: Number of inputs (2-6) - Slider: Number of outputs (2-6) - Dropdown: Activation function (none, ReLU, sigmoid, tanh) - Button: Randomize weights - Button: Randomize input - Toggle: Show bias term - Toggle: Show matrix multiplication step-by-step</p> <p>Default parameters: - Inputs: 3 - Outputs: 2 - Activation: ReLU - Random weights in [-1, 1] - Random inputs in [0, 1]</p> <p>Behavior: - Changing input values updates outputs in real-time - Connection line colors indicate positive (blue) vs negative (red) weights - Step-by-step mode highlights each row-vector dot product - Activation function applied visually to each output</p> <p>Implementation: p5.js with animation</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter introduced matrices as the fundamental data structures of linear algebra with broad applications in AI and machine learning.</p> <p>Core Concepts:</p> <ul> <li>A matrix is a rectangular array of numbers with dimensions \\(m \\times n\\)</li> <li>Matrix entries are indexed by row and column: \\(a_{ij}\\)</li> <li>Row vectors (\\(1 \\times n\\)) and column vectors (\\(m \\times 1\\)) are special cases</li> </ul> <p>Fundamental Operations:</p> <ul> <li>Matrix addition adds corresponding entries (requires same dimensions)</li> <li>Scalar multiplication scales all entries by a constant</li> <li>Matrix-vector product \\(\\mathbf{A}\\mathbf{x}\\) produces a linear combination of columns</li> <li>Matrix multiplication \\(\\mathbf{A}\\mathbf{B}\\) requires matching inner dimensions</li> <li>Transpose \\(\\mathbf{A}^T\\) flips rows and columns</li> </ul> <p>Special Matrix Types:</p> <ul> <li>Identity matrix \\(\\mathbf{I}\\): multiplicative identity</li> <li>Diagonal matrix: nonzeros only on diagonal</li> <li>Triangular matrices: zeros above or below diagonal</li> <li>Symmetric matrix: \\(\\mathbf{A} = \\mathbf{A}^T\\)</li> <li>Orthogonal matrix: \\(\\mathbf{Q}^T\\mathbf{Q} = \\mathbf{I}\\) (preserves lengths/angles)</li> </ul> <p>Inverse and Structure:</p> <ul> <li>Matrix inverse \\(\\mathbf{A}^{-1}\\) satisfies \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\)</li> <li>Invertible matrices have nonzero determinant and linearly independent columns</li> <li>Sparse matrices have few nonzero entries; dense matrices have many</li> <li>Block matrices partition into submatrices for structured computation</li> </ul> <p>Key Properties to Remember:</p> <ul> <li>Matrix multiplication is NOT commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)</li> <li>Product transpose reverses order: \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\)</li> <li>Inverse product reverses order: \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\\)</li> </ul> <p>These operations form the computational vocabulary of machine learning, from basic linear regression to advanced transformer architectures.</p>"},{"location":"chapters/02-matrices-and-matrix-operations/#exercises","title":"Exercises","text":"Exercise 1: Matrix Dimensions <p>Given matrices \\(\\mathbf{A}\\) (3\u00d74), \\(\\mathbf{B}\\) (4\u00d72), and \\(\\mathbf{C}\\) (3\u00d72), which products are defined? What are their dimensions?</p> <ul> <li>\\(\\mathbf{A}\\mathbf{B}\\)</li> <li>\\(\\mathbf{B}\\mathbf{A}\\)</li> <li>\\(\\mathbf{A}\\mathbf{C}\\)</li> <li>\\(\\mathbf{A}\\mathbf{B} + \\mathbf{C}\\)</li> </ul> Exercise 2: Transpose Properties <p>Prove that \\((\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T\\) for 2\u00d72 matrices by computing both sides with generic entries.</p> Exercise 3: Symmetric Matrices <p>Show that for any matrix \\(\\mathbf{A}\\), both \\(\\mathbf{A}^T\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^T\\) are symmetric matrices.</p> Exercise 4: Orthogonal Matrix Verification <p>Verify that the following matrix is orthogonal and determine if it represents a rotation or reflection:</p> <p>\\(\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{bmatrix}\\)</p> Exercise 5: Block Multiplication <p>Compute the product of the following block matrices:</p> <p>\\(\\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ \\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix} \\begin{bmatrix} \\mathbf{I}_2 &amp; \\mathbf{0} \\\\ -\\mathbf{A} &amp; \\mathbf{I}_2 \\end{bmatrix}\\)</p> <p>where \\(\\mathbf{A}\\) is any 2\u00d72 matrix.</p>"},{"location":"chapters/03-systems-of-linear-equations/","title":"Systems of Linear Equations","text":""},{"location":"chapters/03-systems-of-linear-equations/#summary","title":"Summary","text":"<p>This chapter teaches you to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields. You will master Gaussian elimination, understand row echelon forms, and learn to analyze when solutions exist, are unique, or are infinite. These computational techniques are essential for everything from optimization to neural network training.</p>"},{"location":"chapters/03-systems-of-linear-equations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 23 concepts from the learning graph:</p> <ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"chapters/03-systems-of-linear-equations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#introduction","title":"Introduction","text":"<p>Systems of linear equations appear everywhere in science, engineering, and data analysis. Whether you're balancing chemical reactions, analyzing electrical circuits, fitting models to data, or training neural networks, you're solving linear systems. This chapter develops the systematic methods for solving these systems and\u2014equally important\u2014understanding when solutions exist and what form they take.</p> <p>The power of linear algebra lies in its ability to represent complex systems compactly and solve them efficiently. A problem that might involve dozens or thousands of equations becomes a single matrix equation, and the solution emerges through systematic elimination procedures that computers execute with remarkable speed.</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equations-and-systems","title":"Linear Equations and Systems","text":""},{"location":"chapters/03-systems-of-linear-equations/#what-is-a-linear-equation","title":"What is a Linear Equation?","text":"<p>A linear equation in variables \\(x_1, x_2, \\ldots, x_n\\) has the form:</p>"},{"location":"chapters/03-systems-of-linear-equations/#linear-equation-standard-form","title":"Linear Equation Standard Form","text":"<p>\\(a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n = b\\)</p> <p>where:</p> <ul> <li>\\(a_1, a_2, \\ldots, a_n\\) are the coefficients (constants)</li> <li>\\(x_1, x_2, \\ldots, x_n\\) are the variables (unknowns)</li> <li>\\(b\\) is the constant term (right-hand side)</li> </ul> <p>The equation is \"linear\" because each variable appears only to the first power and is not multiplied by other variables. The graph of a linear equation in two variables is a line; in three variables, it's a plane.</p> <p>Examples of linear equations:</p> <ul> <li>\\(3x + 2y = 7\\)</li> <li>\\(x_1 - 4x_2 + x_3 = 0\\)</li> <li>\\(w + 2x - y + 3z = 5\\)</li> </ul> <p>Non-linear equations (not covered by these methods):</p> <ul> <li>\\(x^2 + y = 5\\) (squared term)</li> <li>\\(xy = 3\\) (product of variables)</li> <li>\\(\\sin(x) + y = 1\\) (nonlinear function)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#systems-of-equations","title":"Systems of Equations","text":"<p>A system of equations is a collection of two or more equations involving the same variables. A solution to the system must satisfy all equations simultaneously.</p> <p>Consider a system of \\(m\\) linear equations in \\(n\\) unknowns:</p> <p>\\(a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1\\)</p> <p>\\(a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2\\)</p> <p>\\(\\vdots\\)</p> <p>\\(a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n = b_m\\)</p> <p>Geometrically, each equation defines a hyperplane, and solving the system means finding where all hyperplanes intersect.</p> Equations Variables Geometric Interpretation 2 equations 2 variables Intersection of two lines 3 equations 3 variables Intersection of three planes \\(m\\) equations \\(n\\) variables Intersection of \\(m\\) hyperplanes in \\(\\mathbb{R}^n\\)"},{"location":"chapters/03-systems-of-linear-equations/#diagram-system-of-equations-geometry","title":"Diagram: System of Equations Geometry","text":"System of Equations Geometric Visualization <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: interpret, explain</p> <p>Learning Objective: Help students visualize how the solution to a system of linear equations corresponds to the geometric intersection of lines (2D) or planes (3D).</p> <p>Canvas layout: - Main area: 2D/3D coordinate system with equations plotted - Right panel: Equation display and controls</p> <p>Visual elements: - 2D mode: Two or three lines with intersection point highlighted - 3D mode: Two or three planes with intersection shown - Solution point marked with a distinct marker - Coordinate axes with labels - Grid for reference</p> <p>Interactive controls: - Toggle: 2D / 3D mode - Sliders: Coefficients for each equation (a, b, c values) - Dropdown: Number of equations (2 or 3) - Checkbox: Show solution coordinates - Button: Generate random system - Dropdown: Solution type (unique, infinite, none)</p> <p>Default parameters: - Mode: 2D - Equations: 2 - Example: x + y = 3, x - y = 1 (solution at (2, 1))</p> <p>Behavior: - Lines/planes update in real-time as coefficients change - Intersection point updates dynamically - When lines are parallel (no solution), display message - When lines are coincident (infinite solutions), highlight entire line - 3D mode allows rotation and zoom</p> <p>Implementation: p5.js with WEBGL for 3D mode</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-representation","title":"Matrix Representation","text":""},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form","title":"Matrix Equation Form","text":"<p>Any system of linear equations can be written compactly as a single matrix equation:</p>"},{"location":"chapters/03-systems-of-linear-equations/#matrix-equation-form_1","title":"Matrix Equation Form","text":"<p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\)</p> <p>where:</p> <ul> <li>\\(\\mathbf{A}\\) is the \\(m \\times n\\) coefficient matrix</li> <li>\\(\\mathbf{x}\\) is the \\(n \\times 1\\) vector of unknowns</li> <li>\\(\\mathbf{b}\\) is the \\(m \\times 1\\) vector of constants</li> </ul> <p>For the system:</p> <p>\\(2x + 3y = 8\\)</p> <p>\\(x - y = 1\\)</p> <p>The matrix form is:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 \\\\ 1 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 8 \\\\ 1 \\end{bmatrix}\\)</p> <p>This representation reveals the structure: the coefficient matrix \\(\\mathbf{A}\\) encodes how variables combine, while \\(\\mathbf{b}\\) specifies the targets.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-augmented-matrix","title":"The Augmented Matrix","text":"<p>The augmented matrix combines the coefficient matrix and the right-hand side into a single matrix for manipulation:</p> <p>\\([\\mathbf{A} \\mid \\mathbf{b}] = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1 \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; | &amp; \\vdots \\\\ a_{m1} &amp; a_{m2} &amp; \\cdots &amp; a_{mn} &amp; | &amp; b_m \\end{bmatrix}\\)</p> <p>The vertical bar separates coefficients from constants, though it's sometimes omitted. All solution procedures work on the augmented matrix, treating coefficients and constants together.</p> <p>For our example:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; | &amp; 8 \\\\ 1 &amp; -1 &amp; | &amp; 1 \\end{bmatrix}\\)</p> <p>Why Augmented Matrices?</p> <p>The augmented matrix is the standard data structure for solving linear systems. It keeps all relevant information together and allows us to track how row operations affect both coefficients and constants simultaneously.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-operations","title":"Row Operations","text":"<p>The key to solving linear systems is transforming the augmented matrix into a simpler form while preserving the solution set. Three row operations accomplish this.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-three-elementary-row-operations","title":"The Three Elementary Row Operations","text":"<ol> <li>Row Swap (Interchange): Exchange two rows</li> <li>Notation: \\(R_i \\leftrightarrow R_j\\)</li> <li> <p>Swaps row \\(i\\) with row \\(j\\)</p> </li> <li> <p>Row Scaling (Multiplication): Multiply a row by a nonzero constant</p> </li> <li>Notation: \\(kR_i \\rightarrow R_i\\) (where \\(k \\neq 0\\))</li> <li> <p>Multiplies every entry in row \\(i\\) by \\(k\\)</p> </li> <li> <p>Row Addition (Replacement): Add a multiple of one row to another</p> </li> <li>Notation: \\(R_i + kR_j \\rightarrow R_i\\)</li> <li>Adds \\(k\\) times row \\(j\\) to row \\(i\\)</li> </ol> Operation Effect Preserves Solutions? Row Swap Reorders equations Yes Row Scaling Scales an equation Yes (if \\(k \\neq 0\\)) Row Addition Combines equations Yes <p>These operations correspond to valid algebraic manipulations of equations:</p> <ul> <li>Swapping two equations doesn't change solutions</li> <li>Multiplying an equation by a nonzero constant doesn't change its solutions</li> <li>Adding equations produces a valid new equation satisfied by the same solutions</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-row-operations-interactive","title":"Diagram: Row Operations Interactive","text":"Row Operations Step-by-Step Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, demonstrate</p> <p>Learning Objective: Enable students to practice applying the three elementary row operations and observe how they transform the augmented matrix while preserving solutions.</p> <p>Canvas layout: - Top: Current augmented matrix (large, clear display) - Middle: Operation selector and parameters - Bottom: History of operations performed</p> <p>Visual elements: - Matrix displayed as a grid with clear row/column separation - Active rows highlighted during operation - Animation showing transformation - Vertical bar separating coefficients from constants - Operation history as a scrollable list</p> <p>Interactive controls: - Dropdown: Select operation (Swap, Scale, Add) - For Swap: Two row selectors - For Scale: Row selector + scalar input (prevent 0) - For Add: Target row + source row + multiplier - Button: Apply operation - Button: Undo last operation - Button: Reset to original - Toggle: Show intermediate steps with animation</p> <p>Default parameters: - Starting matrix: 3\u00d74 augmented matrix - Example system: 2x + y - z = 8, -3x - y + 2z = -11, -2x + y + 2z = -3</p> <p>Behavior: - Selected rows highlight before operation - Animation shows values changing - History updates with notation (e.g., \"R\u2081 \u2194 R\u2082\") - Undo restores previous state - Invalid operations (scaling by 0) show error</p> <p>Implementation: p5.js with animation states</p>"},{"location":"chapters/03-systems-of-linear-equations/#gaussian-elimination","title":"Gaussian Elimination","text":"<p>Gaussian elimination is the systematic procedure for reducing an augmented matrix to row echelon form using row operations. Named after Carl Friedrich Gauss, this algorithm is the workhorse of computational linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-algorithm","title":"The Algorithm","text":"<p>Gaussian elimination proceeds column by column, from left to right:</p> <ol> <li>Find a pivot: Locate the leftmost column with a nonzero entry</li> <li>Position the pivot: Use row swaps to move a nonzero entry to the top of the working submatrix</li> <li>Eliminate below: Use row addition to create zeros below the pivot</li> <li>Move down: Repeat for the next column with the remaining rows</li> </ol> <p>The goal is to create an \"upper triangular\" structure where all entries below the main diagonal are zero.</p>"},{"location":"chapters/03-systems-of-linear-equations/#row-echelon-form","title":"Row Echelon Form","text":"<p>A matrix is in row echelon form (REF) if:</p> <ul> <li>All zero rows are at the bottom</li> <li>The leading entry (first nonzero entry) of each row is to the right of the leading entry of the row above</li> <li>All entries below a leading entry are zero</li> </ul> <p>A pivot position is the location of a leading 1 (or leading nonzero entry). A pivot column is a column containing a pivot position.</p> <p>Example of row echelon form:</p> <p>\\(\\begin{bmatrix} \\boxed{2} &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; \\boxed{1} &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; \\boxed{3} &amp; | &amp; 6 \\end{bmatrix}\\)</p> <p>The boxed entries are pivots. Each pivot is to the right of the pivot above it.</p>"},{"location":"chapters/03-systems-of-linear-equations/#back-substitution","title":"Back Substitution","text":"<p>Once a matrix is in row echelon form, we solve for the variables using back substitution\u2014working from the bottom row upward:</p> <ol> <li>Solve the last equation for its variable</li> <li>Substitute into the equation above and solve</li> <li>Continue upward until all variables are found</li> </ol> <p>For the example above:</p> <ul> <li>Row 3: \\(3z = 6 \\Rightarrow z = 2\\)</li> <li>Row 2: \\(y + 4(2) = -2 \\Rightarrow y = -10\\)</li> <li>Row 1: \\(2x + 3(-10) - 2 = 5 \\Rightarrow x = 18.5\\)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-gaussian-elimination-visualizer","title":"Diagram: Gaussian Elimination Visualizer","text":"Gaussian Elimination Step-by-Step Animator <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: execute, implement</p> <p>Learning Objective: Guide students through the complete Gaussian elimination algorithm, showing each row operation and explaining why it's performed.</p> <p>Canvas layout: - Top: Current matrix state with pivot highlighted - Middle: Current operation being performed with explanation - Bottom: Controls and solution display</p> <p>Visual elements: - Matrix with current pivot position highlighted in yellow - Rows being modified highlighted in blue - Zeros created by elimination shown in green (briefly) - Current column being processed indicated - Progress indicator showing algorithm phase - Final solution displayed when complete</p> <p>Interactive controls: - Button: Next Step (advances one row operation) - Button: Auto-solve (animates entire solution) - Slider: Animation speed - Button: Reset - Dropdown: Example system (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show explanatory text for each step - Checkbox: Enable back substitution phase</p> <p>Default parameters: - Matrix size: 3\u00d73 - Animation speed: 1 second per step - Show explanations: true</p> <p>Behavior: - Each step highlights the relevant rows and pivot - Explanation text describes the operation and purpose - Algorithm phases clearly labeled (Forward elimination, Back substitution) - Solution verified by substitution at end - Can step forward/backward through operations</p> <p>Implementation: p5.js with state machine for algorithm steps</p>"},{"location":"chapters/03-systems-of-linear-equations/#reduced-row-echelon-form","title":"Reduced Row Echelon Form","text":"<p>While row echelon form allows back substitution, reduced row echelon form (RREF) goes further to directly reveal solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#definition-and-properties","title":"Definition and Properties","text":"<p>A matrix is in reduced row echelon form if:</p> <ul> <li>It is in row echelon form</li> <li>Each pivot is 1</li> <li>Each pivot is the only nonzero entry in its column</li> </ul> <p>The Gauss-Jordan elimination algorithm extends Gaussian elimination to produce RREF by:</p> <ol> <li>Scaling each pivot row to make the pivot equal to 1</li> <li>Eliminating entries above each pivot (not just below)</li> </ol> <p>Example transformation to RREF:</p> <p>\\(\\begin{bmatrix} 2 &amp; 3 &amp; -1 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; 4 &amp; | &amp; -2 \\\\ 0 &amp; 0 &amp; 3 &amp; | &amp; 6 \\end{bmatrix} \\rightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; | &amp; 18.5 \\\\ 0 &amp; 1 &amp; 0 &amp; | &amp; -10 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>In RREF, the solution is immediately visible: \\(x = 18.5\\), \\(y = -10\\), \\(z = 2\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#comparing-ref-and-rref","title":"Comparing REF and RREF","text":"Property Row Echelon Form Reduced Row Echelon Form Zero rows At bottom At bottom Leading entries Nonzero (any value) Exactly 1 Below pivots All zeros All zeros Above pivots Any value All zeros Solution method Back substitution Direct reading Uniqueness Not unique Unique <p>Computational Efficiency</p> <p>For solving a single system, stopping at REF and using back substitution is often more efficient. RREF requires additional operations. However, RREF is valuable for understanding solution structure and for systems requiring multiple right-hand sides.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-ref-vs-rref-comparison","title":"Diagram: REF vs RREF Comparison","text":"REF vs RREF Side-by-Side Comparison <p>Type: infographic</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Help students understand the difference between row echelon form and reduced row echelon form, and when each is preferable.</p> <p>Layout: Side-by-side matrix displays with transformation arrows</p> <p>Visual elements: - Left panel: Original matrix - Center-left: Row echelon form with pivots highlighted - Center-right: Reduced row echelon form with pivots highlighted - Annotations showing key differences - Color coding: pivots (gold), zeros created (green), coefficients (blue)</p> <p>Interactive features: - Button: Generate new random system - Dropdown: Matrix size (2\u00d72, 3\u00d73, 4\u00d74) - Toggle: Show step count for each form - Toggle: Show solution extraction method - Hover: Show definition of each form</p> <p>Information displayed: - Number of operations to reach each form - Solution method for each (back substitution vs direct) - Uniqueness property highlighted</p> <p>Color scheme: - Pivot positions: gold - Created zeros: light green - Original nonzero entries: blue - Structure indicators: gray outlines</p> <p>Implementation: HTML/CSS/JavaScript with matrix computation</p>"},{"location":"chapters/03-systems-of-linear-equations/#solution-analysis","title":"Solution Analysis","text":"<p>Not every system of linear equations has a solution, and some have infinitely many. Understanding solution existence and uniqueness is as important as computing solutions.</p>"},{"location":"chapters/03-systems-of-linear-equations/#types-of-solution-sets","title":"Types of Solution Sets","text":"<p>The solution set of a system is the collection of all vectors \\(\\mathbf{x}\\) satisfying \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). Three possibilities exist:</p> <ol> <li>Unique Solution: Exactly one solution exists</li> <li>Infinite Solutions: Infinitely many solutions exist (forming a line, plane, or higher-dimensional subspace)</li> <li>No Solution: No solution exists (the system is inconsistent)</li> </ol> <p>The row echelon form reveals which case applies:</p> <ul> <li>No solution: A row of the form \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) where \\(c \\neq 0\\) (equation \\(0 = c\\))</li> <li>Unique solution: Every column is a pivot column (as many pivots as variables)</li> <li>Infinite solutions: Some columns are not pivot columns (fewer pivots than variables)</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#basic-and-free-variables","title":"Basic and Free Variables","text":"<p>In systems with infinitely many solutions, variables split into two types:</p> <ul> <li>Basic variables correspond to pivot columns\u2014they can be expressed in terms of other variables</li> <li>Free variables correspond to non-pivot columns\u2014they can take any value</li> </ul> <p>The number of free variables determines the \"dimension\" of the solution set:</p> Free Variables Solution Set 0 Unique point 1 Line 2 Plane \\(k\\) \\(k\\)-dimensional subspace <p>Example with a free variable:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>Here \\(x_1\\) and \\(x_3\\) are basic variables (columns 1 and 3 are pivot columns), while \\(x_2\\) is free. The general solution is:</p> <p>\\(x_1 = 4 - 2x_2 - 3(2) = -2 - 2x_2\\)</p> <p>\\(x_2 = \\text{free}\\)</p> <p>\\(x_3 = 2\\)</p> <p>Or in vector form: \\(\\mathbf{x} = \\begin{bmatrix} -2 \\\\ 0 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\end{bmatrix}\\)</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-solution-set-visualizer","title":"Diagram: Solution Set Visualizer","text":"Solution Set Type Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: classify, examine</p> <p>Learning Objective: Enable students to explore how different systems produce unique solutions, infinite solutions (lines/planes), or no solution, and to identify the determining factors.</p> <p>Canvas layout: - Left: Augmented matrix (editable or preset) - Center: 2D/3D geometric visualization - Right: Solution analysis panel</p> <p>Visual elements: - Matrix with pivot positions marked - Geometric view showing lines/planes - Solution point, line, or empty set visualized - Pivot columns highlighted - Free variable columns indicated - Parametric solution displayed (for infinite solutions)</p> <p>Interactive controls: - Dropdown: Preset examples (unique, infinite, none) - Editable matrix cells (for custom exploration) - Toggle: 2D / 3D visualization - Checkbox: Show row echelon form - Checkbox: Show reduced row echelon form - Button: Random consistent system - Button: Random inconsistent system</p> <p>Default parameters: - Mode: 2D - Example: unique solution case</p> <p>Behavior: - Matrix edits update visualization in real-time - Inconsistent systems show parallel lines/planes (no intersection) - Infinite solutions show line or plane of solutions - Solution type automatically detected and labeled - Free variables identified and highlighted</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#homogeneous-systems","title":"Homogeneous Systems","text":"<p>A homogeneous system has all zero constants on the right-hand side:</p> <p>\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)</p> <p>Homogeneous systems have special properties that make them particularly important in linear algebra.</p>"},{"location":"chapters/03-systems-of-linear-equations/#the-trivial-solution","title":"The Trivial Solution","text":"<p>Every homogeneous system has at least one solution: \\(\\mathbf{x} = \\mathbf{0}\\). This is called the trivial solution because it's obvious\u2014all zeros satisfy any homogeneous equation.</p> <p>The interesting question is whether nontrivial solutions exist.</p>"},{"location":"chapters/03-systems-of-linear-equations/#existence-of-nontrivial-solutions","title":"Existence of Nontrivial Solutions","text":"<p>A homogeneous system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\) has nontrivial solutions if and only if it has free variables. This happens when:</p> <ul> <li>The number of variables exceeds the number of pivot columns</li> <li>Equivalently, the number of variables exceeds the rank of \\(\\mathbf{A}\\)</li> </ul> <p>Important consequence: If a homogeneous system has more variables than equations, it always has nontrivial solutions.</p> Variables vs Equations Nontrivial Solutions? Variables &gt; Equations Always Variables = Equations Maybe (depends on matrix) Variables &lt; Equations Maybe (depends on matrix)"},{"location":"chapters/03-systems-of-linear-equations/#solution-space-structure","title":"Solution Space Structure","text":"<p>The solution set of a homogeneous system forms a subspace called the null space of \\(\\mathbf{A}\\). This subspace:</p> <ul> <li>Contains the zero vector</li> <li>Is closed under addition (sum of solutions is a solution)</li> <li>Is closed under scalar multiplication (scalar times solution is a solution)</li> </ul> <p>This structure is fundamental to understanding linear transformations and will be explored further in later chapters.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-homogeneous-system-explorer","title":"Diagram: Homogeneous System Explorer","text":"Homogeneous System Solution Space Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that homogeneous systems always have the trivial solution and may have additional solutions forming a subspace through the origin.</p> <p>Canvas layout: - Left: Coefficient matrix A (editable) - Center: 2D/3D visualization of solution space - Right: Solution analysis</p> <p>Visual elements: - Matrix display with rank calculation - Coordinate axes through origin - Trivial solution (origin) always marked - Nontrivial solution space (line or plane through origin) when present - Basis vectors for solution space shown as arrows - Null space dimension displayed</p> <p>Interactive controls: - Matrix size selector (2\u00d72, 2\u00d73, 3\u00d73, 3\u00d74) - Editable matrix entries - Button: Generate random full-rank matrix (only trivial solution) - Button: Generate random rank-deficient matrix (nontrivial solutions) - Toggle: Show basis vectors for null space - Slider: Rotate 3D view</p> <p>Default parameters: - Size: 3\u00d73 - Initial matrix: rank-deficient (has null space)</p> <p>Behavior: - Real-time rank calculation - Solution space updates as matrix changes - Clear indication of trivial-only vs nontrivial solutions - Null space dimension shown (n - rank) - 3D rotation for spatial understanding</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/03-systems-of-linear-equations/#numerical-stability","title":"Numerical Stability","text":"<p>In practice, solving linear systems on computers introduces challenges beyond the pure mathematics.</p>"},{"location":"chapters/03-systems-of-linear-equations/#sources-of-numerical-error","title":"Sources of Numerical Error","text":"<p>Numerical stability refers to how errors propagate through a computation. In linear systems, instability can arise from:</p> <ul> <li>Floating-point representation: Real numbers are stored with limited precision</li> <li>Round-off errors: Each arithmetic operation may introduce small errors</li> <li>Ill-conditioned systems: Some systems amplify small input errors into large output errors</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#partial-pivoting","title":"Partial Pivoting","text":"<p>Standard Gaussian elimination can suffer from numerical instability when small pivots lead to division by near-zero values. Partial pivoting addresses this by selecting the largest available pivot:</p> <ol> <li>Before eliminating in a column, scan downward for the entry with largest absolute value</li> <li>Swap rows to bring this entry to the pivot position</li> <li>Proceed with elimination</li> </ol> <p>This strategy prevents division by small numbers and improves numerical stability.</p>"},{"location":"chapters/03-systems-of-linear-equations/#condition-number","title":"Condition Number","text":"<p>The condition number of a matrix quantifies its sensitivity to perturbations. A high condition number indicates an ill-conditioned system where small changes in input cause large changes in output.</p> <ul> <li>Condition number \u2248 1: Well-conditioned (stable)</li> <li>Condition number large (e.g., \\(10^6\\)): Ill-conditioned (unstable)</li> <li>Condition number = \u221e: Singular matrix (no unique solution)</li> </ul> <p>Practical Implications</p> <p>When working with real data, always consider numerical stability. Libraries like NumPy use sophisticated algorithms (LU decomposition with partial pivoting) that are more stable than naive Gaussian elimination. Never compute \\(\\mathbf{A}^{-1}\\mathbf{b}\\) explicitly\u2014use specialized solvers instead.</p>"},{"location":"chapters/03-systems-of-linear-equations/#diagram-numerical-stability-demonstration","title":"Diagram: Numerical Stability Demonstration","text":"Numerical Stability and Condition Number Explorer <p>Type: microsim</p> <p>Bloom Level: Evaluate (L5) Bloom Verb: assess, judge</p> <p>Learning Objective: Demonstrate how small changes in matrix entries can cause large changes in solutions for ill-conditioned systems, and how partial pivoting improves stability.</p> <p>Canvas layout: - Left: Original system and computed solution - Center: Perturbed system and new solution - Right: Analysis (condition number, error magnification)</p> <p>Visual elements: - Two matrices side-by-side (original and perturbed) - Solutions displayed with precision indicators - Error visualization (bar chart showing input vs output error) - Condition number prominently displayed - Color coding: green (well-conditioned) to red (ill-conditioned) - 2D geometric view showing how solution moves</p> <p>Interactive controls: - Dropdown: Example type (well-conditioned, moderately ill-conditioned, severely ill-conditioned) - Slider: Perturbation magnitude (0.0001 to 0.1) - Toggle: Use partial pivoting - Button: Apply random perturbation - Checkbox: Show geometric interpretation - Display: Precision (decimal places shown)</p> <p>Default parameters: - Example: Hilbert matrix 3\u00d73 (ill-conditioned) - Perturbation: 0.001 - Partial pivoting: off</p> <p>Behavior: - Perturbation applied to random entries - Solution error calculated and displayed - Error magnification factor shown (output error / input error) - Comparison with/without partial pivoting - Geometric view shows solution point movement</p> <p>Implementation: p5.js with high-precision arithmetic library</p>"},{"location":"chapters/03-systems-of-linear-equations/#applications","title":"Applications","text":""},{"location":"chapters/03-systems-of-linear-equations/#balancing-chemical-equations","title":"Balancing Chemical Equations","text":"<p>Chemical equations must balance atoms. For the reaction:</p> <p>\\(a \\text{CH}_4 + b \\text{O}_2 \\rightarrow c \\text{CO}_2 + d \\text{H}_2\\text{O}\\)</p> <p>Balancing each element gives a linear system:</p> <ul> <li>Carbon: \\(a = c\\)</li> <li>Hydrogen: \\(4a = 2d\\)</li> <li>Oxygen: \\(2b = 2c + d\\)</li> </ul> <p>This homogeneous system has a one-dimensional solution space. Setting \\(a = 1\\) gives the balanced equation: \\(\\text{CH}_4 + 2\\text{O}_2 \\rightarrow \\text{CO}_2 + 2\\text{H}_2\\text{O}\\).</p>"},{"location":"chapters/03-systems-of-linear-equations/#network-flow-analysis","title":"Network Flow Analysis","text":"<p>In electrical circuits or traffic networks, conservation laws produce linear systems. At each node, inflow equals outflow. The resulting system determines currents or traffic flows throughout the network.</p>"},{"location":"chapters/03-systems-of-linear-equations/#machine-learning-linear-regression","title":"Machine Learning: Linear Regression","text":"<p>Fitting a linear model \\(\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}\\) to data leads to the normal equations:</p> <p>\\(\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}\\)</p> <p>Solving this system gives the least-squares coefficients \\(\\boldsymbol{\\beta}\\) that minimize prediction error.</p>"},{"location":"chapters/03-systems-of-linear-equations/#neural-network-training","title":"Neural Network Training","text":"<p>Training neural networks involves solving systems of equations (approximately) at each optimization step. The gradient computations that guide learning rely on the same matrix operations covered in this chapter.</p>"},{"location":"chapters/03-systems-of-linear-equations/#computational-implementation","title":"Computational Implementation","text":""},{"location":"chapters/03-systems-of-linear-equations/#numpy-example","title":"NumPy Example","text":"<pre><code>import numpy as np\n\n# Define the system Ax = b\nA = np.array([[2, 3, -1],\n              [4, 4, -3],\n              [1, -1, 2]])\nb = np.array([5, 3, 1])\n\n# Solve using NumPy (LU decomposition with pivoting)\nx = np.linalg.solve(A, b)\nprint(f\"Solution: {x}\")\n\n# Verify: check that Ax = b\nprint(f\"Verification (Ax): {A @ x}\")\nprint(f\"Condition number: {np.linalg.cond(A):.2f}\")\n</code></pre>"},{"location":"chapters/03-systems-of-linear-equations/#key-functions","title":"Key Functions","text":"Function Purpose <code>np.linalg.solve(A, b)</code> Solve \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) <code>np.linalg.matrix_rank(A)</code> Compute rank <code>np.linalg.cond(A)</code> Compute condition number <code>scipy.linalg.lu(A)</code> LU decomposition <code>np.linalg.lstsq(A, b)</code> Least squares solution"},{"location":"chapters/03-systems-of-linear-equations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter developed the theory and practice of solving systems of linear equations.</p> <p>Formulation:</p> <ul> <li>A linear equation has variables appearing to the first power only</li> <li>A system of equations requires simultaneous satisfaction of multiple equations</li> <li>Matrix equation form \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) compactly represents the system</li> <li>The augmented matrix \\([\\mathbf{A} | \\mathbf{b}]\\) combines coefficients and constants</li> </ul> <p>Solution Methods:</p> <ul> <li>Row operations (swap, scale, add) transform systems while preserving solutions</li> <li>Gaussian elimination reduces to row echelon form</li> <li>Back substitution solves from bottom to top</li> <li>Reduced row echelon form allows direct solution reading</li> </ul> <p>Solution Analysis:</p> <ul> <li>Pivot positions and pivot columns determine solution structure</li> <li>Unique solution: all columns are pivot columns</li> <li>Infinite solutions: some columns are free (non-pivot)</li> <li>No solution: inconsistent row \\([0 \\; 0 \\; \\cdots \\; 0 \\; | \\; c]\\) with \\(c \\neq 0\\)</li> <li>Free variables can take any value; basic variables are determined by them</li> </ul> <p>Special Systems:</p> <ul> <li>Homogeneous systems (\\(\\mathbf{A}\\mathbf{x} = \\mathbf{0}\\)) always have the trivial solution</li> <li>Nontrivial solutions exist when there are free variables</li> <li>The solution set of a homogeneous system is a subspace</li> </ul> <p>Computational Considerations:</p> <ul> <li>Numerical stability matters for practical computation</li> <li>Partial pivoting improves stability</li> <li>Condition number measures sensitivity to perturbation</li> <li>Use library functions rather than explicit inverse computation</li> </ul>"},{"location":"chapters/03-systems-of-linear-equations/#exercises","title":"Exercises","text":"Exercise 1: Row Echelon Form <p>Reduce the following augmented matrix to row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 2 &amp; -1 &amp; | &amp; 3 \\\\ 2 &amp; 5 &amp; 1 &amp; | &amp; 8 \\\\ 3 &amp; 7 &amp; 0 &amp; | &amp; 11 \\end{bmatrix}\\)</p> <p>Then use back substitution to find the solution.</p> Exercise 2: Solution Type Identification <p>For each augmented matrix in row echelon form, determine whether the system has a unique solution, infinite solutions, or no solution:</p> <p>a) \\(\\begin{bmatrix} 1 &amp; 3 &amp; | &amp; 5 \\\\ 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>b) \\(\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; | &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; | &amp; 2 \\end{bmatrix}\\)</p> <p>c) \\(\\begin{bmatrix} 1 &amp; 2 &amp; | &amp; 3 \\\\ 0 &amp; 0 &amp; | &amp; 5 \\end{bmatrix}\\)</p> Exercise 3: Free and Basic Variables <p>Given the reduced row echelon form:</p> <p>\\(\\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; | &amp; 3 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; | &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; | &amp; 4 \\end{bmatrix}\\)</p> <p>Identify the pivot columns, free variables, and basic variables. Write the general solution in parametric form.</p> Exercise 4: Homogeneous System <p>Consider the homogeneous system with coefficient matrix:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix}\\)</p> <p>Determine whether nontrivial solutions exist. If so, find a basis for the solution space.</p> Exercise 5: Numerical Stability <p>The Hilbert matrix \\(H_n\\) has entries \\(h_{ij} = \\frac{1}{i+j-1}\\). For \\(n = 3\\):</p> <p>\\(H_3 = \\begin{bmatrix} 1 &amp; 1/2 &amp; 1/3 \\\\ 1/2 &amp; 1/3 &amp; 1/4 \\\\ 1/3 &amp; 1/4 &amp; 1/5 \\end{bmatrix}\\)</p> <p>Compute the condition number of \\(H_3\\). What does this tell you about solving systems with this coefficient matrix?</p>"},{"location":"chapters/04-linear-transformations/","title":"Linear Transformations","text":""},{"location":"chapters/04-linear-transformations/#summary","title":"Summary","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition. You will learn about rotation, scaling, shearing, reflection, and projection transformations, and understand abstract concepts like kernel, range, and change of basis. These ideas are fundamental to computer graphics, robotics, and understanding how neural networks transform data.</p>"},{"location":"chapters/04-linear-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 27 concepts from the learning graph:</p> <ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"chapters/04-linear-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> </ul>"},{"location":"chapters/04-linear-transformations/#introduction","title":"Introduction","text":"<p>Every matrix multiplication \\(\\mathbf{A}\\mathbf{x}\\) can be viewed as a transformation\u2014taking an input vector \\(\\mathbf{x}\\) and producing an output vector \\(\\mathbf{y}\\). This perspective transforms matrices from static arrays of numbers into dynamic operators that rotate, scale, shear, project, and transform geometric objects.</p> <p>Understanding transformations is essential for computer graphics, where every rotation, scaling, and perspective projection is a matrix multiplication. It's equally crucial for machine learning, where neural networks apply layer after layer of linear transformations (with nonlinearities between them) to map inputs to outputs. This chapter develops the mathematical framework for understanding these transformations and their properties.</p>"},{"location":"chapters/04-linear-transformations/#functions-and-transformations","title":"Functions and Transformations","text":""},{"location":"chapters/04-linear-transformations/#functions-between-vector-spaces","title":"Functions Between Vector Spaces","text":"<p>A function \\(T\\) from a set \\(V\\) to a set \\(W\\), written \\(T: V \\rightarrow W\\), is a rule that assigns to each element \\(\\mathbf{v}\\) in \\(V\\) exactly one element \\(T(\\mathbf{v})\\) in \\(W\\).</p> <p>Key terminology:</p> <ul> <li>The domain of \\(T\\) is the set \\(V\\) of all possible inputs</li> <li>The codomain of \\(T\\) is the set \\(W\\) of all possible outputs</li> <li>The image of a vector \\(\\mathbf{v}\\) is its output \\(T(\\mathbf{v})\\)</li> <li>The range (or image of \\(T\\)) is the set of all outputs: \\(\\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</li> </ul> <p>Range vs Codomain</p> <p>The codomain is the set where outputs could live. The range is where outputs actually live. For example, \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\) has codomain \\(\\mathbb{R}^3\\), but its range might be a plane within \\(\\mathbb{R}^3\\).</p>"},{"location":"chapters/04-linear-transformations/#linear-transformations_1","title":"Linear Transformations","text":"<p>A linear transformation (or linear map) is a function \\(T: V \\rightarrow W\\) between vector spaces that preserves vector addition and scalar multiplication:</p>"},{"location":"chapters/04-linear-transformations/#linearity-conditions","title":"Linearity Conditions","text":"<ol> <li> <p>\\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\) for all \\(\\mathbf{u}, \\mathbf{v} \\in V\\)</p> </li> <li> <p>\\(T(c\\mathbf{v}) = cT(\\mathbf{v})\\) for all \\(\\mathbf{v} \\in V\\) and scalars \\(c\\)</p> </li> </ol> <p>These two conditions can be combined into one:</p> <p>\\(T(c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2) = c_1 T(\\mathbf{v}_1) + c_2 T(\\mathbf{v}_2)\\)</p> <p>Linear transformations preserve linear combinations. They map lines to lines (or points), planes to planes (or lines or points), and the origin to the origin.</p> Property Linear Non-Linear Origin maps to origin Always Not necessarily Lines map to... Lines or points Curves possible Parallelism preserved Yes No Grid structure Preserved Distorted"},{"location":"chapters/04-linear-transformations/#the-transformation-matrix","title":"The Transformation Matrix","text":"<p>Every linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) can be represented by an \\(m \\times n\\) matrix \\(\\mathbf{A}\\). The transformation is then:</p> <p>\\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\)</p> <p>To find the transformation matrix, apply \\(T\\) to each standard basis vector:</p> <p>\\(\\mathbf{A} = \\begin{bmatrix} T(\\mathbf{e}_1) &amp; T(\\mathbf{e}_2) &amp; \\cdots &amp; T(\\mathbf{e}_n) \\end{bmatrix}\\)</p> <p>The columns of \\(\\mathbf{A}\\) are the images of the standard basis vectors. This fundamental observation connects abstract transformations to concrete matrix computations.</p>"},{"location":"chapters/04-linear-transformations/#diagram-linear-transformation-visualizer","title":"Diagram: Linear Transformation Visualizer","text":"Linear Transformation Fundamentals Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that linear transformations preserve the grid structure and that the transformation is completely determined by where basis vectors map.</p> <p>Canvas layout: - Left: Original coordinate plane with basis vectors and grid - Right: Transformed plane showing result of applying T - Bottom: Matrix display and controls</p> <p>Visual elements: - Standard basis vectors e\u2081 (red) and e\u2082 (blue) in original space - Transformed basis vectors T(e\u2081) and T(e\u2082) in target space - Grid of points in original space - Corresponding grid in transformed space - Sample vector (user-controlled) showing before/after - 2\u00d72 transformation matrix displayed</p> <p>Interactive controls: - Draggable endpoints for T(e\u2081) and T(e\u2082) to define transformation - Slider: Animation between original and transformed states - Button: Reset to identity - Dropdown: Preset transformations (rotation, scaling, shear, reflection) - Checkbox: Show grid lines - Checkbox: Show sample vector path</p> <p>Default parameters: - Initial transformation: identity - Grid: 5\u00d75 - Sample vector: (1, 1)</p> <p>Behavior: - Dragging basis vector endpoints updates matrix in real-time - Grid morphs smoothly during animation - Matrix entries update as endpoints move - Preset dropdown smoothly transitions to new transformation - Sample vector shows how arbitrary points transform</p> <p>Implementation: p5.js with smooth animation</p>"},{"location":"chapters/04-linear-transformations/#geometric-transformations-in-2d","title":"Geometric Transformations in 2D","text":"<p>The power of linear transformations becomes vivid when we visualize their geometric effects. Each type of transformation has a characteristic matrix structure.</p>"},{"location":"chapters/04-linear-transformations/#rotation-matrix","title":"Rotation Matrix","text":"<p>A rotation matrix rotates vectors around the origin by a fixed angle. For 2D rotation by angle \\(\\theta\\) counterclockwise:</p>"},{"location":"chapters/04-linear-transformations/#2d-rotation-matrix","title":"2D Rotation Matrix","text":"<p>\\(\\mathbf{R}(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(\\theta\\) is the rotation angle (positive = counterclockwise)</li> <li>The columns are the rotated basis vectors</li> </ul> <p>Properties of 2D rotation matrices:</p> <ul> <li>\\(\\mathbf{R}(\\theta)^T = \\mathbf{R}(-\\theta) = \\mathbf{R}(\\theta)^{-1}\\) (orthogonal matrix)</li> <li>\\(\\det(\\mathbf{R}(\\theta)) = 1\\) (preserves area and orientation)</li> <li>\\(\\mathbf{R}(\\alpha)\\mathbf{R}(\\beta) = \\mathbf{R}(\\alpha + \\beta)\\) (rotations compose by adding angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-2d-rotation-interactive","title":"Diagram: 2D Rotation Interactive","text":"2D Rotation Matrix Visualizer <p>Type: microsim</p> <p>Bloom Level: Apply (L3) Bloom Verb: demonstrate, calculate</p> <p>Learning Objective: Enable students to see how the rotation matrix transforms vectors and shapes, and verify the relationship between angle and matrix entries.</p> <p>Canvas layout: - Main area: Coordinate plane with original and rotated shapes - Right panel: Matrix display with cos/sin values - Bottom: Angle control</p> <p>Visual elements: - Unit circle for reference - Original shape (arrow, square, or F-shape) in blue - Rotated shape in red (semi-transparent) - Angle arc showing rotation amount - Basis vectors before and after rotation - Matrix entries updating with angle</p> <p>Interactive controls: - Slider: Rotation angle \u03b8 (-360\u00b0 to 360\u00b0) - Dropdown: Shape to rotate (arrow, square, triangle, F-shape) - Checkbox: Show unit circle - Checkbox: Show angle arc - Button: Animate full rotation - Input: Enter specific angle in degrees</p> <p>Default parameters: - Angle: 45\u00b0 - Shape: F-shape (to show orientation) - Unit circle: visible</p> <p>Behavior: - Shape rotates smoothly as angle slider moves - Matrix entries display cos(\u03b8) and sin(\u03b8) values - Animation shows continuous rotation - F-shape clearly shows orientation preservation</p> <p>Implementation: p5.js with trigonometric calculations</p>"},{"location":"chapters/04-linear-transformations/#3d-rotation","title":"3D Rotation","text":"<p>3D rotation is more complex because we must specify an axis of rotation. The three fundamental rotation matrices rotate around the coordinate axes:</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-x-axis","title":"Rotation Around X-Axis","text":"<p>\\(\\mathbf{R}_x(\\theta) = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\theta &amp; -\\sin\\theta \\\\ 0 &amp; \\sin\\theta &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-y-axis","title":"Rotation Around Y-Axis","text":"<p>\\(\\mathbf{R}_y(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; 0 &amp; \\sin\\theta \\\\ 0 &amp; 1 &amp; 0 \\\\ -\\sin\\theta &amp; 0 &amp; \\cos\\theta \\end{bmatrix}\\)</p>"},{"location":"chapters/04-linear-transformations/#rotation-around-z-axis","title":"Rotation Around Z-Axis","text":"<p>\\(\\mathbf{R}_z(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\)</p> <p>General 3D rotations can be composed from these basic rotations, though the order matters (rotations don't commute in 3D).</p> <p>Gimbal Lock</p> <p>When composing rotations using Euler angles, certain orientations cause \"gimbal lock\" where a degree of freedom is lost. Quaternions provide an alternative representation that avoids this problem, used extensively in robotics and game development.</p>"},{"location":"chapters/04-linear-transformations/#scaling-matrix","title":"Scaling Matrix","text":"<p>A scaling matrix stretches or compresses vectors along the coordinate axes.</p> <p>Uniform scaling scales equally in all directions:</p> <p>\\(\\mathbf{S}_{\\text{uniform}}(k) = \\begin{bmatrix} k &amp; 0 \\\\ 0 &amp; k \\end{bmatrix}\\)</p> <p>Non-uniform scaling scales differently along each axis:</p> <p>\\(\\mathbf{S}(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}\\)</p> <p>where:</p> <ul> <li>\\(s_x\\) is the scaling factor along the x-axis</li> <li>\\(s_y\\) is the scaling factor along the y-axis</li> <li>\\(|s| &gt; 1\\) stretches; \\(|s| &lt; 1\\) compresses</li> <li>\\(s &lt; 0\\) also reflects</li> </ul> Scaling Type Effect Determinant Uniform \\(k &gt; 1\\) Enlarges \\(k^2\\) Uniform \\(0 &lt; k &lt; 1\\) Shrinks \\(k^2\\) Non-uniform Stretches/compresses differently \\(s_x \\cdot s_y\\) Negative factor Also reflects Negative"},{"location":"chapters/04-linear-transformations/#shear-matrix","title":"Shear Matrix","text":"<p>A shear matrix skews shapes by shifting points parallel to an axis, proportional to their distance from that axis.</p> <p>Horizontal shear (shifts x based on y):</p> <p>\\(\\mathbf{H}_x(k) = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Vertical shear (shifts y based on x):</p> <p>\\(\\mathbf{H}_y(k) = \\begin{bmatrix} 1 &amp; 0 \\\\ k &amp; 1 \\end{bmatrix}\\)</p> <p>Shear transformations:</p> <ul> <li>Turn rectangles into parallelograms</li> <li>Preserve area (\\(\\det = 1\\))</li> <li>Are not orthogonal (don't preserve angles)</li> </ul>"},{"location":"chapters/04-linear-transformations/#reflection-matrix","title":"Reflection Matrix","text":"<p>A reflection matrix mirrors points across a line (in 2D) or plane (in 3D).</p> <p>Reflection across the x-axis:</p> <p>\\(\\mathbf{F}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}\\)</p> <p>Reflection across the y-axis:</p> <p>\\(\\mathbf{F}_y = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\)</p> <p>Reflection across a line through the origin at angle \\(\\theta\\):</p> <p>\\(\\mathbf{F}(\\theta) = \\begin{bmatrix} \\cos 2\\theta &amp; \\sin 2\\theta \\\\ \\sin 2\\theta &amp; -\\cos 2\\theta \\end{bmatrix}\\)</p> <p>Reflections have determinant \\(-1\\), indicating they reverse orientation (turning clockwise into counterclockwise).</p>"},{"location":"chapters/04-linear-transformations/#diagram-geometric-transformations-gallery","title":"Diagram: Geometric Transformations Gallery","text":"Geometric Transformations Interactive Gallery <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: compare, differentiate</p> <p>Learning Objective: Enable students to compare and contrast different geometric transformations, understanding their visual effects and matrix structures.</p> <p>Canvas layout: - Main area: Split view showing original and transformed shapes - Right panel: Transformation type selector and parameters - Bottom: Matrix display</p> <p>Visual elements: - Original shape (configurable) in blue - Transformed shape in red - Grid showing deformation - Transformation type label - Matrix with current values - Key properties (determinant, orthogonality)</p> <p>Interactive controls: - Tabs: Rotation | Scaling | Shear | Reflection - For Rotation: Angle slider - For Scaling: sx and sy sliders (or single k for uniform) - For Shear: k slider, direction toggle (horizontal/vertical) - For Reflection: Angle slider for reflection line - Dropdown: Shape (square, circle of points, F-shape, arrow) - Toggle: Show grid deformation - Button: Animate transformation</p> <p>Default parameters: - Transformation: Rotation - Shape: F-shape - Show grid: true</p> <p>Behavior: - Smooth animation between original and transformed states - Matrix updates in real-time with parameter changes - Properties panel shows det, orthogonality, etc. - Grid clearly shows how space is warped - Comparisons possible by switching between tabs</p> <p>Implementation: p5.js with tabbed interface</p>"},{"location":"chapters/04-linear-transformations/#projection","title":"Projection","text":"<p>A projection maps vectors onto a subspace (line, plane, etc.). Unlike the transformations above, projections typically reduce dimension and are not invertible.</p>"},{"location":"chapters/04-linear-transformations/#orthogonal-projection","title":"Orthogonal Projection","text":"<p>An orthogonal projection projects vectors perpendicularly onto a subspace. For projection onto a line through the origin with direction \\(\\mathbf{u}\\):</p>"},{"location":"chapters/04-linear-transformations/#projection-onto-a-line","title":"Projection onto a Line","text":"<p>\\(\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\mathbf{u} \\cdot \\mathbf{u}} \\mathbf{u}\\)</p> <p>The projection matrix onto the line spanned by unit vector \\(\\hat{\\mathbf{u}}\\) is:</p> <p>\\(\\mathbf{P} = \\hat{\\mathbf{u}} \\hat{\\mathbf{u}}^T\\)</p> <p>For projection onto the x-axis:</p> <p>\\(\\mathbf{P}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}\\)</p> <p>Properties of orthogonal projection matrices:</p> <ul> <li>\\(\\mathbf{P}^2 = \\mathbf{P}\\) (applying twice gives same result)</li> <li>\\(\\mathbf{P}^T = \\mathbf{P}\\) (symmetric)</li> <li>Eigenvalues are 0 and 1</li> <li>Not invertible (determinant = 0)</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-orthogonal-projection-visualizer","title":"Diagram: Orthogonal Projection Visualizer","text":"Orthogonal Projection Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students visualize how orthogonal projection maps vectors onto lines or planes, showing the perpendicular relationship between the original vector, its projection, and the error component.</p> <p>Canvas layout: - Main area: 2D/3D coordinate space - Right panel: Vector components and projection formula</p> <p>Visual elements: - Original vector v (blue arrow) - Projection line/plane (gray) - Projected vector proj(v) (red arrow on line) - Error vector (v - proj(v)) shown as dashed green arrow - Right angle indicator showing orthogonality - Unit direction vector u</p> <p>Interactive controls: - Draggable vector v endpoint - Slider: Direction of projection line (angle) - Toggle: 2D / 3D mode - Checkbox: Show error vector - Checkbox: Show right angle indicator - Checkbox: Show projection formula with values - Button: Animate projection process</p> <p>Default parameters: - Mode: 2D - Projection line: 30\u00b0 from x-axis - Vector v: (3, 2)</p> <p>Behavior: - Vector v can be dragged to any position - Projection updates in real-time - Error vector clearly perpendicular to projection line - Formula shows computed values - 3D mode allows projection onto plane</p> <p>Implementation: p5.js with WEBGL for 3D</p>"},{"location":"chapters/04-linear-transformations/#composition-of-transformations","title":"Composition of Transformations","text":"<p>Applying one transformation after another is called composition. If \\(S\\) and \\(T\\) are linear transformations, the composition \\(S \\circ T\\) applies \\(T\\) first, then \\(S\\):</p> <p>\\((S \\circ T)(\\mathbf{x}) = S(T(\\mathbf{x}))\\)</p> <p>For matrix representations, composition corresponds to matrix multiplication:</p> <p>\\(\\mathbf{A}_{S \\circ T} = \\mathbf{A}_S \\mathbf{A}_T\\)</p> <p>Order Matters</p> <p>Matrix multiplication is not commutative, so \\(\\mathbf{A}_S \\mathbf{A}_T \\neq \\mathbf{A}_T \\mathbf{A}_S\\) in general. Rotating then scaling gives different results than scaling then rotating.</p> <p>Common composition examples:</p> <ul> <li>Rotation around a point: Translate to origin, rotate, translate back</li> <li>Scaling about a point: Translate to origin, scale, translate back</li> <li>Euler angles: Compose three axis-aligned rotations</li> </ul> Composition Matrix Product Application Rotate then scale \\(\\mathbf{S}\\mathbf{R}\\) Graphics, robotics Scale then rotate \\(\\mathbf{R}\\mathbf{S}\\) Different result! Multiple rotations \\(\\mathbf{R}_3 \\mathbf{R}_2 \\mathbf{R}_1\\) 3D orientation"},{"location":"chapters/04-linear-transformations/#diagram-transformation-composition","title":"Diagram: Transformation Composition","text":"Transformation Composition Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: examine, compare</p> <p>Learning Objective: Demonstrate that the order of transformations matters and show how composed transformations combine into a single matrix product.</p> <p>Canvas layout: - Top: Two transformation pipelines side by side (T then S vs S then T) - Bottom: Resulting shapes and combined matrices</p> <p>Visual elements: - Original shape in center - Pipeline 1: Original \u2192 T \u2192 S (with intermediate state shown) - Pipeline 2: Original \u2192 S \u2192 T (with intermediate state shown) - Final shapes for each pipeline (different unless transformations commute) - Matrices for T, S, and their products ST and TS</p> <p>Interactive controls: - Dropdown: First transformation type (rotation, scaling, shear) - Parameters for first transformation - Dropdown: Second transformation type - Parameters for second transformation - Checkbox: Show intermediate states - Button: Animate both pipelines - Toggle: Show matrix products</p> <p>Default parameters: - T: Rotation 45\u00b0 - S: Scaling (2, 1) - Shape: unit square</p> <p>Behavior: - Both pipelines animate simultaneously for comparison - Intermediate shapes visible between transformations - Matrix products computed and displayed - Clear visual demonstration that order matters - Commutative cases (e.g., two rotations) show same result</p> <p>Implementation: p5.js with parallel animation</p>"},{"location":"chapters/04-linear-transformations/#kernel-and-range","title":"Kernel and Range","text":"<p>Every linear transformation has two fundamental subspaces that reveal its structure.</p>"},{"location":"chapters/04-linear-transformations/#kernel-null-space","title":"Kernel (Null Space)","text":"<p>The kernel of a linear transformation \\(T: V \\rightarrow W\\) is the set of all vectors that map to zero:</p> <p>\\(\\ker(T) = \\{\\mathbf{v} \\in V : T(\\mathbf{v}) = \\mathbf{0}\\}\\)</p> <p>For a matrix transformation \\(T(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the kernel equals the null space of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Null}(\\mathbf{A}) = \\{\\mathbf{x} : \\mathbf{A}\\mathbf{x} = \\mathbf{0}\\}\\)</p> <p>The kernel is always a subspace of the domain. Its dimension is called the nullity of \\(T\\).</p>"},{"location":"chapters/04-linear-transformations/#range-column-space","title":"Range (Column Space)","text":"<p>The range of \\(T\\) is the set of all possible outputs:</p> <p>\\(\\text{Range}(T) = \\{T(\\mathbf{v}) : \\mathbf{v} \\in V\\}\\)</p> <p>For a matrix \\(\\mathbf{A}\\), the range equals the column space\u2014the span of the columns of \\(\\mathbf{A}\\):</p> <p>\\(\\text{Col}(\\mathbf{A}) = \\text{Span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\\)</p> <p>The range is always a subspace of the codomain. Its dimension is the rank of \\(T\\).</p> Subspace Definition Dimension Name Kernel / Null Space Vectors mapping to zero Nullity Range / Column Space All possible outputs Rank"},{"location":"chapters/04-linear-transformations/#diagram-kernel-and-range-visualizer","title":"Diagram: Kernel and Range Visualizer","text":"Kernel and Range Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Analyze (L4) Bloom Verb: differentiate, examine</p> <p>Learning Objective: Help students visualize the kernel (what maps to zero) and range (what outputs are possible) of a linear transformation, understanding their relationship to the matrix structure.</p> <p>Canvas layout: - Left: Domain space showing kernel - Right: Codomain space showing range - Center: Transformation arrow - Bottom: Matrix and dimension information</p> <p>Visual elements: - Domain with kernel subspace highlighted (line or plane in gray) - Arrows from domain to codomain - Codomain with range subspace highlighted (line or plane in color) - Zero vector in codomain marked - Vectors in kernel shown collapsing to zero - Sample vectors outside kernel shown mapping to range</p> <p>Interactive controls: - Matrix editor (2\u00d72 or 3\u00d72 or 2\u00d73) - Button: Random full-rank matrix - Button: Random rank-deficient matrix - Checkbox: Show kernel vectors - Checkbox: Show how kernel maps to zero - Checkbox: Animate transformation - Display: Rank and nullity values</p> <p>Default parameters: - Matrix: 2\u00d73 with rank 2 (nontrivial kernel)</p> <p>Behavior: - Kernel automatically computed and displayed - Range computed as column space - Arrows show transformation action - Rank and nullity update with matrix changes - Animation shows vectors transforming</p> <p>Implementation: p5.js with linear algebra computations</p>"},{"location":"chapters/04-linear-transformations/#the-rank-nullity-theorem","title":"The Rank-Nullity Theorem","text":"<p>One of the most important theorems in linear algebra connects the dimensions of the kernel and range.</p>"},{"location":"chapters/04-linear-transformations/#statement-of-the-theorem","title":"Statement of the Theorem","text":"<p>For a linear transformation \\(T: V \\rightarrow W\\) where \\(V\\) is finite-dimensional:</p>"},{"location":"chapters/04-linear-transformations/#rank-nullity-theorem","title":"Rank-Nullity Theorem","text":"<p>\\(\\dim(V) = \\text{rank}(T) + \\text{nullity}(T)\\)</p> <p>where:</p> <ul> <li>\\(\\dim(V)\\) is the dimension of the domain (number of columns of \\(\\mathbf{A}\\))</li> <li>\\(\\text{rank}(T)\\) is the dimension of the range</li> <li>\\(\\text{nullity}(T)\\) is the dimension of the kernel</li> </ul> <p>For an \\(m \\times n\\) matrix \\(\\mathbf{A}\\):</p> <p>\\(n = \\text{rank}(\\mathbf{A}) + \\text{nullity}(\\mathbf{A})\\)</p>"},{"location":"chapters/04-linear-transformations/#intuition","title":"Intuition","text":"<p>The theorem says that dimension is conserved: what doesn't go into the kernel must go somewhere (the range). If more vectors collapse to zero (higher nullity), fewer independent output directions remain (lower rank).</p>"},{"location":"chapters/04-linear-transformations/#consequences","title":"Consequences","text":"<p>The Rank-Nullity Theorem has powerful implications:</p> <ul> <li>If \\(\\text{rank}(\\mathbf{A}) = n\\) (full column rank), then \\(\\text{nullity} = 0\\), so \\(T\\) is injective (one-to-one)</li> <li>If \\(\\text{rank}(\\mathbf{A}) = m\\) (full row rank), then \\(T\\) is surjective (onto)</li> <li>If both, \\(T\\) is bijective (invertible) and \\(m = n\\)</li> </ul> Matrix Size Rank Nullity Properties \\(3 \\times 3\\), rank 3 3 0 Invertible \\(3 \\times 3\\), rank 2 2 1 Kernel is a line \\(3 \\times 4\\), rank 3 3 1 Kernel is a line \\(4 \\times 3\\), rank 3 3 0 One-to-one"},{"location":"chapters/04-linear-transformations/#invertible-transformations","title":"Invertible Transformations","text":"<p>An invertible transformation is a linear transformation with an inverse\u2014a transformation that \"undoes\" the original.</p>"},{"location":"chapters/04-linear-transformations/#conditions-for-invertibility","title":"Conditions for Invertibility","text":"<p>A linear transformation \\(T: V \\rightarrow W\\) is invertible if and only if:</p> <ul> <li>\\(T\\) is one-to-one (injective): different inputs give different outputs</li> <li>\\(T\\) is onto (surjective): every output is achieved</li> <li>Equivalently: \\(\\ker(T) = \\{\\mathbf{0}\\}\\) and \\(\\text{Range}(T) = W\\)</li> </ul> <p>For a matrix \\(\\mathbf{A}\\):</p> <ul> <li>Must be square (\\(m = n\\))</li> <li>Must have full rank (\\(\\text{rank}(\\mathbf{A}) = n\\))</li> <li>Must have nullity zero</li> <li>Must have nonzero determinant</li> </ul>"},{"location":"chapters/04-linear-transformations/#inverse-of-geometric-transformations","title":"Inverse of Geometric Transformations","text":"Transformation Matrix Inverse Rotation by \\(\\theta\\) \\(\\mathbf{R}(\\theta)\\) \\(\\mathbf{R}(-\\theta) = \\mathbf{R}^T\\) Scaling by \\((s_x, s_y)\\) \\(\\text{diag}(s_x, s_y)\\) \\(\\text{diag}(1/s_x, 1/s_y)\\) Shear by \\(k\\) \\(\\mathbf{H}(k)\\) \\(\\mathbf{H}(-k)\\) Reflection \\(\\mathbf{F}\\) \\(\\mathbf{F}\\) (self-inverse) Projection \\(\\mathbf{P}\\) Not invertible <p>Projections are never invertible because they collapse dimension\u2014once information is lost, it cannot be recovered.</p>"},{"location":"chapters/04-linear-transformations/#change-of-basis","title":"Change of Basis","text":"<p>Different bases provide different \"viewpoints\" on the same vector space. Change of basis allows us to translate between these viewpoints.</p>"},{"location":"chapters/04-linear-transformations/#basis-transition-matrix","title":"Basis Transition Matrix","text":"<p>Given two bases \\(\\mathcal{B} = \\{\\mathbf{b}_1, \\ldots, \\mathbf{b}_n\\}\\) and \\(\\mathcal{C} = \\{\\mathbf{c}_1, \\ldots, \\mathbf{c}_n\\}\\), the basis transition matrix \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) converts coordinates from \\(\\mathcal{C}\\) to \\(\\mathcal{B}\\).</p> <p>If \\([\\mathbf{v}]_{\\mathcal{C}}\\) are the coordinates of \\(\\mathbf{v}\\) in basis \\(\\mathcal{C}\\), then:</p> <p>\\([\\mathbf{v}]_{\\mathcal{B}} = \\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}} [\\mathbf{v}]_{\\mathcal{C}}\\)</p> <p>The columns of \\(\\mathbf{P}_{\\mathcal{B} \\leftarrow \\mathcal{C}}\\) are the \\(\\mathcal{C}\\) basis vectors expressed in \\(\\mathcal{B}\\) coordinates.</p>"},{"location":"chapters/04-linear-transformations/#similar-matrices","title":"Similar Matrices","text":"<p>If \\(\\mathbf{A}\\) represents a transformation in the standard basis and \\(\\mathbf{P}\\) is the change of basis matrix, then:</p> <p>\\(\\mathbf{A}' = \\mathbf{P}^{-1}\\mathbf{A}\\mathbf{P}\\)</p> <p>represents the same transformation in the new basis. Matrices related this way are called similar matrices\u2014they represent the same transformation in different coordinate systems.</p> <p>Similar matrices have the same:</p> <ul> <li>Determinant</li> <li>Trace</li> <li>Eigenvalues</li> <li>Rank</li> </ul>"},{"location":"chapters/04-linear-transformations/#diagram-change-of-basis-visualizer","title":"Diagram: Change of Basis Visualizer","text":"Change of Basis Interactive Visualizer <p>Type: microsim</p> <p>Bloom Level: Understand (L2) Bloom Verb: explain, interpret</p> <p>Learning Objective: Help students understand that the same vector has different coordinate representations in different bases, and how the transition matrix converts between them.</p> <p>Canvas layout: - Left: Standard basis view with vector - Right: Custom basis view with same vector - Bottom: Coordinate displays and transition matrix</p> <p>Visual elements: - Standard basis vectors (e\u2081, e\u2082) in black - Custom basis vectors (b\u2081, b\u2082) in purple - Same geometric vector shown in both views - Coordinates displayed in each basis - Transition matrix P - Grid lines for each basis</p> <p>Interactive controls: - Draggable endpoints for custom basis vectors - Draggable vector to transform - Button: Reset to standard basis - Checkbox: Show both bases overlaid - Checkbox: Show transition matrix calculation - Dropdown: Preset bases (standard, rotated, skewed)</p> <p>Default parameters: - Custom basis: rotated 30\u00b0 from standard - Vector: (2, 1) in standard coordinates</p> <p>Behavior: - Vector stays fixed geometrically as basis changes - Coordinates update to reflect new basis - Transition matrix updates with basis - Overlay mode shows both grids simultaneously - Clear visualization that vector is unchanged, only representation</p> <p>Implementation: p5.js with coordinate transformation</p>"},{"location":"chapters/04-linear-transformations/#applications","title":"Applications","text":""},{"location":"chapters/04-linear-transformations/#computer-graphics","title":"Computer Graphics","text":"<p>Every transformation in computer graphics\u2014modeling, viewing, projection\u2014is a linear (or affine) transformation represented by matrices. The graphics pipeline applies a sequence of transformations:</p> <ol> <li>Model matrix: Object space \u2192 World space</li> <li>View matrix: World space \u2192 Camera space</li> <li>Projection matrix: Camera space \u2192 Clip space</li> </ol> <p>GPUs are optimized for these matrix multiplications, processing millions of vertices per second.</p>"},{"location":"chapters/04-linear-transformations/#robotics","title":"Robotics","text":"<p>Robot arm kinematics uses transformation matrices to track how joints connect. Each joint applies a rotation or translation, and composing these gives the end-effector position:</p> <p>\\(\\mathbf{T}_{\\text{total}} = \\mathbf{T}_1 \\mathbf{T}_2 \\cdots \\mathbf{T}_n\\)</p>"},{"location":"chapters/04-linear-transformations/#neural-networks","title":"Neural Networks","text":"<p>Each layer of a neural network applies a linear transformation (weights) followed by a nonlinearity:</p> <p>\\(\\mathbf{h} = \\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b})\\)</p> <p>Understanding transformations helps interpret:</p> <ul> <li>What the network \"sees\" at each layer</li> <li>How information flows and transforms</li> <li>Why deep networks can learn complex mappings</li> </ul>"},{"location":"chapters/04-linear-transformations/#principal-component-analysis","title":"Principal Component Analysis","text":"<p>PCA finds a change of basis that:</p> <ul> <li>Aligns axes with directions of maximum variance</li> <li>Decorrelates the data</li> <li>Enables dimensionality reduction by projecting onto top components</li> </ul> <p>This is a change of basis to the eigenvector basis of the covariance matrix.</p>"},{"location":"chapters/04-linear-transformations/#summary-and-key-takeaways","title":"Summary and Key Takeaways","text":"<p>This chapter connected matrix algebra to geometric transformation.</p> <p>Foundations:</p> <ul> <li>A function maps inputs from a domain to outputs in a codomain</li> <li>A linear transformation preserves addition and scalar multiplication</li> <li>Every linear transformation has a transformation matrix whose columns are images of basis vectors</li> </ul> <p>Geometric Transformations:</p> <ul> <li>Rotation matrices rotate while preserving lengths and angles</li> <li>Scaling matrices stretch or compress along coordinate axes</li> <li>Shear matrices skew shapes by sliding parallel to an axis</li> <li>Reflection matrices mirror across a line or plane</li> <li>Projection matrices map onto lower-dimensional subspaces</li> </ul> <p>Composition and Structure:</p> <ul> <li>Composition of transformations corresponds to matrix multiplication</li> <li>Order matters: \\(\\mathbf{AB} \\neq \\mathbf{BA}\\) in general</li> <li>The kernel contains vectors mapping to zero; its dimension is nullity</li> <li>The range is the set of all outputs; its dimension is rank</li> <li>Rank-Nullity Theorem: \\(n = \\text{rank} + \\text{nullity}\\)</li> </ul> <p>Invertibility and Basis:</p> <ul> <li>Invertible transformations have trivial kernel and full range</li> <li>Change of basis provides different coordinate views of the same transformation</li> <li>Similar matrices represent the same transformation in different bases</li> </ul> <p>Key Properties:</p> Transformation Preserves Lengths? Preserves Angles? Invertible? Determinant Rotation Yes Yes Yes 1 Uniform Scaling No Yes Yes (if \\(k \\neq 0\\)) \\(k^n\\) Shear No No Yes 1 Reflection Yes Yes Yes \\(-1\\) Projection No No No 0"},{"location":"chapters/04-linear-transformations/#exercises","title":"Exercises","text":"Exercise 1: Finding Transformation Matrices <p>Find the 2\u00d72 matrix for the linear transformation that:</p> <p>a) Reflects across the line \\(y = x\\)</p> <p>b) Rotates by 90\u00b0 counterclockwise, then scales by factor 2</p> <p>c) Projects onto the line \\(y = 2x\\)</p> Exercise 2: Composition Order <p>Let \\(R\\) be rotation by 45\u00b0 and \\(S\\) be scaling by \\((2, 1)\\). Compute both \\(RS\\) and \\(SR\\), and describe geometrically why they differ.</p> Exercise 3: Kernel and Range <p>For the matrix \\(\\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\end{bmatrix}\\):</p> <p>a) Find a basis for the kernel (null space)</p> <p>b) Find a basis for the range (column space)</p> <p>c) Verify the Rank-Nullity Theorem</p> Exercise 4: Invertibility <p>Determine which transformations are invertible and find the inverse if it exists:</p> <p>a) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{bmatrix}\\mathbf{x}\\)</p> <p>b) \\(T(\\mathbf{x}) = \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 3 \\end{bmatrix}\\mathbf{x}\\)</p> <p>c) Projection onto the x-axis</p> Exercise 5: Change of Basis <p>Let \\(\\mathcal{B} = \\{(1, 1), (1, -1)\\}\\) be a basis for \\(\\mathbb{R}^2\\).</p> <p>a) Find the change of basis matrix from standard coordinates to \\(\\mathcal{B}\\)-coordinates</p> <p>b) Express the vector \\((3, 1)\\) in \\(\\mathcal{B}\\)-coordinates</p> <p>c) If a transformation is rotation by 90\u00b0 in standard coordinates, what matrix represents it in \\(\\mathcal{B}\\)-coordinates?</p>"},{"location":"chapters/05-determinants-and-matrix-properties/","title":"Determinants and Matrix Properties","text":""},{"location":"chapters/05-determinants-and-matrix-properties/#summary","title":"Summary","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes. This chapter covers determinant computation methods, their properties, and geometric interpretation as the volume scaling factor of a transformation. You will also learn Cramer's rule and understand the relationship between determinants and matrix invertibility.</p>"},{"location":"chapters/05-determinants-and-matrix-properties/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 13 concepts from the learning graph:</p> <ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"chapters/05-determinants-and-matrix-properties/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/","title":"Eigenvalues and Eigenvectors","text":""},{"location":"chapters/06-eigenvalues-and-eigenvectors/#summary","title":"Summary","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations. This chapter covers eigenvalues, eigenvectors, characteristic polynomials, and diagonalization. You will learn the spectral theorem for symmetric matrices and the power iteration method. These concepts are essential for PCA, stability analysis, and understanding how neural networks learn.</p>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"chapters/06-eigenvalues-and-eigenvectors/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 5: Determinants and Matrix Properties</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/07-matrix-decompositions/","title":"Matrix Decompositions","text":""},{"location":"chapters/07-matrix-decompositions/#summary","title":"Summary","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction. This chapter covers LU, QR, Cholesky, and Singular Value Decomposition (SVD). Each decomposition has specific use cases: LU for solving systems efficiently, QR for least squares problems, Cholesky for symmetric positive definite matrices, and SVD for low-rank approximations and recommender systems.</p>"},{"location":"chapters/07-matrix-decompositions/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"chapters/07-matrix-decompositions/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 3: Systems of Linear Equations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 8: Vector Spaces and Inner Products (for Gram-Schmidt)</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/","title":"Vector Spaces and Inner Products","text":""},{"location":"chapters/08-vector-spaces-and-inner-products/#summary","title":"Summary","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications. You will learn about abstract vector spaces, inner products, orthogonality, the Gram-Schmidt orthogonalization process, and projections. This chapter also covers the four fundamental subspaces of a matrix and the pseudoinverse, which are essential for least squares and machine learning.</p>"},{"location":"chapters/08-vector-spaces-and-inner-products/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"chapters/08-vector-spaces-and-inner-products/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/09-machine-learning-foundations/","title":"Machine Learning Foundations","text":""},{"location":"chapters/09-machine-learning-foundations/#summary","title":"Summary","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques. You will learn how data is represented as matrices, understand covariance and correlation, master Principal Component Analysis (PCA) for dimensionality reduction, and implement linear regression with regularization. Gradient descent, the workhorse of machine learning optimization, is covered in detail.</p>"},{"location":"chapters/09-machine-learning-foundations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"chapters/09-machine-learning-foundations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 6: Eigenvalues and Eigenvectors</li> <li>Chapter 7: Matrix Decompositions</li> <li>Chapter 8: Vector Spaces and Inner Products</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/","title":"Neural Networks and Deep Learning","text":""},{"location":"chapters/10-neural-networks-and-deep-learning/#summary","title":"Summary","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning, covering neurons, activation functions, weight matrices, forward propagation, and backpropagation. You will also learn about specialized architectures including convolutional layers, batch normalization, and tensor operations.</p>"},{"location":"chapters/10-neural-networks-and-deep-learning/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 26 concepts from the learning graph:</p> <ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"chapters/10-neural-networks-and-deep-learning/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 13: Image Processing (for convolution concepts)</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/11-generative-ai-and-llms/","title":"Generative AI and Large Language Models","text":""},{"location":"chapters/11-generative-ai-and-llms/#summary","title":"Summary","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of transformers and large language models, including embedding spaces, attention mechanisms, query-key-value matrices, and multi-head attention. You will also learn about LoRA for efficient fine-tuning and latent space interpolation in generative models.</p>"},{"location":"chapters/11-generative-ai-and-llms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 19 concepts from the learning graph:</p> <ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"chapters/11-generative-ai-and-llms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 7: Matrix Decompositions (for low-rank approximation)</li> <li>Chapter 9: Machine Learning Foundations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/","title":"Optimization and Learning Algorithms","text":""},{"location":"chapters/12-optimization-and-learning-algorithms/#summary","title":"Summary","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms. This chapter covers the Hessian matrix, convexity, Newton's method, and modern adaptive optimizers like Adam and RMSprop. You will also learn constrained optimization with Lagrange multipliers and KKT conditions.</p>"},{"location":"chapters/12-optimization-and-learning-algorithms/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 14 concepts from the learning graph:</p> <ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"chapters/12-optimization-and-learning-algorithms/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 9: Machine Learning Foundations</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/13-image-processing-and-computer-vision/","title":"Image Processing and Computer Vision","text":""},{"location":"chapters/13-image-processing-and-computer-vision/#summary","title":"Summary","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision. This chapter covers image representation as matrices and tensors, convolution operations, image filtering (blur, sharpen, edge detection), Fourier transforms, and feature detection. You will also learn about homography for perspective transformations.</p>"},{"location":"chapters/13-image-processing-and-computer-vision/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"chapters/13-image-processing-and-computer-vision/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 7: Matrix Decompositions (for SVD-based compression)</li> <li>Chapter 10: Neural Networks and Deep Learning (for tensors)</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/14-3d-geometry-and-transformations/","title":"3D Geometry and Transformations","text":""},{"location":"chapters/14-3d-geometry-and-transformations/#summary","title":"Summary","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers 3D coordinate systems, rotation representations including Euler angles and quaternions, homogeneous coordinates, camera models with intrinsic and extrinsic parameters, stereo vision, and point cloud processing. These concepts form the foundation for any system that operates in 3D space.</p>"},{"location":"chapters/14-3d-geometry-and-transformations/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"chapters/14-3d-geometry-and-transformations/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 4: Linear Transformations</li> <li>Chapter 13: Image Processing and Computer Vision</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/","title":"Autonomous Systems and Sensor Fusion","text":""},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#summary","title":"Summary","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles and robotics. You will learn LIDAR point cloud processing, camera calibration, sensor fusion with Kalman filters, state estimation and prediction, SLAM (Simultaneous Localization and Mapping), object detection and tracking, and path planning algorithms. These are the techniques that enable self-driving cars and autonomous robots.</p>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 20 concepts from the learning graph:</p> <ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"chapters/15-autonomous-systems-and-sensor-fusion/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Vectors and Vector Spaces</li> <li>Chapter 2: Matrices and Matrix Operations</li> <li>Chapter 10: Neural Networks and Deep Learning</li> <li>Chapter 12: Optimization and Learning Algorithms</li> <li>Chapter 13: Image Processing and Computer Vision</li> <li>Chapter 14: 3D Geometry and Transformations</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"learning-graph/","title":"Learning Graph for Applied Linear Algebra for AI and Machine Learning","text":"<p>This section contains the learning graph for this textbook.  A learning graph is a graph of concepts used in this textbook.  Each concept is represented by a node in a network graph.  Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>View the Learning Graph</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts.  They have no outbound edges.  They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts.  At the far right we have the most advanced concepts in the course.  To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long. This course contains 300 concepts covering vectors, matrices, linear systems, transformations, eigentheory, decompositions, machine learning foundations, neural networks, generative AI, optimization, image processing, 3D geometry, and autonomous systems.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG).  DAGs do not have cycles where concepts depend on themselves.  We provide the DAG in two formats.  One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format.  The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties.  This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 2 entry points (Scalar, Function)</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains (19 concepts)</li> <li>Connectivity: all 300 nodes connected to the main cluster</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type.  We use generative AI to create categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>15 concept categories covering the course structure</li> <li>Categories aligned with course chapters and topics</li> <li>Balanced distribution across categories</li> <li>Color-coded for visualization</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This report shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an appropriate number of concepts based on the topic's importance in the course.</p> <ul> <li>Statistical breakdown by category</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List","text":"<p>This document contains 300 concepts for the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-list/#part-1-foundations-of-linear-algebra","title":"Part 1: Foundations of Linear Algebra","text":""},{"location":"learning-graph/concept-list/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<ol> <li>Scalar</li> <li>Vector</li> <li>Vector Notation</li> <li>2D Vector</li> <li>3D Vector</li> <li>N-Dimensional Vector</li> <li>Vector Addition</li> <li>Scalar Multiplication</li> <li>Vector Subtraction</li> <li>Dot Product</li> <li>Cross Product</li> <li>Vector Magnitude</li> <li>Unit Vector</li> <li>Vector Normalization</li> <li>Euclidean Distance</li> <li>L1 Norm</li> <li>L2 Norm</li> <li>L-Infinity Norm</li> <li>Linear Combination</li> <li>Span</li> <li>Linear Independence</li> <li>Linear Dependence</li> <li>Basis Vector</li> <li>Standard Basis</li> <li>Coordinate System</li> <li>Vector Space</li> <li>Dimension of Space</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<ol> <li>Matrix</li> <li>Matrix Notation</li> <li>Matrix Dimensions</li> <li>Row Vector</li> <li>Column Vector</li> <li>Matrix Entry</li> <li>Matrix Addition</li> <li>Matrix Scalar Multiply</li> <li>Matrix-Vector Product</li> <li>Matrix Multiplication</li> <li>Matrix Transpose</li> <li>Symmetric Matrix</li> <li>Identity Matrix</li> <li>Diagonal Matrix</li> <li>Triangular Matrix</li> <li>Upper Triangular</li> <li>Lower Triangular</li> <li>Orthogonal Matrix</li> <li>Matrix Inverse</li> <li>Invertible Matrix</li> <li>Sparse Matrix</li> <li>Dense Matrix</li> <li>Block Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<ol> <li>Linear Equation</li> <li>System of Equations</li> <li>Matrix Equation Form</li> <li>Augmented Matrix</li> <li>Gaussian Elimination</li> <li>Row Operations</li> <li>Row Swap</li> <li>Row Scaling</li> <li>Row Addition</li> <li>Row Echelon Form</li> <li>Reduced Row Echelon Form</li> <li>Pivot Position</li> <li>Pivot Column</li> <li>Free Variable</li> <li>Basic Variable</li> <li>Solution Set</li> <li>Unique Solution</li> <li>Infinite Solutions</li> <li>No Solution</li> <li>Homogeneous System</li> <li>Trivial Solution</li> <li>Numerical Stability</li> <li>Back Substitution</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<ol> <li>Function</li> <li>Linear Transformation</li> <li>Transformation Matrix</li> <li>Domain</li> <li>Codomain</li> <li>Image</li> <li>Rotation Matrix</li> <li>2D Rotation</li> <li>3D Rotation</li> <li>Scaling Matrix</li> <li>Uniform Scaling</li> <li>Non-Uniform Scaling</li> <li>Shear Matrix</li> <li>Reflection Matrix</li> <li>Projection</li> <li>Orthogonal Projection</li> <li>Composition of Transforms</li> <li>Kernel</li> <li>Null Space</li> <li>Range</li> <li>Column Space</li> <li>Rank</li> <li>Nullity</li> <li>Rank-Nullity Theorem</li> <li>Invertible Transform</li> <li>Change of Basis</li> <li>Basis Transition Matrix</li> </ol>"},{"location":"learning-graph/concept-list/#part-2-advanced-matrix-theory","title":"Part 2: Advanced Matrix Theory","text":""},{"location":"learning-graph/concept-list/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<ol> <li>Determinant</li> <li>2x2 Determinant</li> <li>3x3 Determinant</li> <li>Cofactor Expansion</li> <li>Minor</li> <li>Cofactor</li> <li>Determinant Properties</li> <li>Multiplicative Property</li> <li>Transpose Determinant</li> <li>Singular Matrix</li> <li>Volume Scaling Factor</li> <li>Signed Area</li> <li>Cramers Rule</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<ol> <li>Eigenvalue</li> <li>Eigenvector</li> <li>Eigen Equation</li> <li>Characteristic Polynomial</li> <li>Characteristic Equation</li> <li>Eigenspace</li> <li>Algebraic Multiplicity</li> <li>Geometric Multiplicity</li> <li>Diagonalization</li> <li>Diagonal Form</li> <li>Similar Matrices</li> <li>Complex Eigenvalue</li> <li>Spectral Theorem</li> <li>Symmetric Eigenvalues</li> <li>Power Iteration</li> <li>Dominant Eigenvalue</li> <li>Eigendecomposition</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<ol> <li>Matrix Factorization</li> <li>LU Decomposition</li> <li>Partial Pivoting</li> <li>QR Decomposition</li> <li>Gram-Schmidt QR</li> <li>Householder QR</li> <li>Cholesky Decomposition</li> <li>Positive Definite Matrix</li> <li>SVD</li> <li>Singular Value</li> <li>Left Singular Vector</li> <li>Right Singular Vector</li> <li>Full SVD</li> <li>Compact SVD</li> <li>Truncated SVD</li> <li>Low-Rank Approximation</li> <li>Matrix Rank</li> <li>Numerical Rank</li> <li>Condition Number</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<ol> <li>Abstract Vector Space</li> <li>Subspace</li> <li>Vector Space Axioms</li> <li>Inner Product</li> <li>Inner Product Space</li> <li>Norm from Inner Product</li> <li>Cauchy-Schwarz Inequality</li> <li>Orthogonality</li> <li>Orthogonal Vectors</li> <li>Orthonormal Set</li> <li>Orthonormal Basis</li> <li>Gram-Schmidt Process</li> <li>Projection onto Subspace</li> <li>Least Squares Problem</li> <li>Normal Equations</li> <li>Row Space</li> <li>Left Null Space</li> <li>Four Subspaces</li> <li>Pseudoinverse</li> </ol>"},{"location":"learning-graph/concept-list/#part-3-linear-algebra-in-machine-learning","title":"Part 3: Linear Algebra in Machine Learning","text":""},{"location":"learning-graph/concept-list/#chapter-9-ml-foundations","title":"Chapter 9: ML Foundations","text":"<ol> <li>Feature Vector</li> <li>Feature Matrix</li> <li>Data Matrix</li> <li>Covariance Matrix</li> <li>Correlation Matrix</li> <li>Standardization</li> <li>PCA</li> <li>Principal Component</li> <li>Variance Explained</li> <li>Scree Plot</li> <li>Dimensionality Reduction</li> <li>Linear Regression</li> <li>Design Matrix</li> <li>Ridge Regression</li> <li>Lasso Regression</li> <li>Regularization</li> <li>Gradient Vector</li> <li>Gradient Descent</li> <li>Batch Gradient Descent</li> <li>Learning Rate</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<ol> <li>Perceptron</li> <li>Neuron Model</li> <li>Activation Function</li> <li>ReLU</li> <li>Sigmoid</li> <li>Tanh</li> <li>Softmax</li> <li>Weight Matrix</li> <li>Bias Vector</li> <li>Forward Propagation</li> <li>Backpropagation</li> <li>Chain Rule Matrices</li> <li>Loss Function</li> <li>Cross-Entropy Loss</li> <li>Neural Network Layer</li> <li>Hidden Layer</li> <li>Deep Network</li> <li>Convolutional Layer</li> <li>Convolution Kernel</li> <li>Stride</li> <li>Padding</li> <li>Pooling Layer</li> <li>Batch Normalization</li> <li>Layer Normalization</li> <li>Tensor</li> <li>Tensor Operations</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<ol> <li>Embedding</li> <li>Embedding Space</li> <li>Word Embedding</li> <li>Semantic Similarity</li> <li>Cosine Similarity</li> <li>Attention Mechanism</li> <li>Self-Attention</li> <li>Cross-Attention</li> <li>Query Matrix</li> <li>Key Matrix</li> <li>Value Matrix</li> <li>Attention Score</li> <li>Attention Weights</li> <li>Multi-Head Attention</li> <li>Transformer Architecture</li> <li>Position Encoding</li> <li>LoRA</li> <li>Latent Space</li> <li>Interpolation</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<ol> <li>Hessian Matrix</li> <li>Convexity</li> <li>Convex Function</li> <li>Newtons Method</li> <li>Quasi-Newton Method</li> <li>BFGS Algorithm</li> <li>SGD</li> <li>Mini-Batch SGD</li> <li>Momentum</li> <li>Adam Optimizer</li> <li>RMSprop</li> <li>Lagrange Multiplier</li> <li>Constrained Optimization</li> <li>KKT Conditions</li> </ol>"},{"location":"learning-graph/concept-list/#part-4-computer-vision-and-autonomous-systems","title":"Part 4: Computer Vision and Autonomous Systems","text":""},{"location":"learning-graph/concept-list/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<ol> <li>Image Matrix</li> <li>Grayscale Image</li> <li>RGB Image</li> <li>Image Tensor</li> <li>Image Convolution</li> <li>Image Filter</li> <li>Blur Filter</li> <li>Sharpen Filter</li> <li>Edge Detection</li> <li>Sobel Operator</li> <li>Fourier Transform</li> <li>Frequency Domain</li> <li>Image Compression</li> <li>Color Space Transform</li> <li>Feature Detection</li> <li>Homography</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<ol> <li>3D Coordinate System</li> <li>Euler Angles</li> <li>Gimbal Lock</li> <li>Quaternion</li> <li>Quaternion Rotation</li> <li>Homogeneous Coordinates</li> <li>Rigid Body Transform</li> <li>SE3 Transform</li> <li>Camera Matrix</li> <li>Intrinsic Parameters</li> <li>Extrinsic Parameters</li> <li>Projection Matrix</li> <li>Perspective Projection</li> <li>Stereo Vision</li> <li>Triangulation</li> <li>Epipolar Geometry</li> <li>Point Cloud</li> </ol>"},{"location":"learning-graph/concept-list/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<ol> <li>LIDAR Point Cloud</li> <li>Camera Calibration</li> <li>Sensor Fusion</li> <li>Kalman Filter</li> <li>State Vector</li> <li>Measurement Vector</li> <li>Prediction Step</li> <li>Update Step</li> <li>Kalman Gain</li> <li>Extended Kalman Filter</li> <li>State Estimation</li> <li>SLAM</li> <li>Localization</li> <li>Mapping</li> <li>Object Detection</li> <li>Object Tracking</li> <li>Bounding Box</li> <li>Path Planning</li> <li>Motion Planning</li> <li>Trajectory Optimization</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This document defines the categorical taxonomy for organizing the 300 concepts in the Applied Linear Algebra for AI and Machine Learning course.</p>"},{"location":"learning-graph/concept-taxonomy/#taxonomy-categories","title":"Taxonomy Categories","text":"TaxonomyID Category Name Description FOUND Foundation Concepts Basic mathematical building blocks including scalars, vectors, and fundamental operations that form the prerequisite knowledge MATOP Matrix Operations Core matrix concepts, operations, and special matrix types essential for linear algebra computations LINSYS Linear Systems Concepts related to systems of linear equations, solution methods, and matrix equation forms TRANS Transformations Linear transformations, geometric operations (rotation, scaling, shear), and related structural concepts DETERM Determinants Determinant computation, properties, and geometric interpretations EIGEN Eigentheory Eigenvalues, eigenvectors, eigenspaces, and diagonalization concepts DECOMP Decompositions Matrix factorization methods including LU, QR, Cholesky, and SVD INPROD Inner Products Inner product spaces, orthogonality, projections, and related abstract vector space concepts MLBASE ML Foundations Core machine learning concepts including data representation, PCA, regression, and gradient methods NEURAL Neural Networks Deep learning concepts including neurons, layers, activation functions, and backpropagation GENAI Generative AI Embeddings, attention mechanisms, transformers, and large language model concepts OPTIM Optimization Optimization algorithms and methods for training machine learning models IMGPROC Image Processing Computer vision concepts including image representation, convolution, and filtering GEOM3D 3D Geometry Three-dimensional geometry, rotations, coordinate systems, and camera models AUTON Autonomous Systems Sensor fusion, state estimation, SLAM, and autonomous navigation concepts"},{"location":"learning-graph/concept-taxonomy/#category-descriptions","title":"Category Descriptions","text":""},{"location":"learning-graph/concept-taxonomy/#found-foundation-concepts","title":"FOUND - Foundation Concepts","text":"<p>The fundamental building blocks of linear algebra. These concepts are prerequisites for nearly everything else in the course, including scalars, vectors, vector operations, norms, and basic vector space theory.</p>"},{"location":"learning-graph/concept-taxonomy/#matop-matrix-operations","title":"MATOP - Matrix Operations","text":"<p>Essential matrix concepts and operations. Covers matrix notation, types of matrices (diagonal, triangular, symmetric, orthogonal), and core operations like multiplication, transpose, and inverse.</p>"},{"location":"learning-graph/concept-taxonomy/#linsys-linear-systems","title":"LINSYS - Linear Systems","text":"<p>Methods for representing and solving systems of linear equations. Includes Gaussian elimination, row operations, echelon forms, and solution analysis.</p>"},{"location":"learning-graph/concept-taxonomy/#trans-transformations","title":"TRANS - Transformations","text":"<p>How matrices represent geometric transformations. Covers rotation, scaling, shearing, projection, and abstract concepts like kernel, range, and change of basis.</p>"},{"location":"learning-graph/concept-taxonomy/#determ-determinants","title":"DETERM - Determinants","text":"<p>Determinant theory and applications. Includes computation methods, geometric interpretation as volume scaling, and applications like Cramer's rule.</p>"},{"location":"learning-graph/concept-taxonomy/#eigen-eigentheory","title":"EIGEN - Eigentheory","text":"<p>The study of eigenvalues and eigenvectors - one of the most important topics in applied linear algebra. Covers characteristic polynomials, diagonalization, spectral theorem, and power iteration.</p>"},{"location":"learning-graph/concept-taxonomy/#decomp-decompositions","title":"DECOMP - Decompositions","text":"<p>Matrix factorization techniques. Each decomposition has specific use cases: LU for solving systems, QR for least squares, Cholesky for symmetric positive definite matrices, and SVD for general applications.</p>"},{"location":"learning-graph/concept-taxonomy/#inprod-inner-products","title":"INPROD - Inner Products","text":"<p>Abstract theory of inner product spaces. Covers orthogonality, Gram-Schmidt process, projections, least squares, and the four fundamental subspaces.</p>"},{"location":"learning-graph/concept-taxonomy/#mlbase-ml-foundations","title":"MLBASE - ML Foundations","text":"<p>Core machine learning concepts that rely on linear algebra. Includes data representation, covariance analysis, PCA, linear regression, regularization, and gradient descent.</p>"},{"location":"learning-graph/concept-taxonomy/#neural-neural-networks","title":"NEURAL - Neural Networks","text":"<p>Deep learning architecture and computation. Covers the linear algebra of neural networks including weight matrices, forward propagation, backpropagation, and specialized layers.</p>"},{"location":"learning-graph/concept-taxonomy/#genai-generative-ai","title":"GENAI - Generative AI","text":"<p>Modern generative AI concepts. Focuses on the linear algebra behind transformers, attention mechanisms, embeddings, and large language models.</p>"},{"location":"learning-graph/concept-taxonomy/#optim-optimization","title":"OPTIM - Optimization","text":"<p>Optimization algorithms for training. Covers gradient-based methods, second-order optimization, and constrained optimization techniques.</p>"},{"location":"learning-graph/concept-taxonomy/#imgproc-image-processing","title":"IMGPROC - Image Processing","text":"<p>Computer vision fundamentals. Covers image representation, convolution, filtering, frequency domain analysis, and feature detection.</p>"},{"location":"learning-graph/concept-taxonomy/#geom3d-3d-geometry","title":"GEOM3D - 3D Geometry","text":"<p>Three-dimensional geometric concepts. Includes coordinate systems, rotation representations (Euler angles, quaternions), camera models, and stereo vision.</p>"},{"location":"learning-graph/concept-taxonomy/#auton-autonomous-systems","title":"AUTON - Autonomous Systems","text":"<p>Sensor fusion and autonomous navigation. Covers Kalman filtering, SLAM, localization, object tracking, and path planning.</p>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":""},{"location":"learning-graph/course-description-assessment/#course-applied-linear-algebra-for-ai-and-machine-learning","title":"Course: Applied Linear Algebra for AI and Machine Learning","text":"<p>Assessment Date: 2026-01-17 Quality Score: 97/100</p>"},{"location":"learning-graph/course-description-assessment/#scoring-breakdown","title":"Scoring Breakdown","text":"Element Points Assessment Title 5/5 Clear, descriptive: \"Applied Linear Algebra for AI and Machine Learning\" Target Audience 5/5 Specific audiences identified: CS majors (AI/ML), Data Science students, Engineering students (robotics/autonomous systems), Applied Math students, Graduate students Prerequisites 5/5 Clearly listed: College Algebra, Basic programming (Python recommended), Familiarity with calculus (derivatives/integrals) Main Topics Covered 10/10 Comprehensive 15-chapter structure across 4 parts covering foundations through applications Topics Excluded 2/5 Not explicitly stated what is NOT covered Learning Outcomes Header 5/5 Clear statement with 7 high-level objectives Remember Level 10/10 12 specific, actionable outcomes Understand Level 10/10 13 specific, actionable outcomes Apply Level 10/10 13 specific, actionable outcomes Analyze Level 10/10 13 specific, actionable outcomes Evaluate Level 10/10 13 specific, actionable outcomes Create Level 10/10 14 specific outcomes including capstone projects Descriptive Context 5/5 Strong \"Why This Course Matters\" section"},{"location":"learning-graph/course-description-assessment/#strengths","title":"Strengths","text":"<ol> <li>Exceptional Bloom's Taxonomy Coverage: 78 specific learning outcomes across all six cognitive levels</li> <li>Well-Structured Progression: 15 chapters organized into 4 logical parts (Foundations \u2192 Advanced Theory \u2192 ML Applications \u2192 Computer Vision/Autonomous Systems)</li> <li>Strong Application Focus: Every chapter connects theory to practical AI/ML applications</li> <li>Interactive Learning: 8 example microsimulations described for hands-on exploration</li> <li>Clear Assessment Structure: Weekly problem sets, labs, midterm, and capstone project</li> <li>Modern Relevance: Covers transformers, attention mechanisms, LLMs, and autonomous driving</li> </ol>"},{"location":"learning-graph/course-description-assessment/#topics-covered","title":"Topics Covered","text":""},{"location":"learning-graph/course-description-assessment/#part-1-foundations-weeks-1-4","title":"Part 1: Foundations (Weeks 1-4)","text":"<ul> <li>Vectors and Vector Spaces</li> <li>Matrices and Matrix Operations</li> <li>Systems of Linear Equations</li> <li>Linear Transformations</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":"<ul> <li>Determinants and Matrix Properties</li> <li>Eigenvalues and Eigenvectors</li> <li>Matrix Decompositions (LU, QR, Cholesky, SVD)</li> <li>Vector Spaces and Inner Product Spaces</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-3-machine-learning-applications-weeks-9-12","title":"Part 3: Machine Learning Applications (Weeks 9-12)","text":"<ul> <li>Linear Algebra Foundations of ML</li> <li>Neural Networks and Deep Learning</li> <li>Generative AI and Large Language Models</li> <li>Optimization and Learning Algorithms</li> </ul>"},{"location":"learning-graph/course-description-assessment/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":"<ul> <li>Image Processing and Computer Vision</li> <li>3D Geometry and Transformations</li> <li>Autonomous Driving and Sensor Fusion</li> </ul>"},{"location":"learning-graph/course-description-assessment/#minor-improvement-suggestions","title":"Minor Improvement Suggestions","text":"<ol> <li>Add Topics Excluded Section: Consider explicitly stating what is NOT covered (e.g., abstract algebra beyond finite dimensions, proofs of all theorems, advanced numerical analysis)</li> </ol>"},{"location":"learning-graph/course-description-assessment/#concept-estimation","title":"Concept Estimation","text":"<p>Based on the course description, approximately 200-220 distinct concepts can be derived:</p> <ul> <li>~25 foundational vector concepts</li> <li>~25 matrix operation concepts</li> <li>~20 systems of equations concepts</li> <li>~20 linear transformation concepts</li> <li>~15 determinant concepts</li> <li>~25 eigenvalue/eigenvector concepts</li> <li>~20 matrix decomposition concepts</li> <li>~15 inner product space concepts</li> <li>~20 ML foundation concepts</li> <li>~25 neural network/deep learning concepts</li> <li>~20 generative AI concepts</li> <li>~15 optimization concepts</li> <li>~20 computer vision concepts</li> <li>~15 3D geometry concepts</li> <li>~20 autonomous systems concepts</li> </ul>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>PROCEED with learning graph generation. This course description exceeds the quality threshold of 80 points with a score of 97/100.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Foundational Concepts (no dependencies): 2</li> <li>Concepts with Dependencies: 298</li> <li>Average Dependencies per Concept: 1.71</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Scalar</li> <li>74: Function</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 19</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Scalar (ID: 1)</li> <li>Vector (ID: 2)</li> <li>Vector Notation (ID: 3)</li> <li>Vector Addition (ID: 7)</li> <li>Linear Combination (ID: 19)</li> <li>Span (ID: 20)</li> <li>Linear Independence (ID: 21)</li> <li>Basis Vector (ID: 23)</li> <li>Vector Space (ID: 26)</li> <li>Linear Transformation (ID: 75)</li> <li>Transformation Matrix (ID: 76)</li> <li>Eigenvalue (ID: 114)</li> <li>Eigenvector (ID: 115)</li> <li>Eigenspace (ID: 119)</li> <li>Diagonalization (ID: 122)</li> <li>PCA (ID: 175)</li> <li>Principal Component (ID: 176)</li> <li>Variance Explained (ID: 177)</li> <li>Scree Plot (ID: 178)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 97</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>9: Vector Subtraction</li> <li>11: Cross Product</li> <li>14: Vector Normalization</li> <li>16: L1 Norm</li> <li>17: L2 Norm</li> <li>18: L-Infinity Norm</li> <li>22: Linear Dependence</li> <li>27: Dimension of Space</li> <li>34: Matrix Addition</li> <li>49: Dense Matrix</li> <li>50: Block Matrix</li> <li>58: Row Scaling</li> <li>59: Row Addition</li> <li>65: Basic Variable</li> <li>67: Unique Solution</li> <li>68: Infinite Solutions</li> <li>69: No Solution</li> <li>71: Trivial Solution</li> <li>73: Back Substitution</li> <li>81: 2D Rotation</li> </ul> <p>...and 77 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 28 Matrix 25 2 2 Vector 18 3 37 Matrix Multiplication 13 4 10 Dot Product 9 5 186 Gradient Descent 9 6 76 Transformation Matrix 8 7 101 Determinant 8 8 5 3D Vector 7 9 12 Vector Magnitude 7 10 75 Linear Transformation 7"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 2 1 103 2 180 3 14 4 1"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (97): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Long dependency chains (19): Ensure students can follow extended learning paths</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 300</li> <li>Number of Taxonomies: 15</li> <li>Average Concepts per Taxonomy: 20.0</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status Foundation Concepts FOUND 27 9.0% \u2705 Transformations TRANS 27 9.0% \u2705 Neural Networks NEURAL 26 8.7% \u2705 Matrix Operations MATOP 23 7.7% \u2705 Linear Systems LINSYS 23 7.7% \u2705 ML Foundations MLBASE 20 6.7% \u2705 Autonomous Systems AUTON 20 6.7% \u2705 Decompositions DECOMP 19 6.3% \u2705 Inner Products INPROD 19 6.3% \u2705 Generative AI GENAI 19 6.3% \u2705 Eigentheory EIGEN 17 5.7% \u2705 3D Geometry GEOM3D 17 5.7% \u2705 Image Processing IMGPROC 16 5.3% \u2705 Optimization OPTIM 14 4.7% \u2705 Determinants DETERM 13 4.3% \u2705"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>FOUND  \u2588\u2588\u2588\u2588  27 (  9.0%)\nTRANS  \u2588\u2588\u2588\u2588  27 (  9.0%)\nNEURAL \u2588\u2588\u2588\u2588  26 (  8.7%)\nMATOP  \u2588\u2588\u2588  23 (  7.7%)\nLINSYS \u2588\u2588\u2588  23 (  7.7%)\nMLBASE \u2588\u2588\u2588  20 (  6.7%)\nAUTON  \u2588\u2588\u2588  20 (  6.7%)\nDECOMP \u2588\u2588\u2588  19 (  6.3%)\nINPROD \u2588\u2588\u2588  19 (  6.3%)\nGENAI  \u2588\u2588\u2588  19 (  6.3%)\nEIGEN  \u2588\u2588  17 (  5.7%)\nGEOM3D \u2588\u2588  17 (  5.7%)\nIMGPROC \u2588\u2588  16 (  5.3%)\nOPTIM  \u2588\u2588  14 (  4.7%)\nDETERM \u2588\u2588  13 (  4.3%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-found","title":"Foundation Concepts (FOUND)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Scalar</li> </ol> </li> <li> <ol> <li>Vector</li> </ol> </li> <li> <ol> <li>Vector Notation</li> </ol> </li> <li> <ol> <li>2D Vector</li> </ol> </li> <li> <ol> <li>3D Vector</li> </ol> </li> <li> <ol> <li>N-Dimensional Vector</li> </ol> </li> <li> <ol> <li>Vector Addition</li> </ol> </li> <li> <ol> <li>Scalar Multiplication</li> </ol> </li> <li> <ol> <li>Vector Subtraction</li> </ol> </li> <li> <ol> <li>Dot Product</li> </ol> </li> <li> <ol> <li>Cross Product</li> </ol> </li> <li> <ol> <li>Vector Magnitude</li> </ol> </li> <li> <ol> <li>Unit Vector</li> </ol> </li> <li> <ol> <li>Vector Normalization</li> </ol> </li> <li> <ol> <li>Euclidean Distance</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#transformations-trans","title":"Transformations (TRANS)","text":"<p>Count: 27 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Function</li> </ol> </li> <li> <ol> <li>Linear Transformation</li> </ol> </li> <li> <ol> <li>Transformation Matrix</li> </ol> </li> <li> <ol> <li>Domain</li> </ol> </li> <li> <ol> <li>Codomain</li> </ol> </li> <li> <ol> <li>Image</li> </ol> </li> <li> <ol> <li>Rotation Matrix</li> </ol> </li> <li> <ol> <li>2D Rotation</li> </ol> </li> <li> <ol> <li>3D Rotation</li> </ol> </li> <li> <ol> <li>Scaling Matrix</li> </ol> </li> <li> <ol> <li>Uniform Scaling</li> </ol> </li> <li> <ol> <li>Non-Uniform Scaling</li> </ol> </li> <li> <ol> <li>Shear Matrix</li> </ol> </li> <li> <ol> <li>Reflection Matrix</li> </ol> </li> <li> <ol> <li>Projection</li> </ol> </li> <li>...and 12 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#neural-networks-neural","title":"Neural Networks (NEURAL)","text":"<p>Count: 26 concepts (8.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Perceptron</li> </ol> </li> <li> <ol> <li>Neuron Model</li> </ol> </li> <li> <ol> <li>Activation Function</li> </ol> </li> <li> <ol> <li>ReLU</li> </ol> </li> <li> <ol> <li>Sigmoid</li> </ol> </li> <li> <ol> <li>Tanh</li> </ol> </li> <li> <ol> <li>Softmax</li> </ol> </li> <li> <ol> <li>Weight Matrix</li> </ol> </li> <li> <ol> <li>Bias Vector</li> </ol> </li> <li> <ol> <li>Forward Propagation</li> </ol> </li> <li> <ol> <li>Backpropagation</li> </ol> </li> <li> <ol> <li>Chain Rule Matrices</li> </ol> </li> <li> <ol> <li>Loss Function</li> </ol> </li> <li> <ol> <li>Cross-Entropy Loss</li> </ol> </li> <li> <ol> <li>Neural Network Layer</li> </ol> </li> <li>...and 11 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#matrix-operations-matop","title":"Matrix Operations (MATOP)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix</li> </ol> </li> <li> <ol> <li>Matrix Notation</li> </ol> </li> <li> <ol> <li>Matrix Dimensions</li> </ol> </li> <li> <ol> <li>Row Vector</li> </ol> </li> <li> <ol> <li>Column Vector</li> </ol> </li> <li> <ol> <li>Matrix Entry</li> </ol> </li> <li> <ol> <li>Matrix Addition</li> </ol> </li> <li> <ol> <li>Matrix Scalar Multiply</li> </ol> </li> <li> <ol> <li>Matrix-Vector Product</li> </ol> </li> <li> <ol> <li>Matrix Multiplication</li> </ol> </li> <li> <ol> <li>Matrix Transpose</li> </ol> </li> <li> <ol> <li>Symmetric Matrix</li> </ol> </li> <li> <ol> <li>Identity Matrix</li> </ol> </li> <li> <ol> <li>Diagonal Matrix</li> </ol> </li> <li> <ol> <li>Triangular Matrix</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#linear-systems-linsys","title":"Linear Systems (LINSYS)","text":"<p>Count: 23 concepts (7.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Linear Equation</li> </ol> </li> <li> <ol> <li>System of Equations</li> </ol> </li> <li> <ol> <li>Matrix Equation Form</li> </ol> </li> <li> <ol> <li>Augmented Matrix</li> </ol> </li> <li> <ol> <li>Gaussian Elimination</li> </ol> </li> <li> <ol> <li>Row Operations</li> </ol> </li> <li> <ol> <li>Row Swap</li> </ol> </li> <li> <ol> <li>Row Scaling</li> </ol> </li> <li> <ol> <li>Row Addition</li> </ol> </li> <li> <ol> <li>Row Echelon Form</li> </ol> </li> <li> <ol> <li>Reduced Row Echelon Form</li> </ol> </li> <li> <ol> <li>Pivot Position</li> </ol> </li> <li> <ol> <li>Pivot Column</li> </ol> </li> <li> <ol> <li>Free Variable</li> </ol> </li> <li> <ol> <li>Basic Variable</li> </ol> </li> <li>...and 8 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#ml-foundations-mlbase","title":"ML Foundations (MLBASE)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Feature Vector</li> </ol> </li> <li> <ol> <li>Feature Matrix</li> </ol> </li> <li> <ol> <li>Data Matrix</li> </ol> </li> <li> <ol> <li>Covariance Matrix</li> </ol> </li> <li> <ol> <li>Correlation Matrix</li> </ol> </li> <li> <ol> <li>Standardization</li> </ol> </li> <li> <ol> <li>PCA</li> </ol> </li> <li> <ol> <li>Principal Component</li> </ol> </li> <li> <ol> <li>Variance Explained</li> </ol> </li> <li> <ol> <li>Scree Plot</li> </ol> </li> <li> <ol> <li>Dimensionality Reduction</li> </ol> </li> <li> <ol> <li>Linear Regression</li> </ol> </li> <li> <ol> <li>Design Matrix</li> </ol> </li> <li> <ol> <li>Ridge Regression</li> </ol> </li> <li> <ol> <li>Lasso Regression</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#autonomous-systems-auton","title":"Autonomous Systems (AUTON)","text":"<p>Count: 20 concepts (6.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>LIDAR Point Cloud</li> </ol> </li> <li> <ol> <li>Camera Calibration</li> </ol> </li> <li> <ol> <li>Sensor Fusion</li> </ol> </li> <li> <ol> <li>Kalman Filter</li> </ol> </li> <li> <ol> <li>State Vector</li> </ol> </li> <li> <ol> <li>Measurement Vector</li> </ol> </li> <li> <ol> <li>Prediction Step</li> </ol> </li> <li> <ol> <li>Update Step</li> </ol> </li> <li> <ol> <li>Kalman Gain</li> </ol> </li> <li> <ol> <li>Extended Kalman Filter</li> </ol> </li> <li> <ol> <li>State Estimation</li> </ol> </li> <li> <ol> <li>SLAM</li> </ol> </li> <li> <ol> <li>Localization</li> </ol> </li> <li> <ol> <li>Mapping</li> </ol> </li> <li> <ol> <li>Object Detection</li> </ol> </li> <li>...and 5 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#decompositions-decomp","title":"Decompositions (DECOMP)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Matrix Factorization</li> </ol> </li> <li> <ol> <li>LU Decomposition</li> </ol> </li> <li> <ol> <li>Partial Pivoting</li> </ol> </li> <li> <ol> <li>QR Decomposition</li> </ol> </li> <li> <ol> <li>Gram-Schmidt QR</li> </ol> </li> <li> <ol> <li>Householder QR</li> </ol> </li> <li> <ol> <li>Cholesky Decomposition</li> </ol> </li> <li> <ol> <li>Positive Definite Matrix</li> </ol> </li> <li> <ol> <li>SVD</li> </ol> </li> <li> <ol> <li>Singular Value</li> </ol> </li> <li> <ol> <li>Left Singular Vector</li> </ol> </li> <li> <ol> <li>Right Singular Vector</li> </ol> </li> <li> <ol> <li>Full SVD</li> </ol> </li> <li> <ol> <li>Compact SVD</li> </ol> </li> <li> <ol> <li>Truncated SVD</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#inner-products-inprod","title":"Inner Products (INPROD)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Abstract Vector Space</li> </ol> </li> <li> <ol> <li>Subspace</li> </ol> </li> <li> <ol> <li>Vector Space Axioms</li> </ol> </li> <li> <ol> <li>Inner Product</li> </ol> </li> <li> <ol> <li>Inner Product Space</li> </ol> </li> <li> <ol> <li>Norm from Inner Product</li> </ol> </li> <li> <ol> <li>Cauchy-Schwarz Inequality</li> </ol> </li> <li> <ol> <li>Orthogonality</li> </ol> </li> <li> <ol> <li>Orthogonal Vectors</li> </ol> </li> <li> <ol> <li>Orthonormal Set</li> </ol> </li> <li> <ol> <li>Orthonormal Basis</li> </ol> </li> <li> <ol> <li>Gram-Schmidt Process</li> </ol> </li> <li> <ol> <li>Projection onto Subspace</li> </ol> </li> <li> <ol> <li>Least Squares Problem</li> </ol> </li> <li> <ol> <li>Normal Equations</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#generative-ai-genai","title":"Generative AI (GENAI)","text":"<p>Count: 19 concepts (6.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Embedding</li> </ol> </li> <li> <ol> <li>Embedding Space</li> </ol> </li> <li> <ol> <li>Word Embedding</li> </ol> </li> <li> <ol> <li>Semantic Similarity</li> </ol> </li> <li> <ol> <li>Cosine Similarity</li> </ol> </li> <li> <ol> <li>Attention Mechanism</li> </ol> </li> <li> <ol> <li>Self-Attention</li> </ol> </li> <li> <ol> <li>Cross-Attention</li> </ol> </li> <li> <ol> <li>Query Matrix</li> </ol> </li> <li> <ol> <li>Key Matrix</li> </ol> </li> <li> <ol> <li>Value Matrix</li> </ol> </li> <li> <ol> <li>Attention Score</li> </ol> </li> <li> <ol> <li>Attention Weights</li> </ol> </li> <li> <ol> <li>Multi-Head Attention</li> </ol> </li> <li> <ol> <li>Transformer Architecture</li> </ol> </li> <li>...and 4 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#eigentheory-eigen","title":"Eigentheory (EIGEN)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Eigenvalue</li> </ol> </li> <li> <ol> <li>Eigenvector</li> </ol> </li> <li> <ol> <li>Eigen Equation</li> </ol> </li> <li> <ol> <li>Characteristic Polynomial</li> </ol> </li> <li> <ol> <li>Characteristic Equation</li> </ol> </li> <li> <ol> <li>Eigenspace</li> </ol> </li> <li> <ol> <li>Algebraic Multiplicity</li> </ol> </li> <li> <ol> <li>Geometric Multiplicity</li> </ol> </li> <li> <ol> <li>Diagonalization</li> </ol> </li> <li> <ol> <li>Diagonal Form</li> </ol> </li> <li> <ol> <li>Similar Matrices</li> </ol> </li> <li> <ol> <li>Complex Eigenvalue</li> </ol> </li> <li> <ol> <li>Spectral Theorem</li> </ol> </li> <li> <ol> <li>Symmetric Eigenvalues</li> </ol> </li> <li> <ol> <li>Power Iteration</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#3d-geometry-geom3d","title":"3D Geometry (GEOM3D)","text":"<p>Count: 17 concepts (5.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>3D Coordinate System</li> </ol> </li> <li> <ol> <li>Euler Angles</li> </ol> </li> <li> <ol> <li>Gimbal Lock</li> </ol> </li> <li> <ol> <li>Quaternion</li> </ol> </li> <li> <ol> <li>Quaternion Rotation</li> </ol> </li> <li> <ol> <li>Homogeneous Coordinates</li> </ol> </li> <li> <ol> <li>Rigid Body Transform</li> </ol> </li> <li> <ol> <li>SE3 Transform</li> </ol> </li> <li> <ol> <li>Camera Matrix</li> </ol> </li> <li> <ol> <li>Intrinsic Parameters</li> </ol> </li> <li> <ol> <li>Extrinsic Parameters</li> </ol> </li> <li> <ol> <li>Projection Matrix</li> </ol> </li> <li> <ol> <li>Perspective Projection</li> </ol> </li> <li> <ol> <li>Stereo Vision</li> </ol> </li> <li> <ol> <li>Triangulation</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#image-processing-imgproc","title":"Image Processing (IMGPROC)","text":"<p>Count: 16 concepts (5.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Image Matrix</li> </ol> </li> <li> <ol> <li>Grayscale Image</li> </ol> </li> <li> <ol> <li>RGB Image</li> </ol> </li> <li> <ol> <li>Image Tensor</li> </ol> </li> <li> <ol> <li>Image Convolution</li> </ol> </li> <li> <ol> <li>Image Filter</li> </ol> </li> <li> <ol> <li>Blur Filter</li> </ol> </li> <li> <ol> <li>Sharpen Filter</li> </ol> </li> <li> <ol> <li>Edge Detection</li> </ol> </li> <li> <ol> <li>Sobel Operator</li> </ol> </li> <li> <ol> <li>Fourier Transform</li> </ol> </li> <li> <ol> <li>Frequency Domain</li> </ol> </li> <li> <ol> <li>Image Compression</li> </ol> </li> <li> <ol> <li>Color Space Transform</li> </ol> </li> <li> <ol> <li>Feature Detection</li> </ol> </li> <li>...and 1 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#optimization-optim","title":"Optimization (OPTIM)","text":"<p>Count: 14 concepts (4.7%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Hessian Matrix</li> </ol> </li> <li> <ol> <li>Convexity</li> </ol> </li> <li> <ol> <li>Convex Function</li> </ol> </li> <li> <ol> <li>Newtons Method</li> </ol> </li> <li> <ol> <li>Quasi-Newton Method</li> </ol> </li> <li> <ol> <li>BFGS Algorithm</li> </ol> </li> <li> <ol> <li>SGD</li> </ol> </li> <li> <ol> <li>Mini-Batch SGD</li> </ol> </li> <li> <ol> <li>Momentum</li> </ol> </li> <li> <ol> <li>Adam Optimizer</li> </ol> </li> <li> <ol> <li>RMSprop</li> </ol> </li> <li> <ol> <li>Lagrange Multiplier</li> </ol> </li> <li> <ol> <li>Constrained Optimization</li> </ol> </li> <li> <ol> <li>KKT Conditions</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#determinants-determ","title":"Determinants (DETERM)","text":"<p>Count: 13 concepts (4.3%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Determinant</li> </ol> </li> <li> <ol> <li>2x2 Determinant</li> </ol> </li> <li> <ol> <li>3x3 Determinant</li> </ol> </li> <li> <ol> <li>Cofactor Expansion</li> </ol> </li> <li> <ol> <li>Minor</li> </ol> </li> <li> <ol> <li>Cofactor</li> </ol> </li> <li> <ol> <li>Determinant Properties</li> </ol> </li> <li> <ol> <li>Multiplicative Property</li> </ol> </li> <li> <ol> <li>Transpose Determinant</li> </ol> </li> <li> <ol> <li>Singular Matrix</li> </ol> </li> <li> <ol> <li>Volume Scaling Factor</li> </ol> </li> <li> <ol> <li>Signed Area</li> </ol> </li> <li> <ol> <li>Cramers Rule</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Excellent balance: Categories are evenly distributed (spread: 4.7%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"old-sections/","title":"\ud83d\ude80 Navigate the Textbook","text":""},{"location":"old-sections/#section-i-theoretical-foundations-algebraic-structures","title":"\ud83d\udcda Section I: Theoretical Foundations &amp; Algebraic Structures","text":"<p>Explore the core language of linear algebra\u2014from vectors and matrices to subspaces and eigenvalues. You'll build the theoretical intuition necessary to model and analyze real systems. Highlights: Linear transformations, eigenanalysis, vector spaces, inner products, orthogonality.</p>"},{"location":"old-sections/#section-ii-numerical-linear-algebra-scientific-computing","title":"\ud83d\udcbb Section II: Numerical Linear Algebra &amp; Scientific Computing","text":"<p>Shift from abstract concepts to computation. This section focuses on algorithms, decompositions, and numerical stability\u2014critical for scientific computing and simulations. Highlights: LU and QR factorizations, iterative solvers, condition numbers, sparse matrices, SVD.</p>"},{"location":"old-sections/#section-iii-control-systems-electrical-engineering","title":"\u2699\ufe0f Section III: Control Systems &amp; Electrical Engineering","text":"<p>Dive into state-space modeling, controllability, observability, and feedback design\u2014all powered by matrix techniques. Essential for students working in dynamic systems and control theory. Highlights: Kalman decomposition, LQR, matrix exponentiation, model reduction.</p>"},{"location":"old-sections/#section-iv-signal-processing-graph-theory","title":"\ud83d\udd0a Section IV: Signal Processing &amp; Graph Theory","text":"<p>Uncover how linear algebra supports signal transforms, frequency analysis, and network modeling. Graph Laplacians and spectral methods will reveal patterns and enable new optimizations. Highlights: FFT, graph Laplacians, spectral clustering, convolution, flows, geometric transforms.</p>"},{"location":"old-sections/#section-v-data-science-machine-learning","title":"\ud83e\udd16 Section V: Data Science &amp; Machine Learning","text":"<p>Harness linear algebra for machine learning and AI. From PCA and neural networks to optimization and kernel methods, this section gives you the tools to shape data into insight. Highlights: Gradient descent, eigenfaces, kernel tricks, collaborative filtering, SVMs.</p>"},{"location":"old-sections/section-1/section-1/","title":"\ud83d\udcda Section I: Theoretical Foundations &amp; Algebraic Structures","text":"<p>Overview: This section introduces the essential theoretical pillars of linear algebra, equipping students with a rigorous and practical understanding of matrices, vectors, and transformations. Students will build an intuitive and formal foundation that enables the application of linear algebra across computer science and electrical engineering domains.</p>"},{"location":"old-sections/section-1/section-1/#chapter-1-foundations-of-linear-algebra","title":"Chapter 1: Foundations of Linear Algebra","text":"<ul> <li>Key Concepts: Scalars, vectors, matrices, systems of linear equations.</li> <li>Focus: Understand basic objects and how linear systems are expressed and solved mathematically.</li> <li>Skills: Visualize matrices and vectors as fundamental building blocks of computation.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-2-matrix-operations-and-properties","title":"Chapter 2: Matrix Operations and Properties","text":"<ul> <li>Key Concepts: Matrix addition, subtraction, scalar multiplication, matrix multiplication, transpose, identity matrix, zero matrix, and special matrices (diagonal, symmetric, triangular, block matrices).</li> <li>Focus: Master the algebra of matrices and recognize patterns in matrix structure.</li> <li>Skills: Perform and simplify complex matrix operations.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-3-vector-spaces-and-subspaces","title":"Chapter 3: Vector Spaces and Subspaces","text":"<ul> <li>Key Concepts: Vector spaces, subspaces, span, basis, dimension, row space, column space, null space.</li> <li>Focus: Understand spaces generated by vectors and their structural properties.</li> <li>Skills: Classify subspaces and analyze dimensions of solutions to linear systems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-4-linear-independence-and-rank","title":"Chapter 4: Linear Independence and Rank","text":"<ul> <li>Key Concepts: Linear independence, dependence, rank of a matrix, row rank = column rank theorem, rank-nullity theorem.</li> <li>Focus: Explore the relationships between vector combinations, solvability, and matrix structure.</li> <li>Skills: Determine matrix rank and diagnose solution behaviors of linear systems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-5-inner-products-and-orthogonality","title":"Chapter 5: Inner Products and Orthogonality","text":"<ul> <li>Key Concepts: Inner product, norm, distance between vectors, orthogonality, orthogonal projections, orthogonal complement, orthonormal basis, Gram-Schmidt process.</li> <li>Focus: Extend the geometric view of vectors to orthogonality and projection concepts.</li> <li>Skills: Construct orthonormal bases and apply orthogonal projections in applications like least squares problems.</li> </ul>"},{"location":"old-sections/section-1/section-1/#chapter-6-linear-transformations-and-eigenanalysis","title":"Chapter 6: Linear Transformations and Eigenanalysis","text":"<ul> <li>Key Concepts: Linear transformations, matrix representation, kernel, image, change of basis, similar matrices, eigenvalues, eigenvectors, characteristic polynomial, eigenspaces, algebraic and geometric multiplicities.</li> <li>Focus: Translate between abstract transformations and their matrix representations; study intrinsic properties through eigenanalysis.</li> <li>Skills: Diagonalize matrices, compute eigenvalues and eigenvectors, and understand the fundamental significance of spectral properties.</li> </ul>"},{"location":"old-sections/section-1/section-1/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Comprehend the structures and operations central to linear algebra. - Analyze matrix behaviors and subspace relationships. - Apply transformations and eigenvalue techniques in practical settings. - Prepare for advanced topics in numerical methods, control systems, signal processing, and machine learning.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/","title":"\ud83d\udcd8 Chapter 1: Foundations of Linear Algebra","text":"<p>Welcome to the beginning of your journey into linear algebra! This chapter builds the conceptual cornerstones you'll need for everything from circuit design to machine learning. Here, we'll introduce and deeply understand the most fundamental objects: scalars, vectors, matrices, and systems of linear equations.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#11-scalars","title":"1.1 Scalars","text":"<p>A scalar is a single number. Think of it as a single point on a number line \u2014 it could represent temperature, voltage, or the size of a physical quantity.</p> <p>Example: \\( 7 \\), \\( -3.2 \\), and \\( \\pi \\) are all scalars.</p> <p>Why are scalars important? Scalars serve as the \"units\" of information in linear algebra. They're the building blocks that scale vectors and matrices. If vectors are arrows, then scalars are the fuel that make arrows longer or flip them backwards!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#12-vectors","title":"1.2 Vectors","text":"<p>A vector is an ordered list of numbers, arranged either horizontally (a row vector) or vertically (a column vector).</p> <p>Example: A column vector: $$ \\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\end{bmatrix} $$</p> <p>Visualizing Vectors: Imagine a vector as an arrow pointing from the origin to a point in space. Each number tells you how far to move along each axis (like \"walk 2 units east, 1 unit south, and 3 units up\").</p> <p>Key Properties: - Vectors have both magnitude and direction. - Vectors can represent physical quantities like force, velocity, or electric fields.</p> <p>Tip: Vectors don't have a location, just a direction and magnitude. Two identical arrows placed differently are still the same vector!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#13-matrices","title":"1.3 Matrices","text":"<p>A matrix is a rectangular grid of numbers organized into rows and columns.</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} $$ This matrix has 2 rows and 3 columns, so we say it's a \\( 2 \\times 3 \\) matrix.</p> <p>Why matrices? Matrices organize and transform data. They can: - Represent multiple linear equations compactly. - Describe how to rotate, stretch, or shrink objects. - Store pixel data in images, weights in neural networks, and much more!</p> <p>Creative Analogy: Think of matrices like filters: you feed in an input (vector), and the matrix transforms it into a new output (another vector). Just like a coffee filter shapes your coffee's flavor, a matrix shapes your data!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#14-systems-of-linear-equations-sles","title":"1.4 Systems of Linear Equations (SLEs)","text":"<p>A system of linear equations is a collection of equations where each term is either a constant or a constant times a variable.</p> <p>Example: $$ \\begin{aligned} 2x + 3y &amp;= 5 \\ 4x - y &amp;= 1 \\end{aligned} $$</p> <p>How do we express this system using matrices? Let's define: - Matrix of coefficients \\( A \\): $$ \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; -1 \\end{bmatrix} $$ - Vector of variables \\( \\mathbf{x} \\): $$ \\begin{bmatrix} x \\\\ y \\end{bmatrix} $$ - Vector of constants \\( \\mathbf{b} \\): $$ \\begin{bmatrix} 5 \\\\ 1 \\end{bmatrix} $$</p> <p>Then the system becomes: $$ A\\mathbf{x} = \\mathbf{b} $$</p> <p>Why express it this way? Because matrices allow us to efficiently solve systems \u2014 using algorithms like Gaussian elimination or matrix inverses, and they make scaling to large problems possible.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#15-understanding-solutions-to-sles","title":"1.5 Understanding Solutions to SLEs","text":"<p>Depending on the system, there are three possibilities:</p> <ol> <li>One unique solution \u2014 the system is consistent and independent (like two lines crossing at one point).</li> <li>Infinitely many solutions \u2014 the system is dependent (the lines are the same, overlapping entirely).</li> <li>No solution \u2014 the system is inconsistent (the lines are parallel and never meet).</li> </ol> <p>Visual Metaphor: Imagine two roads. They can: - Cross once (one solution), - Overlap perfectly (infinite solutions), - Never touch (no solution).</p> <p>Understanding when and why these happen is foundational for everything else in linear algebra.</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#16-building-toward-whats-next","title":"1.6 Building Toward What's Next","text":"<p>In later chapters, we'll: - Manipulate matrices algebraically, - Understand what spaces vectors create, - Analyze systems deeply through rank, projections, and transformations.</p> <p>Knowing what scalars, vectors, and matrices are \u2014 and how they express systems \u2014 is the bedrock that supports all these future ideas.</p> <p>Mastering this chapter ensures you\u2019re ready to see the world through the lens of linear algebra \u2014 an essential tool for modern engineering and computing!</p>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, we learned:</p> <ul> <li>Scalars are single numbers.</li> <li>Vectors are ordered lists of scalars representing direction and magnitude.</li> <li>Matrices are grids of numbers that can transform vectors.</li> <li>Systems of linear equations model relationships using these structures.</li> <li>Solutions to systems reveal deep information about how equations interact.</li> </ul>"},{"location":"old-sections/section-1/chapter-1/chapter-1/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which of the following systems of equations has no solution, and why?</p> <p>A. A consistent system with full rank</p> <p>B. A system where the augmented matrix has a pivot in every row</p> <p>C. A system where two rows of the augmented matrix are contradictory</p> <p>D. A homogeneous system</p> Show Answer <p>The correct answer is C. A system where two rows of the augmented matrix are contradictory (e.g., one row says \\( 0x + 0y = 5 \\)) indicates inconsistency \u2014 meaning no solution exists.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/","title":"\ud83d\udcd8 Chapter 2: Matrix Operations and Properties","text":"<p>In Chapter 1, we discovered scalars, vectors, matrices, and systems of linear equations. Now, we dive into how matrices interact with each other through operations like addition, multiplication, and transposition, and learn about special types of matrices that have unique, powerful properties.</p> <p>Mastering these operations gives you the toolkit to manipulate data structures, solve complex systems, and build mathematical models of the real world!</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#21-matrix-addition-and-subtraction","title":"2.1 Matrix Addition and Subtraction","text":"<p>Matrices of the same dimensions can be added or subtracted by simply combining their corresponding entries.</p> <p>Example: $$ A =  \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} $$</p> <p>Then: $$ A + B = \\begin{bmatrix} 1+5 &amp; 2+6 \\\\ 3+7 &amp; 4+8 \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{bmatrix} $$</p> <p>Important: - Dimensions must match! You can't add a \\(2 \\times 3\\) matrix to a \\(3 \\times 2\\) matrix.</p> <p>Why Addition/Subtraction? When combining two datasets (e.g., overlaying two images or merging network connections), matrix addition and subtraction help accumulate or compare their contents.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#22-scalar-multiplication","title":"2.2 Scalar Multiplication","text":"<p>Multiplying a matrix by a scalar means multiplying every entry by that scalar.</p> <p>Example: $$ 3A = \\begin{bmatrix} 3\\times1 &amp; 3\\times2 \\\\ 3\\times3 &amp; 3\\times4 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 6 \\\\ 9 &amp; 12 \\end{bmatrix} $$</p> <p>Analogy: Imagine brightening an image: multiplying every pixel's intensity by 3. Scalar multiplication scales the entire matrix.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#23-matrix-multiplication","title":"2.3 Matrix Multiplication","text":"<p>Matrix multiplication is more involved \u2014 but much more powerful.</p> <p>To multiply \\(A\\) (\\(m\\times n\\)) by \\(B\\) (\\(n\\times p\\)): - The number of columns of \\(A\\) must equal the number of rows of \\(B\\). - The result is an \\(m\\times p\\) matrix.</p> <p>How to Multiply: Each entry in the product matrix is the dot product of a row of \\(A\\) and a column of \\(B\\).</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} $$</p> <p>Compute: $$ AB = \\begin{bmatrix} (1)(5)+(2)(7) &amp; (1)(6)+(2)(8) \\\\ (3)(5)+(4)(7) &amp; (3)(6)+(4)(8) \\end{bmatrix} = \\begin{bmatrix} 19 &amp; 22 \\\\ 43 &amp; 50 \\end{bmatrix} $$</p> <p>Key Observations: - Matrix multiplication is not commutative: \\( AB \\neq BA \\) generally. - Associative: \\( (AB)C = A(BC) \\). - Distributive: \\( A(B+C) = AB + AC \\).</p> <p>Creative Analogy: Think of matrices as machines: passing a vector through matrix \\(A\\) and then matrix \\(B\\) is different from passing it through matrix \\(B\\) first!</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#24-the-transpose","title":"2.4 The Transpose","text":"<p>The transpose of a matrix \\(A\\), denoted \\(A^T\\), is created by flipping rows into columns.</p> <p>Example: $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\quad \\Rightarrow \\quad A^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix} $$</p> <p>Why Transpose? - It rearranges information. - Essential for symmetries, simplifying matrix equations, and working with inner products.</p> <p>Tip: If \\(A\\) is a \\(m\\times n\\) matrix, then \\(A^T\\) is \\(n\\times m\\).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#25-special-matrices","title":"2.5 Special Matrices","text":"<p>Certain matrices are particularly important because of their simplicity and properties:</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#identity-matrix-i","title":"Identity Matrix \\(I\\)","text":"<p>An identity matrix acts like the number 1 in multiplication.</p> <p>Example: $$ I_3 = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$</p> <p>Properties: - \\(AI = IA = A\\) for any compatible matrix \\(A\\). - Solving systems often involves creating or approximating \\(I\\).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#zero-matrix","title":"Zero Matrix","text":"<p>All entries are 0. It's the additive identity: $$ (A + 0 = A) $$ $$ (A - 0 = A) $$</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>Only nonzero entries are on the main diagonal.</p> <p>Example: $$ D = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 5 &amp; 0 \\\\ 0 &amp; 0 &amp; 7 \\end{bmatrix} $$</p> <p>Why Important? Diagonal matrices are incredibly easy to compute with: - Multiplying a diagonal matrix by a vector just scales each coordinate. - Finding powers of a diagonal matrix is just raising diagonal entries to powers.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>A matrix \\(A\\) is symmetric if: $$ A^T = A $$</p> <p>Example: $$ \\begin{bmatrix} 1 &amp; 3 \\\\ 3 &amp; 2 \\end{bmatrix} $$</p> <p>Symmetric matrices arise naturally when modeling undirected relationships (like undirected graphs).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#triangular-matrices","title":"Triangular Matrices","text":"<ul> <li>Upper Triangular: All entries below the main diagonal are zero.</li> <li>Lower Triangular: All entries above the main diagonal are zero.</li> </ul> <p>Uses: Critical for simplifying solving systems (e.g., in LU decomposition).</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#block-matrices","title":"Block Matrices","text":"<p>Sometimes matrices are better thought of as blocks of smaller matrices.</p> <p>Example: $$ \\begin{bmatrix} A &amp; B \\\\ C &amp; D \\end{bmatrix} $$</p> <p>where \\(A, B, C, D\\) themselves are matrices.</p> <p>Why Blocks? When dealing with large systems, breaking them into blocks makes operations manageable and efficient.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>How matrices add, subtract, and multiply.</li> <li>That matrix multiplication follows specific dimension rules and is not commutative.</li> <li>What the transpose of a matrix is and why it's important.</li> <li>Key special matrices: identity, zero, diagonal, symmetric, triangular, and block matrices.</li> </ul> <p>These operations are the essential \"verbs\" of linear algebra \u2014 they let us speak the language of systems, transformations, and data flows.</p>"},{"location":"old-sections/section-1/chapter-2/chapter-2/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which property does matrix multiplication NOT generally satisfy?</p> <p>A. Associativity</p> <p>B. Distributivity over addition</p> <p>C. Commutativity</p> <p>D. Compatibility with scalar multiplication</p> Show Answer <p>The correct answer is C. Matrix multiplication is not commutative \u2014 that is, in general, \\( AB \\neq BA \\). However, it is associative and distributive, and scalar multiplication is compatible.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/","title":"\ud83d\udcd8 Chapter 3: Vector Spaces and Subspaces","text":"<p>In Chapters 1 and 2, we explored scalars, vectors, matrices, and how to operate on them. Now, we shift our focus to something deeper: spaces built from vectors.</p> <p>This chapter unlocks a profound idea: vectors aren't isolated \u2014 they live in vast, structured universes called vector spaces. Understanding these spaces is essential for everything from solving systems of equations to building machine learning models.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#31-what-is-a-vector-space","title":"3.1 What is a Vector Space?","text":"<p>A vector space is a collection of vectors where you can: - Add any two vectors and stay inside the space. - Scale any vector by a scalar and stay inside the space.</p> <p>In short: add and stretch without leaving.</p> <p>Formal Definition: A set \\(V\\) is a vector space over a field (like \\(\\mathbb{R}\\) for real numbers) if it satisfies: - Closure under addition and scalar multiplication, - Existence of a zero vector (an \"origin\"), - Existence of additive inverses (every vector has a \"negative\" vector), - and other technical properties (like associativity, distributivity).</p> <p>Example 1: All 2D vectors \\((x, y)\\) where \\(x, y\\in\\mathbb{R}\\) form a vector space: \\(\\mathbb{R}^2\\).</p> <p>Example 2: All 3D vectors \\((x, y, z)\\) where \\(x, y, z\\in\\mathbb{R}\\) form \\(\\mathbb{R}^3\\).</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#32-subspaces","title":"3.2 Subspaces","text":"<p>A subspace is simply a vector space within another vector space.</p> <p>Criteria for a Subspace: A subset \\(W\\) of a vector space \\(V\\) is a subspace if: 1. The zero vector is in \\(W\\), 2. \\(W\\) is closed under addition, 3. \\(W\\) is closed under scalar multiplication.</p> <p>Example: In \\(\\mathbb{R}^3\\), the set of all vectors of the form \\((x, 0, 0)\\) (i.e., lying on the \\(x\\)-axis) is a subspace.</p> <p>Why Subspaces Matter: Subspaces capture constrained movement: movement along a line, inside a plane, or through a lower-dimensional world embedded inside a bigger space.</p> <p>Creative Analogy: Imagine a vector space like a giant 3D room. Subspaces are like wires, sheets, or corners inside the room where all action is confined!</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#33-span","title":"3.3 Span","text":"<p>The span of a set of vectors is the smallest subspace containing them \u2014 it's everything you can build by adding and scaling those vectors.</p> <p>Example: If you have vectors: $$ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} , \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ in \\(\\mathbb{R}^2\\), then their span is the entire plane \\(\\mathbb{R}^2\\).</p> <p>Why Span? When you ask, \"What directions can I move using just these vectors?\" \u2014 you are really asking about the span.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#34-basis-and-dimension","title":"3.4 Basis and Dimension","text":"<p>A basis is a minimal set of vectors that: - Span the space, - Are linearly independent (none of them is redundant).</p> <p>Example: $$ { \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} } $$ is a basis for \\(\\mathbb{R}^2\\).</p> <p>Dimension: The dimension of a vector space is the number of vectors in any basis.</p> <p>Thus: - \\(\\mathbb{R}^2\\) has dimension 2, - \\(\\mathbb{R}^3\\) has dimension 3, - A line through the origin in \\(\\mathbb{R}^3\\) has dimension 1.</p> <p>Creative Analogy: If the vector space is a world, the basis vectors are its fundamental directions \u2014 the minimum GPS instructions you need to navigate everywhere!</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#35-row-space-column-space-and-null-space","title":"3.5 Row Space, Column Space, and Null Space","text":"<p>When dealing with a matrix \\(A\\), we encounter three critical subspaces:</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#row-space","title":"Row Space","text":"<p>The space spanned by the rows of \\(A\\).</p> <ul> <li>It tells us about relationships among equations.</li> <li>Lives in \\(\\mathbb{R}^n\\) if \\(A\\) has \\(n\\) columns.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#column-space","title":"Column Space","text":"<p>The space spanned by the columns of \\(A\\).</p> <ul> <li>It represents all possible outputs of the system \\(A\\mathbf{x}\\).</li> <li>Lives in \\(\\mathbb{R}^m\\) if \\(A\\) has \\(m\\) rows.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#null-space-kernel","title":"Null Space (Kernel)","text":"<p>The set of all vectors \\(\\mathbf{x}\\) such that: $$ A\\mathbf{x} = \\mathbf{0} $$</p> <ul> <li>It captures all the \"invisible\" directions \u2014 inputs that produce zero output.</li> <li>Tells us about solutions to homogeneous systems.</li> </ul>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#36-why-this-chapter-matters","title":"3.6 Why This Chapter Matters","text":"<p>Understanding vector spaces and subspaces helps you:</p> <ul> <li>Solve systems more intelligently (using dimensions and bases),</li> <li>Analyze models (like feature spaces in machine learning),</li> <li>Simplify problems (by recognizing redundancies and patterns).</li> </ul> <p>Link to Previous Chapters: Everything we've learned about vectors and matrices now deepens: Instead of just manipulating individual objects, you begin to analyze entire worlds formed by them.</p> <p>Forward Connection: In the next chapter, we'll ask how to tell if vectors are independent or redundant \u2014 leading us into linear independence and rank.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Vector spaces are collections of vectors closed under addition and scaling.</li> <li>Subspaces are smaller spaces inside vector spaces, satisfying similar rules.</li> <li>The span of a set of vectors is all combinations you can build from them.</li> <li>A basis is a minimal spanning set; the dimension counts its vectors.</li> <li>Matrices naturally give rise to row space, column space, and null space.</li> </ul> <p>These ideas create a bridge from basic matrix operations to the beautiful architecture of linear systems and transformations.</p>"},{"location":"old-sections/section-1/chapter-3/chapter-3/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>Which of the following sets is a subspace of \u211d\u00b3?</p> <p>A. All vectors with \\(x + y + z = 1\\)</p> <p>B. All vectors with \\(x = y = z\\)</p> <p>C. All vectors with \\(x^2 + y^2 + z^2 &lt; 1\\)</p> <p>D. All unit vectors in \u211d\u00b3</p> Show Answer <p>The correct answer is B. The set of all vectors where \\(x = y = z\\) is a subspace: it contains the zero vector, is closed under addition, and is closed under scalar multiplication. Sets defined by non-homogeneous conditions (like \\(x + y + z = 1\\)) or norm constraints (like unit vectors) are not subspaces.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/","title":"\ud83d\udcd8 Chapter 4: Linear Independence and Rank","text":"<p>Building on our understanding of vector spaces and subspaces, we now turn to an important question:  </p> <p>When are vectors truly \"different\" from each other?</p> <p>This chapter focuses on linear independence, dependence, and the crucial matrix property known as rank. These ideas are the keys to understanding when systems have unique solutions, how much information a set of vectors carries, and how efficient our models are.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#41-linear-independence","title":"4.1 Linear Independence","text":"<p>Definition: Vectors are linearly independent if none of them can be written as a combination of the others.</p> <p>Formal Test: Vectors \\( \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n \\) are independent if the only solution to: $$ c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2 + \\dots + c_n\\mathbf{v}_n = \\mathbf{0} $$ is: $$ c_1 = c_2 = \\dots = c_n = 0 $$</p> <p>Otherwise, they are dependent.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#intuitive-picture","title":"Intuitive Picture","text":"<ul> <li>Independent Vectors: Each vector points in a new direction not explained by others.</li> <li>Dependent Vectors: Some vectors \"retrace\" paths created by others.</li> </ul> <p>Creative Analogy: Imagine you're giving directions: - Independent directions are like adding truly new turns. - Dependent directions are like repeating roads you've already traveled!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#simple-examples","title":"Simple Examples","text":"<p>Independent Example: In \\(\\mathbb{R}^2\\), $$ \\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ are independent \u2014 they point along different axes.</p> <p>Dependent Example: $$ \\mathbf{v}_1 = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}, \\quad \\mathbf{v}_2 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $$ are dependent because \\( \\mathbf{v}_1 = 2\\mathbf{v}_2 \\).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#42-the-importance-of-linear-independence","title":"4.2 The Importance of Linear Independence","text":"<ul> <li> <p>Solving Systems:   Independent vectors correspond to systems with unique solutions.</p> </li> <li> <p>Model Building:   In machine learning or statistics, independent features avoid redundancy and boost performance.</p> </li> <li> <p>Efficiency:   Fewer independent vectors mean smaller models without losing information.</p> </li> </ul>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#43-rank-of-a-matrix","title":"4.3 Rank of a Matrix","text":"<p>The rank of a matrix is the dimension of its row space (or equivalently, the column space).</p> <p>In simple terms: - Rank counts the number of truly independent rows or columns.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#finding-rank","title":"Finding Rank","text":"<p>You can find the rank of a matrix by: 1. Row reducing it to Row Echelon Form or Reduced Row Echelon Form (RREF), 2. Counting the number of leading 1s (pivots).</p> <p>Example:</p> <p>Matrix \\(A\\): $$ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\\\ 3 &amp; 6 &amp; 9 \\end{bmatrix} $$</p> <p>Row reducing: $$ \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{bmatrix} $$</p> <p>Thus, \\(\\text{rank}(A) = 1\\).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#key-properties-of-rank","title":"Key Properties of Rank","text":"<ul> <li>\\(\\text{rank}(A) \\leq \\min(m,n)\\) for an \\(m \\times n\\) matrix.</li> <li>If \\(\\text{rank}(A) = n\\) (number of columns), the columns are independent.</li> <li>If \\(\\text{rank}(A) = m\\) (number of rows), the rows are independent.</li> </ul>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#44-row-rank-column-rank","title":"4.4 Row Rank = Column Rank","text":"<p>One of the most beautiful facts in linear algebra:</p> <p>The dimension of the row space equals the dimension of the column space.</p> <p>Thus, row rank = column rank, and we simply call it the rank.</p> <p>Why is this surprising? Rows and columns \"live\" in different spaces \u2014 yet their structure carries the same amount of independent information!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#45-the-rank-nullity-theorem","title":"4.5 The Rank-Nullity Theorem","text":"<p>This elegant theorem relates: - Rank (number of independent columns), - Nullity (dimension of the null space, i.e., number of free variables).</p> <p>Theorem: $$ \\text{rank}(A) + \\text{nullity}(A) = n $$ where \\(n\\) = number of columns.</p> <p>Meaning: Every variable in a system either: - Contributes to a pivot (making the system more constrained), or - Becomes a free variable (adding flexibility).</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#46-why-this-chapter-matters","title":"4.6 Why This Chapter Matters","text":"<p>Understanding independence and rank allows you to:</p> <ul> <li>Diagnose whether a system has a unique solution, infinite solutions, or no solution,</li> <li>Detect redundancy in your models,</li> <li>Simplify matrices for faster computation.</li> </ul> <p>Building on Earlier Ideas: Now we aren't just describing what vectors and matrices are \u2014 we are judging how rich or constrained they are!</p> <p>What's Next: We'll soon build a geometric view of projections and orthogonality \u2014 using these ideas to minimize errors and find best fits.</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Linear independence means no vector can be made from others.</li> <li>Linear dependence reveals redundancy.</li> <li>The rank of a matrix counts independent rows/columns.</li> <li>Row rank and column rank are always equal.</li> <li>The Rank-Nullity Theorem ties together the dimensions of important matrix spaces.</li> </ul> <p>Mastering rank and independence is essential for unlocking the deeper structure hidden inside linear systems and transformations!</p>"},{"location":"old-sections/section-1/chapter-4/chapter-4/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>If a \\(4 \\times 5\\) matrix has rank 3, what is the dimension of its null space?</p> <p>A. 1</p> <p>B. 2</p> <p>C. 3</p> <p>D. 5</p> Show Answer <p>The correct answer is B. By the Rank-Nullity Theorem: $$ \\text{rank} + \\text{nullity} = \\text{number of columns} $$ So: $$ 3 + \\text{nullity} = 5 \\quad \\Rightarrow \\quad \\text{nullity} = 2 $$</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/","title":"\ud83d\udcd8 Chapter 5: Inner Products and Orthogonality","text":"<p>In the last chapter, we explored independence and rank \u2014 how vectors relate in terms of structure. Now, we expand our view to include geometry:  </p> <p>How can we measure angles, distances, and projections between vectors?</p> <p>This chapter introduces inner products, norms, and the powerful idea of orthogonality \u2014 essential for understanding projections, least squares methods, and so much more.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#51-the-inner-product","title":"5.1 The Inner Product","text":"<p>The inner product (also called the dot product) gives a way to \"multiply\" two vectors and produce a single number.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#definition","title":"Definition","text":"<p>For two vectors \\(\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n\\), $$ \\mathbf{u} \\cdot \\mathbf{v} = u_1v_1 + u_2v_2 + \\cdots + u_nv_n $$</p> <p>Example: $$ \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad \\mathbf{v} = \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} \\quad \\Rightarrow \\quad \\mathbf{u} \\cdot \\mathbf{v} = 1\\cdot4 + 2\\cdot5 + 3\\cdot6 = 32 $$</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>The inner product relates to the angle \\(\\theta\\) between vectors: $$ \\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}| |\\mathbf{v}| \\cos\\theta $$</p> <p>Thus: - If \\(\\cos\\theta &gt; 0\\), the vectors point generally in the same direction. - If \\(\\cos\\theta = 0\\), the vectors are orthogonal (perpendicular). - If \\(\\cos\\theta &lt; 0\\), the vectors point opposite directions.</p> <p>Creative Analogy: Think of the inner product as measuring how much two vectors \"agree\" in direction \u2014 like two people pushing an object: are they cooperating, conflicting, or independent?</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#52-norm-and-distance","title":"5.2 Norm and Distance","text":"<p>Using the inner product, we can define a norm, which measures the length (or magnitude) of a vector.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#norm-length","title":"Norm (Length)","text":"<p>The norm of \\(\\mathbf{v}\\) is: $$ |\\mathbf{v}| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $$</p> <p>Example: $$ |\\mathbf{v}| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{77} $$</p> <p>Why Norms Matter: Norms allow us to measure how big a vector is \u2014 crucial for computing distances, speeds, and energy.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#distance-between-vectors","title":"Distance Between Vectors","text":"<p>The distance between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) is: $$ |\\mathbf{u} - \\mathbf{v}| $$</p> <p>It's simply the length of the vector from \\(\\mathbf{u}\\) to \\(\\mathbf{v}\\).</p> <p>Applications: - In machine learning: measuring similarity between data points. - In physics: measuring displacement.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#53-orthogonality","title":"5.3 Orthogonality","text":"<p>Two vectors are orthogonal if: $$ \\mathbf{u} \\cdot \\mathbf{v} = 0 $$</p> <p>Geometric Meaning: They meet at a 90\u00b0 angle \u2014 they are completely independent directionally.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#why-orthogonality-is-powerful","title":"Why Orthogonality Is Powerful","text":"<ul> <li>Simplifies Computations: Orthogonal vectors are easier to work with \u2014 projections, decompositions, and optimizations become cleaner.</li> <li>Decouples Systems: In control systems or signal processing, orthogonal modes can be studied independently.</li> <li>Basis for Least Squares: Approximating solutions in \"best fit\" problems often uses orthogonality.</li> </ul>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#54-orthogonal-projections","title":"5.4 Orthogonal Projections","text":"<p>Sometimes, we want to project a vector onto another vector (or subspace).</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#projecting-onto-a-vector","title":"Projecting onto a Vector","text":"<p>Given a vector \\(\\mathbf{v}\\) and a unit vector \\(\\mathbf{u}\\), the projection of \\(\\mathbf{v}\\) onto \\(\\mathbf{u}\\) is: $$ \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = (\\mathbf{v} \\cdot \\mathbf{u}) \\mathbf{u} $$</p> <p>Creative Picture: Imagine shining a flashlight directly onto \\(\\mathbf{u}\\) \u2014 the \"shadow\" of \\(\\mathbf{v}\\) on \\(\\mathbf{u}\\) is the projection.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#example","title":"Example","text":"<p>Project \\(\\mathbf{v} = [3, 4]\\) onto \\(\\mathbf{u} = [1, 0]\\) (already a unit vector):</p> \\[ \\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = (3\\cdot1 + 4\\cdot0) \\times [1,0] = 3 [1,0] = [3,0] \\]"},{"location":"old-sections/section-1/chapter-5/chapter-5/#55-orthogonal-complement","title":"5.5 Orthogonal Complement","text":"<p>The orthogonal complement of a subspace \\(W\\) consists of all vectors orthogonal to every vector in \\(W\\).</p> <p>Notation: \\(W^\\perp\\)</p> <p>Importance: - It provides a way to \"complete\" spaces: everything not captured by \\(W\\) lies in \\(W^\\perp\\). - Essential for decomposition techniques like the Gram-Schmidt process.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#56-orthonormal-bases-and-gram-schmidt-process","title":"5.6 Orthonormal Bases and Gram-Schmidt Process","text":""},{"location":"old-sections/section-1/chapter-5/chapter-5/#orthonormal-basis","title":"Orthonormal Basis","text":"<p>A set of vectors is orthonormal if: - Each vector is a unit vector (\\(\\|\\mathbf{v}\\| = 1\\)), - Vectors are mutually orthogonal.</p> <p>Example: $$ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$ is an orthonormal basis for \\(\\mathbb{R}^2\\).</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#gram-schmidt-process","title":"Gram-Schmidt Process","text":"<p>The Gram-Schmidt process converts any basis into an orthonormal basis.</p> <p>Outline: 1. Start with a basis \\( \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots\\} \\). 2. Orthogonalize:    - Subtract projections onto earlier vectors. 3. Normalize:    - Scale to unit length.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#why-orthonormal-bases","title":"Why Orthonormal Bases?","text":"<ul> <li>Simplify calculations (e.g., finding coefficients in expansions).</li> <li>Critical for decompositions (QR decomposition, spectral methods).</li> <li>Basis for least squares approximations and many machine learning algorithms.</li> </ul>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>The inner product measures alignment between vectors.</li> <li>The norm measures the size of a vector; the distance measures separation.</li> <li>Orthogonal vectors are directionally independent.</li> <li>Projections help decompose vectors into components along subspaces.</li> <li>Orthonormal bases are simple, efficient building blocks.</li> <li>Gram-Schmidt provides a systematic method for orthogonalization.</li> </ul> <p>This geometric language makes linear algebra a powerful tool for modeling, optimization, and approximation.</p>"},{"location":"old-sections/section-1/chapter-5/chapter-5/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>What is the result of projecting vector \\( \\mathbf{b} \\) onto a unit vector \\( \\mathbf{u} \\)?</p> <p>A. \\( \\mathbf{b} \\cdot \\mathbf{u} \\)</p> <p>B. \\( (\\mathbf{b} \\cdot \\mathbf{u}) \\mathbf{u} \\)</p> <p>C. \\( \\mathbf{u} \\cdot \\mathbf{u} \\)</p> <p>D. \\( \\mathbf{b} \\cdot \\mathbf{b} \\)</p> Show Answer <p>The correct answer is B. The projection of \\( \\mathbf{b} \\) onto a unit vector \\( \\mathbf{u} \\) is \\((\\mathbf{b} \\cdot \\mathbf{u})\\mathbf{u}\\).</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/","title":"\ud83d\udcd8 Chapter 6: Linear Transformations and Eigenanalysis","text":"<p>We've explored matrices, vector spaces, independence, and orthogonality. Now we step into one of the most powerful ideas in linear algebra:  </p> <p>How do matrices act as machines that transform spaces?</p> <p>In this chapter, we'll study linear transformations, their matrix representations, and the profound concepts of eigenvalues and eigenvectors. These ideas form the mathematical core behind control systems, graphics transformations, machine learning models, and beyond.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#61-what-is-a-linear-transformation","title":"6.1 What is a Linear Transformation?","text":"<p>A linear transformation \\( T \\) is a rule that moves vectors around \u2014 but in a special, structure-preserving way.</p> <p>Formally, \\( T: \\mathbb{R}^n \\to \\mathbb{R}^m \\) is linear if:</p> <ol> <li>\\( T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) \\) (Additivity)</li> <li>\\( T(c\\mathbf{u}) = cT(\\mathbf{u}) \\) (Homogeneity)</li> </ol> <p>Creative Analogy: Imagine stretching, rotating, or flipping an entire room \u2014 but no ripping or folding allowed. Every movement is smooth and proportional.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#examples","title":"Examples","text":"<ul> <li>Scaling: Multiply each coordinate by 2.</li> <li>Rotation: Spin vectors around the origin.</li> <li>Projection: Flatten vectors onto a line or plane.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#62-matrix-representation-of-linear-transformations","title":"6.2 Matrix Representation of Linear Transformations","text":"<p>Every linear transformation can be represented by a matrix. Applying a linear transformation is the same as multiplying by its associated matrix.</p> <p>If \\( T \\) is a linear transformation, there exists a matrix \\( A \\) such that: $$ T(\\mathbf{x}) = A\\mathbf{x} $$</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#why-matrices","title":"Why Matrices?","text":"<ul> <li>They encode the action of transformations compactly.</li> <li>They allow easy computation of complex changes.</li> <li>They unify geometric intuition and algebraic manipulation.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#63-kernel-and-image","title":"6.3 Kernel and Image","text":""},{"location":"old-sections/section-1/chapter-6/chapter-6/#kernel-null-space","title":"Kernel (Null Space)","text":"<p>The kernel of a transformation \\(T\\) is the set of vectors sent to zero: $$ \\ker(T) = { \\mathbf{x} \\mid T(\\mathbf{x}) = \\mathbf{0} } $$</p> <p>Interpretation: The kernel captures all directions that are \"flattened\" completely.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#image-range","title":"Image (Range)","text":"<p>The image of a transformation is the set of all possible outputs: $$ \\text{Im}(T) = { T(\\mathbf{x}) \\mid \\mathbf{x} \\in \\mathbb{R}^n } $$</p> <p>Interpretation: The image tells you where vectors can land \u2014 the \"reach\" of the transformation.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#64-change-of-basis-and-similar-matrices","title":"6.4 Change of Basis and Similar Matrices","text":"<p>Sometimes, it's useful to change coordinates to better understand a transformation.</p> <p>Given a basis \\(B\\), we can: - Represent vectors differently (relative to \\(B\\)), - Represent transformations differently (with respect to \\(B\\)).</p> <p>Two matrices are similar if they represent the same transformation but in different bases.</p> <p>Formal Definition: Matrices \\(A\\) and \\(B\\) are similar if: $$ B = P^{-1}AP $$ for some invertible matrix \\(P\\).</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#why-change-basis","title":"Why Change Basis?","text":"<ul> <li>Simplify computations,</li> <li>Reveal hidden structures (like decoupling independent behaviors),</li> <li>Diagonalize matrices when possible.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#65-eigenvalues-and-eigenvectors","title":"6.5 Eigenvalues and Eigenvectors","text":"<p>One of the most astonishing concepts in linear algebra:</p> <p>Some vectors don't change direction when a transformation is applied \u2014 only their magnitude changes.</p> <p>These are eigenvectors.</p> <ul> <li>Given a transformation \\(A\\),</li> <li>An eigenvector \\(\\mathbf{v}\\) satisfies: $$ A\\mathbf{v} = \\lambda \\mathbf{v} $$ where \\(\\lambda\\) is a scalar called the eigenvalue.</li> </ul>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#interpretation","title":"Interpretation","text":"<ul> <li>\\(\\mathbf{v}\\) points along a natural direction of the transformation.</li> <li>\\(\\lambda\\) tells you how much \\(\\mathbf{v}\\) is stretched or shrunk.</li> </ul> <p>Creative Analogy: Imagine pushing a swing: the swing moves back and forth along its natural arc \u2014 it doesn\u2019t spin sideways or move unpredictably. Eigenvectors are the natural \"arcs\" of linear transformations.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#66-finding-eigenvalues-and-eigenvectors","title":"6.6 Finding Eigenvalues and Eigenvectors","text":""},{"location":"old-sections/section-1/chapter-6/chapter-6/#step-1-find-eigenvalues","title":"Step 1: Find Eigenvalues","text":"<p>Solve: $$ \\det(A - \\lambda I) = 0 $$ This is the characteristic equation.</p> <p>The solutions \\(\\lambda\\) are the eigenvalues.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#step-2-find-eigenvectors","title":"Step 2: Find Eigenvectors","text":"<p>For each eigenvalue \\(\\lambda\\), solve: $$ (A - \\lambda I)\\mathbf{v} = \\mathbf{0} $$ to find eigenvectors.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#example","title":"Example","text":"<p>Let: $$ A = \\begin{bmatrix} 4 &amp; 1 \\\\ 2 &amp; 3 \\end{bmatrix} $$</p> <p>Find the eigenvalues:</p> <ol> <li> <p>Compute \\( \\det(A - \\lambda I) \\): $$ \\det \\begin{bmatrix} 4-\\lambda &amp; 1 \\\\ 2 &amp; 3-\\lambda \\end{bmatrix} = (4-\\lambda)(3-\\lambda) - 2 $$ Expand: $$ = \\lambda^2 - 7\\lambda + 10 $$</p> </li> <li> <p>Solve \\( \\lambda^2 - 7\\lambda + 10 = 0 \\).</p> </li> </ol> <p>Solutions: \\( \\lambda = 5 \\) and \\( \\lambda = 2 \\).</p> <ol> <li>For each \\(\\lambda\\), solve \\( (A - \\lambda I)\\mathbf{v} = \\mathbf{0} \\) to find eigenvectors.</li> </ol>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#67-eigenspaces-algebraic-and-geometric-multiplicities","title":"6.7 Eigenspaces, Algebraic and Geometric Multiplicities","text":"<ul> <li>The eigenspace for an eigenvalue \\(\\lambda\\) is the set of all corresponding eigenvectors (plus the zero vector).</li> <li>Algebraic multiplicity: how many times \\(\\lambda\\) appears as a root of the characteristic polynomial.</li> <li>Geometric multiplicity: the dimension of the eigenspace.</li> </ul> <p>Important Fact: Geometric multiplicity is always less than or equal to algebraic multiplicity.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#68-why-this-chapter-matters","title":"6.8 Why This Chapter Matters","text":"<p>Understanding linear transformations and eigenanalysis allows you to:</p> <ul> <li>Decompose systems into simple behaviors,</li> <li>Analyze stability in control systems,</li> <li>Perform dimensionality reduction (PCA in machine learning),</li> <li>Diagonalize matrices to simplify powers of matrices.</li> </ul> <p>Building on Earlier Ideas: Matrices don't just store numbers \u2014 they move vectors in structured ways. Eigenvectors and eigenvalues describe the deep patterns of these movements.</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#summary","title":"\ud83d\udcda Summary","text":"<p>In this chapter, you learned:</p> <ul> <li>Linear transformations preserve structure and can be represented by matrices.</li> <li>Kernels capture vectors that are flattened; images capture vectors that are reachable.</li> <li>Similar matrices describe the same transformation in different coordinate systems.</li> <li>Eigenvalues and eigenvectors reveal natural directions and scaling.</li> <li>Eigenspaces, algebraic multiplicity, and geometric multiplicity structure eigenbehaviors.</li> </ul> <p>This chapter prepares you for real-world applications where transformations need to be analyzed, optimized, and decomposed!</p>"},{"location":"old-sections/section-1/chapter-6/chapter-6/#quiz","title":"\ud83e\udde0 Quiz","text":"<p>What does it mean if a matrix has fewer than \\(n\\) linearly independent eigenvectors?</p> <p>A. It is not square</p> <p>B. It cannot be diagonalized</p> <p>C. It has no eigenvalues</p> <p>D. Its determinant is zero</p> Show Answer <p>The correct answer is B. If a matrix has fewer than \\(n\\) linearly independent eigenvectors, it cannot be diagonalized \u2014 that is, it cannot be written as a diagonal matrix under any basis change.</p>"},{"location":"old-sections/section-2/section-2/","title":"\ud83d\udcda Section II: Numerical Linear Algebra &amp; Scientific Computing","text":"<p>Overview: This section transitions from theoretical constructs to practical, computational techniques essential for modern scientific computing. Students will develop robust skills in solving linear systems, matrix factorization, stability analysis, and iterative algorithms, with an emphasis on numerical precision and efficiency.</p>"},{"location":"old-sections/section-2/section-2/#chapter-7-solving-linear-systems-and-decompositions","title":"Chapter 7: Solving Linear Systems and Decompositions","text":"<ul> <li>Key Concepts: Matrix inverse, determinants and their properties, Cramer's Rule, Gaussian elimination, Reduced Row Echelon Form (RREF), pivot positions, free variables, LU, QR, and Cholesky factorizations.</li> <li>Focus: Understand direct solution methods for systems of equations and learn efficient matrix decomposition strategies.</li> <li>Skills: Solve systems via different matrix decompositions and interpret solution behaviors based on matrix structure.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-8-iterative-methods-and-stability","title":"Chapter 8: Iterative Methods and Stability","text":"<ul> <li>Key Concepts: Iterative methods (Jacobi, Gauss-Seidel, Conjugate Gradient), matrix norms, condition number, stability of linear systems.</li> <li>Focus: Address large-scale or sparse systems where direct methods are impractical; assess numerical stability and error sensitivity.</li> <li>Skills: Implement and analyze iterative algorithms; diagnose and improve system stability based on matrix conditioning.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-9-advanced-matrix-factorizations","title":"Chapter 9: Advanced Matrix Factorizations","text":"<ul> <li>Key Concepts: Singular Value Decomposition (SVD), Moore-Penrose pseudoinverse, Schur decomposition, Householder transformations, Givens rotations.</li> <li>Focus: Explore high-level decompositions that unlock deeper insights into matrix structure and enable powerful numerical applications.</li> <li>Skills: Apply SVD for dimensionality reduction, compute pseudoinverses for under/overdetermined systems, perform orthogonal transformations for stability.</li> </ul>"},{"location":"old-sections/section-2/section-2/#chapter-10-specialized-matrices-and-operations","title":"Chapter 10: Specialized Matrices and Operations","text":"<ul> <li>Key Concepts: Hermitian, unitary, and positive definite matrices; sparse matrices and solvers; block matrix operations; Kronecker products.</li> <li>Focus: Recognize and leverage special matrix properties for computational advantage; manage complex or structured matrix systems efficiently.</li> <li>Skills: Optimize storage and computation for structured matrices, manipulate block structures, and apply Kronecker products in systems modeling.</li> </ul>"},{"location":"old-sections/section-2/section-2/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Solve linear systems efficiently using both direct and iterative methods. - Implement advanced decompositions such as SVD and Schur forms. - Analyze the numerical stability and sensitivity of matrix computations. - Optimize computation for special and sparse matrix types critical in large-scale problems.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/","title":"Chapter 10: Specialized Matrices and Operations","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#overview","title":"Overview","text":"<p>In the previous chapters, we mastered general techniques for solving and analyzing linear systems. In this final chapter of Section II, we focus on special classes of matrices and specialized operations that offer computational shortcuts, deeper insights, and performance optimizations.</p> <p>Recognizing the structure of a matrix allows us to apply tailored methods that are often faster, more stable, and more memory-efficient\u2014essential in fields like signal processing, scientific computing, and machine learning.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#101-hermitian-unitary-and-positive-definite-matrices","title":"10.1 Hermitian, Unitary, and Positive Definite Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#hermitian-matrices","title":"Hermitian Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept","title":"Concept","text":"<p>A Hermitian matrix satisfies:</p> \\[ A = A^* \\] <p>where \\(A^*\\) is the conjugate transpose.</p> <ul> <li>For real matrices, Hermitian reduces to symmetric matrices \\((A = A^T)\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why","title":"How and Why","text":"<ul> <li>Why? Hermitian matrices have real eigenvalues and orthogonal eigenvectors\u2014making them extremely stable in computations.</li> <li>How? Check if each entry satisfies \\(a_{ij} = \\overline{a_{ji}}\\).</li> </ul> Example <p>The matrix \\(\\begin{bmatrix}2 &amp; i \\\\ -i &amp; 3\\end{bmatrix}\\) is Hermitian.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#unitary-matrices","title":"Unitary Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_1","title":"Concept","text":"<p>A unitary matrix satisfies:</p> \\[ U^*U = I \\] <p>meaning its conjugate transpose is also its inverse.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Unitary transformations preserve vector length and angles.</li> <li>How? Columns form an orthonormal set under complex inner product.</li> </ul> Real Analogy <p>In the real case, unitary matrices are just orthogonal matrices \\((Q^T Q = I)\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#positive-definite-matrices","title":"Positive Definite Matrices","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_2","title":"Concept","text":"<p>A matrix \\(A\\) is positive definite if:</p> \\[ \\mathbf{x}^T A \\mathbf{x} &gt; 0 \\] <p>for all nonzero vectors \\(\\mathbf{x}\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Positive definite matrices ensure unique solutions and nice behavior in optimization and numerical methods.</li> <li>How? All eigenvalues are positive.</li> </ul> Quick Test <p>A symmetric matrix is positive definite if all its pivots (or all its eigenvalues) are positive!</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#102-sparse-matrices-and-solvers","title":"10.2 Sparse Matrices and Solvers","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_3","title":"Concept","text":"<p>A sparse matrix has most of its elements equal to zero.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? In large-scale systems (e.g., social networks, finite element analysis), storing all zero entries is wasteful.</li> <li>How?</li> <li>Use compressed storage formats (CSR, CSC).</li> <li>Apply sparse solvers that skip computations with zeros.</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#sparse-techniques","title":"Sparse Techniques","text":"<ul> <li>Iterative methods like Conjugate Gradient are highly effective.</li> <li>Specialized factorizations minimize \"fill-in\" (creation of nonzeros).</li> </ul> Real World <p>Imagine simulating a building's structural stress\u2014the connectivity matrix between elements is sparse!</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#103-block-matrices-and-block-operations","title":"10.3 Block Matrices and Block Operations","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_4","title":"Concept","text":"<p>Block matrices organize data into submatrices rather than individual entries.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? Simplifies calculations by working at a larger granularity.</li> <li>How? Matrix multiplication and inversion rules extend naturally to block structure.</li> </ul> <p>Example: If \\(A\\) and \\(B\\) are block matrices:</p> <p>$$ C = \\begin{bmatrix} A &amp; B \\ 0 &amp; D \\end{bmatrix} $$ then the inverse can be computed block-wise!</p> Efficiency Tip <p>Block operations are especially efficient when blocks are diagonal or triangular.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#104-kronecker-product","title":"10.4 Kronecker Product","text":""},{"location":"old-sections/section-2/chapter-10/chapter-10/#concept_5","title":"Concept","text":"<p>The Kronecker product \\(A \\otimes B\\) creates a larger matrix from two smaller ones, spreading \\(A\\)'s entries across copies of \\(B\\).</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#how-and-why_5","title":"How and Why","text":"<ul> <li>Why? Useful in systems modeling, tensor algebra, and quantum computing.</li> <li>How?</li> </ul> <p>If \\(A\\) is \\(2 \\times 2\\) and \\(B\\) is \\(3 \\times 3\\), then \\(A \\otimes B\\) is \\(6 \\times 6\\).</p> Concrete Example <p>If \\(A = \\begin{bmatrix}1 &amp; 2\\\\ 3 &amp; 4\\end{bmatrix}\\) and \\(B\\) is a 3x3 matrix, then:</p> \\[ A \\otimes B = \\begin{bmatrix} 1B &amp; 2B \\\\ 3B &amp; 4B \\end{bmatrix} \\]"},{"location":"old-sections/section-2/chapter-10/chapter-10/#applications","title":"Applications","text":"<ul> <li>Multidimensional systems modeling.</li> <li>Structured matrix equations.</li> </ul>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#summary","title":"\u2728 Summary","text":"<ul> <li>Hermitian, unitary, and positive definite matrices have special properties that guarantee stability and efficiency.</li> <li>Sparse matrices save memory and computation by exploiting zeros.</li> <li>Block matrices allow solving problems at higher levels of structure.</li> <li>Kronecker products expand modeling capability in structured and multi-dimensional problems.</li> </ul> <p>Recognizing these structures is critical for designing fast and scalable scientific computations.</p>"},{"location":"old-sections/section-2/chapter-10/chapter-10/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>Which type of matrix satisfies \\(A^* = A\\) where \\(A^*\\) denotes the conjugate transpose?</p> <p>A. Unitary matrix B. Positive definite matrix C. Hermitian matrix D. Symmetric matrix</p> Show Answer <p>The correct answer is C. Hermitian matrices satisfy \\(A^* = A\\). For real matrices, Hermitian simply means symmetric \\((A^T = A)\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/","title":"Chapter 7: Solving Linear Systems and Decompositions","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#overview","title":"Overview","text":"<p>In this chapter, we dive into practical methods for solving systems of linear equations, a core application of linear algebra in scientific computing. We will explore both classical techniques and efficient matrix factorizations that not only provide solutions but also reveal important structural insights about the system.</p> <p>Solving linear systems lies at the heart of countless engineering and computing problems: simulating circuits, analyzing networks, training machine learning models, and even 3D graphics rendering. </p> <p>We will build on your knowledge of matrix operations and linear transformations from earlier chapters.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#71-matrix-inverse-and-solving-systems","title":"7.1 Matrix Inverse and Solving Systems","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept","title":"Concept","text":"<p>A matrix inverse, much like the reciprocal of a number, undoes the action of a matrix. If a matrix \\(A\\) is invertible, we can solve \\(A\\mathbf{x} = \\mathbf{b}\\) by computing \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why","title":"How and Why","text":"<ul> <li>Why? It gives a direct way to solve equations.</li> <li>How? If \\(A^{-1}\\) exists, it satisfies \\(A^{-1}A = I\\), where \\(I\\) is the identity matrix.</li> </ul> <p>Warning: Computing \\(A^{-1}\\) is computationally expensive and numerically unstable for large matrices. In practice, we rarely compute the inverse explicitly!</p> Visual Metaphor <p>Think of matrix multiplication as \"squeezing\" or \"stretching\" space. An inverse \"unsqueezes\" or \"unstretches\" space exactly.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#72-determinants-and-their-properties","title":"7.2 Determinants and Their Properties","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_1","title":"Concept","text":"<p>The determinant of a matrix gives a scalar value that indicates whether the matrix is invertible and how it transforms space.</p> <ul> <li>If \\(\\text{det}(A) = 0\\), the matrix is not invertible (it \"flattens\" space).</li> <li>If \\(\\text{det}(A) \\neq 0\\), the matrix is invertible.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? The determinant tells us about volume scaling and collapse.</li> <li>How? Computed recursively or via triangular forms after row reductions.</li> </ul> Example <p>The determinant of a 2x2 matrix \\(\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}\\) is \\(ad - bc\\).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#73-cramers-rule","title":"7.3 Cramer's Rule","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_2","title":"Concept","text":"<p>Cramer's Rule expresses each variable in \\(A\\mathbf{x} = \\mathbf{b}\\) as a ratio of determinants.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? It provides a formulaic method for small systems.</li> <li>How? Replace the column of \\(A\\) corresponding to the unknown with \\(\\mathbf{b}\\), compute the determinant, and divide by \\(\\det(A)\\).</li> </ul> <p>Warning: Cramer's Rule is not practical for large systems because determinant computation grows factorially with matrix size.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#74-gaussian-elimination-and-rref","title":"7.4 Gaussian Elimination and RREF","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_3","title":"Concept","text":"<p>Gaussian elimination systematically applies row operations to solve a linear system, reducing it to an upper triangular or reduced row echelon form (RREF).</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Solving systems becomes easier when matrices are triangular or simple.</li> <li>How?</li> <li>Eliminate variables from equations step by step.</li> <li>Use back substitution to find solutions.</li> </ul> Creative Teaching Tip <p>Think of Gaussian elimination as \"sweeping\" the clutter (variables) under the rug (zeros below the pivots) until only the core (solutions) are visible!</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#75-pivot-positions-and-free-variables","title":"7.5 Pivot Positions and Free Variables","text":""},{"location":"old-sections/section-2/chapter-7/chapter-7/#concept_4","title":"Concept","text":"<p>Pivot positions are the leading nonzero entries in each row after Gaussian elimination.</p> <ul> <li>A variable corresponding to a pivot is called a basic variable.</li> <li>A variable not corresponding to a pivot is called a free variable.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? They determine the structure of the solution set (unique, infinite, or none).</li> <li>How? Identify pivots during elimination, and classify variables accordingly.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#76-lu-qr-and-cholesky-factorizations","title":"7.6 LU, QR, and Cholesky Factorizations","text":"<p>Matrix factorizations break down a complicated matrix into simpler, structured components that make solving systems faster and more stable.</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#lu-decomposition","title":"LU Decomposition","text":"<ul> <li>Concept: Express \\(A = LU\\) where \\(L\\) is lower triangular, \\(U\\) is upper triangular.</li> <li>Application: Solves \\(A\\mathbf{x} = \\mathbf{b}\\) via two easier systems: \\(L\\mathbf{y} = \\mathbf{b}\\), then \\(U\\mathbf{x} = \\mathbf{y}\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#qr-decomposition","title":"QR Decomposition","text":"<ul> <li>Concept: Express \\(A = QR\\) where \\(Q\\) is orthogonal and \\(R\\) is upper triangular.</li> <li>Application: Important in solving least-squares problems and eigenvalue computation.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#cholesky-decomposition","title":"Cholesky Decomposition","text":"<ul> <li>Concept: For symmetric positive definite matrices, \\(A = LL^T\\) where \\(L\\) is lower triangular.</li> <li>Application: Twice as efficient as LU decomposition for these special matrices.</li> </ul> Efficiency Tip <p>Cholesky decomposition is faster and more stable for symmetric positive definite matrices because it avoids duplicating computations!</p>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#summary","title":"\u2728 Summary","text":"<ul> <li>Matrix inverses provide a theoretical solution method but are rarely computed directly.</li> <li>Determinants help determine invertibility and geometric properties.</li> <li>Cramer's Rule, while educational, is inefficient for large systems.</li> <li>Gaussian elimination and RREF are fundamental manual solving methods.</li> <li>Pivot positions and free variables define the nature of the solution set.</li> <li>Matrix decompositions (LU, QR, Cholesky) provide faster, scalable solutions.</li> </ul>"},{"location":"old-sections/section-2/chapter-7/chapter-7/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>What is the most efficient factorization method for solving systems with symmetric, positive definite matrices?</p> <p>A. LU decomposition B. QR decomposition C. Cholesky decomposition D. RREF</p> Show Answer <p>The correct answer is C. Cholesky decomposition is optimized for symmetric positive definite matrices, offering faster and more stable computation than LU or QR.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/","title":"Chapter 8: Iterative Methods and Stability","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#overview","title":"Overview","text":"<p>In Chapter 7, we explored direct methods for solving linear systems. However, direct methods can become computationally impractical or unstable for very large or sparse systems. In this chapter, we shift to iterative methods\u2014techniques that generate a sequence of approximations that converge to the true solution.</p> <p>We'll also study stability and conditioning\u2014crucial concepts that tell us how errors in the data or rounding errors during computation can impact the final solution.</p> <p>Understanding these topics ensures that we can solve real-world problems reliably, even when systems are huge, messy, and prone to error.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#81-introduction-to-iterative-methods","title":"8.1 Introduction to Iterative Methods","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept","title":"Concept","text":"<p>Rather than solving a system in one shot, iterative methods start with an initial guess and improve it step-by-step.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why","title":"How and Why","text":"<ul> <li>Why? Direct methods are too expensive for large or sparse matrices.</li> <li>How? Each iteration refines the solution based on simple matrix-vector operations.</li> </ul> Real World Example <p>Google's PageRank algorithm uses an iterative method to rank web pages!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#basic-structure","title":"Basic Structure","text":"<ol> <li>Start with an initial guess \\( \\mathbf{x}^{(0)} \\).</li> <li>Apply an update rule to get better approximations \\( \\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots \\).</li> <li>Stop when changes become sufficiently small.</li> </ol>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#82-jacobi-and-gauss-seidel-methods","title":"8.2 Jacobi and Gauss-Seidel Methods","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#jacobi-method","title":"Jacobi Method","text":"<ul> <li>Updates each variable based only on the previous iteration's values.</li> <li>Fully parallelizable\u2014each new value can be computed independently.</li> </ul> <p>Update rule: $$ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j \\neq i} a_{ij} x_j^{(k)} \\right) $$</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#gauss-seidel-method","title":"Gauss-Seidel Method","text":"<ul> <li>Updates each variable immediately, using the newest values available.</li> <li>Faster convergence in many cases because it \"learns\" from updates within the same iteration.</li> </ul> <p>Update rule: $$ x_i^{(k+1)} = \\frac{1}{a_{ii}}\\left( b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \\right) $$</p> Analogy <p>Think of Jacobi as students taking a test independently, while Gauss-Seidel is like students sharing answers immediately during the test!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#83-conjugate-gradient-method","title":"8.3 Conjugate Gradient Method","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_1","title":"Concept","text":"<p>The Conjugate Gradient (CG) Method is a more advanced iterative method specialized for symmetric, positive-definite matrices.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Much faster and memory-efficient for large sparse systems arising in physics simulations, machine learning, and more.</li> <li>How? CG searches along conjugate directions to minimize the quadratic form associated with \\(A\\).</li> </ul> <p>Key Features: - Only needs matrix-vector multiplications. - Requires fewer iterations for convergence.</p> When to Use CG <p>Solving \\(Ax = b\\) where \\(A\\) comes from discretizing a 2D Poisson equation\u2014a common physics simulation problem.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#84-matrix-norms","title":"8.4 Matrix Norms","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_2","title":"Concept","text":"<p>A matrix norm measures the \"size\" of a matrix\u2014how much it can stretch a vector.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Helps quantify error and convergence behavior in iterative methods.</li> <li>How? Different norms exist:</li> <li>\\( \\|A\\|_1 \\): maximum absolute column sum.</li> <li>\\( \\|A\\|_\\infty \\): maximum absolute row sum.</li> <li>\\( \\|A\\|_2 \\): largest singular value (spectral norm).</li> </ul> Tip <p>Think of a norm as a \"magnifying glass\"\u2014showing how much a matrix exaggerates changes in input.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#85-condition-number","title":"8.5 Condition Number","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_3","title":"Concept","text":"<p>The condition number measures how sensitive the solution \\( \\mathbf{x} \\) is to small changes in \\(A\\) or \\( \\mathbf{b} \\).</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Predicts if small input errors will cause large output errors.</li> <li>How?</li> </ul> <p>Condition number: $$ \\kappa(A) = |A| \\cdot |A^{-1}| $$</p> <ul> <li>\\( \\kappa(A) \\approx 1 \\): well-conditioned (small errors remain small).</li> <li>\\( \\kappa(A) \\gg 1 \\): ill-conditioned (small errors get amplified).</li> </ul> Important <p>Large condition numbers make solving \\(A\\mathbf{x} = \\mathbf{b}\\) numerically risky, even if \\(A\\) is invertible theoretically!</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#86-stability-and-error-analysis","title":"8.6 Stability and Error Analysis","text":""},{"location":"old-sections/section-2/chapter-8/chapter-8/#concept_4","title":"Concept","text":"<p>Stability refers to whether an algorithm produces nearly correct results even when computations have small errors.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? All real-world computations have rounding errors.</li> <li>How? </li> <li>Stable algorithms control error growth.</li> <li>Unstable algorithms amplify errors unpredictably.</li> </ul>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#forward-vs-backward-error","title":"Forward vs. Backward Error","text":"<ul> <li>Forward error: Difference between true solution and computed solution.</li> <li>Backward error: How much we have to perturb \\(A\\) or \\( \\mathbf{b} \\) to make the computed solution exact.</li> </ul> Example <p>A backward stable algorithm might return an answer slightly wrong for your system but exactly right for a very close system.</p>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#summary","title":"\u2728 Summary","text":"<ul> <li>Iterative methods are essential for solving large or sparse systems.</li> <li>Jacobi and Gauss-Seidel are basic iterative schemes.</li> <li>Conjugate Gradient is highly efficient for symmetric positive-definite matrices.</li> <li>Matrix norms measure \"how big\" matrices are.</li> <li>The condition number quantifies problem sensitivity.</li> <li>Stability ensures reliable computation in the presence of unavoidable errors.</li> </ul>"},{"location":"old-sections/section-2/chapter-8/chapter-8/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>If a matrix has a large condition number, what does it imply about solving \\(Ax = b\\)?</p> <p>A. The matrix is singular. B. Small input errors can cause large output errors. C. The solution is unique and stable. D. Gaussian elimination will fail completely.</p> Show Answer <p>The correct answer is B. A large condition number means the system is sensitive: small changes in input can cause large changes in output.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/","title":"Chapter 9: Advanced Matrix Factorizations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#overview","title":"Overview","text":"<p>Building on our understanding of solving linear systems and iterative methods, we now venture into the world of advanced matrix factorizations. These techniques allow us to uncover deep structural insights into matrices, simplify complex computations, and enable applications in areas like data science, control systems, and optimization.</p> <p>In this chapter, you will learn about powerful tools like the Singular Value Decomposition (SVD), the Moore-Penrose pseudoinverse, Schur decomposition, and structured orthogonal transformations such as Householder reflections and Givens rotations.</p> <p>These methods are fundamental to mastering modern scientific computing.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#91-singular-value-decomposition-svd","title":"9.1 Singular Value Decomposition (SVD)","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept","title":"Concept","text":"<p>SVD expresses any matrix \\(A\\) as a product:</p> \\[ A = U \\Sigma V^T \\] <p>where: - \\(U\\) and \\(V\\) are orthogonal matrices. - \\(\\Sigma\\) is a diagonal matrix with non-negative real numbers (the singular values).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why","title":"How and Why","text":"<ul> <li>Why? It reveals the \"geometry\" of how \\(A\\) stretches and rotates space.</li> <li>How? Through careful decomposition of \\(A\\) into simpler, interpretable parts.</li> </ul> Geometric Intuition <p>Imagine \\(A\\) as first rotating space (via \\(V\\)), then stretching or shrinking (via \\(\\Sigma\\)), then rotating again (via \\(U\\)).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications","title":"Applications","text":"<ul> <li>Principal Component Analysis (PCA) in data science.</li> <li>Solving least-squares problems.</li> <li>Noise reduction and signal compression.</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#92-moore-penrose-pseudoinverse","title":"9.2 Moore-Penrose Pseudoinverse","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_1","title":"Concept","text":"<p>For matrices that are non-square or singular, the pseudoinverse \\(A^+\\) provides a best-fit solution to \\(A\\mathbf{x} = \\mathbf{b}\\).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_1","title":"How and Why","text":"<ul> <li>Why? Not all systems have exact solutions\u2014sometimes we must seek an approximate (least-squares) one.</li> <li>How?</li> </ul> \\[ A^+ = V \\Sigma^+ U^T \\] <p>where \\(\\Sigma^+\\) replaces each nonzero singular value \\(\\sigma\\) with \\(1/\\sigma\\).</p> Best Fit Solutions <p>When the system is inconsistent, \\(A^+\\mathbf{b}\\) gives the solution \\(\\mathbf{x}\\) minimizing \\(\\|A\\mathbf{x} - \\mathbf{b}\\|\\).</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#93-schur-decomposition","title":"9.3 Schur Decomposition","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_2","title":"Concept","text":"<p>Any square matrix \\(A\\) can be decomposed as:</p> \\[ A = Q T Q^* \\] <p>where: - \\(Q\\) is unitary (or orthogonal for real matrices). - \\(T\\) is upper triangular.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_2","title":"How and Why","text":"<ul> <li>Why? Triangular matrices are easier to analyze and work with.</li> <li>How? By applying a sequence of orthogonal transformations to \\(A\\).</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_1","title":"Applications","text":"<ul> <li>Simplifies eigenvalue computation.</li> <li>Forms the basis for many iterative methods (e.g., QR algorithm for eigenvalues).</li> </ul> Triangular Advantage <p>Finding eigenvalues of a triangular matrix \\(T\\) is as easy as reading its diagonal entries!</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#94-householder-transformations","title":"9.4 Householder Transformations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_3","title":"Concept","text":"<p>A Householder transformation reflects a vector across a plane or hyperplane to introduce zeros below the pivot.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_3","title":"How and Why","text":"<ul> <li>Why? Useful in QR decomposition and in reducing matrices to simpler forms.</li> <li>How?</li> </ul> \\[ H = I - 2 \\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T \\mathbf{v}} \\] <p>where \\(\\mathbf{v}\\) is carefully chosen.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_2","title":"Applications","text":"<ul> <li>Efficiently zeroing entries during QR decomposition.</li> <li>Constructing orthogonal matrices numerically stably.</li> </ul> Intuitive Picture <p>Picture a beam of light reflecting off a flat mirror: the vector is flipped symmetrically across a plane!</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#95-givens-rotations","title":"9.5 Givens Rotations","text":""},{"location":"old-sections/section-2/chapter-9/chapter-9/#concept_4","title":"Concept","text":"<p>Givens rotations apply a simple 2D rotation to zero a specific matrix entry.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#how-and-why_4","title":"How and Why","text":"<ul> <li>Why? Targeted and efficient way to introduce zeros while preserving orthogonality.</li> <li>How? Rotates within the plane of two coordinates.</li> </ul>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#applications_3","title":"Applications","text":"<ul> <li>Building QR decompositions especially for sparse matrices.</li> <li>Fine-grained control over numerical stability.</li> </ul> When to Use Givens <p>When you want to zero just one specific off-diagonal element without touching the rest of the matrix too much.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#summary","title":"\u2728 Summary","text":"<ul> <li>SVD decomposes any matrix into orthogonal transformations and singular values.</li> <li>The Moore-Penrose pseudoinverse finds least-squares solutions for non-square systems.</li> <li>Schur decomposition simplifies matrix analysis by reducing to upper triangular form.</li> <li>Householder reflections efficiently zero entire columns below the pivot.</li> <li>Givens rotations perform selective 2D zeroing while maintaining orthogonality.</li> </ul> <p>These tools are essential for stability, performance, and deeper understanding of linear systems in scientific computing.</p>"},{"location":"old-sections/section-2/chapter-9/chapter-9/#quiz","title":"\ud83d\udcdd Quiz","text":"<p>Which decomposition represents any \\(m \\times n\\) matrix as a product of two orthogonal matrices and a diagonal matrix?</p> <p>A. LU Decomposition B. Schur Decomposition C. QR Decomposition D. Singular Value Decomposition (SVD)</p> Show Answer <p>The correct answer is D. Singular Value Decomposition (SVD) expresses any matrix as \\(U\\Sigma V^T\\) with orthogonal \\(U\\) and \\(V\\), and a diagonal \\(\\Sigma\\).</p>"},{"location":"old-sections/section-3/section-3/","title":"\ud83d\udcda Section III: Control Systems &amp; Electrical Engineering","text":"<p>Overview: This section explores how linear algebra provides the mathematical backbone for modeling, analyzing, and designing control systems in electrical engineering. Students will apply matrix techniques to dynamic systems, understand system stability, and master feedback mechanisms critical to modern control theory.</p>"},{"location":"old-sections/section-3/section-3/#chapter-11-linear-systems-in-control-theory","title":"Chapter 11: Linear Systems in Control Theory","text":"<ul> <li>Key Concepts: State-space representation, controllability matrix, observability matrix, Kalman decomposition, transfer function representation.</li> <li>Focus: Model dynamic systems using matrices and analyze their controllability and observability to ensure desired behavior.</li> <li>Skills: Construct and interpret state-space models; evaluate system properties critical for control design.</li> </ul>"},{"location":"old-sections/section-3/section-3/#chapter-12-system-stability-and-feedback","title":"Chapter 12: System Stability and Feedback","text":"<ul> <li>Key Concepts: Stability of state-space systems, feedback control, pole placement, Linear Quadratic Regulator (LQR), Singular Value Decomposition (SVD) applications in control.</li> <li>Focus: Assess system stability and design feedback mechanisms to control system performance effectively.</li> <li>Skills: Design feedback laws to stabilize and optimize dynamic systems; apply SVD in control optimization scenarios.</li> </ul>"},{"location":"old-sections/section-3/section-3/#chapter-13-dynamic-system-modeling","title":"Chapter 13: Dynamic System Modeling","text":"<ul> <li>Key Concepts: Matrix exponentiation, Markov chains (basic overview), matrix powers, model reduction techniques.</li> <li>Focus: Predict system behavior over time and simplify complex models while preserving essential dynamics.</li> <li>Skills: Use matrix exponentials to solve system evolution equations; apply model reduction to improve system efficiency and maintain control integrity.</li> </ul>"},{"location":"old-sections/section-3/section-3/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Model physical and electrical systems dynamically using state-space and transfer function approaches. - Analyze system stability and controllability through eigenstructure and matrix methods. - Design optimal and robust control systems using feedback strategies and optimization techniques. - Simplify high-order systems to manageable models without sacrificing key dynamic behaviors.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/","title":"Chapter 11: Linear Systems in Control Theory","text":""},{"location":"old-sections/section-3/chapter-11/chapter-11/#overview","title":"Overview","text":"<p>In this chapter, we connect the dots between the abstract world of linear algebra and real-world dynamic systems. Specifically, we'll see how state-space models use matrices to represent, analyze, and design control systems in electrical engineering.</p> <p>Understanding controllability and observability is crucial for engineers: if a system cannot be controlled or observed, then no amount of clever engineering will make it behave as desired. These ideas build directly on your earlier understanding of vector spaces, matrix rank, eigenvalues, and transformations.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#111-state-space-representation","title":"11.1 State-Space Representation","text":"<p>Imagine trying to describe every possible behavior of a system like a robot arm or an electric motor. Instead of writing down countless equations, we organize everything neatly into matrices and vectors.</p> <p>A state-space model expresses a system with two main equations:</p> \\[ \\dot{x}(t) = Ax(t) + Bu(t) \\\\\\\\ y(t) = Cx(t) + Du(t) \\] <p>Where: - \\(x(t)\\): State vector (describes the internal condition of the system) - \\(u(t)\\): Input vector (external signals we control) - \\(y(t)\\): Output vector (what we measure or observe) - \\(A\\), \\(B\\), \\(C\\), \\(D\\): Matrices that define how states and inputs relate to state changes and outputs</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#why-does-this-work","title":"Why does this work?","text":"<p>Matrix multiplication naturally captures how multiple inputs affect multiple outputs simultaneously. Instead of writing separate equations for every variable, matrices elegantly compress all the relationships into a compact form.</p> <p>Tip: Think of \\(A\\) as \"how the system evolves,\" \\(B\\) as \"how inputs affect the system,\" \\(C\\) as \"how we observe the system,\" and \\(D\\) as \"direct influence of inputs on outputs.\"</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#112-controllability","title":"11.2 Controllability","text":"<p>Controllability asks: \"Can we move the system to any desired state using available inputs?\"</p> <p>To answer this, we use the Controllability Matrix:</p> \\[ \\mathcal{C} = \\begin{bmatrix} B &amp; AB &amp; A^2B &amp; \\dots &amp; A^{n-1}B \\end{bmatrix} \\] <ul> <li>If \\(\\mathcal{C}\\) has full rank (i.e., rank = number of states \\(n\\)), the system is controllable.</li> </ul>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#intuitive-view","title":"Intuitive View","text":"<p>Picture a video game controller. If certain buttons are broken, you might not be able to move your character anywhere you want. Similarly, if \\(\\mathcal{C}\\) is missing \"directions\" (lacks rank), you can't steer the system to any state.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#how-to-check-controllability","title":"How to check controllability","text":"<ol> <li>Form \\(\\mathcal{C}\\).</li> <li>Compute its rank.</li> <li>Compare to the number of states.</li> </ol> <p>Example: If \\(\\mathcal{C}\\) has rank 2 for a 2-state system, you're good. But if it only has rank 1, you can't reach every state.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#113-observability","title":"11.3 Observability","text":"<p>Observability asks: \"Can we deduce the internal states from measurements?\"</p> <p>To test this, use the Observability Matrix:</p> \\[ \\mathcal{O} = \\begin{bmatrix} C \\\\\\\\ CA \\\\\\\\ CA^2 \\\\\\\\ \\vdots \\\\\\\\ CA^{n-1} \\end{bmatrix} \\] <ul> <li>If \\(\\mathcal{O}\\) has full rank, the system is observable.</li> </ul>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#intuitive-view_1","title":"Intuitive View","text":"<p>Imagine trying to figure out the position of a hidden car by only watching its shadow. If the light source and shadow don't reveal everything, you're missing vital information \u2014 the system is not fully observable.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#how-to-check-observability","title":"How to check observability","text":"<ol> <li>Form \\(\\mathcal{O}\\).</li> <li>Compute its rank.</li> <li>Compare to the number of states.</li> </ol>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#114-kalman-decomposition","title":"11.4 Kalman Decomposition","text":"<p>Sometimes, parts of a system are controllable and observable, while others aren't. The Kalman Decomposition reorganizes the system into blocks that separate these parts.</p> <p>Why is this useful? - You can focus on controllable/observable parts and ignore the rest. - Simplifies design and analysis.</p> <p>Kalman Decomposition uses special coordinate transformations (like changing basis in linear algebra) to reveal these hidden structures.</p> <p>Note: Kalman Decomposition relies heavily on understanding matrix similarity transformations from earlier chapters!</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#115-transfer-function-representation","title":"11.5 Transfer Function Representation","text":"<p>While state-space models are great for multiple-input, multiple-output (MIMO) systems, sometimes it's easier to think about how inputs are transformed into outputs directly.</p> <p>The Transfer Function \\(G(s)\\) relates input to output in the Laplace domain:</p> \\[ G(s) = C(sI - A)^{-1}B + D \\] <ul> <li>\\(s\\) is a complex variable representing frequency.</li> <li>\\((sI - A)^{-1}\\) is the resolvent matrix that describes how system dynamics respond to inputs.</li> </ul> <p>How is this helpful? - Transfer functions give direct frequency response. - Easier to design filters and controllers for single-input, single-output (SISO) systems.</p> <p>Tip: Transfer Functions are like \"black box\" descriptions, while state-space models are \"white box\" internal blueprints.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#chapter-11-quiz","title":"Chapter 11 Quiz","text":""},{"location":"old-sections/section-3/chapter-11/chapter-11/#quiz-controllability","title":"Quiz: Controllability","text":"<p>Which matrix determines whether all states of a system can be influenced by the input?</p> <p>A. Observability Matrix B. Controllability Matrix C. Transfer Function Matrix D. State Transition Matrix</p> Show Answer <p>The correct answer is B. The Controllability Matrix evaluates if the system can reach any state using suitable input sequences. Full rank controllability ensures maximum maneuverability of the system.</p>"},{"location":"old-sections/section-3/chapter-11/chapter-11/#coming-up-next","title":"\u2728 Coming Up Next","text":"<p>In the next chapter, we'll build on this foundation by learning how to assess and design stability and feedback control mechanisms, ensuring that not only can we control a system but that it behaves well over time!</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/","title":"Chapter 12: System Stability and Feedback","text":""},{"location":"old-sections/section-3/chapter-12/chapter-12/#overview","title":"Overview","text":"<p>Building upon Chapter 11's foundation in state-space modeling, this chapter dives into ensuring systems not only move but behave well over time. We will explore how stability and feedback strategies are designed using linear algebra. These ideas are essential \u2014 a controllable system that is unstable is like a rocket ship you can steer but can't keep from exploding!</p> <p>Understanding how eigenvalues (from earlier eigenanalysis topics) control system behavior will become your main tool here.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#121-stability-of-state-space-systems","title":"12.1 Stability of State-Space Systems","text":"<p>In simple terms, a stable system tends to return to equilibrium after a disturbance.</p> <p>For a continuous-time system:</p> \\[ \\dot{x}(t) = Ax(t) \\] <p>the stability depends entirely on the eigenvalues of the matrix A.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#stability-conditions","title":"Stability Conditions","text":"<ul> <li>Stable: All eigenvalues have negative real parts.</li> <li>Unstable: Any eigenvalue has a positive real part.</li> <li>Marginally Stable: Eigenvalues are purely imaginary (on the imaginary axis), and no repeated eigenvalues.</li> </ul>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-eigenvalues","title":"Why Eigenvalues?","text":"<p>Each eigenvalue corresponds to a natural mode of the system. If the real part of an eigenvalue is positive, it describes a mode that grows exponentially over time \u2014 leading to instability.</p> <p>Tip: Think of eigenvalues as \"behavior seeds\" planted inside the system. If they are healthy (negative real parts), the system calms down. If they are poisonous (positive real parts), the system spirals out of control.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#122-feedback-control","title":"12.2 Feedback Control","text":"<p>Sometimes, a system isn't stable by itself \u2014 but we can add feedback to stabilize it!</p> <p>Feedback means using the output (or state) to influence the input.</p> <p>A simple state feedback control law is:</p> \\[  u(t) = -Kx(t) \\] <p>where \\(K\\) is the feedback gain matrix.</p> <p>Substituting this into the state equation gives:</p> \\[ \\dot{x}(t) = (A - BK)x(t) \\]"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-feedback-works","title":"Why Feedback Works","text":"<p>By choosing \\(K\\) wisely, we modify the system matrix from \\(A\\) to \\(A - BK\\). This new matrix can have different eigenvalues, meaning we can move the natural behaviors of the system toward stability.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#123-pole-placement","title":"12.3 Pole Placement","text":"<p>Pole placement is the art of designing \\(K\\) so that the closed-loop system \\(A - BK\\) has desired eigenvalues (poles).</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#procedure","title":"Procedure","text":"<ol> <li>Decide where you want the eigenvalues (e.g., all with large negative real parts for fast decay).</li> <li>Solve for \\(K\\) that achieves those eigenvalues.</li> </ol> <p>Key Requirement: - The system must be controllable (see Chapter 11) to place poles arbitrarily.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#how-it-connects","title":"How it connects","text":"<p>Remember: eigenvalues were first studied in basic matrix theory. Now, they determine real-world system behavior, and we actively design matrices to control them!</p> <p>Example: If you want a motor to settle to rest quickly after a bump, you design \\(K\\) to place poles far into the left half-plane.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#124-linear-quadratic-regulator-lqr","title":"12.4 Linear Quadratic Regulator (LQR)","text":"<p>Sometimes, just placing poles isn't enough \u2014 we want to optimize performance by balancing goals: - Make the system respond quickly - Avoid using huge control inputs (which could burn out hardware)</p> <p>The Linear Quadratic Regulator (LQR) solves this optimization problem:</p> <p>Minimize the cost function:</p> \\[ J = \\int_0^\\infty \\left( x(t)^T Q x(t) + u(t)^T R u(t) \\right) dt \\] <p>where: - \\(Q\\) penalizes bad states. - \\(R\\) penalizes control effort.</p> <p>The resulting optimal feedback gain \\(K\\) satisfies an equation called the Riccati equation.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#why-use-lqr","title":"Why use LQR?","text":"<ul> <li>Energy efficiency: Control effort is minimized.</li> <li>Performance guarantee: System stabilizes optimally according to a clear cost.</li> </ul> <p>Tip: LQR uses matrix optimization to find the \"best\" \\(K\\) rather than just \"any\" \\(K\\).</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#125-svd-applications-in-control","title":"12.5 SVD Applications in Control","text":"<p>The Singular Value Decomposition (SVD), introduced earlier, also plays a powerful role in control theory.</p> <p>SVD helps in: - Diagnosing poorly controllable or observable directions. - Designing robust controllers when certain inputs/outputs are weakly connected. - Understanding how sensitive a system is to disturbances.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#how-it-fits-in","title":"How it fits in","text":"<p>SVD breaks a matrix into \"stretching\" and \"rotating\" components \u2014 revealing hidden structure. When applying feedback, it helps identify which directions are easy or hard to control.</p> <p>Example: If an SVD shows a very small singular value, you know there are directions where the system reacts sluggishly \u2014 caution is needed in control design.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#chapter-12-quiz","title":"Chapter 12 Quiz","text":""},{"location":"old-sections/section-3/chapter-12/chapter-12/#quiz-feedback-control","title":"Quiz: Feedback Control","text":"<p>What is the primary goal of feedback control in a dynamic system?</p> <p>A. Increase computational load B. Stabilize the system and improve performance C. Make the system uncontrollable D. Remove all external inputs</p> Show Answer <p>The correct answer is B. Feedback control modifies the system dynamics to ensure stability, enhance performance, and meet design specifications even in the presence of disturbances or uncertainties.</p>"},{"location":"old-sections/section-3/chapter-12/chapter-12/#coming-up-next","title":"\u2728 Coming Up Next","text":"<p>In Chapter 13, we'll extend our control theory knowledge by learning how to model dynamic systems over time using matrix exponentiation and simplify complex systems without losing essential behavior.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/","title":"Chapter 13: Dynamic System Modeling","text":""},{"location":"old-sections/section-3/chapter-13/chapter-13/#overview","title":"Overview","text":"<p>In this chapter, we shift our focus from designing control systems to predicting and simplifying their behavior over time. We introduce powerful tools like matrix exponentiation to solve dynamic equations, explore Markov chains as discrete-time models, and learn how to reduce complex models while preserving critical dynamics.</p> <p>These concepts extend our earlier work on eigenvalues, matrix powers, and linear transformations into dynamic and probabilistic realms!</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#131-matrix-exponentiation-for-system-evolution","title":"13.1 Matrix Exponentiation for System Evolution","text":"<p>Consider the continuous-time system:</p> \\[ \\dot{x}(t) = Ax(t) \\] <p>We already know that \\(A\\)'s eigenvalues determine stability. But how exactly does \\(x(t)\\) evolve over time?</p> <p>The solution is given by the matrix exponential:</p> \\[ x(t) = e^{At}x(0) \\]"},{"location":"old-sections/section-3/chapter-13/chapter-13/#what-is-eat","title":"What is \\(e^{At}\\)?","text":"<ul> <li>It's defined similarly to the scalar exponential via a power series:</li> </ul> \\[ e^{At} = I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\dots \\] <ul> <li>It \"sums up\" all possible infinitesimal changes over time.</li> </ul> <p>Tip: Think of \\(e^{At}\\) as a \"super-transformation\" that smoothly evolves the state \\(x(0)\\) over time.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#why-matrix-exponentiation-works","title":"Why Matrix Exponentiation Works","text":"<p>Systems governed by linear differential equations can be thought of as being \"constantly nudged\" by \\(A\\). Matrix exponentiation mathematically captures this cumulative nudge.</p> <p>Example: In an RLC electrical circuit, the voltages and currents evolve over time according to matrix exponentials.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#132-markov-chains-and-matrix-powers","title":"13.2 Markov Chains and Matrix Powers","text":"<p>Dynamic systems aren't always continuous \u2014 sometimes they evolve in discrete steps, like a game moving piece by piece.</p> <p>A Markov chain models systems where: - The next state depends only on the current state. - Transitions are governed by probabilities.</p> <p>The evolution is:</p> \\[  x[k+1] = P x[k] \\] <p>where: - \\(x[k]\\) is the state probability vector at step \\(k\\). - \\(P\\) is the transition matrix (stochastic matrix: rows sum to 1).</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#how-to-predict-future-states","title":"How to Predict Future States","text":"<p>By taking powers of \\(P\\):</p> \\[  x[k] = P^k x[0] \\] <p>So \\(P^k\\) tells us how the system evolves after \\(k\\) steps.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#intuitive-view","title":"Intuitive View","text":"<p>Imagine flipping between different webpages. \\(P\\) tells you the chance of jumping from one page to another. Matrix powers predict where you'll likely end up after many clicks.</p> <p>Note: Eigenvalues and eigenvectors of \\(P\\) determine long-term behaviors, connecting back to our spectral analysis from earlier chapters!</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#133-model-reduction-techniques","title":"13.3 Model Reduction Techniques","text":"<p>Real-world systems can be huge \u2014 thousands of states! Simulating or controlling them exactly becomes impractical.</p> <p>Model reduction seeks to create a simpler system that behaves similarly to the original.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#common-strategies","title":"Common Strategies","text":"<ul> <li>Remove unimportant states: Find states that barely affect output and discard them.</li> <li>Approximate eigenvalues: Retain dominant modes (eigenvalues with slow decay) and ignore fast ones.</li> <li>Balanced Truncation: Find a coordinate system where controllability and observability are balanced, then trim small contributions.</li> </ul>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#why-reduce-models","title":"Why Reduce Models?","text":"<ul> <li>Faster simulations.</li> <li>Simpler controller designs.</li> <li>Easier to interpret system behavior.</li> </ul> <p>Tip: Model reduction is like summarizing a long novel into a short but faithful synopsis \u2014 you keep the important plots, lose the irrelevant details.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#134-preserving-key-dynamics","title":"13.4 Preserving Key Dynamics","text":"<p>When simplifying a system, we must ensure that essential behaviors (stability, main oscillations, dominant responses) are preserved.</p> <p>If not, we risk building controllers for a \"fake\" system that doesn't behave like the real one.</p> <p>This connects directly to: - Controllability and Observability: Reduced models should maintain important controllable and observable dynamics. - Spectral Properties: Critical eigenvalues should remain unchanged or closely approximated.</p> <p>Example: When reducing a robotic arm's control model, you can't discard modes that cause major swings \u2014 only tiny vibrations can be safely ignored.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#chapter-13-quiz","title":"Chapter 13 Quiz","text":""},{"location":"old-sections/section-3/chapter-13/chapter-13/#quiz-matrix-exponentiation","title":"Quiz: Matrix Exponentiation","text":"<p>Matrix exponentiation in dynamic systems is primarily used to:</p> <p>A. Compute eigenvalues of a matrix B. Solve time evolution equations C. Find the maximum rank of a matrix D. Perform matrix inversion</p> Show Answer <p>The correct answer is B. Matrix exponentiation solves the system evolution over time, describing how the system's state changes continuously according to the matrix dynamics.</p>"},{"location":"old-sections/section-3/chapter-13/chapter-13/#section-iii-conclusion","title":"\u2728 Section III Conclusion","text":"<p>You've now mastered the tools to model, analyze, and design dynamic systems using the power of linear algebra. Up next, we'll move into Signal Processing and Graph Theory, where you'll see matrices shape signals, images, and networks!</p>"},{"location":"old-sections/section-4/section-4/","title":"\ud83d\udcda Section IV: Signal Processing &amp; Graph Theory","text":"<p>Overview: This section highlights the powerful role of linear algebra in signal processing and graph theory, two cornerstone fields in electrical engineering, computer science, and data analysis. Students will learn to transform, analyze, and interpret signals and networks through matrix-based methods.</p>"},{"location":"old-sections/section-4/section-4/#chapter-14-fourier-and-cosine-transforms","title":"Chapter 14: Fourier and Cosine Transforms","text":"<ul> <li>Key Concepts: Fourier Transform and matrices, Discrete Cosine Transform (DCT), Fast Fourier Transform (FFT) matrix view.</li> <li>Focus: Understand how linear transformations encode frequency information and enable efficient signal processing.</li> <li>Skills: Apply matrix representations of Fourier techniques to analyze signals, compress data, and perform transformations in both time and frequency domains.</li> </ul>"},{"location":"old-sections/section-4/section-4/#chapter-15-graph-theory-and-linear-algebra","title":"Chapter 15: Graph Theory and Linear Algebra","text":"<ul> <li>Key Concepts: Graph adjacency matrices, incidence matrices, graph Laplacian matrices, spectral clustering.</li> <li>Focus: Represent and study graphs using matrices; apply linear algebraic techniques to understand network structure and clustering.</li> <li>Skills: Model complex networks, compute graph partitions, and explore eigenvalue-based methods for network analysis.</li> </ul>"},{"location":"old-sections/section-4/section-4/#chapter-16-applications-in-networks-and-flows","title":"Chapter 16: Applications in Networks and Flows","text":"<ul> <li>Key Concepts: Network flow problems, convolution as matrix multiplication, projections onto subspaces, reflection and rotation matrices (2D and 3D), cross product operations.</li> <li>Focus: Apply matrix techniques to dynamic systems, network flow optimization, and geometric transformations.</li> <li>Skills: Solve flow optimization problems, perform signal convolutions, and execute spatial transformations crucial for graphics, physics, and control.</li> </ul>"},{"location":"old-sections/section-4/section-4/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Transform signals efficiently using Fourier and cosine matrix representations. - Model and analyze networks and graphs through adjacency and Laplacian matrices. - Apply linear algebra techniques to practical problems in signal processing, optimization, and spatial modeling. - Interpret eigenvalues and eigenvectors as tools for clustering, filtering, and network dynamics.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/","title":"\ud83d\udcd6 Chapter 14: Fourier and Cosine Transforms","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#overview","title":"Overview","text":"<p>Signal processing fundamentally relies on understanding how a signal changes over time and across frequencies. In this chapter, you will explore how linear algebra provides the mathematical machinery for these transformations. We will particularly focus on how matrices represent the Fourier Transform, the Discrete Cosine Transform (DCT), and Fast Fourier Transform (FFT) techniques.</p> <p>You already know how matrices act as linear transformations on vectors from previous sections. Now, you will see how carefully chosen matrices can \"rotate\" a signal from the time domain to the frequency domain \u2014 revealing hidden patterns like periodicity and enabling powerful applications like signal compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#141-fourier-transform-and-matrices","title":"14.1 Fourier Transform and Matrices","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#what-is-the-fourier-transform","title":"What is the Fourier Transform?","text":"<p>The Fourier Transform expresses a signal as a sum of complex sinusoids (sines and cosines). It helps us answer the question:</p> <p>\"What frequencies are present in my signal, and how strong are they?\"</p> <p>Instead of looking at how a signal behaves over time, we want to look at its frequency content.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-does-the-fourier-matrix-work","title":"How Does the Fourier Matrix Work?","text":"<p>The Discrete Fourier Transform (DFT) can be represented as multiplication by a special matrix, called the Fourier Matrix \\(F_n\\) of size \\(n \\times n\\).</p> <p>Each entry of \\(F_n\\) is:</p> \\[ (F_n)_{jk} = \\omega_n^{jk} \\quad \\text{where} \\quad \\omega_n = e^{-2\\pi i/n} \\] <p>Here, \\(\\omega_n\\) is a \"primitive root of unity,\" a complex number representing one full circle of rotation split into \\(n\\) parts.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#intuitive-explanation","title":"Intuitive Explanation","text":"<p>Think of \\(\\omega_n\\) as a tiny clock hand. Multiplying by powers of \\(\\omega_n\\) spins vectors around the complex plane. The DFT sums these spins in a way that identifies the signal's dominant frequencies.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#key-properties-of-the-fourier-matrix","title":"Key Properties of the Fourier Matrix:","text":"<ul> <li>Unitary: \\(F_n^* F_n = nI\\) (where \\(F_n^*\\) is the conjugate transpose)</li> <li>Invertible: You can recover the original signal by applying \\(F_n^{-1}\\).</li> <li>Efficient: Although \\(F_n\\) is dense, its structure enables fast computation.</li> </ul>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#example","title":"Example","text":"<p>Given a simple signal: $$ \\mathbf{x} = [1, 0, -1, 0]^T $$ Applying the Fourier matrix transforms it into frequency components. The result tells us which \"pure vibrations\" build up this signal.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#142-discrete-cosine-transform-dct","title":"14.2 Discrete Cosine Transform (DCT)","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#what-is-the-dct","title":"What is the DCT?","text":"<p>While the Fourier Transform uses complex numbers (sines and cosines together), the Discrete Cosine Transform focuses only on cosine terms \u2014 making it entirely real-valued and often more efficient for practical applications.</p> <p>DCT is particularly important for compression, like how JPEG images reduce file size without losing visible quality.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-does-the-dct-matrix-work","title":"How Does the DCT Matrix Work?","text":"<p>The DCT transformation can also be seen as multiplication by a special matrix \\(C_n\\), where each element is a cosine of equally spaced angles.</p> \\[ (C_n)_{jk} = \\cos\\left( \\frac{\\pi}{n} \\left(j + \\frac{1}{2}\\right)k \\right) \\] <p>This formula ensures orthogonality (no redundant information) and compresses most energy into fewer components.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#intuitive-explanation_1","title":"Intuitive Explanation","text":"<p>Imagine drawing a signal as a squiggly line. The DCT breaks this line into a combination of \"smooth hills\" (cosine curves) \u2014 the fewer hills needed, the better the compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#why-use-dct-over-dft","title":"Why Use DCT Over DFT?","text":"<ul> <li>Efficiency: Works better for real, even-symmetric data.</li> <li>Compression: Most of the important information often gets concentrated in the first few coefficients.</li> </ul>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#143-fast-fourier-transform-fft","title":"14.3 Fast Fourier Transform (FFT)","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#why-do-we-need-fft","title":"Why Do We Need FFT?","text":"<p>Directly multiplying a vector by \\(F_n\\) takes \\(O(n^2)\\) operations, which becomes too slow for large \\(n\\). The Fast Fourier Transform (FFT) cleverly reduces this to \\(O(n \\log n)\\) by exploiting the symmetry of the Fourier matrix.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#how-fft-works","title":"How FFT Works","text":"<p>The idea behind FFT is divide and conquer: - Break the signal into even and odd parts. - Apply the Fourier transform recursively to smaller parts. - Combine the results efficiently using simple additions and multiplications.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#visual-analogy","title":"Visual Analogy","text":"<p>Imagine a giant jigsaw puzzle. Instead of solving it all at once, you first assemble smaller regions, then stitch them together. That saves a lot of work!</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#impact","title":"Impact","text":"<p>FFT is one of the most important algorithms in all of computer science and engineering. It powers everything from Wi-Fi and 5G networks to audio compression and digital photography.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#144-connecting-to-previous-topics","title":"14.4 Connecting to Previous Topics","text":"<p>In earlier chapters, you learned about: - Linear transformations: Matrix actions on vectors. - Eigenvalues and eigenvectors: Special vectors that reveal intrinsic properties of transformations.</p> <p>Here, the DFT and DCT are special types of linear transformations \u2014 and the matrices \\(F_n\\) and \\(C_n\\) have eigenproperties that make them extremely useful for diagonalizing convolution operators (used heavily in signal processing).</p> <p>Understanding how signals decompose into simpler parts echoes the ideas of basis change and projection you learned earlier.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-14/chapter-14/#fourier-and-cosine-transforms","title":"Fourier and Cosine Transforms","text":"<p>What does the DCT (Discrete Cosine Transform) primarily help with in signal processing?</p> <p>A. Signal encryption B. Signal compression C. Noise amplification D. Increasing signal frequency  </p> Show Answer <p>The correct answer is B. The DCT is widely used for signal compression. In particular, it concentrates signal energy into fewer coefficients, which allows for efficient storage and transmission, such as in JPEG image compression.</p>"},{"location":"old-sections/section-4/chapter-14/chapter-14/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Fourier Transforms use matrices to uncover frequency components hidden in time signals.</li> <li>DCT focuses only on cosine components, enabling highly efficient compression.</li> <li>FFT accelerates Fourier computations, enabling practical use of frequency analysis in large datasets and real-time applications.</li> </ul> <p>Ready for Chapter 15? We'll now see how matrices help us model and analyze networks in Graph Theory!</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/","title":"\ud83d\udcd6 Chapter 15: Graph Theory and Linear Algebra","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#overview","title":"Overview","text":"<p>Graph theory models relationships between objects, and linear algebra provides the powerful language to study these connections. In this chapter, you'll learn how matrices represent and analyze graphs, and how eigenvalues and eigenvectors reveal deep insights about network structure.</p> <p>By extending your knowledge of matrices, projections, and spectral theory, you will now be able to tackle problems like: - Finding important nodes in a network, - Partitioning a network into communities, - Modeling flow and connectivity in real-world systems.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#151-adjacency-and-incidence-matrices","title":"15.1 Adjacency and Incidence Matrices","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#adjacency-matrix","title":"Adjacency Matrix","text":"<p>An adjacency matrix \\(A\\) for a graph with \\(n\\) nodes is an \\(n \\times n\\) matrix where: - \\(A_{ij} = 1\\) if there is an edge from node \\(i\\) to node \\(j\\), - \\(A_{ij} = 0\\) otherwise.</p> <p>If the graph is undirected, \\(A\\) is symmetric.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#example","title":"Example","text":"<p>A simple graph: <pre><code>1 --- 2\n|     |\n3 --- 4\n</code></pre> Adjacency matrix: $$ A = \\begin{bmatrix}0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#incidence-matrix","title":"Incidence Matrix","text":"<p>An incidence matrix \\(B\\) for a graph with \\(n\\) nodes and \\(m\\) edges is an \\(n \\times m\\) matrix where: - Each column represents an edge. - Entries are \\(1\\), \\(-1\\), or \\(0\\), depending on whether a node is the start or end of the edge (for directed graphs).</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#152-graph-laplacians-and-their-properties","title":"15.2 Graph Laplacians and Their Properties","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#definition","title":"Definition","text":"<p>The graph Laplacian \\(L\\) is defined as: $$ L = D - A $$ where: - \\(D\\) is the degree matrix (diagonal, with node degrees), - \\(A\\) is the adjacency matrix.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#why-the-laplacian","title":"Why the Laplacian?","text":"<ul> <li>It captures both local (neighbor) and global (whole graph) structure.</li> <li>Laplacian eigenvalues reveal important properties like connectivity and clustering.</li> </ul>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#key-properties","title":"Key Properties:","text":"<ul> <li>The smallest eigenvalue is always \\(0\\).</li> <li>The number of zero eigenvalues = the number of connected components.</li> </ul>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#153-spectral-clustering","title":"15.3 Spectral Clustering","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#what-is-spectral-clustering","title":"What is Spectral Clustering?","text":"<p>Spectral clustering uses the eigenvectors of the Laplacian matrix to group nodes into clusters.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#how-it-works","title":"How It Works","text":"<ol> <li>Compute the Laplacian matrix \\(L\\).</li> <li>Find the eigenvectors corresponding to the smallest nonzero eigenvalues.</li> <li>Treat rows of these eigenvectors as new coordinates.</li> <li>Apply a simple clustering algorithm (like k-means) in this new space.</li> </ol>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#visual-analogy","title":"Visual Analogy","text":"<p>Imagine stretching a rubber band model of your graph. Natural clusters will \"pull apart\" \u2014 and that's what the eigenvectors show!</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#why-it-works","title":"Why It Works","text":"<p>The eigenvectors minimize a quantity called graph cut, ensuring that closely connected nodes stay together.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#154-connecting-to-previous-topics","title":"15.4 Connecting to Previous Topics","text":"<p>This chapter builds on: - Matrix representations: Adjacency and incidence matrices extend your matrix vocabulary. - Eigenvalues and eigenvectors: Understanding Laplacian spectra gives you a way to \"feel\" the structure of graphs. - Subspaces and projections: Spectral clustering is really a projection of graph data into meaningful subspaces.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-15/chapter-15/#graph-theory-and-linear-algebra","title":"Graph Theory and Linear Algebra","text":"<p>What matrix is typically used to understand graph connectivity and clustering in spectral graph theory?</p> <p>A. Adjacency matrix B. Laplacian matrix C. Incidence matrix D. Degree matrix  </p> Show Answer <p>The correct answer is B. The Laplacian matrix captures the structure of a graph and its eigenvalues and eigenvectors provide critical information for clustering, connectivity analysis, and spectral graph theory applications.</p>"},{"location":"old-sections/section-4/chapter-15/chapter-15/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Adjacency and incidence matrices encode graph connections.</li> <li>The Laplacian matrix reveals deep structural properties of graphs.</li> <li>Spectral methods use eigenvalues and eigenvectors to partition graphs and analyze network connectivity.</li> </ul> <p>Ready for Chapter 16? We'll explore network flows, convolutions, and spatial transformations next!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/","title":"\ud83d\udcd6 Chapter 16: Applications in Networks and Flows","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#overview","title":"Overview","text":"<p>In this final chapter of Section IV, we apply linear algebra to real-world problems in network flows, signal convolution, and spatial transformations. You'll see how matrices model dynamic systems, optimize resource distribution, and manipulate objects in physical space.</p> <p>Each concept builds on your understanding of matrices as powerful, flexible tools \u2014 not just static arrays of numbers, but active agents that move, combine, and optimize data.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#161-network-flow-problems","title":"16.1 Network Flow Problems","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#modeling-flows-with-matrices","title":"Modeling Flows with Matrices","text":"<p>Imagine a network of pipes, traffic intersections, or internet routers. We want to model how much \"stuff\" flows from point A to point B.</p> <p>We use incidence matrices and flow vectors: - Each row represents a node. - Each column represents an edge (connection).</p> <p>The key idea:</p> <p>Conservation Law: The total inflow minus outflow at each node must equal the external supply/demand.</p> <p>This gives rise to a system of linear equations: $$ B \\mathbf{f} = \\mathbf{s} $$ where: - \\(B\\) is the incidence matrix, - \\(\\mathbf{f}\\) is the flow vector, - \\(\\mathbf{s}\\) is the source/sink vector.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#solving-flow-problems","title":"Solving Flow Problems","text":"<p>You solve for \\(\\mathbf{f}\\) just like you solve any linear system \u2014 using methods such as LU decomposition, least squares, or iterative techniques when necessary.</p> <p>This highlights linear algebra's ability to model and optimize real-world systems!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#162-convolution-as-matrix-multiplication","title":"16.2 Convolution as Matrix Multiplication","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#what-is-convolution","title":"What is Convolution?","text":"<p>Convolution is a way of combining two signals to produce a third. It's crucial in fields like: - Image processing (blur, sharpen filters) - Audio effects (echo, reverb) - Engineering systems (impulse responses)</p> <p>Discrete convolution between two sequences can be expressed as matrix multiplication using a special kind of matrix called a Toeplitz matrix.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#toeplitz-matrix-structure","title":"Toeplitz Matrix Structure","text":"<p>A Toeplitz matrix has constant diagonals: $$ T = \\begin{bmatrix}t_0 &amp; 0 &amp; 0 \\\\ t_1 &amp; t_0 &amp; 0 \\\\ t_2 &amp; t_1 &amp; t_0\\end{bmatrix} $$</p> <p>Multiplying \\(T\\) by a vector \\(\\mathbf{x}\\) performs convolution!</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#why-matrix-formulation","title":"Why Matrix Formulation?","text":"<ul> <li>Unified Framework: Convolution becomes just another matrix operation.</li> <li>Efficient Algorithms: Fast convolution methods often use matrix factorizations.</li> </ul>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#163-reflection-and-rotation-matrices-2d-and-3d","title":"16.3 Reflection and Rotation Matrices (2D and 3D)","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#geometric-transformations","title":"Geometric Transformations","text":"<p>Spatial transformations like rotation and reflection are elegantly expressed using matrices.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#reflection-across-a-line-2d","title":"Reflection Across a Line (2D)","text":"<p>The reflection matrix across a line at angle \\(\\theta\\) is: $$ R = \\begin{bmatrix}\\cos 2\\theta &amp; \\sin 2\\theta \\\\ \\sin 2\\theta &amp; -\\cos 2\\theta\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#rotation-2d","title":"Rotation (2D)","text":"<p>Rotation by angle \\(\\theta\\) counterclockwise: $$ \\text{Rotation} = \\begin{bmatrix}\\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta\\end{bmatrix} $$</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#extension-to-3d","title":"Extension to 3D","text":"<p>In 3D, rotation matrices expand to \\(3 \\times 3\\) matrices, often around principal axes (X, Y, Z).</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Graphics: Moving objects in games and simulations.</li> <li>Physics: Modeling rigid body motion.</li> <li>Robotics: Calculating arm movements.</li> </ul> <p>Matrix formulations allow you to chain multiple transformations simply by multiplying matrices.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#164-cross-product-operations","title":"16.4 Cross Product Operations","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#what-is-the-cross-product","title":"What is the Cross Product?","text":"<p>Given two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) in \\(\\mathbb{R}^3\\), the cross product \\(\\mathbf{u} \\times \\mathbf{v}\\) produces a vector orthogonal to both.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#matrix-formulation","title":"Matrix Formulation","text":"<p>The cross product can be represented as a matrix multiplication: $$ \\mathbf{u} \\times \\mathbf{v} = \\mathbf{u} \\times \\mathbf{v} $$ where \\([\\mathbf{u}]_\\times\\) is the skew-symmetric matrix: $$ [\\mathbf{u}]_\\times = \\begin{bmatrix}0 &amp; -u_3 &amp; u_2 \\\\ u_3 &amp; 0 &amp; -u_1 \\\\ -u_2 &amp; u_1 &amp; 0\\end{bmatrix} $$</p> <p>This clever trick allows cross products to fit neatly into the world of linear transformations.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#165-connecting-to-previous-topics","title":"16.5 Connecting to Previous Topics","text":"<p>This chapter synthesizes ideas from: - Matrix multiplication and structure (Toeplitz, skew-symmetric matrices) - Linear systems (modeling network flows) - Geometric transformations (applying rotations and reflections)</p> <p>By now, you see that linear algebra underpins nearly every structure in signal processing, optimization, and spatial modeling.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#quiz","title":"\u270f\ufe0f Quiz","text":""},{"location":"old-sections/section-4/chapter-16/chapter-16/#applications-in-networks-and-flows","title":"Applications in Networks and Flows","text":"<p>In network flow optimization, what does the Max-Flow Min-Cut Theorem primarily relate?</p> <p>A. The shortest path and the longest path B. Maximum network throughput and minimum cut capacity C. Maximum node degree and minimum spanning tree D. Signal-to-noise ratio  </p> Show Answer <p>The correct answer is B. The Max-Flow Min-Cut Theorem states that the maximum amount of flow passing from a source to a sink in a network equals the minimum capacity that, when removed, would disconnect the source from the sink.</p>"},{"location":"old-sections/section-4/chapter-16/chapter-16/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Network flows are modeled and optimized using incidence matrices.</li> <li>Convolution operations can be framed as matrix multiplications.</li> <li>Geometric transformations (rotations, reflections) are elegantly expressed via matrices.</li> <li>Cross products fit neatly into matrix language via skew-symmetric matrices.</li> </ul> <p>Congratulations on completing Section IV! \ud83d\ude80 Next, you'll dive into applications of linear algebra in machine learning and data science!</p>"},{"location":"old-sections/section-5/section-5/","title":"\ud83d\udcda Section V: Data Science &amp; Machine Learning","text":"<p>Overview: This final section demonstrates how linear algebra forms the computational and conceptual foundation of modern data science and machine learning. Students will explore how matrices and vector spaces drive dimensionality reduction, model training, optimization, and kernel-based learning.</p>"},{"location":"old-sections/section-5/section-5/#chapter-17-principal-component-analysis-and-beyond","title":"Chapter 17: Principal Component Analysis and Beyond","text":"<ul> <li>Key Concepts: Covariance matrices, Principal Component Analysis (PCA), data compression via PCA, whitening transformations, low-rank matrix approximation, matrix completion.</li> <li>Focus: Reduce dimensionality and uncover structure in high-dimensional datasets using spectral techniques.</li> <li>Skills: Perform PCA, compress data while preserving variance, complete missing matrix entries in real-world datasets.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-18-machine-learning-foundations","title":"Chapter 18: Machine Learning Foundations","text":"<ul> <li>Key Concepts: Neural networks and linear layers, weight initialization using SVD, backpropagation and matrix calculus, gradient descent for solving linear systems, batch vs. stochastic methods.</li> <li>Focus: Understand how matrix operations power learning algorithms and neural network training.</li> <li>Skills: Construct and train linear models, differentiate matrix functions, optimize learning dynamics.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-19-advanced-optimization-techniques","title":"Chapter 19: Advanced Optimization Techniques","text":"<ul> <li>Key Concepts: Optimization landscapes, Hessians, Newton\u2019s method with matrices, convexity and linear functions.</li> <li>Focus: Explore curvature and convergence properties of loss functions using second-order information.</li> <li>Skills: Analyze critical points, perform Newton-based updates, and identify convexity in optimization problems.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-20-kernel-methods-and-collaborative-filtering","title":"Chapter 20: Kernel Methods and Collaborative Filtering","text":"<ul> <li>Key Concepts: Support Vector Machines (SVMs), kernel trick, positive semidefinite kernels, Gram matrices, collaborative filtering (matrix factorization), Eigenfaces for face recognition.</li> <li>Focus: Extend linear models to nonlinear settings via kernel methods; apply matrix factorization to recommendation systems and facial recognition.</li> <li>Skills: Use kernels to map data into higher dimensions, perform matrix factorization for prediction, and apply eigenfaces in image classification.</li> </ul>"},{"location":"old-sections/section-5/section-5/#chapter-21-specialized-topics-in-linear-algebra-applications","title":"Chapter 21: Specialized Topics in Linear Algebra Applications","text":"<ul> <li>Key Concepts: Spectral theorem, matrix perturbation theory, tensor basics, matrix calculus fundamentals, Kronecker product, vectorization, sparse matrix solvers, AI-driven applications.</li> <li>Focus: Investigate advanced applications and techniques bridging linear algebra with emerging fields in AI and big data.</li> <li>Skills: Apply theoretical tools in practical machine learning pipelines, use vectorization and sparse matrices for large-scale efficiency.</li> </ul>"},{"location":"old-sections/section-5/section-5/#learning-outcomes","title":"\u2728 Learning Outcomes:","text":"<p>By the end of this section, students will: - Leverage linear algebra to perform feature extraction, dimensionality reduction, and matrix completion. - Implement learning algorithms grounded in matrix calculus and optimization. - Extend linear methods into nonlinear domains via kernels and support vector machines. - Apply advanced linear algebra in recommendation systems, neural networks, and AI-driven tasks.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/","title":"Chapter 17: Principal Component Analysis and Beyond","text":""},{"location":"old-sections/section-5/chapter-17/chapter-17/#introduction","title":"Introduction","text":"<p>Imagine standing in a massive library where every book represents a feature of your dataset. Some books are filled with crucial knowledge; others, not so much. Principal Component Analysis (PCA) helps us figure out which \"books\" contain the most useful information, allowing us to focus only on the essentials.</p> <p>In this chapter, we explore how PCA uses linear algebra \u2014 specifically eigenvalues and eigenvectors \u2014 to reduce the complexity of data while preserving its most important structures. We'll also touch on related ideas like whitening transformations, low-rank approximations, and matrix completion, expanding your toolkit for working with real-world, messy datasets.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#1-covariance-matrices","title":"1. Covariance Matrices","text":"<p>Before we dive into PCA, we must understand covariance matrices.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#what-is-covariance","title":"What is Covariance?","text":"<p>Covariance measures how two variables change together: - Positive covariance: Variables increase together. - Negative covariance: As one increases, the other decreases.</p> <p>Mathematically, for variables \\(X\\) and \\(Y\\):</p> \\[ \\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] \\] <p>where \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of \\(X\\) and \\(Y\\).</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#building-the-covariance-matrix","title":"Building the Covariance Matrix","text":"<p>Given a dataset with multiple features, the covariance matrix summarizes covariances between all pairs of features. For a data matrix \\(X\\) (after centering by subtracting the mean):</p> \\[ \\Sigma = \\frac{1}{n-1}X^T X \\] <p>Key Insight: The covariance matrix is symmetric and its structure tells us how features relate. Eigenvalues and eigenvectors of this matrix reveal the most important directions in the data.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#2-principal-component-analysis-pca","title":"2. Principal Component Analysis (PCA)","text":""},{"location":"old-sections/section-5/chapter-17/chapter-17/#intuitive-idea","title":"Intuitive Idea","text":"<p>PCA finds new axes (directions) where the data variance is maximized. Think of spinning a cloud of data points: PCA aligns a new coordinate system along the directions of greatest spread.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#how-it-works","title":"How It Works","text":"<ol> <li>Center the data: Subtract the mean from each feature.</li> <li>Compute the covariance matrix \\(\\Sigma\\).</li> <li>Find eigenvectors and eigenvalues:<ul> <li>Eigenvectors = Principal components (new axes)</li> <li>Eigenvalues = How much variance each principal component captures</li> </ul> </li> <li>Sort eigenvectors by eigenvalues in descending order.</li> <li>Project data onto the top \\(k\\) eigenvectors to reduce dimensionality.</li> </ol>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#why-pca-works","title":"Why PCA Works","text":"<ul> <li>Energy (variance) is preserved: We keep most of the \"action\" of the data.</li> <li>Orthogonality of principal components ensures that projections are independent and non-redundant.</li> </ul> <p>Quick Visualization</p> <p>Imagine shining a flashlight onto a 3D object. The resulting 2D shadow captures the shape. PCA finds the best way to cast that shadow to preserve maximum detail.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#3-whitening-transformations","title":"3. Whitening Transformations","text":"<p>PCA decorrelates features but may still have different variances along each axis. Whitening goes a step further: it scales the principal components so that they have unit variance.</p> <p>Mathematically:</p> \\[ X_{\\text{white}} = \\Lambda^{-1/2} U^T X \\] <p>where \\(U\\) contains the eigenvectors and \\(\\Lambda\\) is the diagonal matrix of eigenvalues.</p> <p>Use Cases: - Improve stability and convergence in machine learning algorithms. - Standardize features without introducing correlations.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#4-low-rank-matrix-approximation","title":"4. Low-Rank Matrix Approximation","text":"<p>Often, data can be well-approximated by a matrix with lower rank \u2014 fewer \"independent dimensions\" than the original.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#how-it-works_1","title":"How It Works","text":"<ul> <li>Singular Value Decomposition (SVD) decomposes a matrix into \\(U \\Sigma V^T\\).</li> <li>By keeping only the largest singular values and corresponding vectors, we build a lower-rank approximation.</li> </ul> <p>Movie Ratings Dataset</p> <p>Suppose a movie ratings matrix has missing entries. A low-rank approximation guesses missing ratings based on patterns in the data.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#5-matrix-completion","title":"5. Matrix Completion","text":"<p>Matrix completion fills missing values in a matrix, assuming the data lies in a low-dimensional space.</p> <ul> <li>Widely used in recommender systems (e.g., Netflix recommendations).</li> <li>Methods include minimizing the nuclear norm (sum of singular values) subject to constraints from known entries.</li> </ul> <p>The idea is that with only a few observations, if the data is simple enough (low-rank), we can guess the missing pieces accurately.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#summary","title":"Summary","text":"<ul> <li>Covariance matrices reveal relationships between features.</li> <li>PCA finds the directions of maximum variance to compress data.</li> <li>Whitening standardizes data after PCA.</li> <li>Low-rank approximation captures essential structure with fewer dimensions.</li> <li>Matrix completion guesses missing data using low-rank assumptions.</li> </ul> <p>These techniques are central to fields like data science, image compression, genomics, and more.</p>"},{"location":"old-sections/section-5/chapter-17/chapter-17/#quiz-question","title":"Quiz Question","text":"<p>Which matrix operation is central to finding the principal components in PCA?</p> <p>A. Matrix inversion B. Eigen decomposition of the covariance matrix C. LU decomposition D. QR factorization</p> Show Answer <p>The correct answer is B. PCA relies on the eigen decomposition of the data\u2019s covariance matrix to find directions (principal components) that capture the most variance.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/","title":"Chapter 18: Machine Learning Foundations","text":""},{"location":"old-sections/section-5/chapter-18/chapter-18/#introduction","title":"Introduction","text":"<p>Imagine teaching a robot how to recognize handwritten digits or detect objects in photos. How do you get it to learn patterns from numbers and pixels? The answer lies in linear algebra \u2014 particularly the way matrices represent transformations and learning steps.</p> <p>In this chapter, we build on your knowledge of matrices, eigenvalues, and matrix decompositions to understand the mathematical heart of machine learning. We'll explore linear layers, weight initialization, matrix calculus, and gradient descent \u2014 the essential \"training rituals\" behind intelligent systems.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#1-neural-networks-and-linear-layers","title":"1. Neural Networks and Linear Layers","text":"<p>At its core, a neural network is just a collection of matrix operations.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#linear-transformation","title":"Linear Transformation","text":"<p>Each layer in a network applies a matrix multiplication:</p> \\[ Z = W X + b \\] <p>where: - \\(W\\) = weight matrix - \\(X\\) = input vector - \\(b\\) = bias vector</p> <p>This operation transforms input data into a new space, hoping to make patterns easier to recognize.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#building-off-prior-knowledge","title":"Building Off Prior Knowledge","text":"<p>Remember PCA? PCA found new axes (principal components) that reveal structure. Similarly, neural networks learn new transformations (via \\(W\\)) that uncover hidden patterns in data.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#2-weight-initialization-using-svd","title":"2. Weight Initialization Using SVD","text":"<p>Training a neural network can fail if we start with poor weight values.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#the-problem","title":"The Problem","text":"<ul> <li>If weights are too large or small, the outputs can explode or vanish.</li> <li>This makes learning very slow or unstable.</li> </ul>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#the-solution","title":"The Solution","text":"<p>Use ideas from Singular Value Decomposition (SVD): - Initialize weights so that they are \"balanced\" across dimensions. - SVD helps ensure that initial transformations preserve variance, just like PCA!</p> <p>Common strategies: - Xavier Initialization: weights are scaled according to the number of input and output nodes. - He Initialization: scaled for ReLU activations.</p> <p>Why This Works</p> <p>Balanced weights prevent early layers from distorting the data too much, making learning smoother and faster.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#3-backpropagation-and-matrix-calculus","title":"3. Backpropagation and Matrix Calculus","text":"<p>To train a neural network, we must compute how the output error changes with respect to every weight \u2014 this is backpropagation.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#key-ideas","title":"Key Ideas","text":"<ul> <li>Use matrix calculus to compute gradients.</li> <li>Gradients tell us the direction and amount to adjust weights.</li> </ul> <p>If \\(L\\) is the loss function and \\(W\\) is the weight matrix:</p> \\[ \\frac{\\partial L}{\\partial W} \\] <p>computes how a small change in \\(W\\) changes the loss \\(L\\).</p> <p>Matrix calculus rules (like the chain rule) allow efficient computation over entire layers.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#4-gradient-descent-for-solving-linear-systems","title":"4. Gradient Descent for Solving Linear Systems","text":"<p>Gradient Descent is the workhorse of learning.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#how-it-works","title":"How It Works","text":"<ol> <li>Compute the gradient of the loss with respect to parameters.</li> <li>Update parameters in the negative direction of the gradient:</li> </ol> \\[ \\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta) \\] <p>where \\(\\eta\\) is the learning rate.</p> <p>In linear systems, this mirrors methods like the Conjugate Gradient for finding solutions iteratively.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#5-batch-vs-stochastic-methods","title":"5. Batch vs. Stochastic Methods","text":"<p>When updating weights, should we use the entire dataset or just parts?</p> <ul> <li>Batch Gradient Descent: Uses the whole dataset. More accurate but slow.</li> <li>Stochastic Gradient Descent (SGD): Uses one sample at a time. Faster but noisier.</li> <li>Mini-Batch Gradient Descent: A compromise \u2014 small groups of samples.</li> </ul> <p>Choosing the right method depends on dataset size, noise tolerance, and computational resources.</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#summary","title":"Summary","text":"<ul> <li>Neural networks build on linear transformations (matrix multiplication).</li> <li>Weight initialization strategies use SVD concepts to stabilize early learning.</li> <li>Backpropagation applies matrix calculus to compute gradients.</li> <li>Gradient descent is an iterative method similar to solving linear systems.</li> <li>Batch, mini-batch, and stochastic updates balance speed and accuracy.</li> </ul> <p>Understanding these concepts is crucial for anyone wanting to master machine learning foundations!</p>"},{"location":"old-sections/section-5/chapter-18/chapter-18/#quiz-question","title":"Quiz Question","text":"<p>In backpropagation, which mathematical concept is primarily used to compute how the loss changes with respect to weights?</p> <p>A. Matrix multiplication B. Matrix inverse C. Matrix calculus (derivatives) D. Kronecker product</p> Show Answer <p>The correct answer is C. Backpropagation requires calculating derivatives of the loss function with respect to model parameters using matrix calculus.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/","title":"Chapter 19: Advanced Optimization Techniques","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#introduction","title":"Introduction","text":"<p>Imagine climbing a mountain covered in fog, trying to find the highest peak (or the lowest valley if minimizing). You could walk uphill using just the steepness (gradient), but wouldn\u2019t it be faster if you also knew the shape of the land around you? That\u2019s the power of second-order optimization techniques like Newton's method.</p> <p>This chapter delves into the geometry of optimization landscapes, the role of curvature via the Hessian matrix, and how linear algebra unlocks faster and smarter ways to optimize machine learning models.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#1-optimization-landscapes","title":"1. Optimization Landscapes","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#visualizing-optimization","title":"Visualizing Optimization","text":"<p>An optimization landscape maps inputs to outputs (loss values). Picture a rolling 3D terrain: - Peaks = local maxima - Valleys = local minima</p> <p>Finding the lowest point is minimizing the loss function.</p> <p>Gradients tell us the direction of steepest descent, but curvature tells us how steep or flat the landscape is, helping us adjust our steps accordingly.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#building-from-earlier-chapters","title":"Building from Earlier Chapters","text":"<ul> <li>Gradient Descent (Chapter 18) uses first derivatives (gradients).</li> <li>Newton's Method (this chapter) adds second derivatives (curvature) to refine steps.</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#2-the-hessian-matrix","title":"2. The Hessian Matrix","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#what-is-the-hessian","title":"What is the Hessian?","text":"<p>The Hessian matrix captures second-order derivatives of a scalar function with respect to multiple variables.</p> <p>For a function \\(f(x_1, x_2, \\dots, x_n)\\):</p> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix} \\] <p>Key Properties: - Symmetric (if the function is smooth) - Describes local curvature</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#3-newtons-method-with-matrices","title":"3. Newton's Method with Matrices","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#how-it-works","title":"How It Works","text":"<p>Newton\u2019s update formula:</p> \\[ \\theta_{\\text{new}} = \\theta - H^{-1} \\nabla f(\\theta) \\] <ul> <li>\\(\\nabla f(\\theta)\\) = gradient (first derivative)</li> <li>\\(H\\) = Hessian (second derivative)</li> </ul> <p>Instead of just following the slope, we adjust steps using curvature to reach minima faster.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#why-it-works","title":"Why It Works","text":"<ul> <li>When near an optimum, Newton's method can converge quadratically (very fast).</li> <li>Corrects overly aggressive or overly timid steps that plain gradient descent would take.</li> </ul> <p>Trade-Offs</p> <p>Calculating and inverting the Hessian is expensive for high-dimensional data. Newton\u2019s method is powerful but often reserved for smaller problems or when approximations like quasi-Newton methods (e.g., BFGS) are available.</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#4-convexity-and-linear-functions","title":"4. Convexity and Linear Functions","text":""},{"location":"old-sections/section-5/chapter-19/chapter-19/#convexity","title":"Convexity","text":"<p>A function is convex if the line segment between any two points on the graph lies above or on the graph itself.</p> <p>In mathematical terms:</p> \\[ f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y) \\quad \\forall \\lambda \\in [0,1] \\]"},{"location":"old-sections/section-5/chapter-19/chapter-19/#why-convexity-matters","title":"Why Convexity Matters","text":"<ul> <li>No local minima trap: Every local minimum is a global minimum.</li> <li>Optimization algorithms (gradient descent, Newton's method) behave more predictably and converge faster.</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#identifying-convexity","title":"Identifying Convexity","text":"<ul> <li>A twice-differentiable function is convex if its Hessian is positive semidefinite (all eigenvalues \\( \\geq 0 \\)).</li> </ul>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#summary","title":"Summary","text":"<ul> <li>Optimization landscapes describe the shape of loss functions.</li> <li>The Hessian matrix encodes curvature information through second derivatives.</li> <li>Newton's method uses Hessians to achieve faster convergence.</li> <li>Convex functions are ideal because they guarantee global minima and stability.</li> </ul> <p>Understanding these optimization techniques opens doors to training more powerful machine learning models efficiently!</p>"},{"location":"old-sections/section-5/chapter-19/chapter-19/#quiz-question","title":"Quiz Question","text":"<p>What is the main advantage of using Newton\u2019s method over gradient descent in optimization?</p> <p>A. It requires fewer matrix computations B. It uses curvature information to converge faster C. It avoids matrix inverses D. It minimizes computation time at every step</p> Show Answer <p>The correct answer is B. Newton\u2019s method uses second-order information (the Hessian) about the curvature of the function, allowing it to converge more quickly near the optimum.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/","title":"Chapter 20: Kernel Methods and Collaborative Filtering","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#introduction","title":"Introduction","text":"<p>Suppose you want to classify data that isn\u2019t neatly separated by a straight line (like distinguishing spirals or circles). What if we could \"lift\" the data into a higher dimension where it is separable by a line? Kernel methods make this possible \u2014 and they rely on clever applications of linear algebra!</p> <p>This chapter also introduces collaborative filtering, which uses matrix factorization to build recommendation systems, and explores real-world applications like Eigenfaces for facial recognition.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#1-support-vector-machines-svms-and-the-kernel-trick","title":"1. Support Vector Machines (SVMs) and the Kernel Trick","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#linear-svms","title":"Linear SVMs","text":"<p>In simple SVMs, we seek a hyperplane (a line in 2D, a plane in 3D, etc.) that best separates data into two classes by maximizing the margin between them.</p> <p>Mathematically, the hyperplane is defined as:</p> \\[ w^T x + b = 0 \\] <p>where: - \\(w\\) = weight vector (normal to the hyperplane) - \\(b\\) = bias</p> <p>Optimization aims to maximize the margin while correctly classifying as many points as possible.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#when-data-is-nonlinear","title":"When Data is Nonlinear","text":"<p>Often, data isn't linearly separable. To fix this, we can map data into a higher-dimensional space where separation is easier. But computing these higher-dimensional vectors explicitly is expensive!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#enter-the-kernel-trick","title":"Enter the Kernel Trick","text":"<p>Rather than computing transformations explicitly, the kernel trick computes the inner products in the higher-dimensional space directly using a kernel function \\(k(x, y)\\).</p> <p>Examples of kernels: - Linear Kernel: \\(k(x,y) = x^T y\\) - Polynomial Kernel: \\(k(x,y) = (x^T y + c)^d\\) - RBF (Gaussian) Kernel: \\(k(x,y) = \\exp\\left(-\\gamma \\|x-y\\|^2\\right)\\)</p> <p>Key Insight: You never need to know the mapping explicitly; you only need the inner products!</p> <p>Analogy</p> <p>Imagine trying to separate intertwined spaghetti noodles (complex data) on a plate. Instead of untangling them manually, you freeze them into ice, making them rigid and easier to separate. The kernel trick \"freezes\" complexity into manageable pieces.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#2-positive-semidefinite-kernels-and-gram-matrices","title":"2. Positive Semidefinite Kernels and Gram Matrices","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#positive-semidefinite-psd-kernels","title":"Positive Semidefinite (PSD) Kernels","text":"<p>A kernel function must generate a positive semidefinite Gram matrix: - A matrix \\(K\\) where each entry \\(K_{ij} = k(x_i, x_j)\\). - Positive semidefinite means that for any vector \\(z\\):</p> \\[ z^T K z \\geq 0 \\]"},{"location":"old-sections/section-5/chapter-20/chapter-20/#why-psd-matters","title":"Why PSD Matters","text":"<ul> <li>Ensures the kernel represents a valid inner product.</li> <li>Guarantees convexity in SVM optimization (critical for finding global minima).</li> </ul> <p>Important Reminder: PSD matrices have non-negative eigenvalues, linking back to your earlier understanding of eigenvalues from Chapters 5 and 6!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#3-collaborative-filtering-and-matrix-factorization","title":"3. Collaborative Filtering and Matrix Factorization","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#what-is-collaborative-filtering","title":"What is Collaborative Filtering?","text":"<p>Imagine a movie platform like Netflix. How do we recommend movies you haven't seen yet? - Collaborative Filtering predicts your preferences based on patterns in your and others' ratings.</p> <p>This leads to a large, partially-filled user-item matrix where entries are known ratings.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#matrix-factorization","title":"Matrix Factorization","text":"<p>The idea is to factorize the user-item matrix \\(M\\) into two low-rank matrices:</p> \\[ M \\approx U V^T \\] <p>where: - \\(U\\) = user factors (preferences) - \\(V\\) = item factors (attributes)</p> <p>The product \\(U V^T\\) reconstructs known ratings and fills in the missing ones.</p> <p>Optimization typically minimizes the reconstruction error:</p> \\[ \\min_{U, V} \\sum_{(i,j) \\in \\text{known}} (M_{ij} - U_i^T V_j)^2 \\]"},{"location":"old-sections/section-5/chapter-20/chapter-20/#4-eigenfaces-for-face-recognition","title":"4. Eigenfaces for Face Recognition","text":""},{"location":"old-sections/section-5/chapter-20/chapter-20/#eigenfaces-method","title":"Eigenfaces Method","text":"<ul> <li>Treat grayscale face images as large vectors.</li> <li>Compute the covariance matrix of the training faces.</li> <li>Perform eigen decomposition to find principal components (\"Eigenfaces\").</li> <li>Represent each face as a linear combination of a few Eigenfaces.</li> </ul> <p>Key Intuition: Faces live in a much smaller \"face space\" than the full pixel space!</p> <p>This is a direct application of PCA (Chapter 17) to image classification.</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#summary","title":"Summary","text":"<ul> <li>SVMs maximize margins for classification and use the kernel trick to handle nonlinear data.</li> <li>Kernels must create positive semidefinite Gram matrices to guarantee valid optimization.</li> <li>Collaborative filtering uses low-rank matrix factorization for recommendation systems.</li> <li>Eigenfaces apply PCA concepts to facial recognition by finding a lower-dimensional \"face space.\"</li> </ul> <p>These techniques are foundational for modern machine learning applications ranging from Netflix recommendations to facial ID systems!</p>"},{"location":"old-sections/section-5/chapter-20/chapter-20/#quiz-question","title":"Quiz Question","text":"<p>Which concept allows Support Vector Machines to operate in a higher-dimensional space without explicitly computing the mapping?</p> <p>A. Matrix decomposition B. Kernel trick C. Eigen decomposition D. SVD factorization</p> Show Answer <p>The correct answer is B. The kernel trick enables operations in higher-dimensional feature spaces without explicitly computing the transformations.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/","title":"Chapter 21: Specialized Topics in Linear Algebra Applications","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#introduction","title":"Introduction","text":"<p>In previous chapters, we have built a strong foundation in linear algebra techniques powering data science and machine learning. Now, we venture into specialized and advanced applications where linear algebra intersects with modern AI, large-scale systems, and emerging technologies.</p> <p>In this chapter, we study the spectral theorem, matrix perturbation theory, tensor operations, Kronecker products, vectorization techniques, and sparse matrix solvers \u2014 all critical tools for working at scale and with cutting-edge AI systems.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#1-spectral-theorem","title":"1. Spectral Theorem","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#what-it-says","title":"What It Says","text":"<p>If a matrix \\(A\\) is symmetric, it can be diagonalized by an orthogonal matrix:</p> \\[ A = Q \\Lambda Q^T \\] <p>where: - \\(Q\\) = matrix of orthonormal eigenvectors - \\(\\Lambda\\) = diagonal matrix of eigenvalues</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Simplifies computations dramatically.</li> <li>Powers techniques like PCA, SVD, and spectral clustering.</li> </ul> <p>Key Intuition: Symmetric matrices are \"nice\" \u2014 their eigenvectors form an orthonormal basis, making transformations purely scaling operations along those directions.</p> <p>Vibrations of a String</p> <p>In physics, modes of vibration of a string correspond to eigenvectors of a matrix describing the system \u2014 spectral decomposition reveals natural resonances.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#2-matrix-perturbation-theory","title":"2. Matrix Perturbation Theory","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#the-problem","title":"The Problem","text":"<p>Real-world data is noisy. Even small changes to a matrix (perturbations) can affect eigenvalues, eigenvectors, and solutions.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#what-we-study","title":"What We Study","text":"<ul> <li>How sensitive are eigenvalues/eigenvectors to small changes?</li> <li>How robust are algorithms against floating-point errors?</li> </ul>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications","title":"Applications","text":"<ul> <li>Understanding stability in machine learning models.</li> <li>Ensuring numerical safety in simulations.</li> </ul> <p>Connection to Previous Learning: This extends your understanding of eigenanalysis (Chapter 6) into practical, imperfect settings where exact numbers can't be trusted.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#3-tensor-basics","title":"3. Tensor Basics","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#beyond-matrices","title":"Beyond Matrices","text":"<p>A tensor generalizes matrices to higher dimensions: - Scalars (0D tensors) - Vectors (1D tensors) - Matrices (2D tensors) - Higher-order arrays (3D+, e.g., video data)</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#why-tensors-matter","title":"Why Tensors Matter","text":"<p>Modern AI models (like deep learning networks) often handle tensor inputs: - Images: (Height, Width, Channels) - Videos: (Time, Height, Width, Channels)</p> <p>Tensors allow for richer representations of data structures.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#4-matrix-calculus-fundamentals","title":"4. Matrix Calculus Fundamentals","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#matrix-derivatives","title":"Matrix Derivatives","text":"<p>Extends standard calculus to matrix-valued functions.</p> <p>Examples: - Gradient of a scalar function with respect to a vector. - Derivative of matrix products (using chain rules).</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications_1","title":"Applications","text":"<ul> <li>Training deep neural networks.</li> <li>Optimizing functions over matrix variables.</li> </ul> <p>Connection to Earlier Chapters: You already saw matrix calculus in backpropagation (Chapter 18); here, we formalize the broader theory.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#5-kronecker-product-and-vectorization","title":"5. Kronecker Product and Vectorization","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#kronecker-product","title":"Kronecker Product","text":"<p>Given matrices \\(A\\) and \\(B\\):</p> \\[ A \\otimes B \\] <p>produces a large block matrix \u2014 every element of \\(A\\) multiplied by the entire matrix \\(B\\).</p> <p>Usage: - Modeling structured systems. - Compactly representing large linear operations.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#vectorization","title":"Vectorization","text":"<p>\"Flatten\" a matrix into a long column vector by stacking its columns.</p> <p>Notation:</p> \\[ \\text{vec}(A) \\] <p>Key Identity:</p> \\[ \\text{vec}(ABC) = (C^T \\otimes A) \\text{vec}(B) \\] <p>Vectorization allows matrix equations to be manipulated as large linear systems \u2014 a powerful trick for optimization and computation.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#6-sparse-matrix-solvers","title":"6. Sparse Matrix Solvers","text":""},{"location":"old-sections/section-5/chapter-21/chapter-21/#sparse-matrices","title":"Sparse Matrices","text":"<p>Matrices where most entries are zero.</p> <p>Examples: - Social network graphs. - Recommendation systems.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#solving-sparse-systems","title":"Solving Sparse Systems","text":"<p>Standard methods (Gaussian elimination) are inefficient for sparse systems. Specialized solvers: - Store only non-zero entries. - Use iterative methods (like Conjugate Gradient, GMRES).</p> <p>Benefits: - Saves memory. - Speeds up computation dramatically.</p> <p>Connection to Previous Learning: This builds on your knowledge of iterative methods (Chapter 8) with a focus on real-world scalability.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#applications-in-ai-driven-fields","title":"Applications in AI-Driven Fields","text":"<ul> <li>Large language models (LLMs) use massive sparse tensors.</li> <li>Computer vision uses tensor decomposition.</li> <li>Recommender systems leverage matrix factorization and sparse solvers.</li> <li>Scientific computing relies on perturbation analysis to ensure reliable simulations.</li> </ul>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#summary","title":"Summary","text":"<ul> <li>The spectral theorem enables powerful decompositions.</li> <li>Matrix perturbation theory helps manage errors.</li> <li>Tensors model rich, multi-dimensional data.</li> <li>Matrix calculus generalizes derivatives for optimization.</li> <li>Kronecker products and vectorization simplify large linear operations.</li> <li>Sparse matrix solvers enable efficient computation at massive scale.</li> </ul> <p>These specialized tools bridge linear algebra to real-world AI, data science, and large-scale system design.</p>"},{"location":"old-sections/section-5/chapter-21/chapter-21/#quiz-question","title":"Quiz Question","text":"<p>What is a major advantage of using sparse matrix solvers in large-scale machine learning applications?</p> <p>A. They reduce memory usage and computational time B. They increase the density of the matrix C. They ensure perfect numerical accuracy D. They eliminate the need for eigenvalues</p> Show Answer <p>The correct answer is A. Sparse matrix solvers exploit the zero patterns in matrices to save memory and accelerate computations, which is crucial for handling large-scale data.</p>"},{"location":"prompts/cover-image/","title":"Cover image","text":"<p>Please generate a wide-landscape cover image for my new course on applied linear algebra. The format must be a 1.91:1 width:height ratio image for use on social media previews.  Please the title \"Linear Algebra\" in the center and put a montage of images around the title that you infer from the full course description below.</p> <p>Here is the course description:</p>"},{"location":"prompts/cover-image/#course-description-applied-linear-algebra-for-ai-and-machine-learning","title":"Course Description: Applied Linear Algebra for AI and Machine Learning","text":""},{"location":"prompts/cover-image/#course-overview","title":"Course Overview","text":"<p>This one-semester college course provides a comprehensive introduction to linear algebra with a strong emphasis on practical applications in artificial intelligence, machine learning, and computer vision. Students will develop both theoretical understanding and hands-on skills through interactive microsimulations that bring abstract mathematical concepts to life.</p> <p>Linear algebra forms the mathematical backbone of modern AI systems. From the matrix operations that power neural networks to the transformations that enable computer vision, understanding linear algebra is essential for anyone working in data science, machine learning, or AI engineering. This course bridges the gap between abstract mathematics and real-world applications, showing students exactly how vectors, matrices, and linear transformations drive the technologies shaping our world.</p>"},{"location":"prompts/cover-image/#prerequisites","title":"Prerequisites","text":"<ul> <li>College Algebra or equivalent</li> <li>Basic programming experience (Python recommended)</li> <li>Familiarity with calculus concepts (derivatives and integrals)</li> </ul>"},{"location":"prompts/cover-image/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will be able to:</p> <ol> <li>Perform fundamental vector and matrix operations with confidence</li> <li>Understand and apply linear transformations in multiple contexts</li> <li>Decompose matrices using eigenvalue, SVD, and other factorization techniques</li> <li>Apply linear algebra concepts to solve machine learning problems</li> <li>Understand how neural networks use matrix operations for learning</li> <li>Implement linear algebra algorithms for image processing and computer vision</li> <li>Analyze real-world applications in autonomous systems and genertic AI</li> </ol>"},{"location":"prompts/cover-image/#course-structure","title":"Course Structure","text":"<p>The course is divided into four major parts spanning 15 weeks, with each chapter containing interactive microsimulations to reinforce concepts.</p>"},{"location":"prompts/cover-image/#part-1-foundations-of-linear-algebra-weeks-1-4","title":"Part 1: Foundations of Linear Algebra (Weeks 1-4)","text":""},{"location":"prompts/cover-image/#chapter-1-vectors-and-vector-spaces","title":"Chapter 1: Vectors and Vector Spaces","text":"<p>An introduction to vectors as the fundamental building blocks of linear algebra. Students explore vector operations, geometric interpretations, and the concept of vector spaces.</p> <p>Topics: - Vectors in 2D and 3D space - Vector addition and scalar multiplication - Dot products and cross products - Vector norms and distances - Linear combinations and span - Linear independence - Basis vectors and coordinate systems</p> <p>Applications: Feature vectors in machine learning, word embeddings, representing data points in high-dimensional spaces.</p>"},{"location":"prompts/cover-image/#chapter-2-matrices-and-matrix-operations","title":"Chapter 2: Matrices and Matrix Operations","text":"<p>Building on vectors, this chapter introduces matrices as collections of vectors and explores the rich algebra of matrix operations.</p> <p>Topics: - Matrix notation and terminology - Matrix addition and scalar multiplication - Matrix-vector multiplication - Matrix-matrix multiplication - Transpose and symmetric matrices - Special matrices (identity, diagonal, triangular, orthogonal) - Matrix inverses</p> <p>Applications: Data representation, adjacency matrices in graphs, transformation matrices in computer graphics.</p>"},{"location":"prompts/cover-image/#chapter-3-systems-of-linear-equations","title":"Chapter 3: Systems of Linear Equations","text":"<p>Students learn to formulate and solve systems of linear equations, a fundamental skill with applications across all quantitative fields.</p> <p>Topics: - Representing systems as matrix equations - Gaussian elimination - Row echelon form and reduced row echelon form - Existence and uniqueness of solutions - Homogeneous systems - Computational considerations and numerical stability</p> <p>Applications: Solving optimization problems, balancing chemical equations, network flow analysis.</p>"},{"location":"prompts/cover-image/#chapter-4-linear-transformations","title":"Chapter 4: Linear Transformations","text":"<p>This chapter reveals how matrices represent transformations, connecting algebraic operations to geometric intuition.</p> <p>Topics: - Functions between vector spaces - Matrix representation of transformations - Rotation, scaling, shearing, and projection - Composition of transformations - Kernel and range of a transformation - Invertible transformations - Change of basis</p> <p>Applications: Computer graphics transformations, coordinate system changes, feature transformations in ML pipelines.</p>"},{"location":"prompts/cover-image/#part-2-advanced-matrix-theory-weeks-5-8","title":"Part 2: Advanced Matrix Theory (Weeks 5-8)","text":""},{"location":"prompts/cover-image/#chapter-5-determinants-and-matrix-properties","title":"Chapter 5: Determinants and Matrix Properties","text":"<p>Determinants reveal fundamental properties of matrices and transformations, with applications in solving systems and computing volumes.</p> <p>Topics: - Definition and computation of determinants - Properties of determinants - Geometric interpretation (area and volume scaling) - Cramer's rule - Determinants and invertibility - Computational methods for large matrices</p> <p>Applications: Computing volumes in higher dimensions, checking matrix invertibility, understanding transformation behavior.</p>"},{"location":"prompts/cover-image/#chapter-6-eigenvalues-and-eigenvectors","title":"Chapter 6: Eigenvalues and Eigenvectors","text":"<p>One of the most important topics in linear algebra, eigenanalysis reveals the intrinsic structure of linear transformations.</p> <p>Topics: - Definition of eigenvalues and eigenvectors - Characteristic polynomial - Finding eigenvalues and eigenvectors - Diagonalization - Complex eigenvalues - Spectral theorem for symmetric matrices - Power iteration method</p> <p>Applications: Principal Component Analysis (PCA), Google's PageRank algorithm, stability analysis of dynamical systems.</p>"},{"location":"prompts/cover-image/#chapter-7-matrix-decompositions","title":"Chapter 7: Matrix Decompositions","text":"<p>Matrix factorizations provide powerful tools for analysis, computation, and dimensionality reduction.</p> <p>Topics: - LU decomposition - QR decomposition - Cholesky decomposition - Singular Value Decomposition (SVD) - Low-rank approximations - Numerical considerations</p> <p>Applications: Recommender systems, image compression, solving least squares problems, noise reduction.</p>"},{"location":"prompts/cover-image/#chapter-8-vector-spaces-and-inner-product-spaces","title":"Chapter 8: Vector Spaces and Inner Product Spaces","text":"<p>Generalizing concepts from earlier chapters, this section develops the abstract framework underlying all applications.</p> <p>Topics: - Abstract vector spaces - Subspaces and their properties - Inner products and norms - Orthogonality and orthonormal bases - Gram-Schmidt orthogonalization - Projections and least squares - Fundamental subspaces of a matrix</p> <p>Applications: Signal processing, function approximation, optimization in machine learning.</p>"},{"location":"prompts/cover-image/#part-3-linear-algebra-in-machine-learning-weeks-9-12","title":"Part 3: Linear Algebra in Machine Learning (Weeks 9-12)","text":""},{"location":"prompts/cover-image/#chapter-9-linear-algebra-foundations-of-machine-learning","title":"Chapter 9: Linear Algebra Foundations of Machine Learning","text":"<p>This chapter explicitly connects linear algebra concepts to core machine learning algorithms and techniques.</p> <p>Topics: - Data as matrices: features and observations - Covariance matrices and correlation - Principal Component Analysis (PCA) in depth - Linear regression as matrix equations - Regularization: Ridge and Lasso - Gradient descent in matrix form - Batch processing with matrix operations</p> <p>Applications: Feature extraction, dimensionality reduction, predictive modeling, data preprocessing.</p>"},{"location":"prompts/cover-image/#chapter-10-neural-networks-and-deep-learning","title":"Chapter 10: Neural Networks and Deep Learning","text":"<p>Neural networks are fundamentally sequences of linear transformations interleaved with nonlinearities. This chapter reveals the matrix mathematics powering deep learning.</p> <p>Topics: - Neurons as linear functions with activation - Weight matrices and bias vectors - Forward propagation as matrix multiplication - Backpropagation and the chain rule with matrices - Convolutional layers as structured matrix operations - Batch normalization and layer normalization - Attention mechanisms and transformer architecture - Tensor operations and higher-order arrays</p> <p>Applications: Image classification, natural language processing, speech recognition, recommendation systems.</p>"},{"location":"prompts/cover-image/#chapter-11-generative-ai-and-large-language-models","title":"Chapter 11: Generative AI and Large Language Models","text":"<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of these transformative technologies.</p> <p>Topics: - Embedding spaces and semantic similarity - Attention mechanisms as matrix operations - Key, Query, and Value matrices in transformers - Self-attention and cross-attention - Position encodings - Linear projections in multi-head attention - Low-rank adaptations (LoRA) for fine-tuning - Matrix factorization in generative models - Latent spaces and interpolation</p> <p>Applications: Large language models (GPT, Claude), image generation (Stable Diffusion, DALL-E), text-to-speech systems.</p>"},{"location":"prompts/cover-image/#chapter-12-optimization-and-learning-algorithms","title":"Chapter 12: Optimization and Learning Algorithms","text":"<p>Optimization is the engine of machine learning, and linear algebra provides the tools to understand and improve optimization algorithms.</p> <p>Topics: - Gradient vectors and Hessian matrices - Convexity and positive definite matrices - Newton's method and quasi-Newton methods - Stochastic gradient descent - Momentum and adaptive learning rates (Adam, RMSprop) - Second-order optimization methods - Constrained optimization with Lagrange multipliers - Conditioning and numerical stability</p> <p>Applications: Training neural networks, hyperparameter optimization, constrained learning problems.</p>"},{"location":"prompts/cover-image/#part-4-computer-vision-and-autonomous-systems-weeks-13-15","title":"Part 4: Computer Vision and Autonomous Systems (Weeks 13-15)","text":""},{"location":"prompts/cover-image/#chapter-13-image-processing-and-computer-vision","title":"Chapter 13: Image Processing and Computer Vision","text":"<p>Images are matrices of pixel values, making linear algebra the natural language for image processing and computer vision.</p> <p>Topics: - Images as matrices and tensors - Convolution as matrix operations - Image filtering (blur, sharpen, edge detection) - Fourier transforms and frequency domain - Image compression using SVD - Color spaces and transformations - Feature detection and description - Homography and perspective transformations</p> <p>Applications: Photo editing, medical imaging, satellite imagery analysis, facial recognition.</p>"},{"location":"prompts/cover-image/#chapter-14-3d-geometry-and-transformations","title":"Chapter 14: 3D Geometry and Transformations","text":"<p>Understanding 3D geometry is essential for robotics, augmented reality, and autonomous vehicles. This chapter covers the linear algebra of 3D transformations.</p> <p>Topics: - 3D coordinate systems - Rotation matrices and Euler angles - Quaternions and rotation representation - Homogeneous coordinates - Rigid body transformations - Camera models and projection matrices - Stereo vision and triangulation - Point cloud processing</p> <p>Applications: Robotics, augmented reality, 3D reconstruction, motion capture.</p>"},{"location":"prompts/cover-image/#chapter-15-autonomous-driving-and-sensor-fusion","title":"Chapter 15: Autonomous Driving and Sensor Fusion","text":"<p>The capstone chapter applies all course concepts to the complex, safety-critical domain of autonomous vehicles.</p> <p>Topics: - LIDAR point cloud processing - Camera calibration and rectification - Sensor fusion with Kalman filters - State estimation and prediction - Simultaneous Localization and Mapping (SLAM) - Object detection and tracking - Path planning with linear constraints - Safety-critical computation considerations</p> <p>Applications: Self-driving cars, drone navigation, warehouse robots, autonomous delivery systems.</p>"},{"location":"prompts/cover-image/#interactive-microsimulations","title":"Interactive Microsimulations","text":"<p>Each chapter includes interactive microsimulations that allow students to:</p> <ul> <li>Visualize abstract concepts in 2D and 3D</li> <li>Experiment with parameters and see immediate results</li> <li>Build intuition through hands-on exploration</li> <li>Connect mathematical formulas to visual representations</li> <li>Practice computational skills in a forgiving environment</li> </ul> <p>Example microsimulations include:</p> <ul> <li>Vector Operations Playground: Add, subtract, and scale vectors interactively</li> <li>Matrix Transformation Visualizer: See how matrices transform shapes in 2D</li> <li>Eigenvalue Explorer: Watch eigenvectors remain on their span during transformation</li> <li>SVD Image Compressor: Adjust rank and see image quality vs. compression tradeoffs</li> <li>Neural Network Forward Pass: Step through matrix multiplications in a simple network</li> <li>Attention Mechanism Visualizer: See how attention weights are computed</li> <li>Kalman Filter Tracker: Fuse noisy sensor measurements in real-time</li> <li>PCA Dimension Reducer: Project high-dimensional data and see variance preserved</li> </ul>"},{"location":"prompts/cover-image/#assessment","title":"Assessment","text":"<ul> <li>Weekly Problem Sets (30%): Analytical and computational problems</li> <li>Microsimulation Labs (20%): Hands-on exploration with written reflections</li> <li>Midterm Exam (20%): Covering Parts 1 and 2</li> <li>Final Project (30%): Apply linear algebra to a real-world problem in ML, computer vision, or autonomous systems</li> </ul>"},{"location":"prompts/cover-image/#required-materials","title":"Required Materials","text":"<ul> <li>Textbook: This interactive intelligent textbook with embedded microsimulations</li> <li>Software: Python with NumPy, Matplotlib, and scikit-learn</li> <li>Optional: GPU access for deep learning exercises</li> </ul>"},{"location":"prompts/cover-image/#target-audience","title":"Target Audience","text":"<p>This course is designed for:</p> <ul> <li>Computer Science majors pursuing AI/ML specializations</li> <li>Data Science students seeking mathematical foundations</li> <li>Engineering students interested in robotics and autonomous systems</li> <li>Applied Mathematics students wanting practical applications</li> <li>Graduate students needing linear algebra foundations for research</li> </ul>"},{"location":"prompts/cover-image/#why-this-course-matters","title":"Why This Course Matters","text":"<p>Linear algebra is not just a prerequisite checkbox\u2014it is the language in which modern AI systems are written. Understanding matrices and transformations at a deep level enables you to:</p> <ul> <li>Debug ML models by understanding what's happening mathematically</li> <li>Optimize performance by choosing efficient matrix operations</li> <li>Innovate by seeing new ways to apply linear algebra concepts</li> <li>Communicate with researchers and engineers using shared mathematical vocabulary</li> <li>Adapt to new techniques that build on these foundations</li> </ul> <p>The future of technology is built on linear algebra. This course gives you the tools to be part of building that future.</p>"},{"location":"prompts/cover-image/#learning-objectives-sorted-by-blooms-taxonomy","title":"Learning Objectives Sorted by Bloom's Taxonomy","text":"<p>The following learning objectives are organized according to the 2001 revised Bloom's Taxonomy, progressing from foundational cognitive skills to higher-order thinking. Each level builds upon the previous, ensuring students develop comprehensive mastery of applied linear algebra.</p>"},{"location":"prompts/cover-image/#remember","title":"Remember","text":"<p>At this foundational level, students will retrieve and recall essential facts, terminology, and procedures.</p> <ul> <li>Define key terms including vector, matrix, scalar, transpose, determinant, eigenvalue, and eigenvector</li> <li>List the properties of matrix operations (associativity, distributivity, non-commutativity of multiplication)</li> <li>Identify special matrix types: identity, diagonal, symmetric, orthogonal, positive definite, and sparse</li> <li>Recall the conditions for matrix invertibility</li> <li>State the definition of linear independence and span</li> <li>Recognize the notation for vector norms (L1, L2, L-infinity)</li> <li>Name the four fundamental subspaces of a matrix</li> <li>List the steps of Gaussian elimination</li> <li>Identify the components of SVD: U, \u03a3, and V matrices</li> <li>Recall the structure of neural network layers (weights, biases, activations)</li> <li>State the formula for computing attention scores in transformers</li> <li>Recognize common matrix decomposition types (LU, QR, Cholesky, SVD)</li> </ul>"},{"location":"prompts/cover-image/#understand","title":"Understand","text":"<p>At this level, students will demonstrate comprehension by explaining concepts and interpreting mathematical relationships.</p> <ul> <li>Explain the geometric interpretation of the dot product as projection</li> <li>Describe how matrix multiplication represents composition of linear transformations</li> <li>Interpret the meaning of eigenvalues as scaling factors along eigenvector directions</li> <li>Summarize how SVD decomposes a matrix into rotations and scaling</li> <li>Explain why the determinant represents the volume scaling factor of a transformation</li> <li>Describe the relationship between the rank of a matrix and its solution space</li> <li>Interpret covariance matrices in terms of data spread and correlation</li> <li>Explain how PCA uses eigenvectors to find principal components</li> <li>Describe how gradient descent uses the gradient vector to minimize loss functions</li> <li>Summarize the role of weight matrices in neural network forward propagation</li> <li>Explain how attention mechanisms compute relevance between tokens using dot products</li> <li>Describe the purpose of the Kalman filter in combining predictions with measurements</li> <li>Interpret homogeneous coordinates and their role in projective geometry</li> </ul>"},{"location":"prompts/cover-image/#apply","title":"Apply","text":"<p>Students will use learned procedures and concepts to solve problems in familiar and new contexts.</p> <ul> <li>Perform matrix-vector and matrix-matrix multiplication by hand and programmatically</li> <li>Solve systems of linear equations using Gaussian elimination and matrix inverses</li> <li>Compute eigenvalues and eigenvectors for 2\u00d72 and 3\u00d73 matrices</li> <li>Apply the Gram-Schmidt process to orthogonalize a set of vectors</li> <li>Calculate the SVD of a matrix and use it for low-rank approximation</li> <li>Implement PCA to reduce dimensionality of a dataset</li> <li>Use matrix calculus to compute gradients for optimization problems</li> <li>Apply linear regression using the normal equations</li> <li>Implement forward propagation through a neural network layer</li> <li>Construct rotation and transformation matrices for 2D and 3D graphics</li> <li>Apply convolution kernels to perform image filtering operations</li> <li>Use homography matrices to correct perspective in images</li> <li>Implement the power iteration method to find dominant eigenvalues</li> </ul>"},{"location":"prompts/cover-image/#analyze","title":"Analyze","text":"<p>Students will break down complex systems into components and examine relationships between parts.</p> <ul> <li>Analyze the conditioning of a matrix and its impact on numerical stability</li> <li>Decompose the behavior of a linear transformation into its action on eigenspaces</li> <li>Examine the tradeoffs between different matrix decomposition methods for specific applications</li> <li>Analyze how the choice of basis affects the representation of linear transformations</li> <li>Compare the computational complexity of direct vs. iterative methods for solving linear systems</li> <li>Investigate how regularization terms modify the solution space in linear regression</li> <li>Analyze the information flow through neural network layers using matrix dimensions</li> <li>Examine how attention patterns reveal relationships in transformer models</li> <li>Analyze the effect of different kernel sizes and strides on convolutional layer outputs</li> <li>Decompose a camera projection matrix into intrinsic and extrinsic parameters</li> <li>Analyze sensor fusion algorithms to understand how different data sources are weighted</li> <li>Examine the stability of dynamical systems through eigenvalue analysis</li> <li>Investigate the relationship between matrix rank and the information preserved in compression</li> </ul>"},{"location":"prompts/cover-image/#evaluate","title":"Evaluate","text":"<p>Students will make judgments and decisions based on criteria, standards, and evidence.</p> <ul> <li>Assess the numerical stability of different algorithms for computing matrix inverses</li> <li>Evaluate the appropriate rank for SVD truncation based on reconstruction error and compression ratio</li> <li>Judge the suitability of different dimensionality reduction techniques for specific datasets</li> <li>Critique the choice of optimization algorithms based on problem characteristics (convexity, scale, sparsity)</li> <li>Evaluate the effectiveness of different regularization strategies for preventing overfitting</li> <li>Assess the tradeoffs between model complexity and interpretability in linear models</li> <li>Judge the quality of learned embeddings based on semantic similarity measures</li> <li>Evaluate different attention mechanisms for computational efficiency and performance</li> <li>Assess the accuracy of camera calibration by analyzing reprojection errors</li> <li>Critique sensor fusion approaches based on noise characteristics and update rates</li> <li>Evaluate path planning solutions based on optimality and computational constraints</li> <li>Judge the robustness of SLAM algorithms under different environmental conditions</li> <li>Assess when to use dense vs. sparse matrix representations based on memory and speed requirements</li> </ul>"},{"location":"prompts/cover-image/#create","title":"Create","text":"<p>At the highest cognitive level, students will synthesize knowledge to design, construct, and develop novel solutions.</p> <ul> <li>Design a complete data preprocessing pipeline using linear algebra operations</li> <li>Develop a custom dimensionality reduction approach for a specific application domain</li> <li>Construct a neural network architecture with appropriate layer dimensions for a given task</li> <li>Create novel image filters by designing custom convolution kernels</li> <li>Design a feature extraction system using learned linear projections</li> <li>Develop a recommendation system using matrix factorization techniques</li> <li>Construct a real-time object tracking system using Kalman filtering</li> <li>Design a camera calibration procedure for a multi-camera autonomous vehicle system</li> <li>Create a 3D reconstruction pipeline from stereo image pairs</li> <li>Develop a sensor fusion algorithm that combines LIDAR, camera, and IMU data</li> <li>Design an efficient batched matrix computation strategy for GPU acceleration</li> <li>Construct an interpretable linear model that balances accuracy with explainability</li> <li>Create an interactive visualization tool demonstrating linear algebra concepts</li> <li>Design a complete autonomous navigation system integrating perception, localization, and planning</li> </ul>"},{"location":"prompts/logo/","title":"Logo","text":"<p>Create a logo using minimalistic geometry for my new textbook on linear algebra. It should be usable as a favicon.</p>"},{"location":"sims/basis-coordinate-visualizer/","title":"Basis and Coordinate System Visualizer","text":"<p>Run the Basis and Coordinate System Visualizer Fullscreen</p>"},{"location":"sims/basis-coordinate-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization demonstrates that the same geometric vector can have different coordinate representations depending on which basis you use. The left panel shows the standard basis (e\u2081, e\u2082) while the right panel shows a custom basis (b\u2081, b\u2082) that you can modify.</p> <p>Learning Objective: Students will interpret how the same point has different coordinate representations in different bases by visualizing standard and custom basis vectors simultaneously.</p>"},{"location":"sims/basis-coordinate-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Vector: In either panel, drag the green vector endpoint to move it</li> <li>Drag Basis Vectors: In the right panel, drag the endpoints of b\u2081 (red) and b\u2082 (blue) to change the custom basis</li> <li>Use Presets: Click preset buttons to see common basis configurations:</li> <li>Standard: b\u2081 = (1, 0), b\u2082 = (0, 1) - matches the standard basis</li> <li>Rotated 45\u00b0: Basis rotated by 45 degrees</li> <li>Skewed: Non-orthogonal basis vectors</li> <li>Stretched: Basis vectors with different lengths</li> <li>Toggle Options:</li> <li>Show Grid: Toggle grid lines in both panels</li> <li>Show Projections: Toggle dashed projection lines</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/basis-coordinate-visualizer/#coordinate-representation","title":"Coordinate Representation","text":"<p>The same geometric vector v has different coordinates in different bases: - In standard basis: v = (x, y) means v = x\u00b7e\u2081 + y\u00b7e\u2082 - In custom basis: [v]_B = (c\u2081, c\u2082) means v = c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082</p>"},{"location":"sims/basis-coordinate-visualizer/#the-equation","title":"The Equation","text":"<p>The coordinates [v]_B satisfy: \\(\\(\\mathbf{v} = c_1\\mathbf{b}_1 + c_2\\mathbf{b}_2\\)\\)</p>"},{"location":"sims/basis-coordinate-visualizer/#change-of-basis","title":"Change of Basis","text":"<p>When you change the basis vectors, the grid lines in the right panel change to follow the new basis directions. The coordinates change, but the geometric vector stays the same!</p>"},{"location":"sims/basis-coordinate-visualizer/#important-observations","title":"Important Observations","text":"<ol> <li>Same Vector, Different Numbers: Moving to a stretched basis makes coordinates smaller</li> <li>Grid Deformation: The grid follows the basis vectors</li> <li>Parallel Basis Warning: If b\u2081 and b\u2082 become parallel, coordinates become undefined</li> <li>Verification: The formula c\u2081\u00b7b\u2081 + c\u2082\u00b7b\u2082 = v is shown at the bottom</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/basis-coordinate-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/basis-coordinate-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/basis-coordinate-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/basis-coordinate-visualizer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/basis-coordinate-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics</li> <li>Linear combinations</li> <li>Understanding of basis vectors</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Start with \"Standard\" preset and observe both panels show same coordinates</li> <li> <p>Move the vector and verify coordinates match</p> </li> <li> <p>Rotated Basis Investigation (5 min):</p> </li> <li>Click \"Rotated 45\u00b0\"</li> <li>Notice how coordinates change but vector stays the same</li> <li> <p>Find a vector where custom coordinates are simpler than standard</p> </li> <li> <p>Skewed Basis Exploration (5 min):</p> </li> <li>Click \"Skewed\"</li> <li>Observe how grid lines are no longer perpendicular</li> <li> <p>Verify the linear combination formula still works</p> </li> <li> <p>Custom Basis Creation (10 min):</p> </li> <li>Drag b\u2081 and b\u2082 to create your own basis</li> <li>Find a basis where a specific vector has integer coordinates</li> <li>Try to make b\u2081 and b\u2082 parallel and observe the \"undefined\" warning</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why does the same vector have different coordinates in different bases?</li> <li>What happens geometrically when you stretch a basis vector?</li> <li>Why do parallel basis vectors make coordinates undefined?</li> <li>How would you convert coordinates from one basis to another?</li> </ol>"},{"location":"sims/basis-coordinate-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a vector and a basis, calculate the coordinates by hand and verify</li> <li>Explain why the grid deforms when the basis changes</li> <li>Find a basis that makes a given vector have coordinates (1, 1)</li> </ul>"},{"location":"sims/basis-coordinate-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Change of basis - Excellent visual explanation</li> <li>Khan Academy - Change of basis</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.5.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 4.4.</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/","title":"Dot Product and Cross Product Visualizer","text":"<p>Run the Dot Product and Cross Product Visualizer Fullscreen</p>"},{"location":"sims/dot-cross-product-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand the geometric meaning of both the dot product and cross product. The dot product view shows how vectors project onto each other and how the angle between vectors affects the result. The cross product view demonstrates the perpendicular vector and its relationship to parallelogram area.</p> <p>Learning Objective: Students will analyze the geometric relationship between dot product (projection and angle) and cross product (perpendicular vector and area) by manipulating vectors and observing how the products change.</p>"},{"location":"sims/dot-cross-product-visualizer/#how-to-use","title":"How to Use","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product-view-2d","title":"Dot Product View (2D)","text":"<ol> <li>Drag Vectors: Click and drag the endpoints of vectors u (blue) and v (red)</li> <li>Observe Projection: The purple line shows the projection of v onto u</li> <li>Watch the Angle: The orange arc shows the angle \u03b8 between vectors</li> <li>See the Formula: The panel shows how u\u00b7v = |u||v|cos(\u03b8)</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#cross-product-view-3d","title":"Cross Product View (3D)","text":"<ol> <li>Toggle View: Click \"Show Cross Product (3D)\" to switch views</li> <li>Rotate Scene: Click and drag to rotate the 3D view</li> <li>Observe Parallelogram: The yellow area shows the parallelogram formed by u and v</li> <li>See Result Vector: The green vector u\u00d7v is perpendicular to both u and v</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#controls","title":"Controls","text":"<ul> <li>Show Projection: Toggle the projection visualization</li> <li>Show Parallelogram: Toggle the parallelogram in 3D view</li> <li>Show Formula: Toggle the step-by-step formula calculation</li> <li>Animate Angle Sweep: Watch how the products change as angle varies from 0\u00b0 to 180\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/dot-cross-product-visualizer/#dot-product","title":"Dot Product","text":"<ul> <li>Projection: The dot product relates to how much one vector projects onto another</li> <li>Angle Relationship: \\(\\mathbf{u} \\cdot \\mathbf{v} = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)</li> <li>Perpendicularity: When \u03b8 = 90\u00b0, the dot product is zero</li> <li>Sign: Positive when angle &lt; 90\u00b0, negative when angle &gt; 90\u00b0</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#cross-product","title":"Cross Product","text":"<ul> <li>Perpendicular Result: u\u00d7v is perpendicular to both u and v</li> <li>Area: |u\u00d7v| equals the area of the parallelogram formed by u and v</li> <li>Right-Hand Rule: The direction follows the right-hand rule</li> <li>Only in 3D: The cross product is only defined for 3D vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Dot Product: \\(\\(\\mathbf{u} \\cdot \\mathbf{v} = u_x v_x + u_y v_y = |\\mathbf{u}||\\mathbf{v}|\\cos\\theta\\)\\)</p> <p>Cross Product: \\(\\(\\mathbf{u} \\times \\mathbf{v} = \\begin{bmatrix} u_y v_z - u_z v_y \\\\ u_z v_x - u_x v_z \\\\ u_x v_y - u_y v_x \\end{bmatrix}\\)\\)</p> \\[|\\mathbf{u} \\times \\mathbf{v}| = |\\mathbf{u}||\\mathbf{v}|\\sin\\theta\\]"},{"location":"sims/dot-cross-product-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/dot-cross-product-visualizer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/dot-cross-product-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/dot-cross-product-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/dot-cross-product-visualizer/#duration","title":"Duration","text":"<p>25-30 minutes</p>"},{"location":"sims/dot-cross-product-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector basics (magnitude, direction, components)</li> <li>Trigonometry (cosine, sine)</li> <li>Basic understanding of perpendicularity</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Dot Product Exploration (10 min):</li> <li>Start with u = (3, 0) and v = (2, 0) (same direction)</li> <li>Rotate v to 90\u00b0 and observe dot product becomes zero</li> <li>Continue to 180\u00b0 and see negative dot product</li> <li> <p>Run animation to see the full sweep</p> </li> <li> <p>Projection Investigation (5 min):</p> </li> <li>Observe how the projection length changes with angle</li> <li>Find configurations where projection equals |v|</li> <li> <p>Find configurations where projection is negative</p> </li> <li> <p>Cross Product Exploration (10 min):</p> </li> <li>Switch to 3D view</li> <li>Observe the green cross product vector</li> <li>Rotate view to verify it's perpendicular to both u and v</li> <li> <p>Change vectors and observe parallelogram area changes</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Toggle between views for the same vectors</li> <li>Compare how dot product and cross product magnitude change with angle</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the dot product zero when vectors are perpendicular?</li> <li>What does the sign of the dot product tell you about the angle?</li> <li>Why does the cross product only exist in 3D?</li> <li>How can you use dot product to find the angle between two vectors?</li> </ol>"},{"location":"sims/dot-cross-product-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given two vectors, predict whether the dot product is positive, negative, or zero</li> <li>Calculate the area of a parallelogram using the cross product</li> <li>Find a vector perpendicular to two given vectors</li> </ul>"},{"location":"sims/dot-cross-product-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Dot products and duality</li> <li>3Blue1Brown - Cross products</li> <li>Khan Academy - Cross Product Introduction</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.2-1.3.</li> </ol>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>Open Learning Graph Viewer</p> <p>This interactive viewer allows you to explore the learning graph for Applied Linear Algebra for AI and Machine Learning.</p>"},{"location":"sims/graph-viewer/#features","title":"Features","text":"<ul> <li>Search: Type in the search box to find specific concepts</li> <li>Category Filtering: Use checkboxes to show/hide concept categories</li> <li>Interactive Navigation: Click and drag to explore, scroll to zoom</li> <li>Statistics: View real-time counts of visible nodes and edges</li> </ul>"},{"location":"sims/graph-viewer/#using-the-viewer","title":"Using the Viewer","text":"<ol> <li> <p>Search for Concepts: Start typing in the search box to find concepts. Click on a result to focus on that node.</p> </li> <li> <p>Filter by Category: Use the category checkboxes in the sidebar to show or hide groups of related concepts. Use \"Check All\" or \"Uncheck All\" for bulk operations.</p> </li> <li> <p>Navigate the Graph:</p> </li> <li>Drag to pan around the graph</li> <li>Scroll to zoom in and out</li> <li> <p>Click on a node to select it and highlight its connections</p> </li> <li> <p>View Statistics: The sidebar shows counts of visible nodes, edges, and foundational concepts.</p> </li> </ol>"},{"location":"sims/graph-viewer/#graph-structure","title":"Graph Structure","text":"<ul> <li>Foundational Concepts: Prerequisites with no dependencies</li> <li>Advanced Concepts: Topics that build on multiple prerequisites</li> <li>Edges: Arrows point from a concept to its prerequisites</li> </ul>"},{"location":"sims/graph-viewer/#launch-the-viewer","title":"Launch the Viewer","text":""},{"location":"sims/linear-combination-explorer/","title":"Linear Combination Explorer","text":"<p>Run the Linear Combination Explorer Fullscreen</p>"},{"location":"sims/linear-combination-explorer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand linear combinations by allowing them to adjust scalar coefficients and observe how the result vector changes. The challenge mode tests students' ability to find the right coefficients to reach a target point.</p> <p>Learning Objective: Students will apply their understanding of linear combinations by adjusting scalar coefficients to reach target points and observe how span is generated.</p>"},{"location":"sims/linear-combination-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Coefficients: Use the c\u2081 and c\u2082 sliders to change the scalar multipliers</li> <li>Drag Basis Vectors: Click and drag the endpoints of v\u2081 (red) and v\u2082 (blue) to change their directions</li> <li>Observe the Result: The green arrow shows c\u2081v\u2081 + c\u2082v\u2082</li> <li>See Tip-to-Tail: Enable \"Show Components\" to see how the scaled vectors add tip-to-tail</li> <li>Challenge Mode: Click \"New Challenge\" to get a target point (yellow star), then find the coefficients to reach it</li> <li>Get Help: Click \"Show Solution\" to see the answer</li> </ol>"},{"location":"sims/linear-combination-explorer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/linear-combination-explorer/#linear-combination","title":"Linear Combination","text":"\\[c_1\\mathbf{v}_1 + c_2\\mathbf{v}_2\\] <p>A linear combination is a sum of scalar multiples of vectors. Any vector in the plane can be written as a linear combination of two non-parallel vectors.</p>"},{"location":"sims/linear-combination-explorer/#span","title":"Span","text":"<p>The span of vectors is the set of all possible linear combinations. For two non-parallel vectors in 2D, the span is the entire plane.</p>"},{"location":"sims/linear-combination-explorer/#parallel-vectors","title":"Parallel Vectors","text":"<p>When v\u2081 and v\u2082 are parallel (one is a scalar multiple of the other), their span collapses to a line. The visualization shows this with a warning.</p>"},{"location":"sims/linear-combination-explorer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/linear-combination-explorer/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/linear-combination-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/linear-combination-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/linear-combination-explorer/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/linear-combination-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication</li> <li>Basic understanding of coordinate systems</li> </ul>"},{"location":"sims/linear-combination-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Set c\u2081 = 1, c\u2082 = 0 and observe: result equals v\u2081</li> <li>Set c\u2081 = 0, c\u2082 = 1 and observe: result equals v\u2082</li> <li> <p>Set c\u2081 = 1, c\u2082 = 1 and observe: tip-to-tail addition</p> </li> <li> <p>Coefficient Investigation (5 min):</p> </li> <li>What happens when c\u2081 = -1?</li> <li>Find coefficients that put the result at (0, 0)</li> <li> <p>Make the result point in the opposite direction of v\u2081</p> </li> <li> <p>Challenge Mode (10 min):</p> </li> <li>Click \"New Challenge\" to get a target</li> <li>Try to reach the target by adjusting only c\u2081 and c\u2082</li> <li>Record how many attempts it takes</li> <li> <p>After solving, verify by checking the math</p> </li> <li> <p>Span Investigation (5 min):</p> </li> <li>Drag v\u2082 to be parallel to v\u2081</li> <li>Notice the \"span is a line\" warning</li> <li>Try to reach targets outside the line - impossible!</li> </ol>"},{"location":"sims/linear-combination-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why can any point in the plane be reached with two non-parallel vectors?</li> <li>What happens to the span when the vectors become parallel?</li> <li>Is the linear combination c\u2081v\u2081 + c\u2082v\u2082 the same as c\u2082v\u2082 + c\u2081v\u2081?</li> <li>How would you find the coefficients algebraically?</li> </ol>"},{"location":"sims/linear-combination-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, calculate the coefficients by solving a system of equations</li> <li>Explain why two parallel vectors only span a line</li> <li>Predict whether a given point is reachable with given basis vectors</li> </ul>"},{"location":"sims/linear-combination-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors - Excellent visual introduction</li> <li>Khan Academy - Linear Combinations and Span</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Chapter 1.3.</li> <li>Lay, D. C. (2015). Linear Algebra and Its Applications (5th ed.). Section 1.3.</li> </ol>"},{"location":"sims/norm-comparison-visualizer/","title":"Norm Comparison Visualizer","text":"<p>Run the Norm Comparison Visualizer Fullscreen</p>"},{"location":"sims/norm-comparison-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This visualization helps students understand different ways to measure vector \"length\" or distance. By showing the unit \"circles\" for L1, L2, and L-infinity norms simultaneously, students can see how each norm defines what it means for a vector to have \"length 1.\"</p> <p>Learning Objective: Students will compare and contrast L1, L2, and L-infinity norms by observing unit circles and distance measurements for each norm type.</p>"},{"location":"sims/norm-comparison-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Drag the Point: Click and drag the black point to any location on the grid</li> <li>Observe Norm Values: The info panel shows all three norm values for the current point</li> <li>Compare Unit Shapes:</li> <li>Blue Circle: L2 norm (Euclidean) - points at distance 1 from origin</li> <li>Green Diamond: L1 norm (Manhattan) - points with L1 distance 1 from origin</li> <li>Orange Square: L\u221e norm (Maximum) - points with L\u221e distance 1 from origin</li> <li>Toggle Norms: Use checkboxes to show/hide each norm's unit shape</li> <li>Adjust Radius: Use the slider to see how the shapes scale</li> <li>Animate: Watch the point move around the L2 unit circle</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":""},{"location":"sims/norm-comparison-visualizer/#l2-norm-euclidean","title":"L2 Norm (Euclidean)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_2 = \\sqrt{x^2 + y^2}\\)\\) - Standard \"straight-line\" distance - Unit shape is a circle - Used in: Least squares regression, ridge regularization</p>"},{"location":"sims/norm-comparison-visualizer/#l1-norm-manhattan","title":"L1 Norm (Manhattan)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_1 = |x| + |y|\\)\\) - Distance measured along axes (like city blocks) - Unit shape is a diamond (rotated square) - Used in: Lasso regularization, sparse solutions</p>"},{"location":"sims/norm-comparison-visualizer/#l-norm-maximum","title":"L\u221e Norm (Maximum)","text":"<p>\\(\\(\\|\\mathbf{v}\\|_\\infty = \\max(|x|, |y|)\\)\\) - Maximum absolute component value - Unit shape is a square - Used in: Constraining maximum deviation</p>"},{"location":"sims/norm-comparison-visualizer/#why-different-norms-matter","title":"Why Different Norms Matter","text":"<ul> <li>For (3, 4): L1 = 7, L2 = 5, L\u221e = 4</li> <li>The same point can have very different \"distances\" depending on which norm you use</li> <li>In machine learning, L1 promotes sparse solutions while L2 distributes weight evenly</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/norm-comparison-visualizer/main.html\"\n        height=\"502px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/norm-comparison-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/norm-comparison-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or machine learning</p>"},{"location":"sims/norm-comparison-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/norm-comparison-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of vectors and distance</li> <li>Familiarity with absolute value</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min):</li> <li>Drag the point to various locations</li> <li>Observe how the three norm values change differently</li> <li> <p>Find points where two norms give the same value</p> </li> <li> <p>Unit Shape Analysis (5 min):</p> </li> <li>Turn off two norms and examine one at a time</li> <li>Drag the point onto each boundary</li> <li> <p>Notice the indicator when point is \"on boundary\"</p> </li> <li> <p>Comparison Activity (5 min):</p> </li> <li>Find a point where L1 &gt; L2 &gt; L\u221e</li> <li>Find a point where all three norms are equal (hint: on an axis)</li> <li> <p>Predict which norm will be largest for a given point</p> </li> <li> <p>Application Discussion (5 min):</p> </li> <li>Why might we use L1 in machine learning?</li> <li>What does it mean geometrically for L1 to \"promote sparsity\"?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why is the L1 unit \"circle\" a diamond shape?</li> <li>For what points are all three norms equal?</li> <li>Which norm gives the smallest value for most points? Why?</li> <li>How does the relationship between norms change as points move farther from the origin?</li> </ol>"},{"location":"sims/norm-comparison-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Calculate all three norms for a given vector</li> <li>Predict which shape a point is inside/outside of</li> <li>Explain why L1 regularization promotes sparse solutions</li> </ul>"},{"location":"sims/norm-comparison-visualizer/#references","title":"References","text":"<ol> <li>Understanding Different Norms - Wolfram MathWorld</li> <li>L1 vs L2 Regularization - StatQuest</li> <li>Why L1 promotes sparsity - Cross Validated discussion</li> <li>Boyd, S. &amp; Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/","title":"2D and 3D Vector Visualization","text":"<p>Run the 2D and 3D Vector Visualization Fullscreen</p>"},{"location":"sims/vector-2d-3d-visualizer/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive visualization helps students understand how vector components determine position and direction in both 2D and 3D coordinate systems. Students can manipulate the x, y, and z components using sliders and observe how the vector changes in real-time.</p> <p>Learning Objective: Students will interpret vectors geometrically by visualizing how component values determine position and direction in 2D and 3D coordinate systems.</p>"},{"location":"sims/vector-2d-3d-visualizer/#how-to-use","title":"How to Use","text":"<ol> <li>Adjust Components: Use the X, Y, and Z sliders to change the vector components (range: -5 to 5)</li> <li>Switch Views: Click \"Switch to 3D\" to toggle between 2D and 3D visualization modes</li> <li>Toggle Projections: Enable/disable dashed projection lines that show how the vector projects onto each axis</li> <li>Toggle Labels: Show or hide component labels and axis labels</li> <li>Rotate 3D View: In 3D mode, click and drag on the canvas to rotate the view</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Components: How x, y (and z in 3D) values determine the vector endpoint</li> <li>Vector Magnitude: The length of the vector, calculated as \\(\\|v\\| = \\sqrt{x^2 + y^2}\\) in 2D or \\(\\|v\\| = \\sqrt{x^2 + y^2 + z^2}\\) in 3D</li> <li>Coordinate Axes: The standard basis vectors along x, y, and z directions</li> <li>Projection: How a vector projects onto coordinate planes and axes</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<p>You can embed this MicroSim in your own webpage using the following iframe code:</p> <pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-2d-3d-visualizer/main.html\"\n        height=\"562px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-2d-3d-visualizer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-2d-3d-visualizer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-2d-3d-visualizer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-2d-3d-visualizer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of Cartesian coordinate systems</li> <li>Basic knowledge of what vectors represent (magnitude and direction)</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Have students explore the 2D view first, adjusting x and y sliders to see how vectors change</li> <li>Pattern Recognition (5 min): Ask students to find vectors with the same magnitude but different directions</li> <li>3D Extension (5 min): Switch to 3D view and explore how the z-component adds a third dimension</li> <li>Projection Analysis (5 min): Enable projections and discuss how vectors decompose into components</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What happens to the magnitude when you double all components?</li> <li>Can two different vectors have the same magnitude? Give examples.</li> <li>How do the projection lines help you understand vector components?</li> <li>What is the geometric interpretation when x=0 or y=0?</li> </ol>"},{"location":"sims/vector-2d-3d-visualizer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Ask students to create a vector with a specific magnitude</li> <li>Have students predict the direction before adjusting sliders</li> <li>Quiz on calculating magnitudes from given components</li> </ul>"},{"location":"sims/vector-2d-3d-visualizer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Vectors, what even are they? - Excellent visual introduction to vectors</li> <li>Khan Academy - Introduction to Vectors</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press.</li> </ol>"},{"location":"sims/vector-operations-playground/","title":"Vector Operations Playground","text":"<p>Run the Vector Operations Playground Fullscreen</p>"},{"location":"sims/vector-operations-playground/#about-this-microsim","title":"About This MicroSim","text":"<p>This interactive playground allows students to explore vector operations by directly manipulating vectors and observing the results in real-time. Students can perform vector addition, subtraction, and scalar multiplication while seeing both the geometric and numerical representations.</p> <p>Learning Objective: Students will apply vector addition, subtraction, and scalar multiplication by manipulating vectors interactively and predicting results before seeing them visualized.</p>"},{"location":"sims/vector-operations-playground/#how-to-use","title":"How to Use","text":"<ol> <li>Drag Vectors: Click and drag the circular endpoints of vectors u (blue) and v (red) to position them anywhere on the grid</li> <li>Select Operation: Use the radio buttons to choose between:</li> <li>Add: Shows u + v (green result vector)</li> <li>Subtract: Shows u - v (green result vector)</li> <li>Scalar \u00d7: Shows c\u00b7u where c is controlled by the slider</li> <li>Adjust Scalar: When in scalar multiplication mode, use the slider to change the scalar value from -3 to 3</li> <li>Toggle Visualizations:</li> <li>Parallelogram: Shows the parallelogram construction for addition</li> <li>Components: Shows projection lines to the axes</li> <li>Animate: Click to see a step-by-step animation of the operation</li> <li>Reset: Return all vectors to their default positions</li> </ol>"},{"location":"sims/vector-operations-playground/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ul> <li>Vector Addition: The parallelogram rule and tip-to-tail method</li> <li>Vector Subtraction: Finding the difference vector u - v</li> <li>Scalar Multiplication: How scalars stretch, shrink, or reverse vectors</li> <li>Component Operations: How operations work on individual components</li> </ul>"},{"location":"sims/vector-operations-playground/#mathematical-formulas","title":"Mathematical Formulas","text":"<p>Addition: \\(\\mathbf{u} + \\mathbf{v} = (u_x + v_x, u_y + v_y)\\)</p> <p>Subtraction: \\(\\mathbf{u} - \\mathbf{v} = (u_x - v_x, u_y - v_y)\\)</p> <p>Scalar Multiplication: \\(c\\mathbf{u} = (c \\cdot u_x, c \\cdot u_y)\\)</p>"},{"location":"sims/vector-operations-playground/#embedding-this-microsim","title":"Embedding This MicroSim","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-operations-playground/main.html\"\n        height=\"552px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-operations-playground/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-operations-playground/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra or advanced high school mathematics</p>"},{"location":"sims/vector-operations-playground/#duration","title":"Duration","text":"<p>20-25 minutes</p>"},{"location":"sims/vector-operations-playground/#prerequisites","title":"Prerequisites","text":"<ul> <li>Understanding of 2D coordinate systems</li> <li>Basic knowledge of vectors as arrows with magnitude and direction</li> </ul>"},{"location":"sims/vector-operations-playground/#learning-activities","title":"Learning Activities","text":"<ol> <li>Exploration (5 min): Let students freely drag vectors and observe how operations change</li> <li>Addition Investigation (5 min):</li> <li>Enable parallelogram view</li> <li>Ask students to verify the parallelogram rule geometrically</li> <li>Have them predict the sum before moving vectors</li> <li>Subtraction Investigation (5 min):</li> <li>Switch to subtraction mode</li> <li>Explore how u - v relates to the vector from v's tip to u's tip</li> <li>Scalar Multiplication (5 min):</li> <li>Vary the scalar from -3 to 3</li> <li>Observe what happens at c = 0, c = 1, c = -1</li> <li>Synthesis (5 min): Combine concepts to solve problems</li> </ol>"},{"location":"sims/vector-operations-playground/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>What is the geometric meaning of the parallelogram in vector addition?</li> <li>If u + v = w, what is u - v geometrically?</li> <li>What happens to the direction of a vector when multiplied by a negative scalar?</li> <li>Can two different pairs of vectors have the same sum?</li> </ol>"},{"location":"sims/vector-operations-playground/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>Given a target point, find u and v that add to reach it</li> <li>Predict the result of an operation before seeing it</li> <li>Find a scalar that makes cu equal to a specific vector</li> </ul>"},{"location":"sims/vector-operations-playground/#references","title":"References","text":"<ol> <li>3Blue1Brown - Linear combinations, span, and basis vectors</li> <li>Khan Academy - Vector Addition</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Wellesley-Cambridge Press. Chapter 1.</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/","title":"Vector Space Axiom Explorer","text":"<p>Run the Vector Space Axiom Explorer Fullscreen</p>"},{"location":"sims/vector-space-axiom-explorer/#about-this-infographic","title":"About This Infographic","text":"<p>This interactive infographic helps students learn and remember the ten vector space axioms. The axioms are organized into two groups: five for vector addition and five for scalar multiplication. Click on each axiom card to see its definition and a concrete example.</p> <p>Learning Objective: Students will identify and recognize the ten vector space axioms through an interactive concept map with hover definitions and example demonstrations.</p>"},{"location":"sims/vector-space-axiom-explorer/#how-to-use","title":"How to Use","text":"<ol> <li>Hover over the central hub to see what a vector space is and examples of vector spaces and fields</li> <li>Click on axiom cards to expand and see:</li> <li>Full definition of the axiom</li> <li>A concrete numerical example in R\u00b2</li> <li>Track your progress with the counter at the bottom showing how many axioms you've viewed</li> <li>A checkmark appears on viewed axioms</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#the-ten-vector-space-axioms","title":"The Ten Vector Space Axioms","text":"<p>For a set V to be a vector space over a field F, it must satisfy:</p>"},{"location":"sims/vector-space-axiom-explorer/#addition-axioms-1-5","title":"Addition Axioms (1-5)","text":"<ol> <li>Closure: u + v \u2208 V</li> <li>Commutativity: u + v = v + u</li> <li>Associativity: (u + v) + w = u + (v + w)</li> <li>Identity: v + 0 = v</li> <li>Inverse: v + (-v) = 0</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#scalar-multiplication-axioms-6-10","title":"Scalar Multiplication Axioms (6-10)","text":"<ol> <li>Closure: cv \u2208 V</li> <li>Distributivity (vectors): c(u + v) = cu + cv</li> <li>Distributivity (scalars): (c + d)v = cv + dv</li> <li>Associativity: c(dv) = (cd)v</li> <li>Identity: 1\u00b7v = v</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#why-these-axioms-matter","title":"Why These Axioms Matter","text":"<p>These ten axioms are the foundation of linear algebra. Any set that satisfies all ten axioms is a vector space, which means all theorems about vector spaces apply to it. This includes:</p> <ul> <li>R^n (n-dimensional real space)</li> <li>Polynomials of degree \u2264 n</li> <li>Matrices of a given size</li> <li>Continuous functions on an interval</li> <li>Solutions to homogeneous differential equations</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#embedding-this-infographic","title":"Embedding This Infographic","text":"<pre><code>&lt;iframe src=\"https://dmccreary.github.io/linear-algebra/sims/vector-space-axiom-explorer/main.html\"\n        height=\"750px\"\n        width=\"100%\"\n        scrolling=\"no\"&gt;\n&lt;/iframe&gt;\n</code></pre>"},{"location":"sims/vector-space-axiom-explorer/#lesson-plan","title":"Lesson Plan","text":""},{"location":"sims/vector-space-axiom-explorer/#grade-level","title":"Grade Level","text":"<p>Undergraduate introductory linear algebra</p>"},{"location":"sims/vector-space-axiom-explorer/#duration","title":"Duration","text":"<p>15-20 minutes</p>"},{"location":"sims/vector-space-axiom-explorer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Vector addition and scalar multiplication concepts</li> <li>Familiarity with R\u00b2 as an example vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#learning-activities","title":"Learning Activities","text":"<ol> <li>Introduction (3 min):</li> <li>Hover over the central hub to understand what a vector space is</li> <li> <p>Note the two types of operations: addition and scalar multiplication</p> </li> <li> <p>Addition Axioms (5 min):</p> </li> <li>Click through all five addition axioms</li> <li>Work through each example mentally or on paper</li> <li> <p>Note how they formalize intuitive properties</p> </li> <li> <p>Scalar Multiplication Axioms (5 min):</p> </li> <li>Click through all five scalar multiplication axioms</li> <li>Compare distributivity axioms 7 and 8</li> <li> <p>Understand why the identity axiom uses 1</p> </li> <li> <p>Verification Exercise (5 min):</p> </li> <li>Given a candidate vector space (e.g., 2\u00d72 matrices)</li> <li>Check that all ten axioms hold</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#discussion-questions","title":"Discussion Questions","text":"<ol> <li>Why do we need all ten axioms? What would go wrong if one was missing?</li> <li>Can you think of a set with addition that violates one of these axioms?</li> <li>Why is the additive identity (zero vector) unique?</li> <li>What's the difference between the two distributivity axioms?</li> </ol>"},{"location":"sims/vector-space-axiom-explorer/#assessment-ideas","title":"Assessment Ideas","text":"<ul> <li>List all ten axioms from memory</li> <li>Given a set and operations, verify which axioms hold</li> <li>Explain why a given set is NOT a vector space</li> </ul>"},{"location":"sims/vector-space-axiom-explorer/#references","title":"References","text":"<ol> <li>3Blue1Brown - Abstract vector spaces</li> <li>Khan Academy - Vector Spaces</li> <li>Strang, G. (2016). Introduction to Linear Algebra (5th ed.). Section 3.1.</li> <li>Axler, S. (2015). Linear Algebra Done Right (3rd ed.). Chapter 1.</li> </ol>"}]}