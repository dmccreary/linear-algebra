
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="The linear algebra foundations of transformers, attention mechanisms, and modern generative AI">
      
      
        <meta name="author" content="Dan McCreary">
      
      
        <link rel="canonical" href="https://dmccreary.github.io/linear-Algebra/chapters/11-generative-ai-and-llms/">
      
      
        <link rel="prev" href="../10-neural-networks-and-deep-learning/quiz/">
      
      
        <link rel="next" href="quiz/">
      
      
        
      
      
      <link rel="icon" href="../../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Generative AI and Large Language Models - Linear Algebra</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-KC2L3G6KXH"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-KC2L3G6KXH",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-KC2L3G6KXH",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Generative AI and Large Language Models - Linear Algebra" />
<meta property="og:description" content="The linear algebra foundations of transformers, attention mechanisms, and modern generative AI" />
<meta property="og:image" content="https://dmccreary.github.io/linear-Algebra/assets/images/social/chapters/11-generative-ai-and-llms/index.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://dmccreary.github.io/linear-Algebra/chapters/11-generative-ai-and-llms/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Generative AI and Large Language Models - Linear Algebra" />
<meta property="twitter:description" content="The linear algebra foundations of transformers, attention mechanisms, and modern generative AI" />
<meta property="twitter:image" content="https://dmccreary.github.io/linear-Algebra/assets/images/social/chapters/11-generative-ai-and-llms/index.png" />
</head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generative-ai-and-large-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Linear Algebra" class="md-header__button md-logo" aria-label="Linear Algebra" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Linear Algebra
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generative AI and Large Language Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/dmccreary/linear-Algebra" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Linear Algebra" class="md-nav__button md-logo" aria-label="Linear Algebra" data-md-component="logo">
      
  <img src="../../img/logo.png" alt="logo">

    </a>
    Linear Algebra
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dmccreary/linear-Algebra" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../course-description/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Course Description
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Chapters
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Chapters
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../01-vectors-and-vector-spaces/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    1. Vectors and Vector Spaces
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../02-matrices-and-matrix-operations/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    2. Matrices and Matrix Operations
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../03-systems-of-linear-equations/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    3. Systems of Linear Equations
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../04-linear-transformations/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    4. Linear Transformations
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../05-determinants-and-matrix-properties/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    5. Determinants and Matrix Properties
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../06-eigenvalues-and-eigenvectors/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    6. Eigenvalues and Eigenvectors
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../07-matrix-decompositions/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    7. Matrix Decompositions
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../08-vector-spaces-and-inner-products/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    8. Vector Spaces and Inner Products
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../09-machine-learning-foundations/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    9. Machine Learning Foundations
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../10-neural-networks-and-deep-learning/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    10. Neural Networks and Deep Learning
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_12" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="./" class="md-nav__link md-nav__link--active">
              
  
  
  <span class="md-ellipsis">
    
  
    11. Generative AI and LLMs
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link md-nav__link--active" for="__nav_3_12" id="__nav_3_12_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_12_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_12">
            <span class="md-nav__icon md-icon"></span>
            
  
    11. Generative AI and LLMs
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="quiz/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quiz
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../12-optimization-and-learning-algorithms/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    12. Optimization and Learning Algorithms
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../13-image-processing-and-computer-vision/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    13. Image Processing and Computer Vision
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../14-3d-geometry-and-transformations/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    14. 3D Geometry and Transformations
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../15-autonomous-systems-and-sensor-fusion/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    15. Autonomous Systems and Sensor Fusion
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../learning-graph/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    Learning Graph
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Glossary
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    FAQ
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../sims/graph-viewer/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    
  
    MicroSims
  

    
  </span>
  
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#concepts-covered" class="md-nav__link">
    <span class="md-ellipsis">
      
        Concepts Covered
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prerequisites
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#embeddings-from-symbols-to-vectors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Embeddings: From Symbols to Vectors
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Embeddings: From Symbols to Vectors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-an-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Is an Embedding?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Embedding Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Word Embeddings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Word Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-embedding-space-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Embedding Space Visualizer
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#measuring-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Measuring Similarity
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Measuring Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#semantic-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Semantic Similarity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cosine Similarity
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Cosine Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-similarity-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Similarity Comparison
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Attention Mechanism
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Attention Mechanism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#core-idea" class="md-nav__link">
    <span class="md-ellipsis">
      
        Core Idea
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-Attention
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Cross-Attention
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#query-key-value-matrices" class="md-nav__link">
    <span class="md-ellipsis">
      
        Query, Key, Value Matrices
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Query, Key, Value Matrices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-qkv-framework" class="md-nav__link">
    <span class="md-ellipsis">
      
        The QKV Framework
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#intuition" class="md-nav__link">
    <span class="md-ellipsis">
      
        Intuition
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-scores" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention Scores
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention-weights" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention Weights
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#complete-attention-formula" class="md-nav__link">
    <span class="md-ellipsis">
      
        Complete Attention Formula
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Complete Attention Formula">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-attention-mechanism-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Attention Mechanism Visualizer
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-head-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Head Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-Head Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-multiple-heads" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Multiple Heads?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-computation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Head Computation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dimension-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dimension Management
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dimension Management">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-multi-head-attention-visualizer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Multi-Head Attention Visualizer
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Transformer Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Transformer Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-block" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer Block
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-connections" class="md-nav__link">
    <span class="md-ellipsis">
      
        Residual Connections
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Layer Normalization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-encoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        Position Encoding
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Position Encoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Transformer Architecture
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#efficient-fine-tuning-with-lora" class="md-nav__link">
    <span class="md-ellipsis">
      
        Efficient Fine-Tuning with LoRA
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Efficient Fine-Tuning with LoRA">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-lora-idea" class="md-nav__link">
    <span class="md-ellipsis">
      
        The LoRA Idea
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parameter-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Parameter Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-low-rank-works" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why Low-Rank Works
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Implementation
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-lora-visualization" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: LoRA Visualization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#latent-spaces-in-generative-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Latent Spaces in Generative Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Latent Spaces in Generative Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-a-latent-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Is a Latent Space?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structure-in-latent-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Structure in Latent Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpolation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Interpolation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applications
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Applications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#diagram-latent-space-interpolation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Diagram: Latent Space Interpolation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Practical Implementation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
                



  


  <nav class="md-path" aria-label="Navigation" >
    <ol class="md-path__list">
      
        
  
  
    <li class="md-path__item">
      <a href="../.." class="md-path__link">
        
  <span class="md-ellipsis">
    Home
  </span>

      </a>
    </li>
  

      
      
        
  
  
    
    
      <li class="md-path__item">
        <a href="../" class="md-path__link">
          
  <span class="md-ellipsis">
    Chapters
  </span>

        </a>
      </li>
    
  

      
        
  
  
    
    
      <li class="md-path__item">
        <a href="./" class="md-path__link">
          
  <span class="md-ellipsis">
    11. Generative AI and LLMs
  </span>

        </a>
      </li>
    
  

      
    </ol>
  </nav>

              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/dmccreary/linear-Algebra/edit/master/docs/chapters/11-generative-ai-and-llms/index.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  


<h1 id="generative-ai-and-large-language-models">Generative AI and Large Language Models</h1>
<h2 id="summary">Summary</h2>
<p>Modern generative AI systems rely heavily on sophisticated linear algebra. This chapter explores the mathematical foundations of transformers and large language models, including embedding spaces, attention mechanisms, query-key-value matrices, and multi-head attention. You will also learn about LoRA for efficient fine-tuning and latent space interpolation in generative models.</p>
<h2 id="concepts-covered">Concepts Covered</h2>
<p>This chapter covers the following 19 concepts from the learning graph:</p>
<ol>
<li>Embedding</li>
<li>Embedding Space</li>
<li>Word Embedding</li>
<li>Semantic Similarity</li>
<li>Cosine Similarity</li>
<li>Attention Mechanism</li>
<li>Self-Attention</li>
<li>Cross-Attention</li>
<li>Query Matrix</li>
<li>Key Matrix</li>
<li>Value Matrix</li>
<li>Attention Score</li>
<li>Attention Weights</li>
<li>Multi-Head Attention</li>
<li>Transformer Architecture</li>
<li>Position Encoding</li>
<li>LoRA</li>
<li>Latent Space</li>
<li>Interpolation</li>
</ol>
<h2 id="prerequisites">Prerequisites</h2>
<p>This chapter builds on concepts from:</p>
<ul>
<li><a href="../01-vectors-and-vector-spaces/">Chapter 1: Vectors and Vector Spaces</a></li>
<li><a href="../02-matrices-and-matrix-operations/">Chapter 2: Matrices and Matrix Operations</a></li>
<li><a href="../07-matrix-decompositions/">Chapter 7: Matrix Decompositions</a> (for low-rank approximation)</li>
<li><a href="../09-machine-learning-foundations/">Chapter 9: Machine Learning Foundations</a></li>
<li><a href="../10-neural-networks-and-deep-learning/">Chapter 10: Neural Networks and Deep Learning</a></li>
</ul>
<hr />
<h2 id="introduction">Introduction</h2>
<p>Large language models like GPT and Claude have transformed artificial intelligence, demonstrating remarkable capabilities in understanding and generating human language. At their core, these models are sophisticated linear algebra engines—they represent words as vectors, compute relationships through matrix operations, and generate output by manipulating high-dimensional spaces.</p>
<p>This chapter reveals the mathematical machinery behind modern generative AI:</p>
<ul>
<li><strong>Embeddings</strong> map discrete tokens to continuous vector spaces</li>
<li><strong>Attention mechanisms</strong> compute dynamic, context-dependent relationships</li>
<li><strong>Transformers</strong> stack these operations to build powerful models</li>
<li><strong>Low-rank adaptation</strong> enables efficient fine-tuning</li>
</ul>
<p>Understanding these foundations demystifies AI systems and enables you to reason about their capabilities and limitations.</p>
<h2 id="embeddings-from-symbols-to-vectors">Embeddings: From Symbols to Vectors</h2>
<p>Natural language consists of discrete symbols—words, characters, or subword tokens. Neural networks require continuous numerical input. <strong>Embeddings</strong> bridge this gap.</p>
<h3 id="what-is-an-embedding">What Is an Embedding?</h3>
<p>An <strong>embedding</strong> is a learned mapping from discrete items to continuous vectors:</p>
<p><span class="arithmatex">\(e: \{1, 2, \ldots, V\} \to \mathbb{R}^d\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(V\)</span> is the vocabulary size (e.g., 50,000 tokens)</li>
<li><span class="arithmatex">\(d\)</span> is the embedding dimension (e.g., 768, 1024, or 4096)</li>
</ul>
<p>In practice, embeddings are stored as an <strong>embedding matrix</strong> <span class="arithmatex">\(E \in \mathbb{R}^{V \times d}\)</span>:</p>
<p><span class="arithmatex">\(\mathbf{e}_i = E[i, :] \quad \text{(row } i \text{ of } E \text{)}\)</span></p>
<h3 id="embedding-space">Embedding Space</h3>
<p>The <strong>embedding space</strong> is the <span class="arithmatex">\(d\)</span>-dimensional vector space where embeddings live. This space has remarkable properties:</p>
<ul>
<li>Similar items are mapped to nearby points</li>
<li>Relationships are encoded as directions</li>
<li>Arithmetic operations have semantic meaning</li>
</ul>
<p>The famous Word2Vec example: <span class="arithmatex">\(\text{king} - \text{man} + \text{woman} \approx \text{queen}\)</span></p>
<p>This works because the embedding space encodes the "royalty" and "gender" concepts as roughly orthogonal directions.</p>
<h3 id="word-embeddings">Word Embeddings</h3>
<p><strong>Word embeddings</strong> specifically represent words (or subword tokens) as vectors. They are learned from large text corpora by predicting context:</p>
<ul>
<li><strong>Word2Vec:</strong> Predict surrounding words from center word (or vice versa)</li>
<li><strong>GloVe:</strong> Factorize word co-occurrence matrix</li>
<li><strong>Transformer embeddings:</strong> Learned end-to-end with the model</li>
</ul>
<table>
<thead>
<tr>
<th>Method</th>
<th>Approach</th>
<th>Context</th>
</tr>
</thead>
<tbody>
<tr>
<td>Word2Vec (Skip-gram)</td>
<td>Predict context from word</td>
<td>Fixed window</td>
</tr>
<tr>
<td>Word2Vec (CBOW)</td>
<td>Predict word from context</td>
<td>Fixed window</td>
</tr>
<tr>
<td>GloVe</td>
<td>Matrix factorization</td>
<td>Global co-occurrence</td>
</tr>
<tr>
<td>BERT/GPT</td>
<td>End-to-end transformer</td>
<td>Full sequence</td>
</tr>
</tbody>
</table>
<h4 id="diagram-embedding-space-visualizer">Diagram: Embedding Space Visualizer</h4>
<details>
<summary>Embedding Space Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Understand</p>
<p>Learning Objective: Visualize how words are positioned in embedding space and explore semantic relationships</p>
<p>Visual elements:
- 2D projection (t-SNE or PCA) of word embeddings
- Clusters of related words (colors, animals, countries)
- Vector arithmetic visualization
- Distance/similarity measurements</p>
<p>Interactive controls:
- Word search to highlight specific embeddings
- "Find Similar" to show nearest neighbors
- Vector arithmetic input: word1 - word2 + word3 = ?
- Projection method selector (PCA, t-SNE)
- Zoom and pan controls</p>
<p>Default parameters:
- Pre-computed embeddings for 1000 common words
- 2D PCA projection
- Sample clusters highlighted
- Canvas: responsive</p>
<p>Behavior:
- Hover to see word labels
- Click word to show nearest neighbors
- Enter vector arithmetic to see result
- Animate projection computation
- Show similarity scores</p>
<p>Implementation: p5.js with pre-computed embedding data</p>
</details>
<h2 id="measuring-similarity">Measuring Similarity</h2>
<p>How do we determine if two embeddings represent similar concepts?</p>
<h3 id="semantic-similarity">Semantic Similarity</h3>
<p><strong>Semantic similarity</strong> measures how related two concepts are in meaning. In embedding space, semantic similarity corresponds to geometric proximity.</p>
<p>Two approaches:</p>
<ol>
<li><strong>Euclidean distance:</strong> <span class="arithmatex">\(d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|_2\)</span></li>
<li><strong>Cosine similarity:</strong> <span class="arithmatex">\(\cos(\theta) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|}\)</span></li>
</ol>
<h3 id="cosine-similarity">Cosine Similarity</h3>
<p><strong>Cosine similarity</strong> measures the angle between vectors, ignoring magnitude:</p>
<p><span class="arithmatex">\(\text{sim}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u}^T\mathbf{v}}{\|\mathbf{u}\|_2 \|\mathbf{v}\|_2} = \frac{\sum_{i=1}^d u_i v_i}{\sqrt{\sum_{i=1}^d u_i^2} \sqrt{\sum_{i=1}^d v_i^2}}\)</span></p>
<p>Properties:</p>
<ul>
<li>Range: <span class="arithmatex">\([-1, 1]\)</span></li>
<li><span class="arithmatex">\(+1\)</span>: identical direction (most similar)</li>
<li><span class="arithmatex">\(0\)</span>: orthogonal (unrelated)</li>
<li><span class="arithmatex">\(-1\)</span>: opposite direction (most dissimilar)</li>
</ul>
<p>Why cosine over Euclidean?</p>
<ul>
<li>Invariant to vector magnitude (important for variable-length documents)</li>
<li>Efficient computation in high dimensions</li>
<li>More robust to the "curse of dimensionality"</li>
</ul>
<p>For normalized embeddings (<span class="arithmatex">\(\|\mathbf{u}\| = \|\mathbf{v}\| = 1\)</span>):</p>
<p><span class="arithmatex">\(\text{cosine}(\mathbf{u}, \mathbf{v}) = \mathbf{u}^T\mathbf{v}\)</span></p>
<p>This is just the dot product—extremely fast to compute.</p>
<h4 id="diagram-similarity-comparison">Diagram: Similarity Comparison</h4>
<details>
<summary>Cosine vs Euclidean Similarity</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Analyze</p>
<p>Learning Objective: Compare cosine and Euclidean similarity measures and understand when each is appropriate</p>
<p>Visual elements:
- 2D plane with origin
- Two adjustable vectors (draggable endpoints)
- Angle arc between vectors
- Distance line between endpoints
- Similarity scores displayed</p>
<p>Interactive controls:
- Drag vector endpoints
- "Normalize Vectors" toggle
- Show cosine similarity value
- Show Euclidean distance value
- Preset examples: same direction different magnitude, orthogonal, similar angle</p>
<p>Default parameters:
- Two vectors in 2D
- Both similarity measures shown
- Canvas: responsive</p>
<p>Behavior:
- Real-time similarity updates
- Show how cosine ignores magnitude
- Demonstrate normalization effect
- Highlight angle vs distance
- Show formulas with current values</p>
<p>Implementation: p5.js with vector geometry</p>
</details>
<h2 id="the-attention-mechanism">The Attention Mechanism</h2>
<p><strong>Attention</strong> is the key innovation enabling transformers to model long-range dependencies. Rather than processing sequences left-to-right, attention allows every position to directly interact with every other position.</p>
<h3 id="core-idea">Core Idea</h3>
<p>Given a sequence of vectors, attention computes a weighted combination where the weights depend on the content of the vectors themselves. This is "content-based" addressing—the model decides what to focus on based on what it's looking at.</p>
<h3 id="self-attention">Self-Attention</h3>
<p><strong>Self-attention</strong> computes attention within a single sequence. Each position attends to all positions in the same sequence (including itself):</p>
<p>For input sequence <span class="arithmatex">\(X = [\mathbf{x}_1, \ldots, \mathbf{x}_n]^T \in \mathbb{R}^{n \times d}\)</span>:</p>
<ol>
<li>Each position queries: "What should I pay attention to?"</li>
<li>Each position offers keys: "Here's what I contain"</li>
<li>Each position provides values: "Here's my information"</li>
</ol>
<h3 id="cross-attention">Cross-Attention</h3>
<p><strong>Cross-attention</strong> computes attention between two different sequences:</p>
<ul>
<li>Queries come from one sequence (e.g., decoder)</li>
<li>Keys and values come from another sequence (e.g., encoder)</li>
</ul>
<p>This enables:</p>
<ul>
<li>Machine translation (attend to source while generating target)</li>
<li>Image captioning (attend to image regions while generating text)</li>
<li>Question answering (attend to context while generating answer)</li>
</ul>
<table>
<thead>
<tr>
<th>Attention Type</th>
<th>Queries From</th>
<th>Keys/Values From</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-attention</td>
<td>Sequence X</td>
<td>Sequence X</td>
<td>Language modeling</td>
</tr>
<tr>
<td>Cross-attention</td>
<td>Sequence Y</td>
<td>Sequence X</td>
<td>Translation, QA</td>
</tr>
<tr>
<td>Masked self-attention</td>
<td>Sequence X</td>
<td>Past positions of X</td>
<td>Autoregressive generation</td>
</tr>
</tbody>
</table>
<h2 id="query-key-value-matrices">Query, Key, Value Matrices</h2>
<p>The attention mechanism is implemented using three learned linear projections.</p>
<h3 id="the-qkv-framework">The QKV Framework</h3>
<p>For input <span class="arithmatex">\(X \in \mathbb{R}^{n \times d_{model}}\)</span>:</p>
<p><strong>Query Matrix:</strong></p>
<p><span class="arithmatex">\(Q = XW^Q\)</span></p>
<p>where <span class="arithmatex">\(W^Q \in \mathbb{R}^{d_{model} \times d_k}\)</span></p>
<p><strong>Key Matrix:</strong></p>
<p><span class="arithmatex">\(K = XW^K\)</span></p>
<p>where <span class="arithmatex">\(W^K \in \mathbb{R}^{d_{model} \times d_k}\)</span></p>
<p><strong>Value Matrix:</strong></p>
<p><span class="arithmatex">\(V = XW^V\)</span></p>
<p>where <span class="arithmatex">\(W^V \in \mathbb{R}^{d_{model} \times d_v}\)</span></p>
<p>Resulting shapes:</p>
<ul>
<li><span class="arithmatex">\(Q \in \mathbb{R}^{n \times d_k}\)</span> — one query per position</li>
<li><span class="arithmatex">\(K \in \mathbb{R}^{n \times d_k}\)</span> — one key per position</li>
<li><span class="arithmatex">\(V \in \mathbb{R}^{n \times d_v}\)</span> — one value per position</li>
</ul>
<h3 id="intuition">Intuition</h3>
<p>Think of attention as a soft dictionary lookup:</p>
<ul>
<li><strong>Query:</strong> "I'm looking for information about X"</li>
<li><strong>Keys:</strong> "I contain information about Y"</li>
<li><strong>Values:</strong> "Here's my actual content"</li>
</ul>
<p>The query-key dot product measures compatibility. High compatibility means "this key matches my query," so that value should contribute more to my output.</p>
<h3 id="attention-scores">Attention Scores</h3>
<p><strong>Attention scores</strong> measure query-key compatibility:</p>
<p><span class="arithmatex">\(S = QK^T \in \mathbb{R}^{n \times n}\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(S_{ij} = \mathbf{q}_i^T \mathbf{k}_j\)</span> is the score between position <span class="arithmatex">\(i\)</span>'s query and position <span class="arithmatex">\(j\)</span>'s key</li>
<li>Higher score means stronger relevance</li>
</ul>
<p>The scores are scaled to prevent softmax saturation:</p>
<p><span class="arithmatex">\(S_{scaled} = \frac{QK^T}{\sqrt{d_k}}\)</span></p>
<p>The <span class="arithmatex">\(\sqrt{d_k}\)</span> factor keeps the variance of dot products manageable as dimension increases.</p>
<h3 id="attention-weights">Attention Weights</h3>
<p><strong>Attention weights</strong> are normalized scores (probabilities):</p>
<p><span class="arithmatex">\(A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)\)</span></p>
<p>where softmax is applied row-wise:</p>
<p><span class="arithmatex">\(A_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^n \exp(S_{ik})}\)</span></p>
<p>Properties:</p>
<ul>
<li>Each row sums to 1: <span class="arithmatex">\(\sum_j A_{ij} = 1\)</span></li>
<li>All entries non-negative: <span class="arithmatex">\(A_{ij} \geq 0\)</span></li>
<li>Represents a probability distribution over positions</li>
</ul>
<h3 id="complete-attention-formula">Complete Attention Formula</h3>
<p>The attention output combines values weighted by attention:</p>
<p><span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></p>
<p>Output shape: <span class="arithmatex">\(n \times d_v\)</span> — one output vector per position.</p>
<h4 id="diagram-attention-mechanism-visualizer">Diagram: Attention Mechanism Visualizer</h4>
<details>
<summary>Attention Mechanism Step-by-Step</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Apply</p>
<p>Learning Objective: Understand how attention computes weighted combinations through QKV projections</p>
<p>Visual elements:
- Input sequence (tokens with embeddings)
- Q, K, V projection matrices
- Attention score matrix (heatmap)
- Softmax attention weights
- Output vectors as weighted sums</p>
<p>Interactive controls:
- Input sequence editor (3-5 tokens)
- Step-through: "Project Q", "Project K", "Project V", "Compute Scores", "Softmax", "Weighted Sum"
- Query position selector (which position to visualize)
- "Show All Attention Heads" toggle
- Matrix dimension display</p>
<p>Default parameters:
- 4-token sequence
- d_model = 8, d_k = d_v = 4
- Single attention head
- Canvas: responsive</p>
<p>Behavior:
- Animate each projection step
- Show attention score computation
- Visualize softmax normalization
- Demonstrate weighted sum of values
- Highlight which positions attend to which</p>
<p>Implementation: p5.js with matrix visualization</p>
</details>
<h2 id="multi-head-attention">Multi-Head Attention</h2>
<p><strong>Multi-head attention</strong> runs multiple attention operations in parallel, each learning different relationship patterns.</p>
<h3 id="why-multiple-heads">Why Multiple Heads?</h3>
<p>A single attention head might focus on one type of relationship (e.g., syntactic). Multiple heads can capture diverse patterns:</p>
<ul>
<li>Head 1: Subject-verb agreement</li>
<li>Head 2: Coreference (pronouns to antecedents)</li>
<li>Head 3: Positional patterns</li>
<li>Head 4: Semantic similarity</li>
</ul>
<h3 id="multi-head-computation">Multi-Head Computation</h3>
<p>For <span class="arithmatex">\(h\)</span> attention heads:</p>
<p><span class="arithmatex">\(\text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\)</span></p>
<p>where each head has its own projection matrices.</p>
<p>Heads are concatenated and projected:</p>
<p><span class="arithmatex">\(\text{MultiHead}(X) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\)</span></p>
<p>where <span class="arithmatex">\(W^O \in \mathbb{R}^{hd_v \times d_{model}}\)</span> is the output projection.</p>
<h3 id="dimension-management">Dimension Management</h3>
<p>Typical configuration (e.g., <span class="arithmatex">\(d_{model} = 512\)</span>, <span class="arithmatex">\(h = 8\)</span>):</p>
<ul>
<li><span class="arithmatex">\(d_k = d_v = d_{model}/h = 64\)</span></li>
<li>Each head operates on lower dimension</li>
<li>Total computation similar to single full-dimension head</li>
<li>But captures richer patterns</li>
</ul>
<table>
<thead>
<tr>
<th>Component</th>
<th>Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input <span class="arithmatex">\(X\)</span></td>
<td><span class="arithmatex">\(n \times d_{model}\)</span></td>
</tr>
<tr>
<td>Per-head <span class="arithmatex">\(W^Q, W^K\)</span></td>
<td><span class="arithmatex">\(d_{model} \times d_k\)</span></td>
</tr>
<tr>
<td>Per-head <span class="arithmatex">\(W^V\)</span></td>
<td><span class="arithmatex">\(d_{model} \times d_v\)</span></td>
</tr>
<tr>
<td>Per-head output</td>
<td><span class="arithmatex">\(n \times d_v\)</span></td>
</tr>
<tr>
<td>Concatenated</td>
<td><span class="arithmatex">\(n \times hd_v\)</span></td>
</tr>
<tr>
<td>After <span class="arithmatex">\(W^O\)</span></td>
<td><span class="arithmatex">\(n \times d_{model}\)</span></td>
</tr>
</tbody>
</table>
<h4 id="diagram-multi-head-attention-visualizer">Diagram: Multi-Head Attention Visualizer</h4>
<details>
<summary>Multi-Head Attention Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Analyze</p>
<p>Learning Objective: Understand how multiple attention heads capture different patterns and combine their outputs</p>
<p>Visual elements:
- Parallel attention head diagrams
- Per-head attention weight heatmaps
- Concatenation visualization
- Output projection
- Comparison of what each head attends to</p>
<p>Interactive controls:
- Number of heads slider (1-8)
- Input sequence editor
- Head selector to highlight individual heads
- "Compare Heads" mode
- "Show Learned Patterns" toggle</p>
<p>Default parameters:
- 4 attention heads
- 5-token sequence
- Pre-trained attention patterns
- Canvas: responsive</p>
<p>Behavior:
- Show each head's attention independently
- Visualize concatenation step
- Demonstrate different heads learning different patterns
- Compare head outputs before/after combination
- Display dimension flow through computation</p>
<p>Implementation: p5.js with parallel visualization</p>
</details>
<h2 id="the-transformer-architecture">The Transformer Architecture</h2>
<p>The <strong>transformer</strong> stacks attention layers with feedforward networks to build powerful sequence models.</p>
<h3 id="transformer-block">Transformer Block</h3>
<p>A single transformer block contains:</p>
<ol>
<li><strong>Multi-head self-attention</strong> (with residual connection and layer norm)</li>
<li><strong>Feedforward network</strong> (with residual connection and layer norm)</li>
</ol>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code>Input x
    │
    ├──────────────────┐
    ▼                  │
LayerNorm              │
    │                  │
MultiHeadAttn          │
    │                  │
    ▼                  │
    + ◄────────────────┘  (residual)
    │
    ├──────────────────┐
    ▼                  │
LayerNorm              │
    │                  │
FeedForward            │
    │                  │
    ▼                  │
    + ◄────────────────┘  (residual)
    │
Output
</code></pre></div></td></tr></table></div>
<h3 id="residual-connections">Residual Connections</h3>
<p>Residual connections add the input to the output:</p>
<p><span class="arithmatex">\(\text{output} = x + \text{SubLayer}(x)\)</span></p>
<p>Benefits:</p>
<ul>
<li>Enable gradient flow through deep networks</li>
<li>Allow layers to learn "refinements" rather than full transformations</li>
<li>Stabilize training</li>
</ul>
<h3 id="layer-normalization">Layer Normalization</h3>
<p>Layer norm normalizes across features (not batch):</p>
<p><span class="arithmatex">\(\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sigma} + \beta\)</span></p>
<p>Applied before or after each sublayer (pre-norm vs post-norm).</p>
<h3 id="position-encoding">Position Encoding</h3>
<p>Self-attention is permutation-invariant—it treats input as a set, not a sequence. <strong>Position encodings</strong> inject positional information.</p>
<p><strong>Sinusoidal encoding</strong> (original transformer):</p>
<p><span class="arithmatex">\(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})\)</span></p>
<p><span class="arithmatex">\(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(pos\)</span> is the position in the sequence</li>
<li><span class="arithmatex">\(i\)</span> is the dimension index</li>
</ul>
<p>Properties:</p>
<ul>
<li>Unique encoding for each position</li>
<li>Relative positions can be computed from absolute encodings</li>
<li>Generalizes to longer sequences than seen during training</li>
</ul>
<p><strong>Learned position embeddings</strong> (GPT, BERT):</p>
<ul>
<li>Add learnable position embedding matrix <span class="arithmatex">\(P \in \mathbb{R}^{L_{max} \times d_{model}}\)</span></li>
<li><span class="arithmatex">\(\text{input} = \text{token\_embedding} + \text{position\_embedding}\)</span></li>
</ul>
<h4 id="diagram-transformer-architecture">Diagram: Transformer Architecture</h4>
<details>
<summary>Transformer Block Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Understand</p>
<p>Learning Objective: Visualize data flow through a transformer block including attention, FFN, residuals, and layer norms</p>
<p>Visual elements:
- Block diagram of transformer layer
- Data tensors at each stage (shape annotations)
- Residual connection paths
- Layer norm visualization
- Stacking multiple blocks</p>
<p>Interactive controls:
- Number of blocks slider (1-6)
- "Step Through" one operation at a time
- "Show Dimensions" toggle
- "Highlight Residuals" toggle
- Input sequence length slider</p>
<p>Default parameters:
- 2 transformer blocks
- Sequence length 4
- d_model = 512, d_ff = 2048
- Canvas: responsive</p>
<p>Behavior:
- Animate data flow through block
- Show dimension changes at each stage
- Visualize residual addition
- Display parameter count per component
- Compare pre-norm vs post-norm</p>
<p>Implementation: p5.js with architecture diagram</p>
</details>
<h2 id="efficient-fine-tuning-with-lora">Efficient Fine-Tuning with LoRA</h2>
<p>Training all parameters of large models is expensive. <strong>LoRA (Low-Rank Adaptation)</strong> enables efficient fine-tuning.</p>
<h3 id="the-lora-idea">The LoRA Idea</h3>
<p>Instead of updating full weight matrices, LoRA adds trainable low-rank decomposition:</p>
<p><span class="arithmatex">\(W' = W + \Delta W = W + BA\)</span></p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(W \in \mathbb{R}^{d \times k}\)</span> is the frozen pre-trained weight</li>
<li><span class="arithmatex">\(B \in \mathbb{R}^{d \times r}\)</span> and <span class="arithmatex">\(A \in \mathbb{R}^{r \times k}\)</span> are trainable</li>
<li><span class="arithmatex">\(r \ll \min(d, k)\)</span> is the rank (e.g., 4, 8, 16)</li>
</ul>
<h3 id="parameter-efficiency">Parameter Efficiency</h3>
<p>For a weight matrix of size <span class="arithmatex">\(d \times k\)</span>:</p>
<ul>
<li>Full fine-tuning: <span class="arithmatex">\(dk\)</span> parameters</li>
<li>LoRA: <span class="arithmatex">\(r(d + k)\)</span> parameters</li>
</ul>
<p>Example: <span class="arithmatex">\(d = k = 4096\)</span>, <span class="arithmatex">\(r = 8\)</span>:</p>
<ul>
<li>Full: <span class="arithmatex">\(16.7M\)</span> parameters per matrix</li>
<li>LoRA: <span class="arithmatex">\(65K\)</span> parameters per matrix (0.4%)</li>
</ul>
<h3 id="why-low-rank-works">Why Low-Rank Works</h3>
<p>The update <span class="arithmatex">\(\Delta W\)</span> has rank at most <span class="arithmatex">\(r\)</span>. Research suggests that:</p>
<ul>
<li>Model adaptation often lies in a low-dimensional subspace</li>
<li>The "intrinsic dimension" of fine-tuning is much smaller than parameter count</li>
<li>Low-rank updates capture task-specific modifications</li>
</ul>
<h3 id="implementation">Implementation</h3>
<p>During fine-tuning:</p>
<ol>
<li>Freeze all original parameters <span class="arithmatex">\(W\)</span></li>
<li>Initialize <span class="arithmatex">\(A\)</span> with small random values, <span class="arithmatex">\(B\)</span> with zeros</li>
<li>Train only <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span></li>
<li>Forward pass: <span class="arithmatex">\(h = (W + BA)x = Wx + BAx\)</span></li>
</ol>
<p>For inference, can merge: <span class="arithmatex">\(W' = W + BA\)</span> (no added latency).</p>
<h4 id="diagram-lora-visualization">Diagram: LoRA Visualization</h4>
<details>
<summary>LoRA Low-Rank Adaptation Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Apply</p>
<p>Learning Objective: Understand how LoRA approximates weight updates with low-rank matrices</p>
<p>Visual elements:
- Original weight matrix W (large, frozen)
- Low-rank factors A and B
- Product BA visualization
- Updated matrix W' = W + BA
- Parameter count comparison</p>
<p>Interactive controls:
- Matrix dimensions d and k sliders
- Rank r slider (1-32)
- "Show Decomposition" toggle
- "Compare to Full Fine-tune" toggle
- Animation of forward pass</p>
<p>Default parameters:
- d = k = 64 (for visualization)
- r = 4
- Canvas: responsive</p>
<p>Behavior:
- Show A and B shapes change with r
- Display parameter savings percentage
- Visualize low-rank approximation quality
- Compare BA to random full-rank update
- Show merged weight option</p>
<p>Implementation: p5.js with matrix visualization</p>
</details>
<h2 id="latent-spaces-in-generative-models">Latent Spaces in Generative Models</h2>
<p>Generative models learn to map between data and abstract <strong>latent spaces</strong>.</p>
<h3 id="what-is-a-latent-space">What Is a Latent Space?</h3>
<p>A <strong>latent space</strong> is a compressed, continuous representation where:</p>
<ul>
<li>Each point corresponds to a potential data sample</li>
<li>Similar points produce similar outputs</li>
<li>The space is typically lower-dimensional than data space</li>
</ul>
<p>For images:</p>
<ul>
<li>Data space: <span class="arithmatex">\(\mathbb{R}^{H \times W \times 3}\)</span> (millions of pixels)</li>
<li>Latent space: <span class="arithmatex">\(\mathbb{R}^{512}\)</span> (compressed representation)</li>
</ul>
<h3 id="structure-in-latent-space">Structure in Latent Space</h3>
<p>Well-trained generative models learn latent spaces with meaningful structure:</p>
<ul>
<li><strong>Clusters:</strong> Similar items group together</li>
<li><strong>Directions:</strong> Moving along certain directions changes specific attributes</li>
<li><strong>Arithmetic:</strong> Vector operations have semantic meaning</li>
</ul>
<h3 id="interpolation">Interpolation</h3>
<p><strong>Interpolation</strong> generates intermediate points between two latent vectors:</p>
<p><strong>Linear interpolation:</strong></p>
<p><span class="arithmatex">\(\mathbf{z}(t) = (1-t)\mathbf{z}_1 + t\mathbf{z}_2, \quad t \in [0, 1]\)</span></p>
<p><strong>Spherical interpolation (slerp):</strong> Better for normalized latent spaces:</p>
<p><span class="arithmatex">\(\mathbf{z}(t) = \frac{\sin((1-t)\theta)}{\sin\theta}\mathbf{z}_1 + \frac{\sin(t\theta)}{\sin\theta}\mathbf{z}_2\)</span></p>
<p>where <span class="arithmatex">\(\theta = \arccos(\mathbf{z}_1 \cdot \mathbf{z}_2)\)</span> for normalized vectors.</p>
<h3 id="applications">Applications</h3>
<ul>
<li><strong>Image morphing:</strong> Smooth transition between faces</li>
<li><strong>Style mixing:</strong> Combine attributes of different samples</li>
<li><strong>Attribute editing:</strong> Move along discovered directions</li>
<li><strong>Data augmentation:</strong> Generate novel training samples</li>
</ul>
<h4 id="diagram-latent-space-interpolation">Diagram: Latent Space Interpolation</h4>
<details>
<summary>Latent Space Interpolation Visualizer</summary>
<p>Type: microsim</p>
<p>Bloom Taxonomy Level: Apply</p>
<p>Learning Objective: Understand latent space structure through interpolation and vector arithmetic</p>
<p>Visual elements:
- 2D latent space projection
- Two endpoint vectors (selectable)
- Interpolation path (linear vs spherical)
- Generated samples along path
- Vector arithmetic demonstration</p>
<p>Interactive controls:
- Select two points in latent space
- Interpolation method: linear vs slerp
- Number of intermediate steps slider
- "Show Path" toggle
- "Try Vector Arithmetic" mode</p>
<p>Default parameters:
- Pre-computed latent points for simple shapes
- Linear interpolation
- 5 intermediate steps
- Canvas: responsive</p>
<p>Behavior:
- Visualize interpolation path
- Show how generated output changes along path
- Compare linear vs spherical interpolation
- Demonstrate attribute manipulation via vector arithmetic
- Show distance metrics along path</p>
<p>Implementation: p5.js with latent space visualization</p>
</details>
<h2 id="practical-implementation">Practical Implementation</h2>
<p>Here's a simplified attention implementation:</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Numerically stable softmax.&quot;&quot;&quot;</span>
    <span class="n">exp_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp_x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute scaled dot-product attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        Q: Queries, shape (n, d_k)</span>
<span class="sd">        K: Keys, shape (m, d_k)</span>
<span class="sd">        V: Values, shape (m, d_v)</span>
<span class="sd">        mask: Optional attention mask</span>

<span class="sd">    Returns:</span>
<span class="sd">        Attention output, shape (n, d_v)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Compute attention scores</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># (n, m)</span>

    <span class="c1"># Apply mask if provided (for causal attention)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>

    <span class="c1"># Softmax to get attention weights</span>
    <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n, m)</span>

    <span class="c1"># Weighted sum of values</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="o">@</span> <span class="n">V</span>  <span class="c1"># (n, d_v)</span>

    <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attention_weights</span>

<span class="k">def</span><span class="w"> </span><span class="nf">multi_head_attention</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W_Q</span><span class="p">,</span> <span class="n">W_K</span><span class="p">,</span> <span class="n">W_V</span><span class="p">,</span> <span class="n">W_O</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-head attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        X: Input, shape (n, d_model)</span>
<span class="sd">        W_Q, W_K, W_V: Per-head projection weights</span>
<span class="sd">        W_O: Output projection weight</span>
<span class="sd">        num_heads: Number of attention heads</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

    <span class="n">heads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">W_Q</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">W_K</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">W_V</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
        <span class="n">head_output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
        <span class="n">heads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">head_output</span><span class="p">)</span>

    <span class="c1"># Concatenate heads</span>
    <span class="n">concat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Output projection</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">concat</span> <span class="o">@</span> <span class="n">W_O</span>

    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span><span class="w"> </span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute cosine similarity between vectors.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lora_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass with LoRA adaptation.&quot;&quot;&quot;</span>
    <span class="c1"># Original path + low-rank update</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">x</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Example: Embedding lookup</span>
<span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Look up embeddings for token IDs.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">token_ids</span><span class="p">]</span>

<span class="c1"># Example: Position encoding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">positional_encoding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate sinusoidal position encodings.&quot;&quot;&quot;</span>
    <span class="n">positions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="n">dims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>

    <span class="n">angles</span> <span class="o">=</span> <span class="n">positions</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">dims</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Apply sin to even, cos to odd</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">pe</span>

<span class="c1"># Example usage</span>
<span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<span class="c1"># Add position encoding</span>
<span class="n">pos_enc</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<span class="n">X_with_pos</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">pos_enc</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input shape: </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Position encoding shape: </span><span class="si">{</span><span class="n">pos_enc</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<h2 id="summary_1">Summary</h2>
<p>This chapter explored the linear algebra foundations of modern generative AI:</p>
<p><strong>Embeddings:</strong></p>
<ul>
<li><strong>Embeddings</strong> map discrete tokens to continuous vectors</li>
<li><strong>Embedding spaces</strong> encode semantic relationships geometrically</li>
<li><strong>Cosine similarity</strong> measures semantic relatedness: <span class="arithmatex">\(\frac{\mathbf{u}^T\mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|}\)</span></li>
</ul>
<p><strong>Attention Mechanism:</strong></p>
<ul>
<li><strong>Self-attention</strong> allows each position to attend to all others</li>
<li><strong>Query, Key, Value</strong> matrices create content-based addressing</li>
<li><strong>Attention scores</strong> <span class="arithmatex">\(QK^T/\sqrt{d_k}\)</span> measure query-key compatibility</li>
<li><strong>Attention weights</strong> are softmax-normalized scores</li>
</ul>
<p><strong>Multi-Head and Transformers:</strong></p>
<ul>
<li><strong>Multi-head attention</strong> captures diverse relationship patterns</li>
<li><strong>Transformer blocks</strong> stack attention with feedforward layers</li>
<li><strong>Position encodings</strong> inject sequential order information</li>
<li><strong>Residual connections</strong> enable deep network training</li>
</ul>
<p><strong>Efficient Adaptation:</strong></p>
<ul>
<li><strong>LoRA</strong> adds low-rank trainable matrices: <span class="arithmatex">\(W' = W + BA\)</span></li>
<li>Reduces trainable parameters by 100-1000x</li>
</ul>
<p><strong>Latent Spaces:</strong></p>
<ul>
<li><strong>Latent spaces</strong> provide compressed, continuous representations</li>
<li><strong>Interpolation</strong> generates smooth transitions between points</li>
<li>Vector arithmetic enables semantic manipulation</li>
</ul>
<details class="question">
<summary>Self-Check: Why does attention use <span class="arithmatex">\(\sqrt{d_k}\)</span> scaling in the score computation?</summary>
<p>The dot product <span class="arithmatex">\(QK^T\)</span> produces values with variance proportional to <span class="arithmatex">\(d_k\)</span> (the key/query dimension). Without scaling, as <span class="arithmatex">\(d_k\)</span> grows large, the dot products grow large, pushing softmax into regions of extreme gradients (near-zero for most values, near-one for the maximum). Dividing by <span class="arithmatex">\(\sqrt{d_k}\)</span> normalizes the variance to approximately 1, keeping softmax in a region where gradients flow well and the attention distribution isn't too peaked.</p>
</details>







  
  



  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../10-neural-networks-and-deep-learning/quiz/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Quiz">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Quiz
              </div>
            </div>
          </a>
        
        
          
          <a href="quiz/" class="md-footer__link md-footer__link--next" aria-label="Next: Quiz">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Quiz
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["content.code.copy", "navigation.expand", "navigation.path", "navigation.prune", "navigation.indexes", "toc.follow", "navigation.top", "navigation.footer", "content.action.edit"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../js/extra.js"></script>
      
        <script src="../../js/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>